	id	year	title	keywords	abstract	copyright	AU	SO	DE	DOI
0	WOS:000312813800011	2012	Ecological Risk-O-Meter: a risk assessor and manager software tool for better decision making in ecosystems	SECURITY METER QUANTIFY MODEL	"Increased awareness of environmental issues and their effects on ecological systems and human health drive an interest in developing computational methods to reduce detrimental consequences. For example, there are concerns regarding chlorofluorocarbons and their impact on stratospheric ozone, radon and its effect on human health, coal mining and effects on habitat loss, as well as numerous other issues. However, these issues do not exist in a vacuum nor occur just one at a time. There is a need to assess social and ecological risks comprehensively and account for numerous, inter-related potential risks. Given limited funds available for addressing these issues, how can spending for purposes of environmental and ecological mitigation be optimized? What is the magnitude of overall ecological risk for a given region? Novel software, the Ecological Risk-o-Meter, addresses these questions and concerns. The software tool not only assesses the current environmental and ecological risks, but also takes into account potential solutions and provides guidance as to how spending can be optimized to reducing overall environmental risk. We demonstrate this new tool and show how to optimize the costs of risk reduction in recursive cycles based on feedbacks."	" Copyright (c) 2012 John Wiley & Sons, Ltd."	"Sahinoglu, M|Simmons, SJ|Cahoon, LB|Morton, S"	ENVIRONMETRICS	ecological systems vulnerability threat countermeasure risk-o-meter	10.1002/env.2186
1	WOS:000348201000009	2015	Sensitivity analyses and simulations of a full-scale experimental membrane bioreactor system using the activated sludge model No. 3 (ASM3)	RETENTION TIME OPERATION WATER	"An ASM-based model was implemented in the numerical software MATHEMATICA where sensitivity analyses and simulations of a membrane bioreactor (MBR) system were carried out. These results were compared with those obtained using the commercial simulator WEST. Predicted values did not show significant variations between both software and simulations showed that the most influential operational conditions were influent flow rate and concentrations and bioreactor volumes. On the other hand, sensitivity analyses were carried out with both software programs for the same five outputs: COD, ammonium and nitrate concentrations in the effluent, total suspended solids concentration and oxygen uptake rate in the aerobic bioreactor. Similar results were in general obtained in both cases and according to these analyses, the most significant inputs over the model predictions were growth and storage heterotrophic biomass yields and decay coefficient. Other parameters related to the hydrolysis process or to the autotrophic biomass also significantly influenced model outputs."		"Ruiz, LM|Rodelas, P|Perez, JI|Gomez, MA"	JOURNAL OF ENVIRONMENTAL SCIENCE AND HEALTH PART A-TOXIC/HAZARDOUS SUBSTANCES & ENVIRONMENTAL ENGINEERING	simulation modeling sensitivity analysis mbr asm3	10.1080/10934529.2015.981122
2	WOS:000285122400004	2010	Efficient solution for Galerkin-based polynomial chaos expansion systems	STOCHASTIC PROJECTION METHOD FINITE-ELEMENT SYSTEMS ITERATIVE SOLUTION LINEAR-SYSTEMS FLUID-FLOW MODELS	"Iterative solvers and preconditioners are widely used for handling the linear system of equations arising from stochastic finite element method (SFEM) formulations, e.g. galerkin-based polynomial chaos (G-P-C) Expansion method. Especially, Preconditioned Conjugate Gradient (PCG) solver and the Incomplete Cholesky (IC) preconditioner are shown to be adequate choices within this context. In this study, approaches for the automated adjustment of the input parameters for these tools are to be introduced. The proposed algorithms aim to enable the use of the PCG solver and IC preconditioner in a black-box fashion. As a result, the requirement of the expertise for using these tools is removed to a certain extend. Furthermore, these algorithms can be used also for the implementation purposes of SFEM's within general purpose software by increasing the ease of the use of these tools and hence leading to an improved user-comfort. (C) "	 Elsevier Ltd. All rights reserved.	"Panayirci, HM"	ADVANCES IN ENGINEERING SOFTWARE	stochastic finite elements polynomial chaos expansion computational efficiency iterative solvers preconditioners uncertainty quantification	10.1016/j.advengsoft.2010.09.004
4	WOS:000411574400095	2017	The Effect of Vitamin A on Fracture Risk: A Meta-Analysis of Cohort Studies	BONE-MINERAL DENSITY HIP FRACTURE RETINOIC ACID BETA-CAROTENE SERUM RETINOL POSTMENOPAUSAL WOMEN OSTEOCLAST FORMATION HYPERVITAMINOSIS-A CLINICAL-TRIALS FOLLOW-UP	"This meta-analysis evaluated the influence of dietary intake and blood level of vitamin A (total vitamin A, retinol or beta-carotene) on total and hip fracture risk. Cohort studies published before July  were selected through English-language literature searches in several databases. Relative risk (RR) with corresponding % confidence interval (CI) was used to evaluate the risk. Heterogeneity was checked by Chi-square and I- test. Sensitivity analysis and publication bias were also performed. For the association between retinol intake and total fracture risk, we performed subgroup analysis by sex, region, case ascertainment, education level, age at menopause and vitamin D intake. R software was used to complete all statistical analyses. A total of , participants over the age of  years were included. Higher dietary intake of retinol and total vitamin A may slightly decrease total fracture risk (RR with % CI: . (., .) and . (., .), respectively), and increase hip fracture risk (RR with % CI: . (., .) and . (., .), respectively). Lower blood level of retinol may slightly increase total fracture risk (RR with % CI: . (., .)) and hip fracture risk (RR with % CI: . (., .)). In addition, higher beta-carotene intake was weakly associated with the increased risk of total fracture (RR with % CI: . (., .)). Our data suggest that vitamin A intake and level may differentially influence the risks of total and hip fractures. Clinical trials are warranted to confirm these results and assess the clinical applicability."		"Zhang, XG|Zhang, R|Moore, JB|Wang, YG|Yan, HY|Wu, YR|Tan, AR|Fu, JL|Shen, ZG|Qin, GY|Li, R|Chen, GX"	INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH AND PUBLIC HEALTH	vitamin a retinol beta-carotene hip fracture total fracture	10.3390/ijerph14091043
7	WOS:000351458000015	2015	Risk Analysis of Water Demand for Agricultural Crops under Climate Change	OPTIMIZATION HBMO ALGORITHM RESERVOIR OPERATION DESIGN UNCERTAINTY DISCRETE NETWORKS STRATEGY IMPACTS SYSTEM	"This paper assesses the risk of increase in water demand for a wide range of irrigated crops in an irrigation network located downstream of the Aidoghmoush Dam in East Azerbaijan by considering climate change conditions for the period -. Atmosphere-ocean global circulation models (AOGCMs) are used to simulate climatic variables such as temperature and precipitation. The Bayesian approach is used to consider uncertainties of AOGCMs. Climate change scenarios of climatic variables are first weighted by using the mean observed temperature-precipitation (MOTP) method, and related probability distribution functions are produced. Outputs of AOGCMs are used as input to water requirement models. Then, produced by using the Monte Carlo method,  samples (discrete values) from the probability distribution functions of monthly downscaled temperature and precipitation in the study area are extracted by using a software for sensitivity and uncertainty analysis. Time series of climatic variables in future periods are then generated (temperature variable to calculate potential evapotranspiration and rainfall variable to calculate effective rainfall). To estimate crop water requirements, crop evapotranspiration (from the product of potential evapotranspiration in the previous step and coefficient of crop computed) and effective precipitation (from time series of the previous step) are calculated. The Food and Agricultural Organization of the United Nations (FAO) methods, FAO- and Penman-Monteith, were used to compute crop and potential evapotranspiration, respectively. Because of lack of required data, potential evapotranspiration in future periods is computed through the relationship of temperature and potential evapotranspiration in the baseline period; the same procedure is conducted for temperature. Net water requirement (NWR) and the risk of changes in water demand volume of crops (e.g., wheat, barley, alfalfa, soybean, feed corn, forage, potato, and walnut orchards) are computed by entering  monthly time series of downscaled temperature and precipitation in future periods. The results indicate that risk of changes in crop water requirements increases by approximately % for a % risk, approximately % for a % risk, and approximately % for a % risk. Also, based on the current cultivated area, on average, the volume of water demand only for the aforementioned crops will be approximately .(() m()/year) with a risk of %, approximately (() m()/year) with a risk of %, and approximately (() m()/year) with a risk of %. Wheat and barley are more resistant and less sensitive to climate change than other crops considered."	 (C) 2014 American Society of Civil Engineers.	"Ashofteh, PS|Bozorg-Haddad, O|Marino, MA"	JOURNAL OF HYDROLOGIC ENGINEERING	climate change net water requirement risk uncertainty monte carlo	10.1061/(ASCE)HE.1943-5584.0001053
8	WOS:000363949300025	2015	Feasibility analysis of a hybrid off-grid wind-DG-battery energy system for the eco-tourism remote areas	POWER PINCH ANALYSIS MALAYSIA PERFORMANCE GENERATION BANGLADESH SIMULATION STORAGE DESIGN PLANT	"The electrification process of the remote areas and decentralized areas is being a vital fact for the improvement of its eco-tourism issues such as the Cameron Highland of Malaysia. Renewable energy (RE) resources can be used extensively to support and fulfill the demand of the expected loads of these areas. This article presents an analysis of a complete off-grid wind-diesel-battery hybrid RE model. The main objective of the present analysis is to visualize the optimum volume of systems capable of fulfilling the requirements of  kWh/day primary load in coupled with . kW peak for  residential hotels of Cameron Highlands. The hybrid power system can be effective for the tourists of that area as it is a decentralized region of Malaysia. The main motto of this analysis is to minimize the electricity unit cost and ensure the most reliable and feasible system to fulfill the requirements of the desired or expected energy system using HOMER software. From the simulation result, it can be seen that  wind turbines ( kW),  diesel generator ( kW), and  battery (Hoppecke  OPzS) hybrid RE system is the most economically feasible and lowest cost of energy is nearing USD ./kWh and net present cost is USD , . The decrement of the CO emissions also can be identified from the simulation results using that most feasible RE system including the renewable fraction value which is about ., . % capacity shortage and . % electricity as storage as compared to the other energy system."		"Shezan, SKA|Saidur, R|Ullah, KR|Hossain, A|Chong, WT|Julai, S"	CLEAN TECHNOLOGIES AND ENVIRONMENTAL POLICY	renewable energy wind energy wind turbines homer diesel generator sensitivity analysis optimization hybrid model	10.1007/s10098-015-0983-0
10	WOS:000244915000009	2007	Topology optimization of material-nonlinear continuum structures by the element connectivity parameterization	DESIGN SENSITIVITY-ANALYSIS ELASTOPLASTIC STRUCTURES COMPLIANT MECHANISMS PLASTIC-DEFORMATION CONTACT CRASHWORTHINESS	"The application of the element density-based topology optimization method to nonlinear continuum structures is limited to relatively simple problems such as bilinear elastoplastic material problems. Furthermore, it is very difficult to use analytic sensitivity when a commercial nonlinear finite element code is used. As an alternative to the element density formulation, the element connectivity parameterization (ECP) formulation is developed for the topology optimization of isotropic-hardening elastoplastic or hyperelastic continua by using commercial software. ECP varies the stiffness of zero-length linear elastic links that connect design domain-discretizing finite elements. Unloading was not considered. But the advantages of ECP in material-nonlinear problems were demonstrated: considerably simple analytic sensitivity calculation using a commercial code and simple link stiffness penalization regardless of nonlinear material behaviour."	" Copyright (c) 2006 John Wiley & Sons, Ltd."	"Yoon, GH|Kim, YY"	INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING	topology optimization material-nonlinearity element connectivity parameterization	10.1002/nme.1843
11	WOS:000388092400006	2016	"ALBANY: USING COMPONENT-BASED DESIGN TO DEVELOP A FLEXIBLE, GENERIC MULTIPHYSICS ANALYSIS CODE"	EMBEDDED ANALYSIS CAPABILITIES MANAGING SOFTWARE COMPLEXITY FINITE-ELEMENT PARALLEL SIMULATION FRAMEWORK EQUATIONS SYSTEMS LIBRARY	"Albany is a multiphysics code constructed by assembling a set of reusable, general components. It is an implicit, unstructured grid finite element code that hosts a set of advanced features that are readily combined within a single analysis run. Albany uses template-based generic programming methods to provide extensibility and flexibility; it employs a generic residual evaluation interface to support the easy addition and modification of physics. This interface is coupled to powerful automatic differentiation utilities that are used to implement efficient nonlinear solvers and preconditioners, and also to enable sensitivity analysis and embedded uncertainty quantification capabilities as part of the forward solve. The flexible application programming interfaces in Albany couple to two different adaptive mesh libraries; it internally employs generic integration machinery that supports tetrahedral, hexahedral, and hybrid meshes of user specified order. We present the overall design of Albany, and focus on the specifics of the integration of many of its advanced features. As Albany and the components that form it are openly available on the internet, it is our goal that the reader might find some of the design concepts useful in their own work. Albany results in a code that enables the rapid development of parallel, numerically efficient multiphysics software tools. In discussing the features and details of the integration of many of the components involved, we show the reader the wide variety of solution components that are available and what is possible when they are combined within a simulation capability."		"Salinger, AG|Bartlett, RA|Bradley, AM|Chen, QS|Demeshko, IP|Gao, XJ|Hansen, GA|Mota, A|Muller, RP|Nielsen, E|Ostien, JT|Pawlowski, RP|Perego, M|Phipps, ET|Sun, WC|Tezaur, IK"	INTERNATIONAL JOURNAL FOR MULTISCALE COMPUTATIONAL ENGINEERING	partial differential equations finite element analysis template-based generic programming	10.1615/IntJMultCompEng.2016017040
12	WOS:000297595300005	2011	Shape optimisation of preform design for precision close-die forging	FINITE-ELEMENT METHOD TOPOLOGY OPTIMIZATION STRUCTURAL OPTIMIZATION EVOLUTIONARY PROCEDURE SENSITIVITY-ANALYSIS HOMOGENIZATION SIMULATION ALGORITHM BLADE	"Preform design is an essential stage in forging especially for parts with complex shapes. In this paper, based on the evolutionary structural optimisation (ESO) concept, a topological optimisation method is developed for preform design. In this method, a new criterion for element elimination and addition on the workpiece boundary surfaces is proposed to optimise material distribution. To improve the quality of the boundary after element elimination, a boundary smoothing technique is developed using B-spline curve approximation. The developed methods are programmed using C# code and integrated with DEFORM D software package. Two D case problems including forging of an aerofoil shape and forging of rail wheel are evaluated using the developed method. The results suggest that the developed topology optimisation method is an efficient approach for preform design optimisation."		"Lu, B|Ou, HA|Cui, ZS"	STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION	preform design topology optimisation forging	10.1007/s00158-011-0668-1
13	WOS:000223528100004	2004	POLCAGE 1.0 - a possibilistic life-cycle assessment model for evaluating alternative transportation fuels	FUZZY OUTRANKING SYSTEMS ENERGY	"A composite software model for the comparative life-cycle assessment (LCA) of  different fuel options for the Philippine automotive transport sector was developed. It is based on the GREET fuel-cycle inventory model developed by the Argonne National Laboratory for the United States Department of Energy. GREET .a is linked to an impact assessment submodel using the Danish environmental design of industrial products (EDIP) method. This combined inventory-impact assessment model is enhanced further with possibilistic uncertainty propagation (PUP) and possibilistic compromise programming (PCP) features that allow the  alternatives to be ranked in the presence of multiple criteria and uncertain data. Sensitivity and scenario analysis can also be performed within the composite model. Some current and anticipated Philippine conditions, including electricity generation mix, are incorporated in the prototype's built-in database. The software model, designated as POLCAGE . (possibilistic LCA using GREET and EDIP), is coded in Microsoft Excel and Visual Basic. The model's capabilities and features are demonstrated using a case study based on its default scenario. (C) "	 Elsevier Ltd. All rights reserved.	"Tan, RR|Culaba, AB|Purvis, MRI"	ENVIRONMENTAL MODELLING & SOFTWARE	life-cycle assessment (lca) decision support system (dss) alternative fuels	10.1016/j.envsoft.2003.10.004
14	WOS:000248617800003	2007	Automatic sensitivity analysis of a finite volume model for two-dimensional shallow water flows	DENSE URBAN AREA	"Given a numerical model for solving two-dimensional shallow water equations, we are interested in the robustness of the simulation by identifying the rate of change of the water depths and discharges with respect to a change in the bottom friction coefficients. Such a sensitivity analysis can be carried out by computing the corresponding derivatives. Automatic differentiation (AD) is an efficient numerical method, free of approximation errors, to evaluate derivatives of the objective function specified by the computer program, Rubar for example. In this paper AD software tool Tapenade is used to compute forward derivatives. Numerical tests were done to show the robustness of the model and to demonstrate the efficiency of these AD-derivatives."		"Souhar, O|Faure, JB|Paquier, A"	ENVIRONMENTAL FLUID MECHANICS	automatic differentiation hydraulic model sensitivity analysis uncertainty propagation steady flow	10.1007/s10652-007-9028-5
15	WOS:000241177900002	2006	Design parameterization and tool integration for CAD-based mechanism optimization		"This paper presents an open and integrated tool environment that enables engineers to effectively search, in a CAD solid model form, for a mechanism design with optimal kinematic and dynamic performance. In order to demonstrate the feasibility of such an environment, design parameterization that supports capturing design intents in product solid models must be available, and advanced modeling, simulation, and optimization technologies implemented in engineering software tools must be incorporated. In this paper, the design parameterization capabilities developed previously have been applied to support design optimization of engineering products, including a High Mobility Multi-purpose Wheeled Vehicle (HMMWV). In the proposed environment, Pro/ENGINEER and SolidWorks are supported for product model representation, DADS (Dynamic Analysis and Design System) is employed for dynamic simulation of mechanical systems including ground vehicles, and DOT (Design Optimization Tool) is included for a batch mode design optimization. In addition to the commercial tools, a number of software modules have been implemented to support the integration; e.g., interface modules for data retrieval, and model update modules for updating CAD and simulation models in accordance with design changes. Note that in this research, the overall finite difference method has been adopted to support design sensitivity analysis. (C) "	 Elsevier Ltd. All rights reserved.	"Chang, KH|Joo, SH"	ADVANCES IN ENGINEERING SOFTWARE	design optimization design parameterization computer-aided design dynamic simulation tool integration	10.1016/j.advengsoft.2006.05.005
16	WOS:000315974500021	2013	Distributed computation of large scale SWAT models on the Grid	PERSPECTIVES SYSTEMS TOOL	"The increasing interest in larger spatial and temporal scale models and high resolution input data processing comes at a price of higher computational demand. This price is evidently even higher when common modeling routines such as calibration and uncertainty analysis are involved. Likewise, methods and techniques for reducing computation time in large scale socio-environmental modeling software is growing. Recent advancements in distributed computing such as Grid infrastructure have provided further opportunity to this effort. In the interest of gaining computational efficiency, we developed generic tools and techniques for enabling the Soil and Water Assessment Tool (SWAT) model application to run on the EGEE (Enabling Grids for E-science projects in Europe) Grid. Various program components/scripts were written to split a large scale hydrological model of the Soil and Water Assessment Tool (SWAT), to submit the split models to the Grid, and to collect and merge results into single output format. A three-step procedure was applied to take advantage of the Grid. Firstly, a python script was run in order to split the SWAT model into several sub-models. Then, individual sub-models were submitted in parallel for execution on the Grid. Finally, the outputs of the sub-basins were collected and the reach routing process was performed with another script executing a modified SWAT program. We conducted experimental simulations with multiple temporal and spatial scale hydrological models on the Grid infrastructure. Results showed that, in spite of computing overheads, parallel computation of socio-environmental models on the Grid is beneficial for model applications especially with large spatial and temporal scales. In the end, we conclude by recommending methods for further reducing computational overheads while running large scale model applications on the Grid. (c) "	 Elsevier Ltd. All rights reserved.	"Yalew, S|van Griensven, A|Ray, N|Kokoszkiewicz, L|Betrie, GD"	ENVIRONMENTAL MODELLING & SOFTWARE	distributed computing grid computing hydrological models swat gridification	10.1016/j.envsoft.2012.08.002
17	WOS:000388155500003	2016	GTApprox: Surrogate modeling for industrial design	SENSITIVITY-ANALYSIS GAUSSIAN-PROCESSES REGRESSION REGULARIZATION ALGORITHM SELECTION MACHINE SPLINES SAMPLES EXPERTS	"We describe GTApprox - a new tool for medium-scale surrogate modeling in industrial design. Compared to existing software, GTApprox brings several innovations: a few novel approximation algorithms, several advanced methods of automated model selection, novel options in the form of hints. We demonstrate the efficiency of GTApprox on a large collection of test problems. In addition, we describe several applications of GTApprox to real engineering problems. (C) "	 Elsevier Ltd. All rights reserved.	"Belyaev, M|Burnaev, E|Kapushev, E|Panov, M|Prikhodko, P|Vetrov, D|Yarotsky, D"	ADVANCES IN ENGINEERING SOFTWARE	approximation surrogate model surrogate-based optimization	10.1016/j.advengsoft.2016.09.001
19	WOS:000276075900006	2010	Object-oriented design of process line simulation and optimization-A case study in papermaking	MULTIDISCIPLINARY DESIGN PAPER FRAMEWORK SYSTEM FLOW	"Simulation-based optimization for industrial process lines is discussed in this paper. Our approach combines multidisciplinary modeling, modern sensitivity analysis methodology as well as multiobjective optimization by means of object-oriented software design principles. As a result, a simulation and optimization approach that can be extended and modified due to users' needs can be developed. Our approach is illustrated by a real-world example from papermaking industry."		"Madetoja, E|Tarvainen, P"	STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION	process line optimization multiobjective optimization multidisciplinary modeling object-oriented programming	10.1007/s00158-009-0451-8
20	WOS:000361906900013	2015	A bootstrap method for estimating uncertainty of water quality trends	LOAD ESTIMATION REGRESSION STREAM VARIABLES MODELS TESTS	"Estimation of the direction and magnitude of trends in surface water quality remains a problem of great scientific and practical interest. The Weighted Regressions on Time, Discharge, and Season (WRTDS) method was recently introduced as an exploratory data analysis tool to provide flexible and robust estimates of water quality trends. This paper enhances the WRTDS method through the introduction of the WRTDS Bootstrap Test (WBT), an extension of WRTDS that quantifies the uncertainty in WRTDS-estimates of water quality trends and offers various ways to visualize and communicate these uncertainties. Monte Carlo experiments are applied to estimate the Type I error probabilities for this method. WBT is compared to other water-quality trend-testing methods appropriate for data sets of one to three decades in length with sampling frequencies of - observations per year. The software to conduct the test is in the EGRETci R-package."	 Published by Elsevier Ltd.	"Hirsch, RM|Archfield, SA|De Cicco, LA"	ENVIRONMENTAL MODELLING & SOFTWARE	water quality bootstrap trend uncertainty analysis	10.1016/j.envsoft.2015.07.017
23	WOS:000409151600011	2017	"Simulation, identification and statistical variation in cardiovascular analysis (SISCA) - A software framework for multi-compartment lumped modeling"	SENSITIVITY-ANALYSIS BLOOD-FLOW ARTERIAL WINDKESSEL PRESSURE SYSTEM HEART	"It has not yet been possible to obtain modeling approaches suitable for covering a wide range of real world scenarios in cardiovascular physiology because many of the system parameters are uncertain or even unknown. Natural variability and statistical variation of cardiovascular system parameters in healthy and diseased conditions are characteristic features for understanding cardiovascular diseases in more detail. This paper presents SISCA, a novel software framework for cardiovascular system modeling and its MATLAB implementation. The framework defines a multi-model statistical ensemble approach for dimension reduced, multi-compartment models and focuses on statistical variation, system identification and patient-specific simulation based on clinical data. We also discuss a data-driven modeling scenario as a use case example. The regarded dataset originated from routine clinical examinations and comprised typical pre and post surgery clinical data from a patient diagnosed with coarctation of aorta. We conducted patient and disease specific pre/post surgery modeling by adapting a validated nominal multi-compartment model with respect to structure and parametrization using metadata and MRI geometry. In both models, the simulation reproduced measured pressures and flows fairly well with respect to stenosis and stent treatment and by pre-treatment cross stenosis phase shift of the pulse wave. However, with post-treatment data showing unrealistic phase shifts and other more obvious inconsistencies within the dataset, the methods and results we present suggest that conditioning and uncertainty management of routine clinical data sets needs significantly more attention to obtain reasonable results in patient-specific cardiovascular modeling."		"Huttary, R|Goubergrits, L|Schutte, C|Bernhard, S"	COMPUTERS IN BIOLOGY AND MEDICINE	windkessel elements lumped models od modeling multi-compartment modeling cardiovascular simulation distributed parameter modeling clinical data set coarctation of aorta patient-specific models disease-specific models multiscale modeling	10.1016/j.compbiomed.2017.05.021
24	WOS:000258484000001	2008	Software framework for parameter updating and finite-element response sensitivity analysis	INELASTIC STRUCTURES RELIABILITY SERVICES PROGRAM	"The finite-element software framework OpenSees is extended with parameter updating and response sensitivity capabilities to support client applications such as reliability, optimization, and system identification. Using software design patterns, member properties, applied loadings, and nodal coordinates can be identified and repeatedly updated in order to create customized finite-element model updating applications. Parameters are identified using a Chain of Responsibility software pattern, where objects in the finite-element model forward a parameterization request to component objects until the request is handled. All messages to identify and update parameters are passed through a Facade that decouples client applications from the finite-element domain of OpenSees. To support response sensitivity analysis, the Strategy design pattern facilitates multiple approaches to evaluate gradients of the structural response, whereas the Visitor pattern ensures that objects in the finite-element domain make the proper contributions to the equations that govern the response sensitivity. Examples demonstrate the software design and the steps taken by representative finite-element model updating and response sensitivity applications."		"Scott, MH|Haukaas, T"	JOURNAL OF COMPUTING IN CIVIL ENGINEERING		10.1061/(ASCE)0887-3801(2008)22:5(281)
25	WOS:000321813200001	2013	Modelling of groundwater infiltration into sewer systems		"Groundwater infiltration into urban sewers represents a problem that influences costs and management of technical systems. The hydrodynamic groundwater software MODFLOW is used to analyse the influencing variables of the infiltration processes. Besides the hydraulic conductivity of the soil and the piezometric head in the vicinity of the sewer pipe, properties of the sewer trench, the shape and the size of leaks are important influencing factors. A non-linear-regression method is applied to develop a one-dimensional approach in accordance with the MODFLOW results and Darcy's law. Monte Carlo simulations and the developed one-dimensional model are used to assess the leak area and the range of pressure loss in the vicinity of the pipe leaks. By additional sensitivity analysis it was found that the infiltration factor and the conductivity of the backfill are very important for the calculation of the leak area."		"Karpf, C|Krebs, P"	URBAN WATER JOURNAL	infiltration sewer modelling parameter sensitivity monte carlo simulations	10.1080/1573062X.2012.724077
27	WOS:000307721100006	2012	Evolutionary topology optimization of periodic composites for extremal magnetic permeability and electrical permittivity	LEVEL-SET STRUCTURAL OPTIMIZATION DESIGN HOMOGENIZATION METAMATERIALS SHAPE MICROSTRUCTURES	"This paper presents a bidirectional evolutionary structural optimization (BESO) method for designing periodic microstructures of two-phase composites with extremal electromagnetic permeability and permittivity. The effective permeability and effective permittivity of the composite are obtained by applying the homogenization technique to the representative periodic base cell (PBC). Single or multiple objectives are defined to maximize or minimize the electromagnetic properties separately or simultaneously. The sensitivity analysis of the objective function is conducted using the adjoint method. Based on the established sensitivity number, BESO gradually evolves the topology of the PBC to an optimum. Numerical examples demonstrate that the electromagnetic properties of the resulting D and D microstructures are very close to the theoretical Hashin-Shtrikman (HS) bounds. The proposed BESO algorithm is computationally efficient as the solution usually converges in less than  iterations. The proposed BESO method can be implemented easily as a post-processor to standard commercial finite element analysis software packages, e.g. ANSYS which has been used in this study. The resulting topologies are clear black-and-white solutions (with no grey areas). Some interesting topological patterns such as Vigdergauz-type structure and Schwarz primitive structure have been found which will be useful for the design of electromagnetic materials."		"Huang, X|Xie, YM|Jia, B|Li, Q|Zhou, SW"	STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION	topology optimization bidirectional evolutionary structural optimization (beso) homogenization effective permeability effective permittivity	10.1007/s00158-012-0766-8
28	WOS:000243927200008	2007	Reliability-based multiobjective optimization for automotive crashworthiness and occupant safety		"This paper presents a methodology for reliability-based multiobjective optimization of large-scale engineering systems. This methodology is applied to the vehicle crashworthiness design optimization for side impact, considering both structural crashworthiness and occupant safety, with structural weight and front door velocity under side impact as objectives. Uncertainty quantification is performed using two first order reliability method-based techniques: approximate moment approach and reliability index approach. Genetic algorithm-based multiobjective optimization software GDOT, developed in-house, is used to come up with an optimal pareto front in all cases. The technique employed in this study treats multiple objective functions separately without combining them in any form. It shows that the vehicle weight can be reduced significantly from the baseline design and at the same time reduce the door velocity. The obtained pareto front brings out useful inferences about optimal design regions. A decision-making criterion is subsequently invoked to select the ""best"" subset of solutions from the obtained nondominated pareto optimal solutions. The reliability, thus computed, is also checked with Monte Carlo simulations. The optimal solution indicated by knee point on the optimal pareto front is verified with LS-DYNA simulation results."		"Sinha, K"	STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION	reliability-based multiobjective optimization uncertainty quantification form nondominated points gdot pareto optimal solution knee point automotive crashworthiness occupant safety side impact monte carlo simulation	10.1007/s00158-006-0050-x
29	WOS:000247276500009	2007	Application of non-linear automatic optimization techniques for calibration of HSPF	GLOBAL OPTIMIZATION MODELS	"Development of TMDLs (total maximum daily loads) is often facilitated by using the software system BASINS (Better Assessment Science Integrating point and Nonpoint Sources). One of the key elements of BASINS is the watershed model HSPF (Hydrological Simulation Program Fortran) developed by USEPA. Calibration of HSPF is a very tedious and time consuming task, more than  parameters are involved in the calibration process. In the current research, three non-linear automatic optimization techniques are applied and compared, as well an efficient way to calibrate HSPF is suggested. Parameter optimization using local and global optimization techniques for the watershed model is discussed. Approaches to automatic calibration of HSPF using the nonlinear parameter estimator PEST (Parameter Estimation Tool) with its Gauss-Marquardt-L evenberg (GMI-) method, Random multiple Search Method (RSM), and Shuffled Complex Evolution method developed at the University of Arizona (SCE-UA) are presented. Sensitivity analysis was conducted and the most and the least sensitive parameters were identified. It was noted that sensitivity depends on number of adjustable parameters. As more parameters were optimized simultaneously - a wider range of parameter values can maintain the model in the calibrated state. Impact of GML, RSM, and SCE-UA variables on ability to find the global minimum of the objective function (OF) was studied and the best variables are suggested. All three methods proved to be more efficient than manual HSPF calibration. Optimization results obtained by these methods are very similar, although in most cases RSM out performs GML and SCE-UA outperforms RSM. GML is a very fast method, it can perform as well as SCE-UA when the variables are properly adjusted, initial guess is good and insensitive parameters are eliminated from the optimization process. SCE-UA is very robust and convenient to use. Logical definition of key variables in most cases leads to the global minimum."		"Iskra, I|Droste, R"	WATER ENVIRONMENT RESEARCH	calibration hspf pest gauss-marquardt-levenberg method sce-ua	10.2175/106143007X156862
31	WOS:000288865100006	2011	Estimating Water Budgets and Vertical Leakages for Karst Lakes in North-Central Florida (United States) Via Hydrological Modeling	RAINFALL-RUNOFF MODELS AUTOMATIC CALIBRATION CONDUCTANCE	"Newnans, Lochloosa, and Orange Lakes are closely hydrologically connected karst lakes located in north-central Florida, United States. The complex karst hydrology in this region poses a great challenge to the hydrological modeling that is essential to the development of Total Maximum Daily Loads for these lakes. We used a Hydrological Simulation Program - Fortran model coupled with the parallel Parameter ESTimation model calibration and uncertainty analysis software to estimate effectively the hydrological interactions between the lakes and the underlying upper Floridan aquifer and the water budgets for these three lakes. The net results of the lake-groundwater interactions in Newnans and Orange Lakes are that both lakes recharge the underlying upper Floridan aquifer, with the recharge rate of the latter one magnitude greater than that of the former. However, for Lochloosa Lake, the net lake-groundwater interaction is that the lake gains water from groundwater in a significant amount, approximately % of its total terrestrial water input. The annual average vertical leakages estimated for Newnans, Lochloosa, and Orange Lakes are . x , -. x , and . x  m, respectively. The average vertical hydraulic conductance (K(v)/b) of the units between a lake bottom and the underlying upper Floridan aquifer in this region are also estimated to be from . x - to . x - day-."		"Lin, ZL"	JOURNAL OF THE AMERICAN WATER RESOURCES ASSOCIATION	hspf karst hydrology lake water budget optimization surface water groundwater interaction	10.1111/j.1752-1688.2010.00513.x
32	WOS:000362848700009	2015	A Multi-Attribute Decision Analysis for Decommissioning Offshore Oil and Gas Platforms	UTILITY MEASUREMENT	"The  oil and gas platforms off the coast of southern California are reaching the end of their economic lives. Because their decommissioning involves large costs and potential environmental impacts, this became an issue of public controversy. As part of a larger policy analysis conducted for the State of California, we implemented a decision analysis as a software tool (PLATFORM) to clarify and evaluate decision strategies against a comprehensive set of objectives. Key options selected for in-depth analysis are complete platform removal and partial removal to  feet below the water line, with the remaining structure converted in place to an artificial reef to preserve the rich ecosystems supported by the platform's support structure. PLATFORM was instrumental in structuring and performing key analyses of the impacts of each option (e.g., on costs, fishery production, air emissions) and dramatically improved the team's productivity. Sensitivity analysis found that disagreement about preferences, especially about the relative importance of strict compliance with lease agreements, has much greater effects on the preferred option than does uncertainty about specific outcomes, such as decommissioning costs. It found a near-consensus of stakeholders in support of partial removal and ""rigs-to-reefs"" program. The project's results played a role in the decision to pass legislation enabling an expanded California ""rigs-to-reefs"" program that includes a mechanism for sharing cost savings between operators and the state."	 (C) 2015 SETAC	"Henrion, M|Bernstein, B|Swamy, S"	INTEGRATED ENVIRONMENTAL ASSESSMENT AND MANAGEMENT	decision analysis decommissioning multi-attribute utility oil and gas platforms rigs-to-reefs	10.1002/ieam.1693
35	WOS:000369512800012	2016	Integration of a Three-Dimensional Process-Based Hydrological Model into the Object Modeling System	JGRASS-NEWAGE SYSTEM DISTRIBUTED MODEL FRAMEWORK ENERGY BUDGETS BASIN WATER TIME FLOW TERRAIN	"The integration of a spatial process model into an environmental modeling framework can enhance the model's capabilities. This paper describes a general methodology for integrating environmental models into the Object Modeling System (OMS) regardless of the model's complexity, the programming language, and the operating system used. We present the integration of the GEOtop model into the OMS version . and illustrate its application in a small watershed. OMS is an environmental modeling framework that facilitates model development, calibration, evaluation, and maintenance. It provides innovative techniques in software design such as multithreading, implicit parallelism, calibration and sensitivity analysis algorithms, and cloud-services. GEOtop is a physically based, spatially distributed rainfall-runoff model that performs three-dimensional finite volume calculations of water and energy budgets. Executing GEOtop as an OMS model component allows it to: () interact directly with the open-source geographical information system (GIS) uDig-JGrass to access geo-processing, visualization, and other modeling components; and () use OMS components for automatic calibration, sensitivity analysis, or meteorological data interpolation. A case study of the model in a semi-arid agricultural catchment is presented for illustration and proof-of-concept. Simulated soil water content and soil temperature results are compared with measured data, and model performance is evaluated using goodness-of-fit indices. This study serves as a template for future integration of process models into OMS."		"Formetta, G|Capparelli, G|David, O|Green, TR|Rigon, R"	WATER	watershed model environmental modeling framework automatic calibration software integration	10.3390/w8010012
36	WOS:000325218500007	2013	Uncertainty in floodplain delineation: expression of flood hazard and risk in a Gulf Coast watershed		"This paper investigates the development of flood hazard and flood risk delineations that account for uncertainty as improvements to standard floodplain maps for coastal watersheds. Current regulatory floodplain maps for the Gulf Coastal United States present % flood hazards as polygon features developed using deterministic, steady-state models that do not consider data uncertainty or natural variability of input parameters. Using the techniques presented here, a standard binary deterministic floodplain delineation is replaced with a flood inundation map showing the underlying flood hazard structure. Additionally, the hazard uncertainty is further transformed to show flood risk as a spatially distributed probable flood depth using concepts familiar to practicing engineers and software tools accepted and understood by regulators. A case study of the proposed hazard and risk assessment methodology is presented for a Gulf Coast watershed, which suggests that storm duration and stage boundary conditions are important variable parameters, whereas rainfall distribution, storm movement, and roughness coefficients contribute less variability. The floodplain with uncertainty for this coastal watershed showed the highest variability in the tidally influenced reaches and showed little variability in the inland riverine reaches. Additionally, comparison of flood hazard maps to flood risk maps shows that they are not directly correlated, as areas of high hazard do not always represent high risk."	" Copyright (c) 2012 John Wiley & Sons, Ltd."	"Christian, J|Duenas-Osorio, L|Teague, A|Fang, Z|Bedient, P"	HYDROLOGICAL PROCESSES	flood hazard flood risk floodplain mapping uncertainty analysis coastal watersheds storm surge	10.1002/hyp.9360
37	WOS:000323644600031	2013	Nitrous Oxide Emissions from Cropland: a Procedure for Calibrating the DayCent Biogeochemical Model Using Inverse Modelling	CARBON-DIOXIDE SOIL N2O DENITRIFICATION SIMULATIONS COLORADO SYSTEMS DNDC	"DayCent is a biogeochemical model of intermediate complexity widely used to simulate greenhouse gases (GHG), soil organic carbon and nutrients in crop, grassland, forest and savannah ecosystems. Although this model has been applied to a wide range of ecosystems, it is still typically parameterized through a traditional ""trial and error"" approach and has not been calibrated using statistical inverse modelling (i.e. algorithmic parameter estimation). The aim of this study is to establish and demonstrate a procedure for calibration of DayCent to improve estimation of GHG emissions. We coupled DayCent with the parameter estimation (PEST) software for inverse modelling. The PEST software can be used for calibration through regularized inversion as well as model sensitivity and uncertainty analysis. The DayCent model was analysed and calibrated using NO flux data collected over  years at the Iowa State University Agronomy and Agricultural Engineering Research Farms, Boone, IA. Crop year  data were used for model calibration and  data were used for validation. The optimization of DayCent model parameters using PEST significantly reduced model residuals relative to the default DayCent parameter values. Parameter estimation improved the model performance by reducing the sum of weighted squared residual difference between measured and modelled outputs by up to  %. For the calibration period, simulation with the default model parameter values underestimated mean daily NO flux by  %. After parameter estimation, the model underestimated the mean daily fluxes by  %. During the validation period, the calibrated model reduced sum of weighted squared residuals by  % relative to the default simulation. Sensitivity analysis performed provides important insights into the model structure providing guidance for model improvement."		"Rafique, R|Fienen, MN|Parkin, TB|Anex, RP"	WATER AIR AND SOIL POLLUTION	daycent model inverse modelling parameter estimation (pest) nitrous oxide sensitivity analysis automatic calibration validation	10.1007/s11270-013-1677-z
38	WOS:000395607700003	2017	Infiltration under snow cover: Modeling approaches and predictive uncertainty	ENERGY-BALANCE HYDRAULIC CONDUCTIVITY WATER EQUIVALENT MELT SIMULATIONS PILOT POINTS TEMPERATURE INDEX RADIATION PARAMETER SURFACE	"Groundwater recharge from snowmelt represents a temporal redistribution of precipitation. This is extremely important because the rate and timing of snowpack drainage has substantial consequences to aquifer recharge patterns, which in turn affect groundwater availability throughout the rest of the year. The modeling methods developed to estimate drainage from a snowpack, which typically rely on temporally dense point-measurements or temporally-limited spatially-dispersed calibration data, range in complexity from the simple degree-day method to more complex and physically-based energy balance approaches. While the gamut of snowmelt models are routinely used to aid in water resource management, a comparison of snowmelt models' predictive uncertainties had previously not been done. Therefore, we established a snowmelt model calibration dataset that is both temporally dense and represents the integrated snowmelt infiltration signal for the Vers Chez le Brandt research catchment, which functions as a rather unique natural lysimeter. We then evaluated the uncertainty associated with the degree-day, a modified degree-day and energy balance snowmelt model predictions using the null space Monte Carlo approach. All three melt models underestimate total snowpack drainage, underestimate the rate of early and midwinter drainage and overestimate spring snowmelt rates. The actual rate of snowpack water loss is more constant over the course of the entire winter season than the snowmelt models would imply, indicating that mid-winter melt can contribute as significantly as springtime snow melt to groundwater recharge in low alpine settings. Further, actual groundwater recharge could be between  and % greater than snowmelt models suggest, over the total winter season. This study shows that snowmelt model predictions can have considerable uncertainty, which may be reduced by the inclusion of more data that allows for the use of more complex approaches such as the energy balance method. Further, our study demonstrated that an uncertainty analysis of model predictions is easily accomplished due to the low computational demand of the models and efficient calibration software and is absolutely worth the additional investment. Lastly, development of a systematic instrumentation that evaluates the distributed, temporal evolution of snowpack drainage is vital for optimal understanding and management of cold-climate hydrologic systems."	 (C) 2017 Elsevier B.V. All rights reserved.	"Meeks, J|Moeck, C|Brunner, P|Hunkeler, D"	JOURNAL OF HYDROLOGY	uncertainty snowmelt energy balance day degree recharge karst groundwater	10.1016/j.jhydrol.2016.12.042
41	WOS:000323981900016	2013	Analyzing the effects of geological and parameter uncertainty on prediction of groundwater head and travel time	MULTIPLE-POINT STATISTICS GLUE METHODOLOGY CAPTURE ZONE FLOW SIMULATION MODEL HETEROGENEITY INCOHERENCE CALIBRATION DENMARK	"Uncertainty of groundwater model predictions has in the past mostly been related to uncertainty in the hydraulic parameters, whereas uncertainty in the geological structure has not been considered to the same extent. Recent developments in theoretical methods for quantifying geological uncertainty have made it possible to consider this factor in groundwater modeling. In this study we have applied the multiple-point geostatistical method (MPS) integrated in the Stanford Geostatistical Modeling Software (SGeMS) for exploring the impact of geological uncertainty on groundwater flow patterns for a site in Denmark. Realizations from the geostatistical model were used as input to a groundwater model developed from Modular three-dimensional finite-difference ground-water model (MODFLOW) within the Groundwater Modeling System (GMS) modeling environment. The uncertainty analysis was carried out in three scenarios involving simulation of groundwater head distribution and travel time. The first scenario implied  stochastic geological models all assigning the same hydraulic parameters for the same geological units. In the second scenario the same  geological models were subjected to model optimization, where the hydraulic parameters for each of them were estimated by calibration against observations of hydraulic head and stream discharge. In the third scenario each geological model was run with  randomized sets of parameters. The analysis documented that the uncertainty on the conceptual geological model was as significant as the uncertainty related to the embedded hydraulic parameters."		"He, X|Sonnenborg, TO|Jorgensen, F|Hoyer, AS|Moller, RR|Jensen, KH"	HYDROLOGY AND EARTH SYSTEM SCIENCES		10.5194/hess-17-3245-2013
42	WOS:000270759400017	2009	Quantifying predictive uncertainty for a mountain-watershed model	HYDROLOGIC-MODELS AUTOMATIC CALIBRATION GLOBAL OPTIMIZATION SWAT MODEL VALIDATION SENSITIVITY	"Watershed models require calibration before they are utilized as a decision-making tool. This paper describes a rigorous sensitivity analysis, automated parameter estimation and evaluation of prediction uncertainty for a Watershed Analysis Risk Management Framework (WARMF) model of the Turkey Creek Watershed. Sensitivity analysis was conducted using UCODE calibration and uncertainty-analysis software. Simulated stream flow is strongly sensitive to  of the  parameters evaluated: hydraulic conductivity, field capacity, total porosity, precipitation weighting factor, evaporation magnitude, evaporation skewness and snow melting rates; and parameter sensitivity is dependent on site-specific climate and soil conditions. Simulated stream flow matched observed stream flow fairly well with an R() value of ., Nash-Sutcliffe coefficient of efficiency (NSE) value of . and Root Mean Squared Error (RMSE) of . m()/s. The calibrated model was used to predict changes in stream flow that would result from changes in land use, including development of forested areas in parts of the watershed to commercial and residential areas. As expected, new development resulted in increased peak flows and reduced low flows. Uncertainty associated with all model parameters, including those not estimated by calibration by enhancing the parameter variance/covariance matrix, was considered when evaluating prediction uncertainties. Seventy percent of the time, predicted flows had uncertainties less than % with more of the uncertainty during low flow conditions."	 (C) 2009 Elsevier B.V. All rights reserved.	"Geza, M|Poeter, EP|McCray, JE"	JOURNAL OF HYDROLOGY	sensitivity analysis automatic calibration prediction uncertainty ucode warmf	10.1016/j.jhydrol.2009.07.025
45	WOS:000312884600010	2013	"Development of a Long-term, Ecologically Oriented Dam Release Plan for the Lake Baiyangdian Sub-basin, Northern China"	RESERVOIR OPERATION RIVER-BASIN ALLOCATION	"Using China's Lake Baiyangdian sub-basin for a case study, we developed an ecologically oriented dam release plan that can be used to define an optimal dam operation scheme that provides both the environmental flows required by bodies of water and wetlands downstream from the Xidayang Reservoir dam and enough water for agricultural, and industrial water users. In addition, we evaluated the benefits that might be provided by modifying releases of water from the reservoir. To attain ecological sustainability in the sub-basin, we used the supply for each water user as a decision variable based on three objectives: () to achieve sustainable socioeconomic development; () to keep the water volume as close as possible to the ideal environmental flows in the urban rivers of Baoding City; and () to keep the water amount as close as possible to Lake Baiyangdian's ideal environmental water requirements. We used the ideal-point method to provide dimensionless values for the first objective, and then used a weighting method to integrate the three objectives into a single holistic goal. We then used the GAMS/CONOPT software to solve the nonlinear model and predict the optimal results. We discuss the optimal water allocation and ecologically oriented dam release plans for the three scenarios. To determine the limitations of the method, we performed a sensitivity analysis, and discuss the optimal results for different weightings of objectives provided by decision-makers. The results of the optimization analysis provide a set of effective compromises among the target objectives that can guide future management of water releases from the reservoir."		"Yang, W|Yang, ZF"	WATER RESOURCES MANAGEMENT	ecologically oriented dam release plans multi-objective optimization environmental water requirements lake baiyangdian	10.1007/s11269-012-0198-7
46	WOS:000239466700009	2006	The comparison of four dynamic systems-based software packages: Translation and sensitivity analysis	WETNESS	"Dynamic model development for describing complex ecological systems continues to grow in popularity. For both academic research and project management, understanding the benefits and limitations of systems-based software could improve the accuracy of results and enlarge the user audience. A Surface Wetness Energy Balance (SWEB) model for canopy surface wetness has been translated into four software packages and their strengths and weaknesses were compared based on 'novice' user interpretations. We found expression-based models such as Simulink and GoldSim with Expressions were able to model the SWEB more accurately; however, stock and flow-based models such as STELLA, Madonna, and GoldSim with Flows provided the user a better conceptual understanding of the ecologic system. Although the original objective of this study was to identify an 'appropriate' software package for predicting canopy surface wetness using SWEB, our outcomes suggest that many factors must be considered by the stakeholders when selecting a model because the modeling software becomes part of the model and of the calibration process. These constraints may include user demographics, budget limitations, built-in sensitivity and optimization tools, and the preference of user friendliness vs. computational power. Furthermore, the multitude of closed proprietary software may present a disservice to the modeling community, creating model artifacts that originate somewhere deep inside the undocumented features of the software, and masking the underlying properties of the model. (c) "	 Elsevier Ltd. All rights reserved.	"Rizzo, DM|Mouser, PJ|Whitney, DH|Mark, CD|Magarey, RD|Voinov, AA"	ENVIRONMENTAL MODELLING & SOFTWARE	model comparison dynamic simulation system-based models canopy surface energy balance	10.1016/j.envsoft.2005.07.009
48	WOS:000306034100006	2012	Sensitivity analysis for volcanic source modeling quality assessment and model selection	UNCERTAINTY IMPORTANCE MEASURE AKAIKE INFORMATION CRITERION COUPLED REACTION SYSTEMS SURFACE DEFORMATION RATE COEFFICIENTS F-TEST INDEXES MOTION	"The increasing knowledge and understanding of volcanic sources has led to the development and implementation of sophisticated and complex mathematical models with the main goal of describing field and experimental data. Quantification of the model's ability in describing the data becomes fundamental for a realistic estimate of the model parameters. The analysis of sensitivity can help us in identifying the parameters that significantly affect the model's output and in assessing its quality factor. In this paper, we describe the Global Sensitivity Analysis (GSA) methods based both on Fourier Amplitude Sensitivity Test and on the Sobol' approach and discuss their implementation in a Mat lab software tool (GSAT). We also introduce a new criterion for model selection based on sensitivity analysis. The proposed approach is tested and applied to quantify the fitting ability of an analytic volcanic source model on a synthetic deformation data. Results show the validity of the method, against the traditional approaches, in supporting the volcanic model selection and the flexibility of the GSAT software tool in analyzing the model sensitivity. (C) "	 Elsevier Ltd. All rights reserved.	"Cannavo, F"	COMPUTERS & GEOSCIENCES	sensitivity analysis inverse problem volcanic source modeling model selection	10.1016/j.cageo.2012.03.008
50	WOS:000328724500002	2014	SGEMS-UQ: An uncertainty quantification toolkit for SGEMS	DISTANCES RESERVOIR MODELS	"While algorithms and methodologies to study uncertainty in the Earth Sciences are constantly evolving, there is currently no free integrated software that allows the general practitioners access to these developments. This paper presents SGEMS-UQ a plugin for the SGEMS platform, that is used to perform distance-based uncertainty analysis on geostatistical simulations, and the resulting forward transfer function responses used in subsurface modeling and engineering. A versatile XML-derived dialect is defined for communicating with external programs that reduces the need for ad-hoc linking of codes, and a relational database system is implemented to automate many of the steps in data mining the spatial and forward model parameters. Through a graphical user interface, one can map a set of realizations and forward transfer function responses into a multidimensional scaling (MDS) space where visualization utilities, and clustering techniques are available. Once mapped in the MDS space, the user can explore linkage between simulation parameters and forward transfer function responses using a module based on a SQL database. Consideration is given to the use of software engineering paradigms and design patterns to produce a code-base that is manageable, efficient, and extensible for future applications, while being scalable to work with large datasets. Finally, we illustrate the versatility of the code-base on an application of modeling uncertainty in reservoir forecasts for an oil reservoir in the West Coast of Africa. (C) "	 Elsevier Ltd. All rights reserved.	"Li, L|Boucher, A|Caers, J"	COMPUTERS & GEOSCIENCES	uncertainty quantification software design xml sql database	10.1016/j.cageo.2013.09.009
51	WOS:000332448800030	2014	PERSiST: a flexible rainfall-runoff modelling toolkit for use with the INCA family of models	MEDITERRANEAN FORESTED CATCHMENT CLIMATE-CHANGE IMPACTS WATER-QUALITY MULTISITE CALIBRATION NITROGEN MODEL HYDROLOGY UNCERTAINTY DYNAMICS FLOW ACIDIFICATION	"Runoff generation processes and pathways vary widely between catchments. Credible simulations of solute and pollutant transport in surface waters are dependent on models which facilitate appropriate, catchment-specific representations of perceptual models of the runoff generation process. Here, we present a flexible, semi-distributed landscape-scale rainfall-runoff modelling toolkit suitable for simulating a broad range of user-specified perceptual models of runoff generation and stream flow occurring in different climatic regions and landscape types. PERSiST (the Precipitation, Evapotranspiration and Runoff Simulator for Solute Transport) is designed for simulating present-day hydrology; projecting possible future effects of climate or land use change on runoff and catchment water storage; and generating hydrologic inputs for the Integrated Catchments (INCA) family of models. PERSiST has limited data requirements and is calibrated using observed time series of precipitation, air temperature and runoff at one or more points in a river network. Here, we apply PERSiST to the river Thames in the UK and describe a Monte Carlo tool for model calibration, sensitivity and uncertainty analysis."		"Futter, MN|Erlandsson, MA|Butterfield, D|Whitehead, PG|Oni, SK|Wade, AJ"	HYDROLOGY AND EARTH SYSTEM SCIENCES		10.5194/hess-18-855-2014
54	WOS:000367774700005	2015	Chaospy: An open source tool for designing methods of uncertainty quantification		"The paper describes the philosophy, design, functionality, and usage of the Python software toolbox Chaospy for performing uncertainty quantification via polynomial chaos expansions and Monte Carlo simulation. The paper compares Chaospy to similar packages and demonstrates a stronger focus on defining reusable software building blocks that can easily be assembled to construct new, tailored algorithms for uncertainty quantification. For example, a Chaospy user can in a few lines of high-level computer code define custom distributions, polynomials, integration rules, sampling schemes, and statistical metrics for uncertainty analysis. In addition, the software introduces some novel methodological advances, like a framework for computing Rosenblatt transformations and a new approach for creating polynomial chaos expansions with dependent stochastic variables. (C)  The Authors."	 Published by Elsevier B.V.	"Feinberg, J|Langtangen, HP"	JOURNAL OF COMPUTATIONAL SCIENCE	uncertainty quantification polynomial chaos expansions monte carlo simulation rosenblatt transformations python package	10.1016/j.jocs.2015.08.008
55	WOS:000282087300021	2010	TAMkin: A Versatile Package for Vibrational Analysis and Chemical Kinetics	MOLECULAR-ORBITAL METHODS FREE-RADICAL POLYMERIZATIONS BLOCK HESSIAN APPROACH FREQUENCY NORMAL-MODES AB-INITIO CALCULATION PHASE N-ALKANES HARMONIC-ANALYSIS LARGE SYSTEMS BASIS-SET THERMOCHEMICAL KINETICS	"TAMkin is a program for the calculation and analysis of normal modes, thermochemical properties and chemical reaction rates. At present, the output from the frequently applied software programs ADF, CHARMM, CPMD, CPK, Gaussian, Q-Chem, and VASP can be analyzed. The normal-mode analysis can be performed using a broad variety of advanced models, including the standard full Hessian, the Mobile Block Hessian, the Partial Hessian Vibrational approach, the Vibrational Subsystem Analysis with or without mass matrix correction, the Elastic Network Model, and other combinations. TAMkin is readily extensible because of its modular structure. Chemical kinetics of unimolecular and bimolecular reactions can be analyzed in a straightforward way using conventional transition state theory, including tunneling corrections and internal rotor refinements. A sensitivity analysis can also be performed, providing important insight into the theoretical error margins on the kinetic parameters. Two extensive examples demonstrate the capabilities of TAMkin: the conformational change of the biological system adenylate kinase is studied, as well as the reaction kinetics of the addition of ethene to the ethyl radical. The important feature of batch processing large amounts of data is highlighted by performing an extended level of theory study, which TAMkin can automate significantly."		"Ghysels, A|Verstraelen, T|Hemelsoet, K|Waroquier, M|Van Speybroeck, V"	JOURNAL OF CHEMICAL INFORMATION AND MODELING		10.1021/ci100099g
56	WOS:000374807600014	2016	Towards uncertainty quantification and parameter estimation for Earth system models in a component-based modeling framework	EQUIFINALITY	"Component-based modeling frameworks make it easier for users to access, configure, couple, run and test numerical models. However, they do not typically provide tools for uncertainty quantification or data-based model verification and calibration. To better address these important issues, modeling frameworks should be integrated with existing, general-purpose toolkits for optimization, parameter estimation and uncertainty quantification. This paper identifies and then examines the key issues that must be addressed in order to make a component-based modeling framework interoperable with general-purpose packages for model analysis. As a motivating example, one of these packages, DAKOTA, is applied to a representative but nontrivial surface process problem of comparing two models for the longitudinal elevation profile of a river to observational data. Results from a new mathematical analysis of the resulting nonlinear least squares problem are given and then compared to results from several different optimization algorithms in DAKOTA. (C) "	 Elsevier Ltd. All rights reserved.	"Peckham, SD|Kelbert, A|Hill, MC|Hutton, EWH"	COMPUTERS & GEOSCIENCES	model uncertainty modeling frameworks component-based modeling optimization inverse problems nonlinear least squares parameter estimation longitudinal river elevation profiles	10.1016/j.cageo.2016.03.005
58	WOS:000188167100002	2003	Uncertainty analysis of hydrologic and water quality predictions for a small watershed using SWAT2000	SOIL HYDRAULIC-PROPERTIES RECHARGE RATE ESTIMATION SPATIAL VARIABILITY ARID ENVIRONMENTS SOLUTE TRANSPORT RISK-ASSESSMENT PART 1 MODEL CONSTRAINTS VARIABLES	"Hydrologic and water quality (H/WQ) models are being used with increasing frequency to devise alternative pollution control strategies. It has been recognized that such models may have a large degree of uncertainty associated with their predictions, and that this uncertainty can significantly impact the utility of the model. In this study, ARRAMIS (Advanced Risk & Reliability Assessment Model) software package was used to analyze the uncertainty of the SWAT (Soil and Water Assessment Tool) outputs concerning nutrients and sediment losses from agricultural lands. ARRAMIS applies Monte Carlo simulation technique connected with Latin hypercube sampling (LHS) scheme. This technique is applied to the Warner Creek watershed located in the Piedmont physiographic region of Maryland, and it provides an interval estimate of a range of values with an associated probability instead of a point estimate of a particular pollutant constituent. Uncertainty of model outputs was investigated using LHS scheme with restricted pairing for the model input sampling. Probability distribution functions (pdfs) for each of the  model simulations were constructed from these results. Model output distributions of interest in this analysis were stream flow, sediment, organic nitrogen (organic-N), organic phosphorus (organic-P), nitrate, ammonium, and mineral phosphorus (mineral-P) transported with water. Developed probability distribution functions for the model provided information with desirable probability. Results indicate that consideration of input parameter uncertainty produces % less mean stream flow along with approximately .% larger sediment loading than obtained using mean input parameters. On the contrary, mean of outputs regarding nutrients such as nitrate, ammonia, organic-N, and organic-P (but not mineral-P) were almost the same as the one using mean input parameters. The uncertainty in predicted stream flow and sediment loading is large, but that for nutrient loadings is the same as that of the corresponding input parameters. This study concluded that using a best possible distribution for the input parameters to reflect the impact of soils and land use diversity in a small watershed on SWAT model outputs may be more accurate than using average values for each input parameter."		"Sohrabi, TM|Shirmohammadi, A|Chu, TW|Montas, H|Nejadhashemi, AP"	ENVIRONMENTAL FORENSICS	uncertainty swat2000 latin hypercube sampling monte carlo nonpoint pollution nutrient	10.1080/714044368
60	WOS:000384855300018	2016	On ISSM and leveraging the Cloud towards faster quantification of the uncertainty in ice-sheet mass balance projections	NORTHEAST GREENLAND MODEL FLOW SENSITIVITY CREEP	"With the Amazon EC Cloud becoming available as a viable platform for parallel computing, Earth System Models are increasingly interested in leveraging its capabilities towards improving climate projections. In particular, faced with long wait periods on high-end clusters, the elasticity of the Cloud presents a unique opportunity of potentially ""infinite"" availability of small-sized clusters running on high-performance instances. Among specific applications of this new paradigm, we show here how uncertainty quantification in climate projections of polar ice sheets (Antarctica and Greenland) can be significantly accelerated using the Cloud. Indeed, small-sized clusters are very efficient at delivering sensitivity and sampling analysis, core tools of uncertainty quantification. We demonstrate how this approach was used to carry out an extensive analysis of ice-flow projections on one of the largest basins in Greenland, the North-East Greenland Glacier, using the Ice Sheet System Model, the public-domain NASA-funded ice-flow modeling software. We show how errors in the projections were accurately quantified using Monte-Carlo sampling analysis on the EC Cloud, and how a judicious mix of high-end parallel computing and Cloud use can best leverage existing infrastructures, and significantly accelerate delivery of potentially ground-breaking climate projections, and in particular, enable uncertainty quantification that were previously impossible to achieve. (C) "	 Elsevier Ltd. All rights reserved.	"Larour, E|Schlegel, N"	COMPUTERS & GEOSCIENCES	polar ice sheet modeling cloud uncertainty quantification	10.1016/j.cageo.2016.08.007
61	WOS:000383683800015	2016	Scalable subsurface inverse modeling of huge data sets with an application to tracer concentration breakthrough data from magnetic resonance imaging	COMPONENT GEOSTATISTICAL APPROACH GENERALIZED COVARIANCE FUNCTIONS HETEROGENEOUS POROUS-MEDIA PARAMETER-ESTIMATION UNCERTAINTY QUANTIFICATION HYDRAULIC CONDUCTIVITY TEMPORAL MOMENTS EQUATIONS TRANSPORT SYSTEMS	"Characterizing subsurface properties is crucial for reliable and cost-effective groundwater supply management and contaminant remediation. With recent advances in sensor technology, large volumes of hydrogeophysical and geochemical data can be obtained to achieve high-resolution images of subsurface properties. However, characterization with such a large amount of information requires prohibitive computational costs associated with ""big data'' processing and numerous large-scale numerical simulations. To tackle such difficulties, the principal component geostatistical approach (PCGA) has been proposed as a ""Jacobian-free'' inversion method that requires much smaller forward simulation runs for each iteration than the number of unknown parameters and measurements needed in the traditional inversion methods. PCGA can be conveniently linked to any multiphysics simulation software with independent parallel executions. In this paper, we extend PCGA to handle a large number of measurements (e.g.,  or more) by constructing a fast preconditioner whose computational cost scales linearly with the data size. For illustration, we characterize the heterogeneous hydraulic conductivity (K) distribution in a laboratory-scale -D sand box using about  million transient tracer concentration measurements obtained using magnetic resonance imaging. Since each individual observation has little information on the K distribution, the data were compressed by the zeroth temporal moment of breakthrough curves, which is equivalent to the mean travel time under the experimental setting. Only about  forward simulations in total were required to obtain the best estimate with corresponding estimation uncertainty, and the estimated K field captured key patterns of the original packing design, showing the efficiency and effectiveness of the proposed method."		"Lee, JH|Yoon, HK|Kitanidis, PK|Werth, CJ|Valocchi, AJ"	WATER RESOURCES RESEARCH		10.1002/2015WR018483
62	WOS:000170761700007	2001	Integration of topology and shape optimization for design of structural components		"This paper presents an integrated approach that supports the topology optimization and CAD-based shape optimization. The main contribution of the paper is using the geometric reconstruction technique that is mathematically sound and error bounded for creating solid models of the topologically optimized structures with smooth geometric boundary. This geometric reconstruction method extends the integration to -D applications. In addition, commercial Computer-Aided Design (CAD), finite element analysis (FEA), optimization, and application software tools are incorporated to support the integrated optimization process. The integration is carried out by first converting the geometry of the topologically optimized structure into smooth and parametric B-spline curves and surfaces. The B-spline curves and surfaces are then imported into a parametric CAD environment to build solid models of the structure. The control point movements of the B-spline curves or surfaces are defined as design variables for shape optimization, in which CAD-based design velocity field computations, design sensitivity analysis (DSA), and nonlinear programming are performed. Both -D plane stress and -D solid examples are presented to demonstrate the proposed approach."		"Tang, PS|Chang, KH"	STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION	cad design sensitivity analysis fea shape optimization topology optimization	10.1007/PL00013282
63	WOS:000378360600027	2016	Operational snow mapping with simplified data assimilation using the seNorge snow model	WATER EQUIVALENT COVERED AREA SWISS ALPS DEPTH PREDICTION CALIBRATION CHALLENGES RADIATION NORWAY SCHEME	"Frequently updated maps of snow conditions are useful for many applications, e.g., for avalanche and flood forecasting services, hydropower energy situation analysis, as well as for the general public. Numerical snow models are often applied in snow map production for operational hydrological services. However, inaccuracies in the simulated snow maps due to model uncertainties and the lack of suitable data assimilation techniques to correct them in near-real time may often reduce the usefulness of the snow maps in operational use. In this paper the revised seNorge snow model (v...) for snow mapping is described, and a simplified data assimilation procedure is introduced to correct detected snow model biases in near real-time. The data assimilation procedure is theoretically based on the Bayesian updating paradigm and is meant to be pragmatic with modest computational and input data requirements. Moreover, it is flexible and can utilize both point-based snow depth and satellite-based areal snow-covered area observations, which are generally the most common data-sources of snow observations. The model and analysis codes as well as the ""R"" statistical software are freely available. All these features should help to lower the challenges and hurdles hampering the application of data-assimilation techniques in operational hydrological modeling. The steps of the data assimilation procedure (evaluation, sensitivity analysis, optimization) and their contribution to significantly increased accuracy of the snow maps are demonstrated with a case from eastern Norway in winter /."	 (C) 2016 Elsevier B.V. All rights reserved.	"Saloranta, TM"	JOURNAL OF HYDROLOGY	snow modeling snow mapping data assimilation	10.1016/j.jhydrol.2016.03.061
67	WOS:000371777100004	2015	Sensitivity of algorithm parameters and objective function scaling in multi-objective optimisation of water distribution systems	OF-THE-ART DISTRIBUTION NETWORKS GENETIC ALGORITHMS EVOLUTIONARY ALGORITHMS OPTIMAL OPERATION NSGA-II DECISION-MAKING TOTAL-COST DESIGN QUALITY	"This paper presents an extensive analysis of the sensitivity of multi-objective algorithm parameters and objective function scaling tested on a large number of parameter setting combinations for a water distribution system optimisation problem. The optimisation model comprises two operational objectives minimised concurrently, the pump energy costs and deviations of constituent concentrations as a water quality measure. This optimisation model is applied to a regional non-drinking water distribution system, and solved using the optimisation software GANetXL incorporating the NSGA-II linked with the network analysis software EPANet. The sensitivity analysis employs a set of performance metrics, which were designed to capture the overall quality of the computed Pareto fronts. The performance and sensitivity of NSGA-II parameters using those metrics is evaluated. The results demonstrate that NSGA-II is sensitive to different parameter settings, and unlike in the single-objective problems, a range of parameter setting combinations appears to be required to reach a Pareto front of optimal solutions. Additionally, inadequately scaled objective functions cause the NSGA-II bias towards the second objective. Lastly, the methodology for performance and sensitivity analysis may be used for calibration of algorithm parameters."		"Mala-Jetmarova, H|Barton, A|Bagirov, A"	JOURNAL OF HYDROINFORMATICS	algorithm parameters multi-objective optimisation performance metrics scaling sensitivity water distribution systems	10.2166/hydro.2015.062
68	WOS:000392165500023	2017	Shale gas flowback water desalination: Single vs multiple-effect evaporation with vapor recompression cycle and thermal integration	PERFORMANCE EVALUATION MEMBRANE DISTILLATION COMPRESSION SYSTEM MANAGEMENT OPTIMIZATION DESIGN UNCERTAINTY RESOURCES SELECTION	"This paper introduces a new optimization model for the single and multiple-effect evaporation (SEE/MEE) systems design, including vapor recompression cycle and thermal integration. The SEE/MEE model is specially developed for shale gas flowback water desalination. A superstructure is proposed to solve the problem, comprising several evaporation effects coupled with intermediate flashing tanks that are used to enhance thermal integration by recovering condensate vapor. Multistage equipment with intercooling is used to compress the vapor formed by flashing and evaporation. The compression cycle is driven by electricity to operate on the vapor originating from the SEE/MEE system, providing all the energy needed in the process. The mathematical model is formulated as a nonlinear programming (NLP) problem optimized under GAMS software by minimizing the total annualized cost. The SEE/MEE system application for zero liquid discharge (ZLD) is investigated by allowing brine salinity discharge near to salt saturation conditions. Additionally, sensitivity analysis is carried out to evaluate the optimal process configuration and performance under distinct feed water salinity conditions. The results highlight the potential of the proposed model to cost-effectively optimize SEE/MEE systems by producing fresh water and reducing brine discharges and associated environmental impacts. (C)  The Authors."	 Published by Elsevier B.V.	"Onishi, VC|Carrero-Parreno, A|Reyes-Labarta, JA|Ruiz-Femenia, R|Salcedo-Diaz, R|Fraga, ES|Caballero, JA"	DESALINATION	shale gas single-effect evaporation (see) multiple-effect evaporation (mee) mechanical vapor recompression (mvr) thermal integration zero liquid discharge (zld)	10.1016/j.desal.2016.11.003
69	WOS:000330079500009	2014	Exploring incomplete information in maintenance materials inventory optimization	SPARE PARTS INTERMITTENT DEMAND STOCK CONTROL CONTROL PERFORMANCE REORDER POINT PARAMETERS SYSTEM MODEL	"Purpose Ensuring the sufficient service level is essential for critical materials in industrial maintenance. This study aims to evaluate the use of statistically imperfect data in a stochastic simulation-based inventory optimization where items' failure characteristics are derived from historical consumption data, which represents a real-life situation in the implementation of such an optimization model. Design/methodology/approach - The risks of undesired shortages were evaluated through a service-level sensitivity analysis. The service levels were simulated within the error of margin of the key input variables by using StockOptim optimization software and real data from a Finnish steel mill. A random sample of  inventory items was selected. Findings - Service-level sensitivity is item specific, but, for many items, statistical imprecision in the input data causes significant uncertainty in the service level. On the other hand, some items seem to be more resistant to variations in the input data than others. Research limitations/implications - The case approach, with one simulation model, limits the generalization of the results. The possibility that the simulation model is not totally realistic exists, due to the model's normality assumptions. Practical implications - Margin of error in input data estimation causes a significant risk of not achieving the required service level. It is proposed that managers work to improve the preciseness of the data, while the sensitivity analysis against statistical uncertainty, and a correction mechanism if necessary, should be integrated into optimization models. Originality/value - The output limitations in the optimization, i.e. service level, are typically stated precisely, but the capabilities of the input data have not been addressed adequately. This study provides valuable insights into ensuring the availability of-critical materials."		"Puurunen, A|Majava, J|Kess, P"	INDUSTRIAL MANAGEMENT & DATA SYSTEMS	inventory optimization maintenance materials service-level sensitivity spare parts stockoptim	10.1108/IMDS-01-2013-0025
70	WOS:000303035400007	2012	Comparison of different uncertainty techniques in urban stormwater quantity and quality modelling	FORMAL BAYESIAN METHOD SENSITIVITY-ANALYSIS STREAMFLOW SIMULATION HYDRAULIC-PROPERTIES GLUE APPROACH WATER OPTIMIZATION CALIBRATION PARAMETER QUANTIFICATION	"Urban drainage models are important tools used by both practitioners and scientists in the field of stormwater management. These models are often conceptual and usually require calibration using local datasets. The quantification of the uncertainty associated with the models is a must, although it is rarely practiced. The International Working Group on Data and Models, which works under the IWA/IAHR Joint Committee on Urban Drainage, has been working on the development of a framework for defining and assessing uncertainties in the field of urban drainage modelling. A part of that work is the assessment and comparison of different techniques generally used in the uncertainty assessment of the parameters of water models. This paper compares a number of these techniques: the Generalized Likelihood Uncertainty Estimation (GLUE), the Shuffled Complex Evolution Metropolis algorithm (SCEM-UA), an approach based on a multi-objective auto-calibration (a multialgorithm, genetically adaptive multi-objective method, AMALGAM) and a Bayesian approach based on a simplified Markov Chain Monte Carlo method (implemented in the software MICA). To allow a meaningful comparison among the different uncertainty techniques, common criteria have been set for the likelihood formulation, defining the number of simulations, and the measure of uncertainty bounds. Moreover, all the uncertainty techniques were implemented for the same case study, in which the same stormwater quantity and quality model was used alongside the same dataset. The comparison results for a well-posed rainfall/runoff model showed that the four methods provide similar probability distributions of model parameters, and model prediction intervals. For ill-posed water quality model the differences between the results were much wider; and the paper provides the specific advantages and disadvantages of each method. In relation to computational efficiency (i.e. number of iterations required to generate the probability distribution of parameters), it was found that SCEM-UA and AMALGAM produce results quicker than GLUE in terms of required number of simulations. However, GLUE requires the lowest modelling skills and is easy to implement. All non-Bayesian methods have problems with the way they accept behavioural parameter sets, e.g. GLUE, SCEM-UA and AMALGAM have subjective acceptance thresholds, while MICA has usually problem with its hypothesis on normality of residuals. It is concluded that modellers should select the method which is most suitable for the system they are modelling (e.g. complexity of the model's structure including the number of parameters), their skill/knowledge level, the available information, and the purpose of their study. (C) "	 Elsevier Ltd. All rights reserved.	"Dotto, CBS|Mannina, G|Kleidorfer, M|Vezzaro, L|Henrichs, M|McCarthy, DT|Freni, G|Rauch, W|Deletic, A"	WATER RESEARCH	urban drainage models uncertainties parameter probability distributions bayesian inference glue scem-ua mica amalgam mcmc multi-objective auto-calibration	10.1016/j.watres.2012.02.009
73	WOS:000413245200004	2017	A user-friendly software package for VIC hydrologic model development	GLOBAL SENSITIVITY-ANALYSIS PRECIPITATION PRODUCTS MULTISITE CALIBRATION SOIL-MOISTURE RUNOFF-MODEL WATER BASIN SIMULATION DATASET FLUXES	"The Variable Infiltration Capacity (VIC) hydrologic and river routing model simulates the water and energy fluxes that occur near the land surface and provides useful information regarding the quantity and timing of available water within a watershed system. However, despite its popularity, wider adoption is hampered by the considerable effort required to prepare model inputs and calibrate the model parameters. This study presents a user-friendly software package, named VIC-Automated Setup Toolkit (VIC-ASSIST), accessible through an intuitive MATLAB graphical user interface. VIC-ASSIST enables users to navigate the model building process through prompts and automation, with the intention to promote the use of the model for practical, educational, and research purposes. The automated processes include watershed delineation, climate and geographical input set-up, model parameter calibration, sensitivity analysis, and graphical output generation. We demonstrate the package's utilities in various case studies. (C) "	 Elsevier Ltd. All rights reserved.	"Wi, S|Ray, P|Demaria, EMC|Steinschneider, S|Brown, C"	ENVIRONMENTAL MODELLING & SOFTWARE	vic hydrologic model vic setup assistant tool matlab graphic user interface automatic calibration sensitivity analysis	10.1016/j.envsoft.2017.09.006
76	WOS:000280543800002	2010	An evolutionary optimization of diffuser shapes based on CFD simulations	STRAIGHT 2-DIMENSIONAL DIFFUSERS HEAT-TRANSFER OPTIMIZATION INTERNALLY FINNED TUBES SENSITIVITY ANALYSIS ADJOINT FORMULATION PARALLEL COMPUTERS LAMINAR-FLOW DESIGN PERFORMANCE CHANNELS	"An efficient and robust algorithm is presented for the optimum design of plane symmetric diffusers handling incompressible turbulent flow. The indigenously developed algorithm uses the CFD software: Fluent for the hydrodynamic analysis and employs a genetic algorithm (GA) for optimization. For a prescribed inlet velocity and outlet pressure, pressure recovery coefficient C-p* (the objective function) is estimated computationally for various design options. The CFD software and the GA have been combined in a monolithic platform for a fully automated operation using some special control commands. Based on the developed algorithm, an extensive exercise has been made to optimize the diffuser shape. Different methodologies have been adopted to create a large number of design options. Interestingly, not much difference has been noted in the optimum C-p* values obtained through different approaches. However, in all the approaches, a better design has been obtained through a proper selection of the number of design variables. Finally, the effect of diffuser length on the optimum shape has also been studied."	" Copyright (C) 2009 John Wiley & Sons, Ltd."	"Ghosh, S|Pratihar, DK|Maiti, B|Das, PK"	INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN FLUIDS	incompressible flow optimization genetic algorithm 2d planar diffuser 2d planar duct	10.1002/fld.2124
78	WOS:000240365500002	2006	Environmental and ecological hydroinformatics to support the implementation of the European Water Framework Directive for river basin management	DECISION-SUPPORT MODELING APPROACH SWAT MODEL SPECIES RICHNESS QUALITY MODELS LANDSCAPE INTEGRATION DESIGN SYSTEM UNCERTAINTY	"Research and development in hydroinformatics can play an important role in environmental impact assessment by integrating physically-based models, data-driven models and other information and Communication Tools (ICT). An illustration is given in this paper describing the developments around the Soil and Water Assessment Tool (SWAT) to support the implementation of the EU water Framework Directive. SWAT operates on the river basin scale and includes processes for the assessment of complex diffuse pollution; it is open-source software, which allows for site-specific modifications to the source and easy linkage to other hydroinformatics tools. A crucial step in the world-wide applicability of SWAT was the integration of the model into a GIS environment, allowing for a quick model set-up using digital information on terrain elevation, land use and management, soil properties and weather conditions. Model analysis tools can be integrated with SWAT to assist in the tedious tasks of model calibration, parameter optimisation, sensitivity and uncertainty analysis and allows better understanding of the model before addressing scientific and societal questions. Finally, further linkage of SWAT to ecological assessment tools, Land Use prediction tools and tools for optimal Experimental Design shows that SWAT can play an important role in multi-disciplinary eco-environmental impact assessment studies."		"van Griensven, A|Breuer, L|Di Luzio, M|Vandenberghe, V|Goethals, P|Meixner, T|Arnold, J|Srinivasan, R"	JOURNAL OF HYDROINFORMATICS	catchment modelling eco-hydrology environmental hydroinformatics eu water framework directive model integration swat	10.2166/hydro.2006.010
81	WOS:000303082000011	2012	An environmental and economic analysis for geotube coastal structures retaining dredge material	EROSION	"This paper investigates the environmental and economic sensitivity of coastal structures for two different construction methods: a traditional rubble mound structure and a geotube coastal structure using dredged material. The analysis is undertaken for two projects: a small scale coastal protection project using a revetment and a medium size capital harbour expansion using a breakwater. This work provides further insight into previously published work by Sheehan et al. () on the economic aspects of geotube technology and identifies the optimum method of construction for each type of coastal structure. An economic sensitivity analysis is undertaken on the key logistical parameters involved in the construction of these coastal structures. An environmental sensitivity analysis focuses on the CO emissions produced from the construction of the coastal structures for both construction methods. These sensitivity analyses are undertaken using a decision support software program (DMMAP), developed to assist users at the planning stages of a project to achieve sustainable dredge material management. The key logistical parameters are analysed to generate environmental and economic ranking tables. The analyses highlight that the size of the structure and the distance to the source of the quarry material are crucial factors in determining the optimum construction method. This work shows that geotubes are a viable alternative to traditional rubble mound coastal structures. It also shows that traditional construction methods may be more economical than geotube structures when considering small coastal structures. In general, the larger the scale of the project the greater the potential savings in CO emissions and cost that can be achieved through the use of geotube technology. Geotubes, with the use of dredge material, may provide a sustainable beneficial use for dredge material and offer a serious economic and environmental alternative to traditional rubble mound structures."	 (C) 2012 Elsevier B.V. All rights reserved.	"Sheehan, C|Harrington, J"	RESOURCES CONSERVATION AND RECYCLING	geotube revetment breakwater coastal structures dredging beneficial use	10.1016/j.resconrec.2012.01.011
82	WOS:000358060800012	2015	A software development framework for structural optimization considering non linear static responses	DYNAMIC TOPOLOGY OPTIMIZATION EQUIVALENT LOADS	"In the real world, structural systems may not have linear static characteristics. However, structural optimization has been developed based on static responses because sensitivity analysis regarding static finite element analysis is developed quite well. Analyses other than static analysis are heavily required in the engineering community these days. Techniques for such analyses have been extensively developed and many software systems using the finite element method are easily available in the market. On the other hand, development of structural optimization using such analyses is fairly slow due to many obstacles. One obstacle is that it is very difficult and expensive to consider the nonlinearities or dynamic effects in the way of conventional optimization. Recently, the equivalent static loads method for non linear static response structural optimization (ESLSO) has been proposed for structural optimization with various responses: linear dynamic response, nonlinear static response, and nonlinear dynamic response. In ESLSO, finite element analysis other than static analysis is performed, equivalent static loads (ESLs) are generated, linear static response structural optimization is carried out with the ESLs and the process iterates. A software system for the automatic use of ESLSO is developed and described. One of the advantages of ESLSO is that it can use well developed commercial software systems for structural analysis and linear static response structural optimization. Various analysis and optimization systems are integrated in the developed system. The structure of the system is systematically defined and the software is developed by the C++ language on the Windows operating system."		"Lee, HA|Park, GJ"	STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION	structural optimization equivalent static loads (esls) equivalent static loads method for non linear static response structural optimization (eslso)	10.1007/s00158-015-1228-x
86	WOS:000375818700005	2016	River-to-sea pressure retarded osmosis: Resource utilization in a full-scale facility	POWER-GENERATION SALINITY GRADIENTS REVERSE ELECTRODIALYSIS OSMOTIC POWER SEAWATER DESALINATION ENERGY EFFICIENCY RO-PRO WATER SYSTEM PERFORMANCE	"Pressure retarded osmosis (PRO) is a technology that could be utilized to recover energy from the mixing of freshwater with seawater. This source of renewable energy is sizeable and in the past decade several investigations analyzed its potential. The vast majority of studies focused on mass transfer problems across the membrane in order to improve membrane productivity and just recently studies started to look at membrane module efficiencies and parasitic loads within the PRO facility. In this article, the net specific energy production from a facility-scale PRO system was determined and optimized by using a novel simulation method that integrates parasitic loads and efficiencies of the PRO facility components and combines the model with an optimization software in a linked system optimization scheme. It was found that the overall net specific energy that may be recovered by a river-to-sea PRO facility is approximately . kWh per m() of permeate. Furthermore, a sensitivity analysis was performed to elucidate the relationship between net specific energy and power density as functions of membrane area, flow rates, and operating pressures. In general, in order to maximize resource recovery, a low power density, thus a low membrane productivity, must be accepted."	 (C) 2016 Elsevier B.V. All rights reserved.	"O'Toole, G|Jones, L|Coutinho, C|Hayes, C|Napoles, M|Achilli, A"	DESALINATION	renewable energy salinity gradient power pressure retarded osmosis net specific energy power density facility analysis	10.1016/j.desal.2016.01.012
87	WOS:000331776000033	2014	Characterisation factors for life cycle impact assessment of sound emissions	ROAD TRAFFIC NOISE SENSITIVITY-ANALYSIS LCA FRAMEWORK	"Noise is a serious stressor affecting the health of millions of citizens. It has been suggested that disturbance by noise is responsible for a substantial part of the damage to human health. However, no recommended approach to address noise impacts was proposed by the handbook for life cycle assessment (LCA) of the European Commission, nor are characterisation factors (CFs) and appropriate inventory data available in commonly used databases. This contribution provides CFs to allow for the quantification of noise impacts on human health in the LCA framework. Noise propagation standards and international reports on acoustics and noise impacts were used to define the model parameters. Spatial data was used to calculate spatially-defined CFs in the form of -by--km maps. The results of this analysis were combined with data from the literature to select input data for representative archetypal situations of emission (e.g. urban day with a frequency of  Hz, rural night at  Hz, etc.). A total of  spatial and  archetypal CFs were produced to evaluate noise impacts at a European level (i.e. EU). The possibility of a user-defined characterisation factor was added to support the possibility of portraying the situation of full availability of information, as well as a highly-localised impact analysis. A Monte Carlo-based quantitative global sensitivity analysis method was applied to evaluate the importance of the input factors in determining the variance of the output. The factors produced are ready to be implemented in the available LCA databases and software. The spatial approach and archetypal approach may be combined and selected according to the amount of information available and the life cycle under study. The framework proposed and used for calculations is flexible enough to be expanded to account for impacts on target subjects other than humans and to continents other than Europe."	 (C) 2013 Elsevier B.V. All rights reserved.	"Cucurachi, S|Heijungs, R"	SCIENCE OF THE TOTAL ENVIRONMENT	noise noise impacts life cycle lcia lca annoyance	10.1016/j.scitotenv.2013.07.080
88	WOS:000209099900002	2011	ASSESSMENT OF COLLOCATION AND GALERKIN APPROACHES TO LINEAR DIFFUSION EQUATIONS WITH RANDOM DATA		"We compare the performance of two methods, the stochastic Galerkin method and the stochastic collocation method, for solving partial differential equations (PDEs) with random data. The stochastic Galerkin method requires the solution of a single linear system that is several orders larger than linear systems associated with deterministic PDEs. The stochastic collocation method requires many solves of deterministic PDEs, which allows the use of existing software. However, the total number of degrees of freedom in the stochastic collocation method can be considerably larger than the number of degrees of freedom in the stochastic Galerkin system. We implement both methods using the Trilinos software package and we assess their cost and performance. The implementations in Trilinos are known to be efficient, which allows for a realistic assessment of the computational complexity of the methods. We also develop a cost model for both methods which allows us to examine asymptotic behavior."		"Elman, HC|Miller, CW|Phipps, ET|Tuminaro, RS"	INTERNATIONAL JOURNAL FOR UNCERTAINTY QUANTIFICATION	uncertainty quantification stochastic partial differential equations polynomial chaos stochastic galerkin method stochastic sparse grid collocation karhunen-loeve expansion	10.1615/Int.J.UncertaintyQuantification.v1.i1.20
89	WOS:000325074400040	2013	Estimation of nitrate load from septic systems to surface water bodies using an ArcGIS-based software	GROUND-WATER RIPARIAN ZONE DENITRIFICATION NITROGEN MODEL TRANSPORT SIMULATION TOPOGRAPHY AQUIFERS STREAM	"Nitrate, as a commonly identified groundwater and surface water pollutant, poses serious threats to human health and the environment. One important source of nitrate in the environment is due to wastewater treatment using Onsite Sewage Treatment and Disposal Systems (OSTDS) (a.k.a., septic systems). To facilitate water resources and environmental management, an ArcGIS-Based Nitrate Load Estimation Toolkit (ArcNLET) is developed to simulate nitrate transport and estimate nitrate load from septic systems and collocated fertilizer applications in groundwater to surface water bodies. It is a screening tool based on a simplified conceptual model of groundwater flow and nitrate transport. It is used in this study to estimate nitrate load from thousands of septic systems to surface water bodies in two neighborhoods located in Jacksonville, FL, USA, where nitrate due to septic systems is believed to be one of the reasons of nutrient enrichment and an isotope study indicates that denitrification is significant. A global sensitivity analysis is performed to identify critical parameters for model calibration, and the most critical parameter is the first-order decay coefficient used to simulate the denitrification process. Hydraulic conductivities at different soil zones have different levels of influence on simulated nitrate concentrations at different locations. By manually adjusting model parameters, simulated shapes of water table and nitrate concentration agree reasonably with average field observations, suggesting that ArcNLET is able to simulate spatial variability of field observations. Estimated nitrate loads exhibit spatial variability, which is useful to facilitate decisions on the conversion of OSTDS into sewers in certain areas for reducing nitrate load from septic systems to surface water bodies."		"Wang, LY|Ye, M|Rios, JF|Fernandes, R|Lee, PZ|Hicks, RW"	ENVIRONMENTAL EARTH SCIENCES	gis-based screening model nitrate transport denitrification nitrate loads sensitivity analysis morris method	10.1007/s12665-013-2283-5
91	WOS:000236011600030	2006	A standard interface between simulation programs and systems analysis software	TRANSPORT PARAMETERS AQUATIC SYSTEMS UNCERTAINTY MODELS MINIMIZATION FLOW	"A simple interface between simulation programs and systems analytical software is proposed. This interface is designed to facilitate linkage of environmental simulation programs with systems analytical software and thus can contribute to remedying the deficiency in applying systems analytical techniques to environmental modelling studies. The proposed concept, consisting of a text file interface combined with a batch mode simulation program call, is independent of model structure, operating system and programming language. It is open for implementation by academic and commercial simulation and systems analytical software developers and is very simple to implement. Its practicability is demonstrated by implementations for three environmental simulation packages (AQUASIM, SWAT and LEACHM) and two systems analytical program packages (UNCSIM, SUR). The properties listed above and the demonstration of the ease of implementation of the approach are prerequisites for the stimulation of a widespread implementation of the proposed interface that would be beneficial for the dissemination of systems analytical techniques in the environmental and engineering sciences. Furthermore, such a development could stimulate the transfer of systems analytical techniques between different fields of application."		"Reichert, P"	WATER SCIENCE AND TECHNOLOGY	systems analytical techniques environmental simulation programs statistical inference sensitivity analysis identifiability analysis uncertainty analysis	10.2166/wst.2006.029
92	WOS:000356196000011	2015	PUQ; A code for non-intrusive uncertainty propagation in computer simulations	STRENGTH MAXIMUM SCIENCE CHAOS	"We present a software package for the non-intrusive propagation of uncertainties in input parameters through computer simulation codes or mathematical models and associated analysis; we demonstrate its use to drive micromechanical simulations using a phase field approach to dislocation dynamics. The PRISM uncertainty quantification framework (PUQ) offers several methods to sample the distribution of input variables and to obtain surrogate models (or response functions) that relate the uncertain inputs with the quantities of interest (QoIs); the surrogate models are ultimately used to propagate uncertainties. PUQ requires minimal changes in the simulation code, just those required to annotate the QoI(s) for its analysis. Collocation methods include Monte Carlo, Latin Hypercube and Smolyak sparse grids and surrogate models can be obtained in terms of radial basis functions and via generalized polynomial chaos. PUQ uses the method of elementary effects for sensitivity analysis in Smolyak runs. The code is available for download and also available for cloud computing in nanoHUB. PUQ orchestrates runs of the nanoPLASTICITY tool at nanoHUB where users can propagate uncertainties in dislocation dynamics simulations using simply a web browser, without downloading or installing any software. Program summary Program title: PUQ Catalogue identifier: AEWP_v_ Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEWP_v_.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: MIT license No. of lines in distributed program, including test data, etc.:  No. of bytes in distributed program, including test data, etc.:  Distribution format: tar.gz Programming language: Python, C. Computer: Workstations. Operating system: Linux, Mac OSX. Classification: ., ., ., External routines: SciPy, Matplotlib, hpy Nature of problem: Uncertainty propagation and creation of response surfaces. Solution method: Generalized Polynomial Chaos (gPC) using Smolyak sparse grids. Running time: PUQ performs uncertainty quantification and sensitivity analysis by running a simulation multiple times using different values for input parameters. Its run time will be the product of the run time of the chosen simulation code and the number of runs required to achieve the desired accuracy."	 (C) 2015 Elsevier B.V. All rights reserved.	"Hunt, M|Haley, B|McLennan, M|Koslowski, M|Murthy, J|Strachan, A"	COMPUTER PHYSICS COMMUNICATIONS	uncertainty quantification surrogate model sensitivity analysis	10.1016/j.cpc.2015.04.011
95	WOS:000378360600044	2016	Tools for investigating the prior distribution in Bayesian hydrology	PARAMETER-ESTIMATION INFERENCE MODELS CALIBRATION	"Bayesian inference is one of the most popular tools for uncertainty analysis in hydrological modeling. While much emphasis has been placed on the selection of appropriate likelihood functions within Bayesian hydrology, few researchers have evaluated the importance of the prior distribution in deriving appropriate posterior distributions. This paper describes tools for the evaluation of parameter sensitivity to the prior distribution to provide guidelines for defining meaningful priors. The tools described here consist of two measurements, the Kullback-Leibler Divergence (KLD) and the prior information elasticity. The Kullback-Leibler Divergence (KLD) is applied to calculate differences between the prior and posterior distributions for different cases. The prior information elasticity is then used to quantify the responsiveness of the KLD values to the change of prior distributions and length of available data. The tools are demonstrated via a Bayesian framework using an MCMC algorithm for a conceptual hydrologic model with both synthetic and real cases. The results of the application of this toolkit suggest the prior distribution can have a significant impact on the posterior distribution and should be more routinely assessed in hydrologic studies."	 (C) 2016 Elsevier B.V. All rights reserved.	"Tang, YT|Marshall, L|Sharma, A|Smith, T"	JOURNAL OF HYDROLOGY	bayesian hydrological modeling prior distribution parameter sensitivity kullback-leibler divergence elasticity	10.1016/j.jhydrol.2016.04.032
97	WOS:000389785300007	2017	An automated decision support system for aided assessment of variogram models	GLOBAL SENSITIVITY-ANALYSIS ENVIRONMENTAL-MODELS VALIDATION FRAMEWORK NETWORKS ROBUST SOIL	"In the present paper, an extensive cross-validation procedure, based on the analysis of numerical indices and graphical tools, is described and discussed. The procedure has been implemented in a software application designed to support practitioners in the variogram model assessment. It provides an extensive report, which summarizes a large post-processing stage and suggests how to interpret the performed analysis to rate the model to be validated. Besides classical accuracy indices, two new integrated tools based on the variogram of residuals are introduced, which take the spatial nature of the dataset into account. Finally, inspecting the summary report, the user can decide whether the considered model is satisfactory for his/her goals or it needs to be improved. Finally, a case study is presented related to the variogram assessment of groundwater level measured in a porous shallow aquifer of the Apulia Region (South -Italy). (C) "	 Elsevier Ltd. All rights reserved.	"Barca, E|Porcu, E|Bruno, D|Passarella, G"	ENVIRONMENTAL MODELLING & SOFTWARE	geostatistics extensive cross-validation rank coefficient of spatial correlation decision-support system	10.1016/j.envsoft.2016.11.004
103	WOS:000302212800003	2012	Effect of Temporal and Spatial Rainfall Resolution on HSPF Predictive Performance and Parameter Estimation	HYDROLOGICAL SIMULATION PROGRAM AUTOMATIC CALIBRATION MODEL PARAMETERS VARIABILITY RUNOFF PRECIPITATION RADAR UNCERTAINTY IMPACT FLOW	"Watershed-scale rainfall-runoff models are used for environmental management and regulatory modeling applications, but their effectiveness is limited by predictive uncertainties associated with model input data. This study evaluated the effect of temporal and spatial rainfall resolution on the predictive performance of Hydrological Simulation Program-Fortran (HSPF) using manual and automatic calibration procedures. Furthermore, the effect of automatic parameter estimation on the physical significance of calibrated parameter values was evaluated. Temporal resolutions examined included  min,  min,  h, and  h, and spatial resolution effects evaluated included the effect of a spatially averaged network of four rain gauges and Next-Generation Radar (NEXRAD) for selected rain events. Model efficiencies ranged from . to . when individual rain gauges (RG, RG, RG, and RG) were used one at a time. Model efficiency improved and ranged from . to . when a spatially averaged network of four rain gauges was used. The effect of temporal resolution on model performance varied with rain gauge location in the watershed and with use of a single gauge or spatially averaged rain gauges for model calibration. Rainfall resolution has a strong influence on parameter estimation because, to achieve high model performance, parameter values must shift whenever the resolution of the rainfall data is changed. Despite a shift in parameter values as a result of changes in rainfall resolution, the results showed that Parameter Estimation Software (PEST)-calibrated values remained within their parameter bounds. In summary, results obtained from a medium-sized Piedmont watershed in Georgia, USA, revealed that model performance was more sensitive to spatial resolution than temporal resolution. DOI: ./(ASCE)HE.-.."	 (C) 2012 American Society of Civil Engineers.	"Mohamoud, YM|Prieto, LM"	JOURNAL OF HYDROLOGIC ENGINEERING	hspf spatial resolution temporal resolution parameter estimation model performance watershed modeling	10.1061/(ASCE)HE.1943-5584.0000457
105	WOS:000378854600008	2016	"Towards the integration of process design, control and scheduling: Are we getting closer?"	MODEL-PREDICTIVE CONTROL INTEGER DYNAMIC OPTIMIZATION CONSTRAINED LINEAR-SYSTEMS OPTIMAL GRADE TRANSITION CHEMICAL-PROCESSES BATCH PROCESSES UNCERTAINTY FRAMEWORK FLEXIBILITY PARAMETERS	"The integration of design and control, control and scheduling and design, control and scheduling, all have been core PSE challenges. While significant progress has been achieved over the years, it is fair to say that at the moment there is not a generally accepted methodology and/or ""protocol"" for such an integration - it is also interesting to note that currently, there is not a commercially available software [or even in a prototype form] system to fully support such an activity. Here, we present the foundations for such an integrated framework and especially a software platform that enables such integration based on research developments over the last  years. In particular, we describe PAROC, a prototype software system which allows for the representation, modeling and solution of integrated design, scheduling and control problems. Its main features include: (i) a high-fidelity dynamic model representation, also involving global sensitivity analysis, parameter estimation and mixed integer dynamic optimization capabilities; (ii) a suite/toolbox of model approximation methods; (iii) a host of multi-parametric programming solvers for mixed continuous/integer problems; (iv) a state-space modeling representation capability for scheduling and control problems; and (v) an advanced toolkit for multi-parametric/explicit Model Predictive Control and moving horizon reactive scheduling problems. Algorithms that enable the integration capabilities of the systems for design, scheduling and control are presented on a case of a series of cogeneration units. (C) "	 Elsevier Ltd. All rights reserved.	"Pistikopoulos, EN|Diangelakis, NA"	COMPUTERS & CHEMICAL ENGINEERING	multi-parametric receding horizon policies control design optimization optimal scheduling integration	10.1016/j.compchemeng.2015.11.002
107	WOS:000355760000014	2015	Optimization of the motion control mechanism of the hatch door of airliner	FLEXIBLE MULTIBODY SYSTEMS DESIGN SENSITIVITY-ANALYSIS FINITE SEGMENT APPROACH DYNAMIC-SYSTEMS RECURSIVE FORMULATION PARALLEL MANIPULATORS LOOP SYSTEMS EQUATIONS METHODOLOGY ALGORITHM	"This paper deals with the problem of parameter optimization of the motion control mechanism of the hatch door of ARJ-, a regional airliner of China. Motion improvement of the hatch door is implemented by two kinds of passive designs. Firstly, a single-layer optimization model for trajectory modification is developed to find the optimum size of the key parts of the control mechanism. Secondly, a novel nested bi-level optimization model is presented for the design of the size tolerance limits of the selected parts. The design objective is minimization of the total extremum deviation of the motion trajectory of the objective point of the hatch door, where the extremum deviation is obtained by solution of the inner-level size optimization problem for the fixed size tolerance limits. The optimization models for motion control of the hatch door mechanism are solved using the response surface method. Numerical examples show that the precision of the real running trajectory of the objective point of the hatch door mechanism may be improved effectively by using the methods presented. A home-made multi-body dynamics solver (THUSOLVER) and the corresponding optimization software have been developed to implement the above tasks."		"Du, JB|Huang, ZT|Yang, RZ"	STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION	bi-level optimization model motion control mechanism tolerance design response surface method size optimization	10.1007/s00158-014-1191-y
108	WOS:000256243200015	2008	Sensitivity of population viability to spatial and nonspatial parameters using grip	METAPOPULATION DYNAMICS PREDICTIVE ABILITY PVA MODELS MATRIX REINTRODUCTION	"Metapopulation dynamics are influenced by spatial parameters including the amount and arrangement of suitable habitat, yet these parameters may be uncertain when deciding how to manage species or their habitats. Sensitivity analyses of population viability analysis (PVA) models can help measure relative parameter influences on predictions, identify research priorities for reducing uncertainty, and evaluate management strategies. Few spatial PVAs, however, include sensitivity analyses of both spatial and nonspatial parameters, perhaps because computationally efficient tools for such analyses are lacking or inaccessible. We developed GRIP, a program to facilitate sensitivity analysis of spatial and nonspatial input parameters for PVAs created in RAMAS Metapop, a widely applied software program. GRIP creates random sets of input files by varying parameters specified in the PVA model including vital rates and their correlations among populations, the number and configuration of populations, dispersal rates, dispersal survival, initial population abundances, carrying capacities, and the probability, intensity, and spatial extent of catastrophes, while drawing on specified parameter distributions. We evaluated GRIP's performance as a tool for sensitivity analysis of spatial PVAs and explored the consequences of varying spatial input parameters for predictions of a published PVA model of the sand lizard (Lacerta agilis). We used GRIP output to generate standardized regression coefficients (SRCs) and nonparametric correlation coefficients as indices of the relative sensitivity of predicted conservation status to input parameters. GRIP performed well; with a single analysis we were able to rank the relative influence of input parameters identified as influential by the PVA's original author, S. A. Berglind, who used three separate forms of sensitivity analysis. Our analysis, however, also underscored the value of exploring the relative influence of spatial parameters on PVA predictions; both SRCs and correlation coefficients indicated that the most influential parameters in the sand lizard model were spatial in nature. We provide annotated code so that GRIP may be modified to reflect particular species biology, customized for more complex spatial PVA models, upgraded to incorporate features added in newer versions of RAMAS Metapop, used as a template to develop similar programs, or used as it is for computationally efficient sensitivity analyses in support of conservation planning."		"Curtis, JMR|Naujokaitis-Lewis, I"	ECOLOGICAL APPLICATIONS	decision-support tool lacerta agilis recovery planning sand lizard sensitivity analysis spatial population viability analysis uncertainty	10.1890/07-1306.1
110	WOS:000227978000007	2005	"Investigating uncertainty and sensitivity in integrated, multimedia environmental models: tools for FRAMES-3MRA"	FRAMEWORK SYSTEM	"Elucidating uncertainty and sensitivity structures in environmental models can be a difficult task, even for low-order, single-medium constructs driven by a unique set of site-specific data. Quantitative assessment of integrated, multimedia models that simulate hundreds of sites, spanning multiple geographical and ecological regions, will ultimately require a comparative approach using several techniques, coupled with sufficient computational power. The Framework for Risk Analysis in Multimedia Environmental Systems - Multimedia, Multipathway, and Multireceptor Risk Assessment (FRAMES-MRA) is an important software model being developed by the United States Environmental Protection Agency for use in risk assessment of hazardous waste management facilities. The MRA modeling system includes a set of  science modules that collectively simulate release, fate and transport, exposure, and risk associated with hazardous contaminants disposed of in land-based waste management units (WMU). The MRA model encompasses  multi-dimensional input variables, over  of which are explicitly stochastic. Design of SuperMUSE, a  GHz PC-based, Windows-based Supercomputer for Model Uncertainty and Sensitivity Evaluation is described. Developed for MRA and extendable to other computer models, an accompanying platform-independent, Java-based parallel processing software toolset is also discussed. For MRA, comparison of stand-alone PC versus SuperMUSE simulation executions showed a parallel computing overhead of only . seconds/simulation, a relative cost increase of .% over average model runtime. Parallel computing software tools represent a critical aspect of exploiting the capabilities of such modeling systems. The Java toolset developed here readily handled machine and job management tasks over the Windows cluster, and is currently capable of completing over  million MRA model simulations per month on SuperMUSE. Preliminary work is reported for an example uncertainty analysis of Benzene disposal that describes the relative importance of various exposure pathways in driving risk levels for ecological receptors and human health. Incorporating landfills, waste piles, aerated tanks, surface impoundments, and land application units, the site-based data used in the analysis included  facilities across the United States representing  site-WMU combinations."	 Published by Elsevier Ltd.	"Babendreier, JE|Castleton, KJ"	ENVIRONMENTAL MODELLING & SOFTWARE	multimedia model parallel computing pc-based supercomputing uncertainty analysis sensitivity analysis benzene disposal java	10.1016/j.envsoft.2004.09.013
112	WOS:000241572400011	2006	Optimal control of open-channel flow using adjoint sensitivity analysis	MANNINGS ROUGHNESS COEFFICIENTS CONTAMINANT RELEASES HAZARD MITIGATION WAVE CONTROL IDENTIFICATION OPTIMIZATION RIVERS MODEL	"An optimal flow control methodology based on adjoint sensitivity analysis for controlling nonlinear open channel flows with complex geometries is presented. The adjoint equations, derived from the nonlinear Saint-Venant equations, are generally capable of evaluating the time-dependent sensitivities with respect to a variety of control variables under complex flow conditions and cross-section shapes. The internal boundary conditions of the adjoint equations at a confluence (junction) derived by the variational approach make the flow control model applicable to solve optimal flow control problems in a channel network over a watershed. As a result, an optimal flow control software package has been developed, in which two basic modules, i.e., a hydrodynamic module and a bound constrained optimization module using the limited-memory quasi-Newton algorithm, are integrated. The effectiveness and applicability of this integrated optimal control tool are demonstrated thoroughly by implementing flood diversion controls in rivers, from one reach with a single or multiple floodgates (with or without constraints), to a channel network with multiple floodgates. This new optimal flow control model can be generally applied to make optimal decisions in real-time flood control and water resource management in a watershed."		"Ding, Y|Wang, SSY"	JOURNAL OF HYDRAULIC ENGINEERING-ASCE	open channel flow sensitivity analysis floods geometry optimization	10.1061/(ASCE)0733-9429(2006)132:11(1215)
113	WOS:000331916100026	2014	Life cycle assessment of corn-based ethanol production in Argentina	TILLAGE SYSTEMS BIOENERGY PRODUCTION UNITED-STATES LAND-USE EMISSIONS BIOFUELS IMPACTS PAMPAS ENERGY DYNAMICS	"The promotion of biofuels as energy for transportation in the world is mainly driven by the perspective of oil depletion, the concerns about energy security and global warming. In Argentina, the legislation has imposed the use of biofuels in blend with fossil fuels ( to %) in the transport sector. The aim of this paper is to assess the environmental impact of corn-based ethanol production in the province of Santa Fe in Argentina based on the life cycle assessment methodology. The studied system includes from raw materials production to anhydrous ethanol production using dry milling technology. The system is divided into two subsystems: agricultural system and refinery system. The treatment of stillage is considered as well as the use of co-products (distiller's dried grains with solubles), but the use and/or application of the produced biofuel is not analyzed: a cradle-to-gate analysis is presented. As functional unit,  MJ of anhydrous ethanol at biorefinery is chosen. Two life cycle impact assessment methods are selected to perform the study: Eco-indicator  and ReCiPe. SimaPro is the life cycle assessment software used. The influence of the perspectives on the model is analyzed by sensitivity analysis for both methods. The two selected methods identify the same relevant processes. The use of fertilizers and resources, seeds production, harvesting process, corn drying, and phosphorus fertilizers and acetamide-anillide-compounds production are the most relevant processes in agricultural system. For refinery system, corn production, supplied heat and burned natural gas result in the higher contributions. The use of distiller's dried grains with solubles has an important positive environmental impact."	 (C) 2013 Elsevier B.V. All rights reserved.	"Pieragostini, C|Aguirre, P|Mussati, MC"	SCIENCE OF THE TOTAL ENVIRONMENT	life cycle assessment corn-based ethanol eco-indicator 99 recipe sensitivity analysis perspectives analysis	10.1016/j.scitotenv.2013.11.012
115	WOS:000412192800042	2017	Dynamic optimization of beer fermentation: Sensitivity analysis of attainable performance vs. product flavour constraints	VERTICAL ELECTRICAL FURNACE PERLITE GRAIN EXPANSION BATCH DISTILLATION OPTIMAL OPERATION FORMULATION SIMULATION MODEL	"The declining alcohol industry in the UK and the concurrent surge in supply and variety of beer products has created extremely competitive environment for breweries, many of which are pursuing the benefits of process intensification and optimization. To gain insight into the brewing process, an investigation into the influence of by-product threshold levels on obtainable fermentation performance has been performed, by computing optimal operating temperature profiles for a range of constraint levels on by-product concentrations in the final product. The DynOpt software package has been used, converting the continuous control vector optimization problem into nonlinear programming (NLP) form via collocation on finite elements, which has then been solved with an interior point algorithm. This has been performed for increasing levels of time discretization, by means of a range of initializing solution profiles, for a wide spectrum of imposed by-product flavour constraints. Each by-product flavour threshold affects process performance in a unique way. Results indicate that the maximum allowable diacetyl concentration in the final product has very strong influence on batch duration, with lower limits requiring considerably longer batches. The maximum allowable ethyl acetate concentration is shown to dictate the attainable ethanol concentration, and lower limits adversely affect the desired high alcohol content in the final product. (C) "	 Elsevier Ltd. All rights reserved.	"Rodman, AD|Gerogiorgis, DI"	COMPUTERS & CHEMICAL ENGINEERING	beer fermentation dynamic optimization multi-objective optimization orthogonal collocation on finite elements sensitivity analysis flavour constraints	10.1016/j.compchemeng.2017.06.024
116	WOS:000266820000004	2009	Finite element response sensitivity analysis of multi-yield-surface J(2) plasticity model by direct differentiation method	ELASTOPLASTIC SEISMIC RESPONSE FOUNDATION-GROUND SYSTEM STRESS-STRAIN BEHAVIOR 3-D EARTH DAMS DYNAMIC-RESPONSE SOFTWARE RELIABILITY	"Finite element (FE) response sensitivity analysis is an essential tool for gradient-based optimization methods used in various sub-fields of civil engineering such as structural optimization, reliability analysis, system identification, and finite element model updating. Furthermore, stand-alone sensitivity analysis is invaluable for gaining insight into the effects and relative importance of various system and loading parameters on system response. The direct differentiation method (DDM) is a general, accurate and efficient method to compute FE response sensitivities to FE model parameters. In this paper, the DDM-based response sensitivity analysis methodology is applied to a pressure independent multi-yield-surface J() plasticity material model, which has been used extensively to simulate the nonlinear undrained shear behavior of cohesive soils subjected to static and dynamic loading conditions. The complete derivation of the DDM-based response sensitivity algorithm is presented. This algorithm is implemented in a general-purpose nonlinear finite element analysis program. The work presented in this paper extends significantly the framework of DDM-based response sensitivity analysis, since it enables numerous applications involving the use of the multi-yield-surface J() plasticity material model. The new algorithm and its software implementation are validated through two application examples, in which DDM-based response sensitivities are compared with their counterparts obtained using forward finite difference (FFD) analysis. The normalized response sensitivity analysis results are then used to measure the relative importance of the soil constitutive parameters on the system response."	 (C) 2009 Elsevier B.V. All rights reserved.	"Quan, G|Conte, JP|Elgamal, A|Yang, ZH"	COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING	nonlinear finite element analysis response sensitivity analysis multi-yield-surface plasticity model direct differentiation method soil material model	10.1016/j.cma.2009.02.030
119	WOS:000366074400041	2016	Soil solution concentrations and chemical species of copper and zinc in a soil with a history of pig slurry application and plant cultivation	DISSOLVED ORGANIC-MATTER SANDY TYPIC HAPLUDALF CONTAMINATED SOILS HEAVY-METALS VINEYARD SOILS DEEP-LITTER ZN CU COMPLEXATION PHOSPHORUS	"Successive pig slurry applications may increase soil copper(Cu) and zinc (Zn) concentrations and change the proportions of free chemical species in solution when combined with plant cultivation. The aim of this study was to assess the soluble, available,,and total Cu and Zn concentrations and the distribution of their chemical species in the solution in a Hapludalf soil with a history of pig slurry application and plant cultivation. The study was conducted in undisturbed soil columns that originated from an -year-long experiment conducted at the experimental unit of the Federal University of Santa Maria in Santa Maria, southern Brazil. The soil was a Typic Hapludalf soil fertilized with pig slurry at rates of , , , and  m() ha(-). The soil was collected from depth intervals of -., .-., .-., .-., -., and .-. m before and after cultivation with black oat and maize in a greenhouse to assess the total and available Cu and Zn concentrations and to extract the solution. The soil solution concentrations of the main cations, anions, and dissolved organic carbon (DOC) and pH were assessed. The distribution of Cu and Zn chemical species was assessed using the Visual Minteq software. The history of  pig slurry applications increased the concentration of Cu and Zn in surface soil intervals, but the concentration of Cu also increased in the soil solution at depth. The phytotoxicity caused by Cu and Zn may not occur even after several years of pig slurry application because the plants provide soil conditions in which chemical species complexed with dissolved organic carbon predominate and Cu and Zn in free forms are present only in small amounts."	 (C) 2015 Elsevier B.V. All rights reserved.	"De Conti, L|Ceretta, CA|Ferreira, PAA|Lourenzi, CR|Girotto, E|Lorensini, F|Tiecher, TL|Marchezan, C|Anchieta, MG|Brunetto, G"	AGRICULTURE ECOSYSTEMS & ENVIRONMENT	poaceae pig slurry speciation modeling dissolved organic carbon	10.1016/j.agee.2015.09.040
124	WOS:000331341400011	2014	Adaptive stochastic Galerkin FEM	GENERALIZED POLYNOMIAL CHAOS FINITE-ELEMENT-METHOD ELLIPTIC SPDES CONVERGENCE PDES	"A framework for residual-based a posteriori error estimation and adaptive mesh refinement and polynomial chaos expansion for general second order linear elliptic PDEs with random coefficients is presented. A parametric, deterministic elliptic boundary value problem on an infinite-dimensional parameter space is discretized by means of a Galerkin projection onto finite generalized polynomial chaos (gpc) expansions, and by discretizing each gpc coefficient by a FEM in the physical domain. An anisotropic residual-based a posteriori error estimator is developed. It contains bounds for both contributions to the overall error: the error due to gpc discretization and the error due to Finite Element discretization of the gpc coefficients in the expansion. The reliability of the residual estimator is established. Based on the explicit form of the residual estimator, an adaptive refinement strategy is presented which allows to steer the polynomial degree adaptation and the dimension adaptation in the stochastic Galerkin discretization, and, embedded in the gpc adaptation loop, also the Finite Element mesh refinement of the gpc coefficients in the physical domain. Asynchronous mesh adaptation for different gpc coefficients is permitted, subject to a minimal compatibility requirement on meshes used for different gpc coefficients. Details on the implementation with the open-source software framework ALEA are presented; it is generic, and is based on available stiffness and mass matrices of a FEM for the deterministic, nonparametric nominal problem evaluated in the FEniCS environment. Preconditioning of the resulting matrix equation and iterative solution are discussed. Numerical experiments in two spatial dimensions for membrane and plane stress boundary value problems on polygons are presented. They indicate substantial savings in total computational complexity due to FE mesh coarsening in high gpc coefficients."	 (C) 2013 Elsevier B.V. All rights reserved.	"Eigel, M|Gittelson, CJ|Schwab, C|Zander, E"	COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING	uncertainty quantification stochastic finite element methods operator equations alea fenics adaptive methods	10.1016/j.cma.2013.11.015
125	WOS:000287437100013	2011	"NCNA: Integrated platform for constructing, visualizing, analyzing and sharing human-mediated nitrogen biogeochemical networks"		"Human alterations to the nitrogen (N) cycle are closely associated with global environmental and climate change. New tools are necessary to model and analyze the highly complex N cycles emerging from human-mediated ecosystems. We developed a new software. NCNA, to provide three functions: a) rigorous reconstruction of quasi-empirical models (QEMs), b) computer-aided interface for data collection and automatic sensitivity analysis, and c) automatic generation, visualization and network environ analysis (NEA) of N cycling networks. (c) "	 Elsevier Ltd. All rights reserved.	"Min, Y|Gong, W|Jin, XG|Chang, J|Gu, BJ|Han, Z|Ge, Y"	ENVIRONMENTAL MODELLING & SOFTWARE	quasi-empirical model reconstruction visualization network environ analysis urbanization	10.1016/j.envsoft.2010.11.002
126	WOS:000278842000024	2010	"Reducing lost-sales rate in (T,R,L) inventory model with controllable lead time"	BACKORDERS MIXTURE REDUCTION DISCOUNT	"In order to establish a good image and to enhance customer's loyalty, many efforts such as upgrading the servicing facilities, maintaining a high quality of products and increasing expenditure on advertisement could be made by a selling shop. Naturally, an extra-added cost must be spent for these efforts and it is expected to have a result to reduce the shortage cost of lost-sales and the total expected annual cost. This paper explores a probabilistic inventory model with optimal lost-sales caused by investment due to two different types of cost functions. We consider that the lead time can be shortened at an extra crashing cost, which depends on the length of the lead time. Moreover, we assume that the lost-sales rate can also be reduced by capital investment. The purpose of this paper is to establish a (T,R,L) inventory model with controllable lead time and to analyze the effects of increasing two different types of investments to reduce the lost-sales rate, in which the review period, lead time and lost-sales rate are treated as decision variables. We first formulate the basic periodic review model mathematically with the capital investment to reduce lost-sales rate. Then two models are discussed, one with normally distributed protection interval demand and another with distribution-free case. For each model, two investment cost functional forms, logarithmic and power, are employed for lost-sales rate reduction. Two computational algorithms with the help of the software Matlab are furnished to determine the optimal solution. In addition, six numerical examples and sensitivity analysis are presented to illustrate the theoretical results and obtain some managerial insights. Finally, the effect of lost-sales rate reduction is investigated. By framing this new model, we observe that a significant amount of savings can be easily achieved to increase the competitive edge in business. The results in the numerical examples indicate that the savings of expected annual total cost are realized through lost-sales reduction."	 (C) 2010 Elsevier Inc. All rights reserved.	"Annadurai, K|Uthayakumar, R"	APPLIED MATHEMATICAL MODELLING	inventory lost sales rate variable lead time minimax distribution-free procedure optimization	10.1016/j.apm.2010.02.035
127	WOS:000238960100008	2006	Variance-based sensitivity analysis of the probability of hydrologically induced slope instability	STABILITY MODEL UNSATURATED SOILS UNCERTAINTY DESIGN	"Analysis of the sensitivity of predictions of slope instability to input data and model uncertainties provides a rationale for targeted site investigation and iterative refinement of geotechnical models. However, sensitivity methods based on local derivatives do not reflect model behaviour over the whole range of input variables. whereas methods based on standardised regression or correlation coefficients cannot detect non-linear and non-monotonic relationships between model input and output. Variance-based sensitivity analysis (VBSA) provides a global, model-independent sensitivity measure. The approach is demonstrated using the Combined Hydrology and Stability Model (CHASM) and is applicable to a wide variety of computer models. The method of Sobol', assuming independence between input variables, was used to identify interactions between model input variables, whilst replicated Latin Hypercube Sampling (LHS) is used to investigate the effects of statistical dependence between the input variables. The SIMLAB software was used, both to generate the input sample and to calculate the sensitivity indices. The analysis provided quantified evidence of well-known sensitivities as well demonstrating how uncertainty in slope failure during rainfall is, for the examples tested here. more attributable to uncertainty in the soil strength than to uncertainty in the rainfall. (c) "	 Elsevier Ltd. All rights reserved.	"Hamm, NAS|Hall, JW|Anderson, MG"	COMPUTERS & GEOSCIENCES	site investigation slope stability analysis statistical analysis sensitivity analysis uncertainty analysis	10.1016/j.cageo.2005.10.007
128	WOS:000259363300011	2008	Methods for assessing uncertainty in fundamental assumptions and associated models for cancer risk assessment	EXPERT JUDGMENT PROBABILITY-DISTRIBUTIONS COMPREHENSIVE REALISM INHALED FORMALDEHYDE PHARMACOKINETIC DATA CLIMATE-CHANGE LUNG-CANCER F344 RAT INFORMATION ELICITATION	"The distributional approach for uncertainty analysis in cancer risk assessment is reviewed and extended. The method considers a combination of bioassay study results, targeted experiments, and expert judgment regarding biological mechanisms to predict a probability distribution for uncertain cancer risks. Probabilities are assigned to alternative model components, including the determination of human carcinogenicity, mode of action, the dosimetry measure for exposure, the mathematical form of the dose-response relationship, the experimental data set(s) used to fit the relationship, and the formula used for interspecies extrapolation. Alternative software platforms for implementing the method are considered, including Bayesian belief networks (BBNs) that facilitate assignment of prior probabilities, specification of relationships among model components, and identification of all output nodes on the probability tree. The method is demonstrated using the application of Evans, Sielken, and co-workers for predicting cancer risk from formaldehyde inhalation exposure. Uncertainty distributions are derived for maximum likelihood estimate (MLE) and th percentile upper confidence limit (UCL) unit cancer risk estimates, and the effects of resolving selected model uncertainties on these distributions are demonstrated, considering both perfect and partial information for these model components. A method for synthesizing the results of multiple mechanistic studies is introduced, considering the assessed sensitivities and selectivities of the studies for their targeted effects. A highly simplified example is presented illustrating assessment of genotoxicity based on studies of DNA damage response caused by naphthalene and its metabolites. The approach can provide a formal mechanism for synthesizing multiple sources of information using a transparent and replicable weight-of-evidence procedure."		"Small, MJ"	RISK ANALYSIS	bayesian belief network cancer risk assessment distributional method expert judgment genotoxicity mode of action uncertainty analysis weight of evidence	10.1111/j.1539-6924.2008.01134.x
130	WOS:000305299400013	2012	Adjoint sensitivity in PDE constrained least squares problems as a multiphysics problem		"Purpose - The purpose of this paper is to provide a framework for the implementation of an adjoint sensitivity formulation for least-squares partial differential equations constrained optimization problems exploiting a multiphysics finite elements package. The estimation of the diffusion coefficient in a Poisson-type diffusion equation is used as an example. Design/methodology/approach - The authors derive the adjoint formulation in a continuous setting allowing to attribute to the direct and adjoint states the role of different fields to be solved for. They are one-way coupled through the mismatch between measured and direct states acting as a source term in the adjoint equation. Having solved for the direct and adjoint state, the sensitivity of the cost function with respect to the design variables can then be obtained by a suitable post-processing procedure. This sensitivity can then be used to efficiently solve the least-squares problem. Findings - The authors derived the adjoint formulation in a continuous setting allowing the direct and adjoint states to be attributed the role of different fields to be solved. They are one-way coupled through the mismatch between measured and direct states acting as a source term in the adjoint equation. It is found that, having solved for the direct and adjoint state, the sensitivity of the cost function with respect to the design variables can then be obtained by a suitable post-processing procedure. Research limitations/implications - This paper implies that modern multiphysics finite elements packages provide a flexible and extendable software environment for the experimentation with different adjoint formulations. Such tools are therefore expected to become increasingly important in solving notoriously difficult partial differential equation (PDE)-constrained least-squares problems. The framework also provides the possibility of experimentation with different regularization techniques (total variation and multiscale techniques for instance) to handle the ill-posedness of the problem. Originality/value - In this paper the adjoint sensitivity computation is casted as a multiphysics problem allowing for a flexible and extendable implementation."		"Lahaye, D|Mulckhuyse, W"	COMPEL-THE INTERNATIONAL JOURNAL FOR COMPUTATION AND MATHEMATICS IN ELECTRICAL AND ELECTRONIC ENGINEERING	sensitivity analysis differential equations computation computer software adjoint sensitivity non-linear least squares problems multiphysics finite elements software	10.1108/03321641211209780
132	WOS:000295716700008	2011	Uncertainty propagation or box propagation	INITIAL-VALUE PROBLEMS VALIDATED SOLUTIONS TAYLOR ODES	This paper discusses the use of recently developed techniques and software in the numerical propagation of uncertainties in initial coordinates and/or parameters for initial value problems. We present an approach based on several validated numerical integration techniques but focusing on the propagation of boxes. The procedure uses a multivariable high order Taylor series development of the solution of the system whose Taylor coefficients are calculated via extended automatic differentiation rules for all the basic operations. These techniques are implemented in the recent free-software TIDES. The classical two-body and Lorenz problems are chosen as examples to show the benefits of the approach. The results show that the solution of uncertainties can be approximated in an analytic form by means of a Taylor series and that these techniques can be extremely useful in different practical applications. (C) 	 Elsevier Ltd. All rights reserved.	"Barrio, R|Rodriguez, M|Abad, A|Serrano, S"	MATHEMATICAL AND COMPUTER MODELLING	taylor series method automatic differentiation uncertainty propagation freeware software	10.1016/j.mcm.2011.06.036
135	WOS:000385907200039	2016	MINFIT: A Spreadsheet-Based Tool for Parameter Estimation in an Equilibrium Speciation Software Program	SURFACE COMPLEXATION MODELS URANIUM(VI) ADSORPTION CONSISTENT MODEL TITRATION DATA LAYER MODEL SORPTION OXIDE MONTMORILLONITE PHOSPHATE HEMATITE	"Determination of equilibrium constants describing chemical reactions in the aqueous phase and at solid-water interface relies on inverse modeling and parameter estimation. Although there are existing tools available, the steep learning curve prevents the wider community of environmental engineers and chemists to adopt those tools. Stemming from classical chemical equilibrium codes, MINEQL+ has been one of the most widely used chemical equilibrium software programs. We developed a spreadsheet-based tool, which we are calling MINFIT, that interacts with MINEQL+ to perform parameter estimations that optimize model fits to experimental data sets. MINFIT enables automatic and convenient screening of a large number of parameter sets toward the optimal solutions by calling MINEQL+ to perform iterative forward calculations following either exhaustive equidistant grid search or randomized search algorithms. The combined use of the two algorithms can securely guide the searches for the global optima. We developed interactive interfaces so that the optimization processes are transparent. Benchmark examples including both aqueous and surface complexation problems illustrate the parameter estimation and associated sensitivity analysis. MINFIT is accessible at http://minfit.strikingly.com."		"Xie, XF|Giammar, DE|Wang, ZM"	ENVIRONMENTAL SCIENCE & TECHNOLOGY		10.1021/acs.est.6b03399
136	WOS:000265171300023	2009	Artificial neural networks to predict daylight illuminance in office buildings	CONTROL-SYSTEMS COST ESTIMATION PERFORMANCE COEFFICIENT SAVINGS DESIGN MODELS FUZZY	"A prediction model was developed to determine daylight illuminance for the office buildings by using artificial neural networks (ANNs). Illuminance data were collected for  months by applying a field measuring method. Utilizing weather data from the local weather station and building parameters from the architectural drawings, a three-layer ANN model of feed-forward type (with one output node) was constructed. Two variables for time (date, hour),  weather determinants (outdoor temperature, solar radiation, humidity, UV index and UV dose) and  building parameters (distance to windows. number of windows, orientation of rooms, floor identification, room dimensions and point identification) were considered as input variables. Illuminance was used as the output variable. In ANN modeling, the data were divided into two groups; the first  of these data sets were used for training and the remaining  for testing. Microsoft Excel Solver used simplex optimization method for the optimal weights. The model's performance was then measured by using the illuminance percentage error. As the prediction power of the model was almost %, predicted data had close matches with the measured data. The prediction results were successful within the sample measurements. The model was then subjected to sensitivity analysis to determine the relationship between the input and output variables. NeuroSolutions Software by NeuroDimensions Inc., was adopted for this application. Researchers and designers will benefit from this model in daylighting performance assessment of buildings by making predictions and comparisons and in the daylighting design process by determining illuminance. (C) "	 Elsevier Ltd. All rights reserved.	"Kazanasmaz, T|Gunaydin, M|Binol, S"	BUILDING AND ENVIRONMENT	modeling building daylighting artificial neural networks	10.1016/j.buildenv.2008.11.012
138	WOS:000291785400002	2011	Concurrent Decisions on Design Concept and Material Using Analytical Hierarchy Process at the Conceptual Design Stage	MATERIALS SELECTION SYSTEM	"There is an increased study for considering the precise decisions on the design concept (DC) and material concurrently at the early stage of development of product. Inappropriate decisions on DC and material always lead to huge cost involvement and ultimately drive toward premature component or product failure. To overcome this problem, concurrent engineering (CE) is an approach which allows designers to consider early decision making (EDM) need to be implemented. To illustrate the use of CE principle at the early stage of design process, a concept selection framework called concurrent DC selection and materials selection (CDCSMS) was proposed. In order to demonstrate the proposed CDCSMS framework, eight DC s and six different types of composite materials of automotive bumper beam have been considered. Both of these decisions were then verified by performing various scenarios of sensitivity analysis by using analytical hierarchy process through utilizing Expert Choice software."		"Hambali, A|Sapuan, SM|Rahim, AS|Ismail, N|Nukman, Y"	CONCURRENT ENGINEERING-RESEARCH AND APPLICATIONS	concurrent engineering analytical hierarchy process early decision making	10.1177/1063293X11408138
139	WOS:000326685400009	2013	Uncertainty analysis in urban drainage modelling: should we break our back for normally distributed residuals?	PARAMETER-ESTIMATION CALIBRATION	"This study presents results on the assessment of the application of a Bayesian approach to evaluate the sensitivity and uncertainty associated with urban rainfall-runoff models. The software MICA was adopted, in which the prior information about the parameters is updated to generate the parameter posterior distribution. The likelihood function adopted in MICA assumes that the residuals between the measured and modelled values have a normal distribution. This is a trait of many uncertainty/sensitivity procedures. This study compares the results from three different scenarios: (i) when normality of the residuals was checked but if they were not normal then nothing was done (unverified); (ii) normality assumption was checked, verified (using data transformations) and a weighting strategy was used that gives more importance to high flows; and (iii) normality assumption was checked and verified, but no weights were applied. The modelling implications of such scenarios were analysed in terms of model efficiency, sensitivity and uncertainty assessment. The overall results indicated that verifying the normality assumption required the models to fit a wider portion of the hydrograph, allowing a more detailed inspection of parameters and processes simulated in both models. Such an outcome provided important information about the advantages and limitations of the models' structure."		"Dotto, CBS|Deletic, A|McCarthy, DT"	WATER SCIENCE AND TECHNOLOGY	bayesian approach normality assumption uncertainty analysis urban drainage models	10.2166/wst.2013.360
140	WOS:000344676000001	2014	Kinetic Study of Nonequilibrium Plasma-Assisted Methane Steam Reforming	DIELECTRIC-BARRIER DISCHARGE CONVERSION TRANSPORT MECHANISM REACTOR	"To develop a detailed reaction mechanism for plasma-assisted methane steam reforming, a comprehensive numerical and experimental study of effect laws on methane conversion and products yield is performed at different steam to methane molar ratio (S/C), residence time s, and reaction temperatures. A CHEMKIN-PRO software with sensitivity analysis module and path flux analysis module was used for simulations. A set of comparisons show that the developed reaction mechanism can accurately predict methane conversion and the trend of products yield in different operating conditions. Using the developed reaction mechanism in plasma-assisted kinetic model, the reaction path flux analysis was carried out. The result shows that CH recombination is the limiting reaction for CO production and O is the critical species for CO production. Adding wt.% Ni/SiO in discharge region has significantly promoted the yield of H-, CO, or CO in dielectric packed bed (DPB) reactor. Plasma catalytic hybrid reforming experiment verifies the reaction path flux analysis tentatively."		"Zheng, HT|Liu, Q"	MATHEMATICAL PROBLEMS IN ENGINEERING		10.1155/2014/938618
142	WOS:000364452300007	2015	Uncertainties propagations in 1D hydraulic modeling		"Numerical modeling tools, like Crue the D modeling software developed by CNR, are widely used to analyze hydraulic and hydrological behavior of rivers. Those tools are based on input parameters, with physical or numerical meaning; theses inputs are generally known with some uncertainties. The tool Promethee, developed by IRSN, is able to realize uncertainties propagations, and two kinds of sensibility analysis: the first one, a determinist method (Morris) based on screening, is able to identify factors which influenced outputs variability; the second one, a probabilistic method (FAST) based on variance analysis of outputs regarding inputs variances, performs inputs ranking in function of outputs sensibilities. Uncertainties propagations studies require an important computational capacity; to do so; the Promethee/Crue coupling is used. The coupled tool is able to parameter Crue files for the hydraulic computations, to run lots of computation, and then to analyze results with statistic tools. This coupled tool gives the possibility to realize sensitivity studies by probabilistic method, to parameter realistic and complex model rivers, and to study the influence of several inputs variations."		"Nguyen, TM|Richet, Y|Balayn, P|Bardet, L"	HOUILLE BLANCHE-REVUE INTERNATIONALE DE L EAU	promethee crue9 sensitivity analysis fast morris	10.1051/lhb/20150055
143	WOS:000250352400010	2007	Parallel computing techniques for sensitivity analysis in optimum structural design	FINITE-ELEMENT-ANALYSIS OPTIMIZATION SYSTEMS ENVIRONMENT LOADS	"Among different activities of the optimum structural design using the gradient-based optimization approaches, design sensitivity analysis is the most time-consuming computational process. By introducing parallel computing techniques for sensitivity computation, significant speedup has been obtained in optimum structural design. Computation of design sensitivities is characteristically uncoupled, thus opening the door to parallelization. In this paper, two types of approaches viz. single-level and multilevel parallelisms are pursued for design sensitivities. The design sensitivities are computed using analytical and finite-difference methods. Numerical studies show that the performance of the parallel algorithms for design sensitivities on message passing systems is very good. Good speedups have been achieved in parallel multilevel sensitivity calculation. The parallel algorithms for design sensitivity analysis have been implemented on message passing parallel systems within the software platform of Parallel Computer Adaptive Language."		"Umesha, PK|Venuraju, MT|Hartmann, D|Leimbach, KR"	JOURNAL OF COMPUTING IN CIVIL ENGINEERING		10.1016/(ASCE)0887-3801(2007)21:6(463)
144	WOS:000348756700001	2015	Pi 4U: A high performance computing framework for Bayesian uncertainty quantification of complex models	LIQUID WATER EVOLUTIONARY STRATEGIES PROBABILISTIC APPROACH MARGINAL LIKELIHOOD DYNAMICAL-SYSTEMS INVERSE PROBLEMS UPDATING MODELS SIMULATION OPTIMIZATION RELIABILITY	"We present Pi U,() an extensible framework, for non-intrusive Bayesian Uncertainty Quantification and Propagation (UQ+P) of complex and computationally demanding physical models, that can exploit massively parallel computer architectures. The framework incorporates Laplace asymptotic approximations as well as stochastic algorithms, along with distributed numerical differentiation and task-based parallelism for heterogeneous clusters. Sampling is based on the Transitional Markov Chain Monte Carlo (TMCMC) algorithm and its variants. The optimization tasks associated with the asymptotic approximations are treated via the Covariance Matrix Adaptation Evolution Strategy (CMA-ES). A modified subset simulation method is used for posterior reliability measurements of rare events. The framework accommodates scheduling of multiple physical model evaluations based on an adaptive load balancing library and shows excellent scalability. In addition to the software framework, we also provide guidelines as to the applicability and efficiency of Bayesian tools when applied to computationally demanding physical models. Theoretical and computational developments are demonstrated with applications drawn from molecular dynamics, structural dynamics and granular flow."	 (C) 2014 Elsevier Inc. All rights reserved.	"Hadjidoukas, PE|Angelikopoulos, P|Papadimitriou, C|Koumoutsakos, P"	JOURNAL OF COMPUTATIONAL PHYSICS	uncertainty quantification parallel computing distributed computing bayesian inference reliability	10.1016/j.jcp.2014.12.006
147	WOS:000265341800006	2009	Estimating storm discharge and water quality data uncertainty: A software tool for monitoring and modeling applications	AGRICULTURAL WATERSHEDS SAMPLE PRESERVATION REACTIVE PHOSPHORUS INORGANIC-PHOSPHATE RISK-ASSESSMENT RIVER WATER NITROGEN STRATEGIES SEDIMENT SORPTION	"Uncertainty estimates corresponding to measured hydrologic and water quality data can contribute to improved monitoring design, decision-making, model application, and regulatory formulation. With these benefits in mind, the Data Uncertainty Estimation Tool for Hydrology and Water Quality (DUET-H/WQ) was developed from an existing uncertainty estimation framework for small watershed discharge, sediment, and N and P data. Both the software and its framework-basis utilize the root mean square error propagation methodology to provide uncertainty estimates instead of more rigorous approaches requiring detailed statistical information, which is rarely available. DUET-H/WQ lists published uncertainty information for data collection procedures to assist the user in assigning appropriate data-specific uncertainty estimates and then calculates the uncertainty for individual discharge, concentration, and load values. Results of DUET-H/WQ application in several studies indicated that substantial uncertainty can be contributed by each procedural category (discharge measurement, sample collection, sample preservation/storage, laboratory analysis, and data processing and management). For storm loads, the uncertainty was typically least for discharge (+/- -%), greater for sediment (+/- -%) and dissolved N and P (+/- -%) loads, and greater yet for total N and P (+/- -%). When these uncertainty estimates for individual values were aggregated within study periods (i.e. total discharge, average concentration, and total load), uncertainties followed the same pattern (Q < TSS < dissolved N and P < total N and P). This rigorous demonstration of uncertainty in discharge and water quality data illustrates the importance of uncertainty analysis and the need for appropriate tools. It is our hope that DUET-H/WQ contributes to making uncertainty estimation a routine data collection and reporting procedure and thus enhances environmental monitoring, modeling, and decision-making. Hydrologic and water quality data are too important for scientists to continue to ignore the inherent uncertainty."	 Published by Elsevier Ltd.	"Harmel, RD|Smith, DR|King, KW|Slade, RM"	ENVIRONMENTAL MODELLING & SOFTWARE	error propagation data collection hydrology nutrients watershed models	10.1016/j.envsoft.2008.12.006
148	WOS:000321313800102	2013	Techno-economic analysis of two bio-oil upgrading pathways	BIOMASS FAST PYROLYSIS HYDROGEN-PRODUCTION GREEN GASOLINE MODEL FUELS	"We evaluate the economic feasibility for two bio-oil upgrading pathways: two-stage hydrotreating followed by fluid catalytic cracking (FCC) or single-stage hydrotreating followed by hydrocracking. In the hydrotreating/FCC pathway, two options are available as the hydrogen source for hydrotreating: merchant hydrogen or hydrogen from natural gas reforming. The primary products of the hydrotreating/FCC pathway are commodity chemicals whereas the primary products for the hydrotreating/hydrocracking pathway are transportation fuels and hydrogen. The two pathways are modeled using Aspen Plus (R) for a  metric tons/day facility. Equipment sizing and cost calculations are based on Aspen Economic Evaluation (R) software. The bio-oil yield via fast pyrolysis is assumed to be % of biomass. We calculate the internal rate of return (IRR) for each pathway as a function of feedstock cost, fixed capital investment (FCI), hydrogen and catalyst costs, and facility revenues. The results show that a facility employing the hydrotreating/ FCC pathway with hydrogen production via natural gas reforming option generates the highest IRR of .%. Sensitivity analysis demonstrates that product yield, FCI, and biomass cost have the greatest impacts on facility IRR. Monte-Carlo analysis shows that two-stage hydrotreating and FCC of the aqueous phase bio-oil with hydrogen produced via natural gas reforming has a relatively low risk for project investment."	 (c) 2013 Published by Elsevier B.V.	"Zhang, YN|Brown, TR|Hu, GP|Brown, RC"	CHEMICAL ENGINEERING JOURNAL	fast pyrolysis bio-oil upgrading commodity chemicals transportation fuels hydrogen	10.1016/j.cej.2013.01.030
149	WOS:000411869000009	2017	A Scenario Based Impact Assessment of Trace Metals on Ecosystem of River Ganges Using Multivariate Analysis Coupled with Fuzzy Decision-Making Approach	WATER-QUALITY MANAGEMENT HEAVY-METALS INDIA BASIN FISH	"The growing consciousness about the health risks associated with environmental pollutants has brought a major shift in global concern towards prevention of hazardous/trace metals discharge in water bodies. Majority of these trace metals gets accumulated in the body of aquatic lives, which are considered as potential indicators of hazardous content. This results in an ecological imbalance in the form of poisoning, diseases and even death of fish and other aquatic lives, and ultimately affect humans through food chain. Trace metals such as Cd, Cr, Cu, Mn, Ni, Pb and Zn originated from various industrial operations containing metallic solutions and agricultural practices, have been contributing significantly to cause aquatic pollution. The present study develops a novel approach of expressing sustainability of river's ecosystem based on health of the fish by coupling fuzzy sensitivity analysis into multivariate analysis. A systematic methodology has been developed by generating monoplot, two dimensional biplot and rotated component matrix (using 'Analyze it' and 'SPSS' software), which can simultaneously identify critical trace metals and their industrial sources, critical sampling stations, and adversely affected fish species along with their interrelationships. A case study of assessing the impact of trace metals on the aquatic life of river Ganges, India has also been presented to demonstrate effectiveness of the model. The clusters pertaining to various water quality parameters have been identified using Principal Component Analysis (PCA) to determine actual sources of pollutants and their impact on aquatic life. The fuzzy sensitivity analysis reveals the cause-effect relationship of these critical parameters. The study suggests pollution control agencies to enforce appropriate regulations on the wastewater dischargers responsible for polluting river streams with a particular kind of trace metal(s)."		"Srinivas, R|Singh, AP|Sharma, R"	WATER RESOURCES MANAGEMENT	aquatic ecosystem fuzzy decision-making impact assessment multivariate analysis river ecosystem water quality	10.1007/s11269-017-1738-y
150	WOS:000295845900012	2011	Groundwater drawdown at Nankou site of Beijing Plain: model development and calibration	FLOW	"Water shortage and groundwater pollution have become two primary environmental concerns to Beijing since the s. The local aquifers, as the dominant sources for domestic and agricultural water supply, are depleting due to groundwater abstraction and continuous drought in recent years with rapid urbanization and increasing water consumption. Therefore, understanding the hydrogeological system is fundamental for a sustainable water resources management. In this article, the numerical analysis of a -D regional groundwater flow model for the Nankou area is presented. The hydrogeological system is reproduced according to sparsely distributed boreholes data. The numerical analysis is carried out using the scientific software OpenGeoSys, which is based on the finite element method. The model calibration and sensitivity analysis are accomplished with inverse methods by applying a model independent parameter estimation system (PEST). The results of the calibrated model show reasonable agreements with observed water levels. The transient groundwater flow simulations reflect the observed drawdown of the last  years and show the formation of a depression cone in an intensively pumped area."		"Sun, F|Shao, HB|Kalbacher, T|Wang, WQ|Yang, ZS|Huang, ZF|Kolditz, O"	ENVIRONMENTAL EARTH SCIENCES	groundwater modeling opengeosys pest nankou	10.1007/s12665-011-0957-4
152	WOS:000340977000075	2014	Groundwater fluxes in a shallow seasonal wetland pond: The effect of bathymetric uncertainty on predicted water and solute balances	MASS-BALANCE DEPENDENT ECOSYSTEMS LAKES MODEL DISCHARGE STORAGE RN-222 VOLUME CALIBRATION AUSTRALIA	"The successful management of groundwater dependent shallow seasonal wetlands requires a sound understanding of groundwater fluxes. However, such fluxes are hard to quantify. Water volume and solute mass balance models can be used in order to derive an estimate of groundwater fluxes within such systems. This approach is particularly attractive, as it can be undertaken using measurable environmental variables, such as; rainfall, evaporation, pond level and salinity. Groundwater fluxes estimated from such an approach are subject to uncertainty in the measured variables as well as in the process representation and in parameters within the model. However, the shallow nature of seasonal wetland ponds means water volume and surface area can change rapidly and non-linearly with depth, requiring an accurate representation of the wetland pond bathymetry. Unfortunately, detailed bathymetry is rarely available and simplifying assumptions regarding the bathymetry have to be made. However, the implications of these assumptions are typically not quantified. We systematically quantify the uncertainty implications for eight different representations of wetland bathymetry for a shallow seasonal wetland pond in South Australia. The predictive uncertainty estimation methods provided in the Model-Independent Parameter Estimation and Uncertainty Analysis software (PEST) are used to quantify the effect of bathymetric uncertainty on the modelled fluxes. We demonstrate that bathymetry can be successfully represented within the model in a simple parametric form using a cubic Sexier curve, allowing an assessment of bathymetric uncertainty due to measurement error and survey detail on the derived groundwater fluxes compared with the fixed bathymetry models. Findings show that different bathymetry conceptualisations can result in very different mass balance components and hence process conceptualisations, despite equally good fits to observed data, potentially leading to poor management decisions for the wetlands. Model predictive uncertainty increases with the crudity of the bathymetry representation, however, approximations that capture the general shape of the wetland pond such as a power law or Bezier curve show only a small increase in prediction uncertainty compared to the full dGPS surveyed bathymetry, implying these may be sufficient for most modelling purposes."	 (C) 2014 Elsevier B.V. All rights reserved.	"Trigg, MA|Cook, PG|Brunner, P"	JOURNAL OF HYDROLOGY	wetland ponds bathymetry uncertainty pest solute balance bezier curve	10.1016/j.jhydrol.2014.06.020
153	WOS:000185985700004	2003	Structural design optimization on thermally induced vibration	TRANSIENT HEAT-CONDUCTION PRECISE TIME INTEGRATION SENSITIVITY ANALYSIS DYNAMIC LOADS DERIVATIVES	"The numerical method of design optimization for structural thermally induced vibration is originally studied in this paper and implemented in, the software JIFEX The direct and adjoint methods of sensitivity analysis for thermal-induced vibration coupled with both linear and non-linear transient heat conduction is firstly proposed. Based on the finite element method, the linear structural dynamics is treated simultaneously with linear and non-linear transient heat conduction. In the heat conduction, the non-linear factors include the radiation and temperature-dependent materials. The sensitivity analysis of transient linear and non-linear heat conduction is performed with the precise time integration method; and then, the sensitivity analysis of structural transient responses is performed by the Newmark method. Both the direct method and the adjoint method are employed to derive the sensitivity equations of thermal vibration. In the adjoint method, two adjoint vectors of structure and of heat conduction are used to derive the adjoint equations. The coupling effect of heat conduction on thermal vibration in the sensitivity analysis is particularly investigated. With the coupling sensitivity analysis, the optimization model is constructed and solved by the sequential linear programming or sequential quadratic programming algorithm. Numerical examples are given to validate the proposed methods and to demonstrate the importance of the coupled design optimization."	" Copyright (C) 2003 John Wiley Sons, Ltd."	"Chen, BS|Gu, YX|Zhang, HW|Zhao, GZ"	INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING	heat conduction thermal vibration sensitivity analysis design optimization precise time integration thermal coupled structural system	10.1002/nme.814
158	WOS:000414081000003	2017	Multi-scale equation of state computations for confined fluids	EQUILIBRIUM	"Fluid properties of five binary mixtures relevant to shale gas and light tight oil in confined nano-channels are studied. Canonical (NVT) Monte Carlo simulations are used to determine internal energies of departure of pure fluids using the RASPA software system (Dubbeldam et al., ). The linear mixing rule proposed by Lucia et al. () is used to determine internal energies of departure for mixtures, U-M(D), in confined spaces and compared to U-M(D) from direct NVT Monte Carlo simulation. The sensitivity of the mixture energy parameter, a(M), for the Gibbs-Helmholtz constrained (GHC) equation, confined fluid molar volume, V-M, and bubble point pressure are studied as a function of uncertainty in U-M(D). Results show that the sensitivity of confined fluid molar volume to % uncertainty in U-M(D) is less than % and that the GHC equation predicts physically meaningful reductions in bubble point pressure for light tight oils. (C) "	 Elsevier Ltd. All rights reserved.	"Thomas, E|Lucia, A"	COMPUTERS & CHEMICAL ENGINEERING	confined fluids monte carlo simulation ghc equation of state sensitivity analysis bubble point pressure	10.1016/j.compchemeng.2017.05.028
160	WOS:000322557200002	2013	Comparison of sediment transport computations using hydrodynamic versus hydrologic models in the Simiyu River in Tanzania	BED-LOAD TRANSPORT SENSITIVITY-ANALYSIS PARAMETERS SWAT TOOL	"This paper presents the results of a study that compares the sediment routing of the Simiyu River using the hydrologic model, Soil and Water Assessment Tool (SWAT) and the D hydrodynamic simulation software for Rivers and Estuaries (SOBEK-RE) model. Routing in SWAT is completed using the simplified Bagnold's equation and in the SOBEK-RE model is undertaken using the Saint Venant equation. The upstream boundary conditions for the routing modules were derived from the subcatchments sediment yields that were estimated by SWAT using the Modified Universal Soil Loss Equation (MUSLE). The sediment loads extrapolated or interpolated from the sediment rating curve for the catchment outlet were used for calibration and validation purposes. The SWAT model predicted an erosion rate of . Mt/yr. The total sediment load transported to the main outlet of the catchment simulated by the SWAT and SOBEK-RE models was equal to . and . Mt/yr, respectively. Thus the models computed a net erosion in the channels of . Mt/yr (SWAT) and . Mt/yr (SOBEK-RE). When comparing the results of the models for the different reaches of the main channel and main tributaries, the models showed different results both in magnitude and in sign (erosion/deposition). However, in a situation where data is scarce (such as grain size, channel geometry), the more complex hydrodynamic model does not necessarily lead to more reliable results. (c) "	 Elsevier Ltd. All rights reserved.	"van Griensven, A|Popescu, L|Abdelhamid, MR|Ndomba, PM|Beevers, L|Betrie, GD"	PHYSICS AND CHEMISTRY OF THE EARTH	sediment routing sediment transport swat sober-re simiyu river basin	10.1016/j.pce.2013.02.003
161	WOS:000314802700001	2013	An educational model for ensemble streamflow simulation and uncertainty analysis	CLIMATE-CHANGE SENSITIVITY HYDROLOGY FUTURE TOOL	"This paper presents the hands-on modeling toolbox, HBV-Ensemble, designed as a complement to theoretical hydrology lectures, to teach hydrological processes and their uncertainties. The HBV-Ensemble can be used for in-class lab practices and homework assignments, and assessment of students' understanding of hydrological processes. Using this modeling toolbox, students can gain more insights into how hydrological processes (e.g., precipitation, snowmelt and snow accumulation, soil moisture, evapotranspiration and runoff generation) are interconnected. The educational toolbox includes a MATLAB Graphical User Interface (GUI) and an ensemble simulation scheme that can be used for teaching uncertainty analysis, parameter estimation, ensemble simulation and model sensitivity. HBV-Ensemble was administered in a class for both in-class instruction and a final project, and students submitted their feedback about the toolbox. The results indicate that this educational software had a positive impact on students understanding and knowledge of uncertainty in hydrological modeling."		"AghaKouchak, A|Nakhjiri, N|Habib, E"	HYDROLOGY AND EARTH SYSTEM SCIENCES		10.5194/hess-17-445-2013
163	WOS:000368869200001	2016	A GUI platform for uncertainty quantification of complex dynamical models	RAINFALL-RUNOFF MODELS GLOBAL SENSITIVITY MEASURES AUTOMATIC CALIBRATION OPTIMIZATION INDEXES DESIGN MACHINE OUTPUT	"Uncertainty quantification (UQ) refers to quantitative characterization and reduction of uncertainties present in computer model simulations. It is widely used in engineering and geophysics fields to assess and predict the likelihood of various outcomes. This paper describes a UQ platform called UQ-PyL (Uncertainty Quantification Python Laboratory), a flexible software platform designed to quantify uncertainty of complex dynamical models. UQ-PyL integrates different kinds of UQ methods, including experimental design, statistical analysis, sensitivity analysis, surrogate modeling and parameter optimization. It is written in Python language and runs on all common operating systems. UQ-PyL has a graphical user interface that allows users to enter commands via pull-down menus. It is equipped with a model driver generator that allows any computer model to be linked with the software. We illustrate the different functions of UQ-PyL by applying it to the uncertainty analysis of the Sacramento Soil Moisture Accounting Model. We will also demonstrate that UQ-PyL can be applied to a wide range of applications. (C)  The Authors."	 Published by Elsevier Ltd.	"Wang, C|Duan, QY|Tong, CH|Di, ZH|Gong, W"	ENVIRONMENTAL MODELLING & SOFTWARE	uncertainty quantification design of experiments sensitivity analysis surrogate modeling parameter optimization uq-pyl	10.1016/j.envsoft.2015.11.004
165	WOS:000341218800002	2014	An environmental assessment system for environmental technologies	LIFE-CYCLE ASSESSMENT WASTE MANAGEMENT-SYSTEMS SOLID-WASTE LCA INCINERATION COLLECTION EASEWASTE MODEL	"A new model for the environmental assessment of environmental technologies, EASETECH, has been developed. The primary aim of EASETECH is to perform life-cycle assessment (LCA) of complex systems handling heterogeneous material flows. The objectives of this paper are to describe the EASETECH framework and the calculation structure. The main novelties compared to other LCA software are as follows. First, the focus is put on material flow modelling, as each flow is characterised as a mix of material fractions with different properties and flow compositions are computed as a basis for the LCA calculations. Second, the tool has been designed to allow for the easy set-up of scenarios by using a toolbox, the processes within which can handle heterogeneous material flows in different ways and have different emission calculations. Finally, tools for uncertainty analysis are provided, enabling the user to parameterise systems fully and propagate probability distributions through Monte Carlo analysis. (C) "	 Elsevier Ltd. All rights reserved.	"Clavreul, J|Baumeister, H|Christensen, TH|Damgaard, A"	ENVIRONMENTAL MODELLING & SOFTWARE	easetech life cycle assessment waste lca model uncertainty flow modelling	10.1016/j.envsoft.2014.06.007
166	WOS:000327903400013	2013	Addressing ten questions about conceptual rainfall-runoff models with global sensitivity analyses in R	MULTIOBJECTIVE CALIBRATION ENVIRONMENTAL-MODEL CATCHMENT MODEL CLIMATE-CHANGE UNCERTAINTY PERFORMANCE INDEXES PARAMETERS AUSTRALIA HYDROLOGY	"Sensitivity analysis (SA) is generally recognized as a worthwhile step to diagnose and remedy difficulties in identifying model parameters, and indeed in discriminating between model structures. An analysis of papers in three journals indicates that SA is a standard omission in hydrological modeling exercises. We provide some answers to ten reasonably generic questions using the Morris and Sobol SA methods, including to what extent sensitivities are dependent on parameter ranges selected, length of data period, catchment response type, model structures assumed and climatic forcing. Results presented demonstrate the sensitivity of four target functions to parameter variations of four rainfall runoff models of varying complexity (- parameters). Daily rainfall, streamflow and pan evaporation data are used from four -year data sets and from five catchments in the Australian Capital Territory (ACT) region. Similar results are obtained using the Morris and Sobol methods. It is shown how modelers can easily identify parameters that are insensitive, and how they might improve identifiability. Using a more complex objective function, however, may not result in all parameters becoming sensitive. Crucially, the results of the SA can be influenced by the parameter ranges selected. The length of data period required to characterize the sensitivities assuredly is a minimum of five years. The results confirm that only the simpler models have well-identified parameters, but parameter sensitivities vary between catchments. Answering these ten questions in other case studies is relatively easy using freely available software with the Hydromad and Sensitivity packages in R."	 (C) 2013 Elsevier B.V. All rights reserved.	"Shin, MJ|Guillaume, JHA|Croke, BFW|Jakeman, AJ"	JOURNAL OF HYDROLOGY	sensitivity analysis rainfall-runoff model identifiability	10.1016/j.jhydrol.2013.08.047
168	WOS:000390183000039	2016	Identifiability of sorption parameters in stirred flow-through reactor experiments and their identification with a Bayesian approach	NONEQUILIBRIUM SOLUTE TRANSPORT CHAIN MONTE-CARLO POROUS-MEDIA MODELS ADSORPTION EQUILIBRIUM KINETICS VALUES	"This paper addresses the methodological conditions particularly experimental design and statistical inference ensuring the identifiability of sorption parameters from breakthrough curves measured during stirred flow-through reactor experiments also known as continuous flow stirred-tank reactor (CSTR) experiments. The equilibrium-kinetic (EK) sorption model was selected as nonequilibrium parameterization embedding the Kd approach. Parameter identifiability was studied.formally on the equations governing outlet concentrations. It was also studied numerically on  simulated CSTR experiments on a soil with known equilibrium-kinetic sorption parameters. EK sorption parameters can not be identified from a single breakthrough curve of a CSTR experiment, because K-d,K- and k(-) were diagnosed collinear. For pairs of CSTR experiments, Bayesian inference allowed to select the correct models of sorption and error among sorption alternatives. Bayesian inference was conducted with SAMCAT software (Sensitivity Analysis and Markov Chain simulations Applied to Transfer models) which launched the simulations through the embedded simulation engine GNU-MCSim, and automated their configuration and post-processing. Experimental designs consisting in varying flow rates between experiments reaching equilibrium at contamination stage were found optimal, because they simultaneously gave accurate sorption parameters and predictions. Bayesian results were comparable to maximum likehood method but they avoided convergence problems, the marginal likelihood allowed to compare all modeK and credible interval gave directly the uncertainty of sorption parameters theta. Although these findings are limited to the specific conditions studied here, in particular the considered sorption model, the chosen parameter values and error structure, they help in the conception and analysis of future CSTR experiments with radionuclides whose kinetic behaviour is suspected. (C) "	 Elsevier Ltd. All rights reserved.	"Nicoulaud-Gouin, V|Garcia-Sanchez, L|Giacalone, M|Attard, JC|Martin-Garin, A|Bois, FY"	JOURNAL OF ENVIRONMENTAL RADIOACTIVITY	bayesian inference sorption parameters identifiability mcmc convergence monitoring equilibrium kinetic model	10.1016/j.jenvrad.2016.06.008
172	WOS:000403512500013	2017	"The rocky road to extended simulation frameworks covering uncertainty, inversion, optimization and control"	REAL-TIME MANAGEMENT COMPUTATIONAL SCIENCE CO2 STORAGE FLOW ASSIMILATION VERIFICATION ALGORITHMS COMPLEXITY TRANSPORT SELECTION	"In the past decades, simulation frameworks have greatly increased in complexity, due to coupling of models from various disciplines into so-called integrated models. Recently, the combination with tools for uncertainty quantification, inverse modelling, optimization and control started a development towards what we call extended simulation frameworks. While there is an ongoing discussion on quality assurance and reproducibility for simulation frameworks, we have not observed a similar discussion for the extended case. Particularly for extended frameworks, the need for quality assurance is high: The overwhelming range of options and algorithms is unmanageable by a domain expert and opaque to decision makers or the public. The resulting demand for 'intelligent software' with automated configuration can lead to a blind trust in simulation results even if they are incorrect. This is a threatening scenario due to potential consequences in simulation-based engineering or political decisions. In this paper, we analyze the increasing complexity of scientific computing workflows, and discuss the corresponding problems of extended scientific simulation frameworks. We propose a paradigm that regulates the allowable properties of framework components, supports the framework configuration for complex simulations, enforces automatic self-tests of configured frameworks, and communicates automated algorithm choices, potentially critical user settings or convergence issues with adaptive detail level and urgency to the end-user. Our goal is to start transferring the quality assurance discussion in the field of integrated modeling and conventional software frameworks to the area of extended simulation frameworks. With this, we hope to increase the reliability and transparency of (extended) frameworks, framework use and of the corresponding simulation results. (C) "	 Elsevier Ltd. All rights reserved.	"Wirtz, D|Nowak, W"	ENVIRONMENTAL MODELLING & SOFTWARE	scientific computing extended software frameworks no free lunch reproducibility software trust	10.1016/j.envsoft.2016.10.003
173	WOS:000222719700006	2004	Interactive software for material parameter characterization of advanced engineering constitutive models	STATE	"The development of an overall strategy to estimate the material parameters for a class of viscoplastic material models is presented. The procedure is automated through the integrated software COMPARE (Constitutive Material PARameter Estimator) that enables the determination of an 'optimum' set of material parameters by minimizing the errors between the experimental test data and the model's predicted response. The core ingredients of COMPARE are (i) primal analysis, which utilizes a finite element-based solution scheme, (ii) sensitivity analysis utilizing a direct-differentiation approach for the material response sensitivities, and (iii) a gradient-based optimization technique of an error/cost function. Now that the COMPARE core code has reached a level of maturity, a graphical user interface (GUI) was deemed necessary. Without such an interface, use of COMPARE was previously restricted to very experienced users with the additional cumbersome, and sometimes tedious, task of preparing the required input files manually. The complexity of the input containing massive amounts of data has previously placed severe limitations on the use of such optimization procedures by the general engineering community. By using C+ + and the Microsoft Foundation Classes to develop a GUI, it is believed that an advanced code such as COMPARE can now make the transition to general usability in an engineering environment. (C) "	 Elsevier Ltd. All rights reserved.	"Saleeb, AF|Marks, JR|Wilt, TE|Arnold, SM"	ADVANCES IN ENGINEERING SOFTWARE	c plus graphical user interface optimization material characterization viscoplasticity	10.1016/j.advengsoft.2004.03.010
175	WOS:000399586700038	2017	Accelerating Monte Carlo estimation with derivatives of high-level finite element models	SENSITIVITY DERIVATIVES CHAOS	In this paper we demonstrate the ability of a derivative-driven Monte Carlo estimator to accelerate the propagation of uncertainty through two high-level non-linear finite element models. The use of derivative information amounts to a correction to the standard Monte Carlo estimation procedure that reduces the variance under certain conditions. We express the finite element models in variational form using the high-level Unified Form Language (UFL). We derive the tangent linear model automatically from this high-level description and use it to efficiently calculate the required derivative information. To study the effectiveness of the derivative-driven method we consider two stochastic PDEs; a one-dimensional Burgers equation with stochastic viscosity and a three-dimensional geometrically non-linear Mooney-Rivlin hyperelastic equation with stochastic density and volumetric material parameter. Our results show that for these problems the first-order derivative-driven Monte Carlo method is around one order of magnitude faster than the standard Monte Carlo method and at the cost of only one extra tangent linear solution per estimation problem. We find similar trends when comparing with a modern non-intrusive multi-level polynomial chaos expansion method. We parallelise the task of the repeated forward model evaluations across a cluster using the ipyparallel and mpipy software tools. A complete working example showing the solution of the stochastic viscous Burgers equation is included as supplementary material.	 (C) 2017 Published by Elsevier B.V.	"Hauseux, P|Hale, JS|Bordas, SPA"	COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING	monte carlo methods uncertainty propagation tangent linear models partially intrusive methods polynomial chaos expansion parallel computing	10.1016/j.cma.2017.01.041
180	WOS:000299324200005	2012	Yield improvement analysis with parameter-screening factorials	SENSITIVITY-ANALYSIS SYSTEM MODEL	"This paper presents a technique for the critical parameter analysis of the disk drive manufacturing process. The objective of the work is to improve the manufacturing yield by tuning the parameters that significantly affect the yield. Several techniques were studied including the sensitivity analysis framework, which is currently used at several disk drive plants. From our initial experiments, we found that the sensitivity analysis results were not sufficiently good and the interactions between parameters were not identified. We then designed a new technique based on factorial designs, the parameter-screening factorials algorithm. Our method can work with a large number of inputs within reasonable computing time, and can identify both the parameter and the interaction effects. The results can be obtained more quickly and are better in comparison with the currently used technique. Moreover, by applying the technique to the full list instead of the pre-selected list of the manufacturing parameters, we discovered that the parameters watch list previously identified by the experts should be adjusted to include some extra parameters. After the results were validated by the experts, we designed software that automates the critical parameter analysis process. The software should greatly benefit the daily yield analysis at the disk drive manufacturing plant greatly."	 (C) 2011 Elsevier B.V. All rights reserved.	"Yamwong, W|Achalakul, T"	APPLIED SOFT COMPUTING	critical parameter identification yield improvement analysis factorial designs	10.1016/j.asoc.2011.11.021
181	WOS:000174593500001	2002	Use of the most likely failure point method for risk estimation and risk uncertainty analysis		"The most likely failure point (MLFP) method, developed within the field of structural reliability analysis (where it is known as the FORM/SORM method) is a technique for estimating the risk (probability) that a calculated quantity Q exceeds a set limit Q(lim) when some or all of the inputs to the calculation are uncertain. It can be used as an efficient stand-alone method for this type of risk calculation. However, for application within the field of toxic hazards, it is proposed as a means for performing sensitivity analyses, possibly in parallel with a risk calculation carried out by conventional methods. The basis of the method is outlined and its use is demonstrated by means of an example calculation of the risk arising froth an installation containing chlorine. The calculation uses, as a consequence model, commercial software for the prediction of dense gas transport. The risk estimate is shown to be acceptably close to that obtained by the Monte Carlo method. The use of a proposed screening procedure utilising the sensitivity formulas that the method provides, in order to identify the most significant uncertainties, is demonstrated. The identification of a single set of input values containing sufficient information to summarise (at least approximately) the entire risk analysis is considered to be an important feature of the method and is proposed as the basis of a means for assessing the validity of the consequence model."	 (C) 2002 Elsevier Science B.V All rights reserved.	"Mitchell, B"	JOURNAL OF HAZARDOUS MATERIALS	risk toxic hazard uncertainty sensitivity	10.1016/S0304-3894(01)00378-8
182	WOS:000183089700001	2003	Direct treatment of uncertainty: I - Applications in aquatic invertebrate risk assessment and soil metabolism for chlorpyrifos	RESPONSE VARIABILITY DYNAMIC-RESPONSE ACUTE TOXICITY SYSTEMS MODELS	"Environmental processes are wrought with uncertainty. Therefore, an efficient means to propagate uncertainty is advantageous, especially if regulatory decisions are based upon any research or data analysis where uncertainty is present. The Deterministic Equivalent Modeling Method (DEMM) propagates parametric uncertainties in model input parameters to output predictions. DEMM is used to calculate uncertainty in output parameters based upon the direct effect of every uncertain input parameter. Rather than sampling input distributions and running hundreds or thousands of model calculations as in Monte Carlo or Latin Hypercube Sampling, DEMM carries a representation of each distribution throughout the calculation of the dependent variable. An overview of DEMM is provided. Once DEMM algorithms are established using symbolic mathematical software program(s), and the dependent variable expansion hypothesized, then the additional overhead required to set up and solve algebraic or differential systems is small. Examples of DEMM using literature values for chlorpyrifos (a widely used insecticide) effects and fate illustrate DEMM's capability for uncertainty propagation. Determination of chlorpyrifos risk quotients for invertebrates (algebraic system) and chlorpyrifos metabolic fate in soil (differential equation system) are presented. These examples illustrate DEMM methodology on problems of interest in environmental fate and risk assessment. Multiple data sets and field/laboratory observations for chlorpyrifos were assembled and utilized with DEMM to propagate uncertainty in output predictions. Chlorpyrifos environmental fate (environmental degradation and metabolite formation/degradation) and risk for aquatic invertebrates, with uncertainty characterized using DEMM are discussed."		"Cryer, SA|Applequist, GE"	ENVIRONMENTAL ENGINEERING SCIENCE	deterministic equivalent modeling method demm uncertainty chlorpyrifos soil metabolism risk quotient	10.1089/109287503321671375
184	WOS:000283340800004	2010	A DECISION SUPPORT TOOL FOR IRRIGATION INFRASTRUCTURE INVESTMENTS	OPTIMAL ALLOCATION WATER-RESOURCES SYSTEM MODEL MANAGEMENT IMPACTS FARM	"Increasing water scarcity, climate change and pressure to provide water for environmental flows urge irrigators to be more efficient. In Australia, ongoing water reforms and most recently the National Water Security Plan offer incentives to irrigators to adjust their farming practices by adopting water-saving Irrigation infrastructures to match soil, crop and climatic conditions. Water Works is a decision support tool to facilitate irrigators to make long- and short-term irrigation infrastructure investment decisions at the farm level. It helps irrigators to improve the economic efficiency, water use efficiency and environmental performance of their farm businesses. Water Works has been tested, validated and accepted by the irrigation community and researchers in NSW, Australia. The interface of Water Works is user-friendly and flexible. The simulation and optimisation module in Water Works provides an opportunity to evaluate Infrastructure investment decisions to suit their seasonal or long-term water availability. The sensitivity analysis allows substantiation of the impact of major variables Net present value, internal rate of return, benefit cost ratio and payback period are used to analyse the costs and benefits of modern irrigation technology. Application of Water Works using a whole farm-level case study indicates its effectiveness in making long- and short-term investment decisions Water Works can be easily integrated into commercial software such as spreadsheets, GIS, real-time data acquisition and control systems to further enhance its usability. Water Works can also be used in regional development planning."	" Copyright (C) 2009 John Wiley & Sons, Ltd."	"Khan, S|Mushtaq, S|Chen, C"	IRRIGATION AND DRAINAGE	decision support tool water management seasonal and long-term investment optimisation simulation benefit-cost analysis whole farm water trading water saving	10.1002/ird.501
185	WOS:000355932400009	2015	"Parameter sensitivity analysis and optimization of Noah land surface model with field measurements from Huaihe River Basin, China"	MESOSCALE ETA-MODEL WATERSHED MODEL ENVIRONMENTAL-MODELS UNCERTAINTY CALIBRATION IMPLEMENTATION HYDROLOGY SYSTEMS IMPACT EVAPORATION	"This study aims to identify the parameters that are most important in controlling the Noah land surface model (LSM), the analysis of parameter interactions, and the evaluation of the performance of parameter optimization using the parameter estimation software PEST. We found it necessary to analyze parameter sensitivity in order to properly simulate hydrological variables such as latent heat flux in the Huaihe River Basin, China. The parameters under study in the Noah LSM link thermodynamic and hydrological parts into a complete model. To our knowledge, this parameter interaction in the Noah LSM has never been studied before. There are, however, several studies concerning the influence of vegetation types and climate conditions on parameter sensitivity of the Noah LSM. Three sensitivity analysis methods, the including local sensitivity analysis method SENSAN, regional sensitivity analysis, and Sobol's method, were tested. Five experimental sites in the Huaihe River Basin were chosen to perform the simulations. The results show that the Noah LSM parameter sensitivities were impacted by the choice of the analysis method. The local method SENSAN often produced significant differences in results compared to the two global methods. The parameter interactions investigated made a significant contribution towards elucidating how one process influences another in the Noah LSM. The results show that parameters were not transferable solely based on vegetation types but also rely on climate conditions. According to the sensitivity analysis results, four sensitive parameters were chosen to be optimized using the PEST method. PEST is a widely used method for estimating parameters in models. Root-mean-square error was used to evaluate the effect of the optimization. Generally in all sites, the optimized parameters values perform better than the original parameter values."		"Hou, T|Zhu, YH|Lu, HS|Sudicky, E|Yu, ZB|Ouyang, F"	STOCHASTIC ENVIRONMENTAL RESEARCH AND RISK ASSESSMENT	noah lsm huaihe river sensitivity analysis rsa sobol's method pest	10.1007/s00477-015-1033-5
186	WOS:000352642900010	2015	"A computational framework for dynamic data-driven material damage control, based on Bayesian inference and model selection"	APPLICATIONS SYSTEMS CREEP DAMAGE SIMULATIONS MECHANICS GROWTH	"In the present study, a general dynamic data-driven application system (DDDAS) is developed for real-time monitoring of damage in composite materials using methods and models that account for uncertainty in experimental data, model parameters, and in the selection of the model itself. The methodology involves (i) data data from uniaxial tensile experiments conducted on a composite material; (ii) continuum damage mechanics based material constitutive models; (iii) a Bayesian framework for uncertainty quantification, calibration, validation, and selection of models; and (iv) general Bayesian filtering, as well as Kalman and extended Kalman filters. A software infrastructure is developed and implemented in order to integrate the various parts of the DDDAS. The outcomes of computational analyses using the experimental data prove the feasibility of the Bayesian-based methods for model calibration, validation, and selection. Moreover, using such DDDAS infrastructure for real-time monitoring of the damage and degradation in materials results in results in an improved prediction of failure in the system."	" Copyright (C) 2014 John Wiley & Sons, Ltd."	"Prudencio, EE|Bauman, PT|Faghihi, D|Ravi-Chandar, K|Oden, JT"	INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING	bayesian model selection extended kalman filter dynamic data-driven application systems material damage	10.1002/nme.4669
189	WOS:000418736700067	2017	"A Practical, Robust Methodology for Acquiring New Observation Data Using Computationally Expensive Groundwater Models"	EMPIRICAL ORTHOGONAL FUNCTIONS BAYESIAN EXPERIMENTAL-DESIGN REDUCED-ORDER MODEL PREDICTIVE UNCERTAINTY PARAMETER-IDENTIFICATION GENETIC ALGORITHM NETWORK DESIGN DATA-WORTH REDUCTION FLOW	"Regional groundwater flow models play an important role in decision making regarding water resources; however, the uncertainty embedded in model parameters and model assumptions can significantly hinder the reliability of model predictions. One way to reduce this uncertainty is to collect new observation data from the field. However, determining where and when to obtain such data is not straightforward. There exist a number of data-worth and experimental design strategies developed for this purpose. However, these studies often ignore issues related to real-world groundwater models such as computational expense, existing observation data, high-parameter dimension, etc. In this study, we propose a methodology, based on existing methods and software, to efficiently conduct such analyses for large-scale, complex regional groundwater flow systems for which there is a wealth of available observation data. The method utilizes the well-established d-optimality criterion, and the minimax criterion for robust sampling strategies. The so-called Null-Space Monte Carlo method is used to reduce the computational burden associated with uncertainty quantification. And, a heuristic methodology, based on the concept of the greedy algorithm, is proposed for developing robust designs with subsets of the posterior parameter samples. The proposed methodology is tested on a synthetic regional groundwater model, and subsequently applied to an existing, complex, regional groundwater system in the Perth region of Western Australia. The results indicate that robust designs can be obtained efficiently, within reasonable computational resources, for making regional decisions regarding groundwater level sampling. Plain Language Summary Water supply for the public, industry, and the environment can be heavily reliant on groundwater resources. Therefore, decision makers must be able to make predictions about how a groundwater system will respond to management options. These predictions often contain a significant degree of uncertainty. This uncertainty must be reduced in order for decision makers to make optimal use of groundwater resources with minimal risk to the environment. One way to reduce this uncertainty is to obtain more information about the nature of the groundwater system by collecting new measurement data from the study site. However, it is often not clear where and when to collect this data. This study proposes a new methodology for collecting data in an optimal fashion so that the information acquired is maximized. The method incorporates any existing information, examines the characteristics of uncertainty, and alleviates the high computing costs associated with conducting the necessary calculations. The procedure is applied to a regional groundwater system in the Perth area of Western Australia. The results are consistent with the hydrogeologic conceptualization of the Perth system, and provide important insight into where new observation wells could be constructed to obtain information about the hydraulic nature of faults."		"Siade, AJ|Hall, J|Karelse, RN"	WATER RESOURCES RESEARCH	groundwater modeling uncertainty assessment experimental design calibration monitoring network	10.1002/2017WR020814
190	WOS:000344387000014	2014	Non-stationary extreme value analysis in a changing climate	DIFFERENTIAL EVOLUTION MODEL PROJECTIONS RETURN LEVELS SIMULATIONS EVENTS TEMPERATURE VARIABILITY ENSEMBLE IMPACTS RECORDS	"This paper introduces a framework for estimating stationary and non-stationary return levels, return periods, and risks of climatic extremes using Bayesian inference. This framework is implemented in the Non-stationary Extreme Value Analysis (NEVA) software package, explicitly designed to facilitate analysis of extremes in the geosciences. In a Bayesian approach, NEVA estimates the extreme value parameters with a Differential Evolution Markov Chain (DE-MC) approach for global optimization over the parameter space. NEVA includes posterior probability intervals (uncertainty bounds) of estimated return levels through Bayesian inference, with its inherent advantages in uncertainty quantification. The software presents the results of non-stationary extreme value analysis using various exceedance probability methods. We evaluate both stationary and non-stationary components of the package for a case study consisting of annual temperature maxima for a gridded global temperature dataset. The results show that NEVA can reliably describe extremes and their return levels."		"Cheng, LY|AghaKouchak, A|Gilleland, E|Katz, RW"	CLIMATIC CHANGE		10.1007/s10584-014-1254-5
191	WOS:000338551800010	2014	Risk and uncertainty analysis of gas pipeline failure and gas combustion consequence	MODELS DETONATION METHANE	"Taking into account a general concept of risk parameters and knowing that natural gas provides very significant portion of energy, firstly, it is important to insure that the infrastructure remains as robust and reliable as possible. For this purpose, authors present available statistical information and probabilistic analysis related to failures of natural gas pipelines. Presented historical failure data is used to model age-dependent reliability of pipelines in terms of Bayesian methods, which have advantages of being capable to manage scarcity and rareness of data and of being easily interpretable for engineers. The performed probabilistic analysis enables to investigate uncertainty and failure rates of pipelines when age-dependence is significant and when it is not relevant. The results of age-dependent modeling and analysis of gas pipeline reliability and uncertainty are applied to estimate frequency of combustions due to natural gas release when pipeline failure occurs. Estimated age-dependent combustion frequency is compared and proposed to be used instead of conservative and age-independent estimate. The rupture of a high-pressure natural gas pipeline can lead to consequences that can pose a significant threat to people and property in the close vicinity to the pipeline fault location. The dominant hazard is combustion and thermal radiation from a sustained fire. The second purpose of the paper is to present the combustion consequence assessment and application of probabilistic uncertainty analysis for modeling of gas pipeline combustion effects. The related work includes performance of the following tasks: to study gas pipeline combustion model, to identify uncertainty of model inputs noting their variation range, and to apply uncertainty and sensitivity analysis for results of this model. The performed uncertainty analysis is the part of safety assessment that focuses on the combustion consequence analysis. Important components of such uncertainty analysis are qualitative and quantitative analysis that identifies the most uncertain parameters of combustion model, assessment of uncertainty, analysis of the impact of uncertain parameters on the modeling results, and communication of the results' uncertainty. As outcome of uncertainty analysis the tolerance limits and distribution function of thermal radiation intensity are given. The measures of uncertainty and sensitivity analysis were estimated and outcomes presented applying software system for uncertainty and sensitivity analysis. Conclusions on the importance of the parameters and sensitivity of the results are obtained using a linear approximation of the model under analysis. The outcome of sensitivity analysis confirms that distance from the fire center has the greatest influence on the heat flux caused by gas pipeline combustion."		"Alzbutas, R|Iesmantas, T|Povilaitis, M|Vitkute, J"	STOCHASTIC ENVIRONMENTAL RESEARCH AND RISK ASSESSMENT	risk parameters natural gas pipelines age-dependent reliability bayesian inference gas combustion uncertainty and sensitivity analysis	10.1007/s00477-013-0845-4
193	WOS:000231963300008	2005	Parameterization based shape optimization: theory and practical implementation aspects	DESIGN SENSITIVITY-ANALYSIS TOPOLOGY OPTIMIZATION MECHANICAL SYSTEMS EFFICIENT HOMOGENIZATION TOOL	"Purpose - To present an approach to parameterization based shape optimization of statically loaded structures and to propose its practical implementation. Design/methodology/approach - In order to establish a convenient shape parameterization, the design element technique is employed. A rational Bezier body is used to serve as the design element. The design element is used to retrieve the nodal geometrical data of finite elements (FEs). Their field geometrical data are obtained using the FE own internal functions. For practical implementation it is proposed to establish the optimization cycle by two separately running processes. The data exchange is established by using self-descriptive and platform-independent XML conforming data files. Findings - The proposed approach offers an unified approach to shape optimization of skeletal, as well as continuous structures. Structural shape may be varied smoothly with a relative small set of design variables. The employment of a gradient-based optimization algorithm assures computational efficiency. Research limitations/implications - The aspects of FE mesh deterioration are not considered in this work. This would be necessary if for the actual problem at hand major and excessively non-uniform shape changes of the FE mesh are expected. Practical implications - A useful source of information for someone who is planning to develop a general or special-purpose integrated structural analysis and shape optimization software. Originality/value - The paper offers a rather simple, but quite powerful approach to structural shape optimization together with practical hints for its computational implementation."		"Kegl, M"	ENGINEERING COMPUTATIONS	optimization techniques structures numerical analysis	10.1108/02644400510603041
195	WOS:000304926200026	2012	Numerical Modelling of Waste Stabilization Ponds: Where Do We Stand?	DYNAMIC MATHEMATICAL-MODEL AERATED LAGOON TREATMENT EFFICIENCY MATURATION PONDS STEADY-STATE PAPER-MILL PREDICTION REMOVAL TRANSFORMATION CALIBRATION	"Waste stabilization pond (WSP) technology has been an active area of research for the last three decades. In spite of its relative simplicity of design, operation and maintenance, the various processes taking place in WSP have not been entirely quantified. Lately, modelling has served as an important, low-cost tool for a better description and an improved understanding of the system. Although several papers on individual pond models have been published, there is no specific review on different models developed so far. This paper aims at filling this gap. Models are compared by focussing on their key features like the presence and comprehensiveness of a water quality sub-model in terms of aerobic/anoxic and anaerobic carbon removal and nutrient removal; the type of hydraulic sub-model used (D, D, D or D); the software used for implementation and simulation; and whether or not sensitivity analysis, calibration and validation were done. This paper also recommends future directions of research in this area. In-depth study of the published models reveals a clear evolution over time in the concept of modelling, from just hydraulic empirical models to D ones and from simple first-order water quality models to complex ones which describe key biochemical processes as a set of mathematical equations. Due to the inherent complexity, models tend to focus only on specific aspects whilst ignoring or simplifying others. For instance, many models have been developed that either focus solely on hydrodynamics or solely on biochemical processes. Models which integrate both aspects in detail are still rare. Furthermore, it is evident from the review of the different models that calibration and validation with full-scale WSP data is also scarce. Hence, we believe that there is a need for the development of a comprehensive, calibrated model for waste stabilization ponds that can reliably serve as a support tool for the improvement and optimization of pond design and performance."		"Sah, L|Rousseau, DPL|Hooijmans, CM"	WATER AIR AND SOIL POLLUTION	computational fluid dynamics (cfd) hydrodynamics modelling waste stabilization pond (wsp) water quality	10.1007/s11270-012-1098-4
196	WOS:000267193100001	2009	Evaluating uncertainty in integrated environmental models: A review of concepts and tools	RAINFALL-RUNOFF MODELS GENERALIZED POLYNOMIAL CHAOS WATER-QUALITY MODELS SENSITIVITY-ANALYSIS RISK-ASSESSMENT AUTOMATIC CALIBRATION GLOBAL OPTIMIZATION RELIABILITY METHODS HYDROLOGIC-MODELS DIFFERENTIAL-EQUATIONS	"This paper reviews concepts for evaluating integrated environmental models and discusses a list of relevant software-based tools. A simplified taxonomy for sources of uncertainty and a glossary of key terms with ""standard'' definitions are provided in the context of integrated approaches to environmental assessment. These constructs provide a reference point for cataloging  different model evaluation tools. Each tool is described briefly (in the auxiliary material) and is categorized for applicability across seven thematic model evaluation methods. Ratings for citation count and software availability are also provided, and a companion Web site containing download links for tool software is introduced. The paper concludes by reviewing strategies for tool interoperability and offers guidance for both practitioners and tool developers."		"Matott, LS|Babendreier, JE|Purucker, ST"	WATER RESOURCES RESEARCH		10.1029/2008WR007301
198	WOS:000407370700097	2017	Modeling Nitrogen Dynamics in a Waste Stabilization Pond System Using Flexible Modeling Environment with MCMC	SENSITIVITY-ANALYSIS CONSTRUCTED WETLAND WATER TREATMENT PREDICTIVE UNCERTAINTY NUTRIENT RECOVERY GLUE METHODOLOGY BAYESIAN METHOD UNITED-STATES REMOVAL PERFORMANCE	"This study presents an approach for obtaining realization sets of parameters for nitrogen removal in a pilot-scale waste stabilization pond (WSP) system. The proposed approach was designed for optimal parameterization, local sensitivity analysis, and global uncertainty analysis of a dynamic simulation model for the WSP by using the R software package Flexible Modeling Environment (R-FME) with the Markov chain Monte Carlo (MCMC) method. Additionally, generalized likelihood uncertainty estimation (GLUE) was integrated into the FME to evaluate the major parameters that affect the simulation outputs in the study WSP. Comprehensive modeling analysis was used to simulate and assess nine parameters and concentrations of ON-N, NH-N and NO-N. Results indicate that the integrated FME-GLUE-based model, with good Nash-Sutcliffe coefficients (.-.) and correlation coefficients (.-.), successfully simulates the concentrations of ON-N, NH-N and NO-N. Moreover, the Arrhenius constant was the only parameter sensitive to model performances of ON-N and NH-N simulations. However, Nitrosomonas growth rate, the denitrification constant, and the maximum growth rate at  degrees C were sensitive to ON-N and NO-N simulation, which was measured using global sensitivity."		"Mukhtar, H|Lin, YP|Shipin, OV|Petway, JR"	INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH AND PUBLIC HEALTH	flexiblemodeling environment waste stabilization pond nitrogen dynamic parameterization sensitivity mcmc glue global uncertainty	10.3390/ijerph14070765
200	WOS:000304453300004	2012	Standardized uncertainty analysis for hydrometry: a review of relevant approaches and implementation examples		"The water-centric community has continuously made efforts to identify, assess and implement rigorous uncertainty analyses for routine hydrological measurements. This paper reviews some of the most relevant efforts and subsequently demonstrates that the Guide to the expression of uncertainty in measurement (GUM) is a good candidate for estimation of uncertainty intervals for hydrometry. The demonstration is made by implementing the GUM to typical hydrometric applications and comparing the analysis results with those obtained using the Monte Carlo method. The results show that hydrological measurements would benefit from the adoption of the GUM as the working standard, because of its soundness, the availability of software for practical implementation and potential for extending the GUM to hydrological/hydraulic numerical simulations."		"Muste, M|Lee, K|Bertrand-Krajewski, JL"	HYDROLOGICAL SCIENCES JOURNAL-JOURNAL DES SCIENCES HYDROLOGIQUES	uncertainty analysis hydrometric measurements monte carlo uncertainty estimation sensitivity analysis	10.1080/02626667.2012.675064
201	WOS:000168569900007	2001	COOPT - a software package for optimal control of large-scale differential-algebraic equation systems		"This paper describes the functionality and implementation of COOPT. This software package implements a direct method with modified multiple shooting type techniques for solving optimal control problems of large-scale differential-algebraic equation (DAE) systems. The basic approach in COOPT is to divide the original time interval into multiple shooting intervals, with the DAEs solved numerically on the subintervals at each optimization iteration. Continuity constraints are imposed across the subintervals, The resulting optimization problem is solved by sparse sequential quadratic programming (SQP) methods. Partial derivative matrices needed for the optimization are generated by DAE sensitivity software. The sensitivity equations to be solved are generated via automatic differentiation. COOPT has been successfully used in solving optimal control problems arising from a wide variety of applications, such as chemical vapor deposition of superconducting thin films, spacecraft trajectory design and contingency/recovery problems, and computation of cell traction forces in tissue engineering. (C)  IMACS."	 Published by Elsevier Science B.V. All rights reserved.	"Serban, R|Petzold, LR"	MATHEMATICS AND COMPUTERS IN SIMULATION	differential-algebraic equations sensitivity analysis optimal control sequential quadratic programming methods	10.1016/S0378-4754(01)00289-0
203	WOS:000321425100048	2013	Life cycle assessment of the production of hydrogen and transportation fuels from corn stover via fast pyrolysis	LAND-USE CHANGE BIOMASS FAST PYROLYSIS BIO-OIL TECHNOECONOMIC ANALYSIS ETHANOL-PRODUCTION AQUEOUS FRACTION COMBUSTION CATALYSTS BIOFUELS SWITCHGRASS	"This life cycle assessment evaluates and quantifies the environmental impacts of the production of hydrogen and transportation fuels from the fast pyrolysis and upgrading of corn stover. Input data for this analysis come from Aspen Plus modeling, a GREET (Greenhouse Gases, Regulated Emissions, and Energy Use in Transportation) model database and a US Life Cycle Inventory Database. SimaPro . software is employed to estimate the environmental impacts. The results indicate that the net fossil energy input is . MJ and . MJ per km traveled for a light-duty vehicle fueled by gasoline and diesel fuel, respectively. Bio-oil production requires the largest fossil energy input. The net global warming potential (GWP) is . kg CO()eq and . kg CO()eq per km traveled for a vehicle fueled by gasoline and diesel fuel, respectively. Vehicle operations contribute up to % of the total positive GWP, which is the largest greenhouse gas footprint of all the unit processes. The net GWPs in this study are % and % lower than for petroleum-based gasoline and diesel fuel ( baseline), respectively. Biomass transportation has the largest impact on ozone depletion among all of the unit processes. Sensitivity analysis shows that fuel economy, transportation fuel yield, bio-oil yield, and electricity consumption are the key factors that influence greenhouse gas emissions."		"Zhang, YN|Hu, GP|Brown, RC"	ENVIRONMENTAL RESEARCH LETTERS	life cycle assessment fast pyrolysis bio-oil upgrading greenhouse gas emission energy demand	10.1088/1748-9326/8/2/025001
205	WOS:000345254600006	2014	Quantification of the uncertainties related to velocity-area streamgauging data		"Estimating the contribution of the different error sources in a given streamgauging offers a practical tool to improve the measurement strategy. To address the limitations of the method proposed by the ISO  standard, a generalized approach is introduced for computing the uncertainty associated with velocity-area discharge measurements. Direct computation methods are suggested for estimating the uncertainty components related to the vertical integration of velocity and to the transversal integration of velocity and depth. Discharge extrapolations to the edges and in the top/bottom layers are explicitly taken into account, as well as the distribution of the verticals throughout the cross-section. The new uncertainty analysis method was applied to streamgauging data which are representative of varied site conditions and field procedures. The new method appears to be more versatile than the ISO  method, and better suited to the diversity of streamgauging procedures. It is possible to implement it in discharge computation software such as BAREME."		"Le Coz, J|Bechon, PM|Camenen, B|Dramais, G"	HOUILLE BLANCHE-REVUE INTERNATIONALE DE L EAU	gauging uncertainty velocity area procedure	10.1051/lhb/2014047
206	WOS:000395108800010	2017	Pattern Mixture Models for Quantifying Missing Data Uncertainty in Longitudinal Invariance Testing	STRUCTURAL EQUATION MODELS FACTORIAL INVARIANCE DROP-OUT PSYCHOLOGICAL-RESEARCH NONIGNORABLE DROPOUT SENSITIVITY-ANALYSIS CLINICAL-TRIALS INCOMPLETE DATA MIXED MODELS GROWTH	"Many psychology applications assess measurement invariance of a construct (e.g., depression) over time. These applications are often characterized by few time points (e.g., ), but high rates of dropout. Although such applications routinely assume that the dropout mechanism is ignorable, this assumption may not always be reasonable. In the presence of nonignorable dropout, fitting a conventional longitudinal factor model (LFM) to assess longitudinal measurement invariance can yield misleading inferences about the level of invariance, along with biased parameter estimates. In this article we develop pattern mixture longitudinal factor models (PM-LFMs) for quantifying uncertainty in longitudinal invariance testing due to an unknown, but potentially nonignorable, dropout mechanism. PM-LFMs are a kind of multiple group model wherein observed missingness patterns define groups, LFM parameters can differ across these pattern-groups subject to identification constraints, and marginal inference about longitudinal invariance is obtained by pooling across pattern-groups. When dropout is nonignorable, we demonstrate via simulation that conventional LFMs can indicate longitudinal noninvariance, even when invariance holds in the overall population; certain PM-LFMs are shown to ameliorate this problem. On the other hand, when dropout is ignorable, PM-LFMs are shown to provide results comparable to conventional LFMs. Additionally, we contrast PM-LFMs to a latent mixture approach for accommodating nonignorable dropoutwherein missingness patterns can differ across latent groups. In an empirical example assessing longitudinal invariance of a harsh parenting construct, we employ PM-LFMs to assess sensitivity of results to assumptions about nonignorable missingness. Software implementation and recommendations for practice are discussed."		"Sterba, SK"	STRUCTURAL EQUATION MODELING-A MULTIDISCIPLINARY JOURNAL	longitudinal factor model longitudinal invariance nonignorable missing data pattern mixture model	10.1080/10705511.2016.1250635
212	WOS:000417943600002	2017	Parameter sensitivity analysis of a 1-D cold region lake model for land-surface schemes	GLOBAL SENSITIVITY GENERAL-CIRCULATION HYDROLOGIC MODEL ORGANIC-MATTER CLIMATE MODELS WATER-QUALITY GREAT-LAKES UNCERTAINTY ATMOSPHERE HEAT	"Lakes might be sentinels of climate change, but the uncertainty in their main feedback to the atmosphere heat- exchange fluxes - is often not considered within climate models. Additionally, these fluxes are seldom measured, hindering critical evaluation of model output. Analysis of the Canadian Small Lake Model (CSLM), a one-dimensional integral lake model, was performed to assess its ability to reproduce diurnal and seasonal variations in heat fluxes and the sensitivity of simulated fluxes to changes in model parameters, i.e., turbulent transport parameters and the light extinction coefficient (K-d). A C++ open-source software package, Problem Solving environment for Uncertainty Analysis and Design Exploration (PSUADE), was used to perform sensitivity analysis (SA) and identify the parameters that dominate model behavior. The generalized likelihood uncertainty estimation (GLUE) was applied to quantify the fluxes' uncertainty, comparing daily-averaged eddy-covariance observations to the output of CSLM. Seven qualitative and two quantitative SA methods were tested, and the posterior likelihoods of the modeled parameters, obtained from the GLUE analysis, were used to determine the dominant parameters and the uncertainty in the modeled fluxes. Despite the ubiquity of the equifinality issue - different parameter-value combinations yielding equivalent results-the answer to the question was unequivocal: K-d, a measure of how much light penetrates the lake, dominates sensible and latent heat fluxes, and the uncertainty in their estimates is strongly related to the accuracy with which K-d is determined. This is important since accurate and continuous measurements of K-d could reduce modeling uncertainty."		"Guerrero, JL|Pernica, P|Wheater, H|Mackay, M|Spence, C"	HYDROLOGY AND EARTH SYSTEM SCIENCES		10.5194/hess-21-6345-2017
213	WOS:000321690600011	2013	Data-driven sensitivity analysis to detect missing data mechanism with applications to structural equation modelling	GOODNESS-OF-FIT IMPROPER SOLUTIONS STANDARD ERRORS VALUES	"Missing data are a common problem in almost all areas of empirical research. Ignoring the missing data mechanism, especially when data are missing not at random (MNAR), can result in biased and/or inefficient inference. Because MNAR mechanism is not verifiable based on the observed data, sensitivity analysis is often used to assess it. Current sensitivity analysis methods primarily assume a model for the response mechanism in conjunction with a measurement model and examine sensitivity to missing data mechanism via the parameters of the response model. Recently, Jamshidian and Mata (Post-modelling sensitivity analysis to detect the effect of missing data mechanism, Multivariate Behav. Res.  (), pp. -) introduced a new method of sensitivity analysis that does not require the difficult task of modelling the missing data mechanism. In this method, a single measurement model is fitted to all of the data and to a sub-sample of the data. Discrepancy in the parameter estimates obtained from the the two data sets is used as a measure of sensitivity to missing data mechanism. Jamshidian and Mata describe their method mainly in the context of detecting data that are missing completely at random (MCAR). They used a bootstrap type method, that relies on heuristic input from the researcher, to test for the discrepancy of the parameter estimates. Instead of using bootstrap, the current article obtains confidence interval for parameter differences on two samples based on an asymptotic approximation. Because it does not use bootstrap, the developed procedure avoids likely convergence problems with the bootstrap methods. It does not require heuristic input from the researcher and can be readily implemented in statistical software. The article also discusses methods of obtaining sub-samples that may be used to test missing at random in addition to MCAR. An application of the developed procedure to a real data set, from the first wave of an ongoing longitudinal study on aging, is presented. Simulation studies are performed as well, using two methods of missing data generation, which show promise for the proposed sensitivity method. One method of missing data generation is also new and interesting in its own right."		"Jamshidian, M|Yuan, KH"	JOURNAL OF STATISTICAL COMPUTATION AND SIMULATION	factor analysis generating missing data incomplete data missing at random missing not at random sensitivity analysis simulation sub-sample	10.1080/00949655.2012.660486
216	WOS:000317821300006	2013	Sensitivity Analysis of Multiple Informant Models When Data Are Not Missing at Random	MARITAL SATISFACTION DROP-OUT SYMPTOMS ADOPTION PARENT CHILD	"Missing data are common in studies that rely on multiple informant data to evaluate relationships among variables for distinguishable individuals clustered within groups. Estimation of structural equation models using raw data allows for incomplete data, and so all groups can be retained for analysis even if only  member of a group contributes data. Statistical inference is based on the assumption that data are missing completely at random or missing at random. Importantly, whether or not data are missing is assumed to be independent of the missing data. A saturated correlates model that incorporates correlates of the missingness or the missing data into an analysis and multiple imputation that might also use such correlates offer advantages over the standard implementation of SEM when data are not missing at random because these approaches could result in a data analysis problem for which the missingness is ignorable. This article considers these approaches in an analysis of family data to assess the sensitivity of parameter estimates and statistical inferences to assumptions about missing data, a strategy that could be easily implemented using SEM software."		"Blozis, SA|Ge, XJ|Xu, S|Natsuaki, MN|Shaw, DS|Neiderhiser, JM|Scaramella, LV|Leve, LD|Reiss, D"	STRUCTURAL EQUATION MODELING-A MULTIDISCIPLINARY JOURNAL	auxiliary variables missing data missing not at random multiple imputation multiple informant data	10.1080/10705511.2013.769393
217	WOS:000299625100003	2012	A topological optimization approach for structural design of a high-speed low-load mechanism using the equivalent static loads method	DYNAMIC LOADS SYSTEMS ALGORITHM	"In high-speed low-load mechanisms, the principal loads are the inertial forces caused by the high accelerations and velocities. Hence, mechanical design should consider lightweight structures to minimize such loads. In this paper, a topological optimization method is presented on the basis of the equivalent static loads method. Finite element (FE) models of the mechanism in different positions are constructed, and the equivalent loads are obtained using flexible multibody dynamics simulation. Kinetic DOFs are used to simulate the motion joints, and a quasi-static analysis is performed to obtain the structural responses. The element sensitivity is calculated according to the static-load-equivalent equilibrium, in such a way that the influence on the inertial force is considered. A dimensionless component sensitivity factor (strain energy caused by unit load divided by kinetic energy from unit velocity) is used, which quantifies the significance of each element. Finally, the topological optimization approach is presented on the basis of the evolutionary structural optimization method, where the objective is to find the maximum ratio of strain energy to kinetic energy. In order to show the efficiency of the presented method, we presented two numerical cases. The results of these analyses show that the presented method is more efficient and can be easily implemented in commercial FE analysis software."	" Copyright (C) 2011 John Wiley & Sons, Ltd."	"Yang, ZJ|Chen, X|Kelly, R"	INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING	topological optimization high-speed low-load mechanism equivalent static loads method quasi-static analysis sensitivity analysis	10.1002/nme.3253
218	WOS:000313918200054	2013	Stochastic approach to municipal solid waste landfill life based on the contaminant transit time modeling using the Monte Carlo (MC) simulation	EARTHEN BARRIERS COMPACTED CLAY SATURATED SOIL TRANSPORT MIGRATION DIFFUSION SORPTION SITES DECAY	"The paper is concerned with application and benefits of MC simulation proposed for estimating the life of a modern municipal solid waste (MSW) landfill. The software Crystal Ball (R) (CB), simulation program that helps analyze the uncertainties associated with Microsoft (R) Excel models by MC simulation, was proposed to calculate the transit time contaminants in porous media. The transport of contaminants in soil is represented by the one-dimensional (D) form of the advection-dispersion equation (ADE). The computer program CONTRANS written in MATLAB language is foundation to simulate and estimate the thickness of landfill compacted clay liner. In order to simplify the task of determining the uncertainty of parameters by the MC simulation, the parameters corresponding to the expression Z taken from this program were used for the study. The tested parameters are: hydraulic gradient (HG), hydraulic conductivity (HC), porosity (POROS), linear thickness (TH) and diffusion coefficient (EDC). The principal output report provided by CB and presented in the study consists of the frequency chart, percentiles summary and statistics summary. Additional CB options provide a sensitivity analysis with tornado diagrams. The data that was used include available published figures as well as data concerning the Mittal Steel Poland (MSP) S.A. in Krakow, Poland. This paper discusses the results and show that the presented approach is applicable for any MSW landfill compacted clay liner thickness design."	 (C) 2012 Elsevier B.V. All rights reserved.	"Bieda, B"	SCIENCE OF THE TOTAL ENVIRONMENT	poland mc simulation cb (r) sensitivity analysis advection dispersion equation stochastic process	10.1016/j.scitotenv.2012.10.032
219	WOS:000233938100033	2005	Utility of dynamic-landscape metapopulation models for sustainable forest management	BIRDS INDICATORS VIABILITY FRAGMENTATION BIODIVERSITY FIRE	"We evaluated the utility of combining metapopulation models with landscape-level forest-dynamics models to assess the sustainability of forest management practices. We used the Brown Creeper (Certhia americana) in the boreal forests of northern Ontario as a case study. We selected the Brown Creeper as a potential indicator of sustainability because it is relatively common in the region but is dependent on snags and old trees for nesting and foraging; hence, it may be sensitive to timber harvesting. For the modeling we used RAMAS Landscape, a software package that integrates RAMAS GIS, population-modeling software, and LANDIS, forest-dynamics modeling software. Predictions about the future floristic composition and structure of the landscape tinder a variety of management and natural disturbance scenarios were derived using LANDIS. We modeled eight alternative forest management scenarios, ranging in intensity from no timber harvesting and a natural fire regime to intensive timber harvesting with salvage logging after fire. We predicted the response of The Brown Creeper metapopulation over a -year period and used future population size and expected minimum population size to compare the sustainability of the various management scenarios. The modeling methods were easy to apply and model predictions were sensitive to the differences among management scenarios, indicating that these methods may be useful for assessing and ranking the sustainability of forest management options. Primary concerns about the method are the practical difficulties associated with incorporating fire stochasticity in prediction uncertainty and the number of model assumptions that must be made and tested with sensitivity analysis. We wrote new software to bell) quantify the contribution of landscape stochasticity to model prediction uncertainty."		"Wintle, BA|Bekessy, SA|Venier, LA|Pearce, JL|Chisholm, RA"	CONSERVATION BIOLOGY	brown creeper landscape ecology population model population viability analysis succession model	10.1111/j.1523-1739.2005.00276.x
221	WOS:000387850900012	2016	Two-scale topology design optimization of stiffened or porous plate subject to out-of-plane buckling constraint	STRUCTURAL OPTIMIZATION HOMOGENIZATION METHOD HIERARCHICAL-OPTIMIZATION OPTIMUM STRUCTURE MICROSTRUCTURES PENALIZATION STIFFNESS LAYOUT BEAM	"This paper studies maximum out-of-plane buckling load design of thin bending plates for a given amount of material. Two kinds of plates are considered. One is made of periodic homogeneous porous material. Another is uniformly stiffened solid plate. The plate material, thickness, design domain of its middle plane and boundary conditions are given. The pattern of prescribed in-plane external load or displacements along the part of boundaries, which move freely, is given. Both plate topology and micro-structural topology of porous material or stiffener layout are concurrently optimized. The artificial element material densities in both macro and micro-scale are chosen as design variables. The volume preserving nonlinear density filter is applied to obtain the black-white optimum topology and comparison of its different sensitivities is made to show the reason for oscillation during optimization process in Appendix. The new numerical implementation of asymptotic homogenization method (NIAH, Cheng (Acta Mech Sinica (): -, ) and Cai (Int J Solids Struct (), -, ) is applied to homogenization of periodic plate structures and analytic sensitivity analysis of effective stiffness with respect to the topological design variables in both macro-scale and micro-scale. On basis of that, this paper implements the sensitivity analysis of out-of-plane buckling load by using commercial FEA software and enables the application of gradient-based search algorithm in optimization. Several numerical implementation details are discussed. Three numerical examples are given to show the validity of this method."		"Cheng, GD|Xu, L"	STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION	two-scale analysis concurrent topology optimization buckling constraints niah method	10.1007/s00158-016-1542-y
226	WOS:000276271600005	2010	Application of generic data assimilation tools (DATools) for flood forecasting purposes		"This paper describes the generic data assimilation software tool DATools. DATools can be used as standalone or within Delft-FEWS. DATools is completely configurable via XML configuration. DATools is built up of three components: a Filter, a Stochastic Modeler, and a Stochastic Observer. Configuration of all these three parts is explained in detail. At the moment two data assimilation filters are available within DATools: () ensemble Kalman Filter and () the residual resampling filter. Results of a twin experiment with both filters with DATtools show similar results as a previous study performed with custom implementations. It is also shown that DATools can function inside Delft-FEWS software used for operational flood forecasting. Applying EnKF to a D hydrodynamic SOBEK-RE model of the river Rhine within the operational system FEWS-NL Rhine and Meuse improves the forecasts at the Lobith gaugin station and downstream of Lobith. DATools has been coupled with the HBV-, SOBEK, and REW models and will be coupled to MODFLOW, Delft-D, and the geotechnical model MSetlle in the near future. Uncertainty analysis with this tool is also possible and calibration will be added later this year. (C) "	 Elsevier Ltd. All rights reserved.	"Weerts, AH|El Serafy, GY|Hummel, S|Dhondia, J|Gerritsen, H"	COMPUTERS & GEOSCIENCES	data assimilation hydrology ensemble kalman filter residual resampling filter operational system delft-fews	10.1016/j.cageo.2009.07.009
228	WOS:000318057900003	2013	A model-independent Particle Swarm Optimisation software for model calibration	RAINFALL-RUNOFF MODELS SHUFFLED COMPLEX EVOLUTION PAMPA DEL TAMARUGAL GLOBAL OPTIMIZATION SENSITIVITY-ANALYSIS DIFFERENTIAL EVOLUTION UNCERTAINTY ASSESSMENT REGIONAL AQUIFER PARAMETERS ALGORITHM	"This work presents and illustrates the application of hydroPSO, a novel multi-OS and model-independent R package used for model calibration. hydroPSO allows the modeller to perform a standard modelling work flow including, sensitivity analysis, parameter calibration, and assessment of the calibration results, using a single piece of software. hydroPSO implements several state-of-the-art enhancements and fine-tuning options to the Particle Swarm Optimisation (PSO) algorithm to meet specific user needs. hydroPSO easily interfaces the calibration engine to different model codes through simple ASCII files and/or R wrapper functions for exchanging information on the calibration parameters. Then, optimises a user-defined goodness-of-fit measure until a maximum number of iterations or a convergence criterion are met. Finally, advanced plotting functionalities facilitate the interpretation and assessment of the calibration results. The current hydroPSO version allows easy parallelization and works with single-objective functions, with multi-objective functionalities being the subject of ongoing development. We compare hydroPSO against standard algorithms (SCE_UA, DE, DREAM, SPSO-, and GML) using a series of benchmark functions. We further illustrate the application of hydroPSO in two real-world case studies: we calibrate, first, a hydrological model for the Ega River Basin (Spain) and, second, a groundwater flow model for the Pampa del Tamarugal Aquifer (Chile). Results from the comparison exercise indicate that hydroPSO is: i) effective and efficient compared to commonly used optimisation algorithms, ii) ""scalable"", i.e. maintains a high performance for increased problem dimensionality, and iii) versatile to adapt to different response surfaces of the objective function. Case study results highlight the functionality and ease of use of hydroPSO to handle several issues that are commonly faced by the modelling community such as: working on different operating systems, single or batch model execution, transient- or steady-state modelling conditions, and the use of alternative goodness-of-fit measures to drive parameter optimisation. Although we limit the application of hydroPSO to hydrological models, flexibility of the package suggests it can be implemented in a wider range of models requiring some form of parameter optimisation. (C) "	 Elsevier Ltd. All rights reserved.	"Zambrano-Bigiarini, M|Rojas, R"	ENVIRONMENTAL MODELLING & SOFTWARE	global optimisation evolutionary algorithm surface water modelling groundwater modelling swat-2005 modflow-2005 r	10.1016/j.envsoft.2013.01.004
229	WOS:000388119400013	2016	ASSESSMENT OF INDUSTRIAL SOLID WASTE USING THE INTELLIGENT DECISION SYSTEM (IDS) METHOD	EVIDENTIAL REASONING APPROACH MANAGEMENT UNCERTAINTY SAFETY	"About  tons of daily solid waste disposal is one of the consequences of the speedy industrial expansion in the province of Khuzestan in the south west of Iran. There are more often than not diverse criteria for assessing the resulted pollution loads from solid waste disposal. In this paper, a new application for the Intelligent Decision System (IDS) is demonstrated for industrial solid waste assessment. IDS software is a Windows-based package for handling Multiple Criteria Decision Making (MCDM) problems considering both qualitative and quantitative criteria under uncertainties. The basis of IDS is a recently developed theory named the Evidential Reasoning ( ER) approach. The major features, superiority and excellence of IDS will be clarified through its application to the ranking of the industrial units located in the Khuzestan Province. Moreover, as a complimentary assessment, a sensitivity analysis is carried out in which the effect of decision maker's attitude toward risk on the total utility which each industry would gain is investigated. The results show that Ahwaz, Abadan, and Khoramshahr are respectively the three most polluting cities in the Khuzestan province. It is concluded that IDS can be utilized not only to handle problems which traditional methods can work out, but also to arrange and evaluate more difficult decision problems that traditional methods are not sufficiently expert of handling."		"Abed-Elmdoust, A|Kerachian, R"	ENVIRONMENTAL ENGINEERING AND MANAGEMENT JOURNAL	evidential reasoning approach industrial solid waste assessment intelligent decision system khuzestan province multiple-criteria-decision-making (mcdm)	10.30638/eemj.2016.191
230	WOS:000306623200008	2012	Sensitivity analysis of some critical factors affecting simulated intrusion volumes during a low pressure transient event in a full-scale water distribution system	HYDRAULIC TRANSIENTS LEAKAGE PIPES	"Intrusion events caused by transient low pressures may result in the contamination of a water distribution system (DS). This work aims at estimating the range of potential intrusion volumes that could result from a real downsurge event caused by a momentary pump shutdown. A model calibrated with transient low pressure recordings was used to simulate total intrusion volumes through leakage orifices and submerged air vacuum valves (AVVs). Four critical factors influencing intrusion volumes were varied: the external head of (untreated) water on leakage orifices, the external head of (untreated) water on submerged air vacuum valves, the leakage rate, and the diameter of AVVs' outlet orifice (represented by a multiplicative factor). Leakage orifices' head and AVVs' orifice head levels were assessed through fieldwork. Two sets of runs were generated as part of two statistically designed experiments. A first set of  runs was based on a complete factorial design in which each factor was varied over  levels. A second set of  runs was based on a latin hypercube design, better suited for experimental runs on a computer model. The simulations were conducted using commercially available transient analysis software. Responses, measured by total intrusion volumes, ranged from  to  L. A second degree polynomial was used to analyze the total intrusion volumes. Sensitivity analyses of both designs revealed that the relationship between the total intrusion volume and the four contributing factors is not monotonic, with the AVVs' orifice head being the most influential factor. When intrusion through both pathways occurs concurrently, interactions between the intrusion flows through leakage orifices and submerged AVVs influence intrusion volumes. When only intrusion through leakage orifices is considered, the total intrusion volume is more largely influenced by the leakage rate than by the leakage orifices' head. The latter mainly impacts the extent of the area affected by intrusion. (C) "	 Elsevier Ltd. All rights reserved.	"Ebacher, G|Besner, MC|Clement, B|Prevost, M"	WATER RESEARCH	water distribution system intrusion volumes sensitivity analysis transient analysis downsurge event	10.1016/j.watres.2012.05.006
231	WOS:000400267300014	2017	Hierarchical approach to hydrological model calibration	WATER-QUALITY MODELS UNCERTAINTY ANALYSIS RIVER-BASIN SWAT OPTIMIZATION FLOW EQUIFINALITY PREDICTIONS SENSITIVITY VALIDATION	"Hydrological models have been widely used for water resources management. Successful application of hydrological models depends on careful calibration and uncertainty analysis. Spatial unit of water balance calculations may differ widely in different models from grids to hydrological response units (HRU). The Soil and Water Assessment Tool (SWAT) software uses HRU as the spatial unit. SWAT simulates hydrological processes at sub-basin level by deriving HRUs by thresholding areas of soil type, land use, and slope combinations. This may ignore some important areas, which may have great impact on hydrological processes in the watershed. In this study, a hierarchical HRU approach was developed in order to increase model performance and reduce computational complexity simultaneously. For hierarchical optimization, HRUs are first divided into two-HRU types and are optimized with respect to some relevant influence parameters. Then, each HRU is further divided into two. Each child HRU inherits the optimum parameter values of the parent HRU as its initial value. This approach decreases the total calibration time while obtaining a better result. The performance of the hierarchical methodology is demonstrated on two basins, namely Sarisu-Eylikler and Namazgah Dam Lake Basins in Turkey. In Sarisu-Eylikler, we obtained good results by a combination of curve number (CN), soil hydraulic conductivity, and slope for generating HRUs, while in Namazgah use of only CN gave better results."		"Ozdemir, A|Leloglu, UM|Abbaspour, KC"	ENVIRONMENTAL EARTH SCIENCES	swat calibration hydrological response unit sufi2 optimization	10.1007/s12665-017-6560-6
232	WOS:000260992800007	2009	The OECD software tool for screening chemicals for persistence and long-range transport potential	ORGANIC-CHEMICALS MULTIMEDIA MODELS SPATIAL RANGE UNCERTAINTY BEHAVIOR FATE	"We present the software implementation of The OECD P-OV & LRTP Screening Tool (The Tool) that is used to assess the environmental hazard of organic chemicals using metrics of overall persistence (P-OV) and long-range transport potential (LRTP). The Tool is designed to support decision making for chemical management and includes features that are recommended by the Organization for Economic Cooperation and Development (OECD) expert group on multimedia modeling. The Tool is useful for screening the environmental hazard potential of non-ionizing organic chemicals whose environmental partitioning can be described by absorptive capacities of environmental media estimated from partitioning between air, water and octanol in the laboratory. The software includes data storage functionality, and a user interface that is designed to facilitate simple data input and straightforward interpretation of the model results. The effect of uncertainties in input properties describing chemicals can be assessed with a Monte Carol analysis. The software is evaluated and illustrated by comparing results from The Tool with those from other models and by evaluating four substances that are candidates for regulation or ban under the Stockholm convention on Persistent Organic Pollutants. (C) "	 Elsevier Ltd. All rights reserved.	"Wegmann, F|Cavin, L|MacLeod, M|Scheringer, M|Hungerbuhler, K"	ENVIRONMENTAL MODELLING & SOFTWARE	chemicals persistent organic pollutants multimedia modeling hazard assessment uncertainty analysis decision support	10.1016/j.envsoft.2008.06.014
233	WOS:000088879600005	2000	Scales and similarities in runoff processes with respect to geomorphometry	SPATIAL VARIABILITY CATCHMENT SOIL	"Numerous investigations using various techniques have been carried out towards a more detailed understanding of relationships and interactions between catchment morphometry and rainfall-runoff processes. Recently, this research question has become more relevant through the need for accurate, yet simple, computer models simulating the water balance of large areas. Moreover, advances in the analysis of landform morphometry through the availability of high-resolution digital elevation models (DEMs) and powerful geographical information systems (GIS) have enhanced research efforts with this aim. In this study several computer techniques and models were applied to investigate the effects of geomorphometry on rainfall-runoff processes at different scales. The sensitivity of dynamic hydrological processes to comparatively static boundary conditions requires different methods for modelling, analysis and visualization of different kinds of data appropriate to different scales. Therefore an approach integrating several geocomputational concepts, including spatial analysis of different types of geodata, static modelling of spatial structures, dynamic four-dimensional modelling of hydrological processes and statistical techniques was chosen. Geomorphometric analysis of the study sites was carried out with GIS packages (including ARC/INFO and GRASS), special purpose software and self-developed tools. Soil-morphometry relationships were modelled within a GIS environment. Hydrological models (SAKE and TOPMODEL) were then used to simulate rainfall-runoff processes, and finally statistical tools and sensitivity analysis were applied to gain an insight into the hydrological significance of the various geomorphometric properties. The results demonstrate the importance of small subregions of the catchment, particularly those having low slope angles, low flow lengths and concavities. The spatial distribution of soil types significantly influences modelled runoff. Spatial distributions of soil types are partly related to morphometry and can be captured using soil-morphometry models. Further results show that catchments which differ significantly in morphometry show different runoff responses and different hydrological sensitivity to changes in boundary conditions. A crude derivation of geomorphometric-hydrological landform types could be reached. Therefore, geomorphometric classifications of catchment types could form a basis for representative hydrological modelling at the large scale. Models describing soil distribution in relation to geomorphometry could assist regionalization of spatial heterogeneity and structure of soil parameters relevant in hydrological modelling. Moreover, quantification of geomorphometric catchment structure, e.g. in terms of contributing areas, is needed to describe significant geomorphometric catchment characteristics."	" Copyright (C) 2000 John Wiley & Sons, Ltd."	"Schmidt, J|Hennrich, K|Dikau, R"	HYDROLOGICAL PROCESSES	hydrological modelling geomorphometry gis dem soil-morphometry relationship	10.1002/1099-1085(20000815/30)14:11/12<1963::AID-HYP48>3.0.CO;2-M
234	WOS:000087885500003	2000	Buckling design optimization of complex built-up structures with shape and size variables	SENSITIVITY ANALYSIS	"The design optimization of buckling behaviour is studied for complex built-up structures composed of various kinds of elements and implemented within JIFEX, a general-purpose software for finite element analysis and design optimization. The direct and adjoint methods of sensitivity analysis for critical buckling loads are presented with detailed computational procedures. Particularly, the variations of prebuckling stresses and external loads have been accounted, for. The design model and solution methods presented in this paper are available for both shape and size optimization, and buckling optimization can also be combined with static, frequency and dynamic response optimization. The numerical examples show the applications of the buckling optimization method and the effectiveness of the methods and the program of this paper."		"Gu, YX|Zhao, GZ|Zhang, HW|Kang, Z|Grandhi, RV"	STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION	shape optimization size optimization buckling natural frequency dynamic response built-up structures	10.1007/s001580050101
241	WOS:000386560600005	2016	SOFTWARE RELIABILITY GROWTH MODEL WITH TEMPORAL CORRELATION IN A NETWORK ENVIRONMENT	CHANGE-POINT NOISE	"Increasingly software systems are developed to provide great flexibility to customers but also introduce great uncertainty for system development. The uncertain behavior of fault-detection rate has irregular fluctuation and is described as a Markovian stochastic processes (white noise). However, in many cases the white noise idealization is insufficient, and real fluctuations are always correlated and correlated fluctuations (or colored noise) are non-Markovian stochastic processes. We develop a new model to quantify the uncertainties within non-homogeneous Poisson process (NHPP) in the noisy environment. Based on a stochastic model of the software fault detection process, the environmental uncertainties collectively are treated as a noise of arbitrary distribution and correlation structure. Based on the stochastic model, the analytical solution can be derived. To validate our model, we consider five particular scenarios with distinct environmental uncertainty. Experimental comparisons with existing methods demonstrate that the new framework shows a closer fitting to actual data and exhibits a more accurately predictive power."		"Xu, JJ|Yao, SZ|Yang, SK|Wang, P"	INTERNATIONAL JOURNAL FOR UNCERTAINTY QUANTIFICATION	uncertainty quantification reliability nhpp noise correlation	10.1615/Int.J.UncertaintyQuantification.2016016194
243	WOS:000295845300022	2011	Modeling the effects of completion techniques and formation heterogeneity on CO2 sequestration in shallow and deep saline aquifers	STORAGE	"This work studied the effect of completion techniques and reservoir heterogeneity on CO storage and injectivity in saline aquifers using a compositional reservoir simulator, CMG-GEM. Two reservoir models were built based on the published data to represent a deep saline aquifer and a shallow aquifer. The effect of various completion conditions on CO storage was then discussed, including partial perforation of the reservoir net pay (partial completion), well geometry, orientation, location, and length. The heterogeneity effect was addressed by considering three parameters: mean permeability, the vertical to horizontal permeability ratio, and permeability variation. Sensitivity analysis was carried out using iSIGHT software (design of experiments) to determine the dominant factors affecting CO storage capacity and injectivity. Simulation results show that the most favorable option is the perforation of all layers with horizontal wells - m long set in the upper layers. Mean permeability has the most effect on CO storage capacity and injectivity; k(v)/k(h) affects CO injectivity storage capacity more than permeability variation, V-k. More CO can be stored in the heterogeneous reservoirs with low mean permeability; however, high injectivity can be achieved in the uniform reservoirs with high mean permeability."		"Yang, F|Bai, BJ|Dunn-Norman, S"	ENVIRONMENTAL EARTH SCIENCES	co2 sequestration saline aquifer completion heterogeneity reservoir simulation	10.1007/s12665-011-0908-0
244	WOS:000353971300004	2015	Enhancing the Characterization of Epistemic Uncertainties in PM2.5 Risk Analyses	PARTICULATE AIR-POLLUTION LONG-TERM EXPOSURE UNITED-STATES FOLLOW-UP 6 CITIES MORTALITY FINE COHORT VETERANS QUALITY	"The Environmental Benefits Mapping and Analysis Program (BenMAP) is a software tool developed by the U.S. Environmental Protection Agency (EPA) that is widely used inside and outside of EPA to produce quantitative estimates of public health risks from fine particulate matter (PM.). This article discusses the purpose and appropriate role of a risk analysis tool to support risk management deliberations, and evaluates the functions of BenMAP in this context. It highlights the importance in quantitative risk analyses of characterization of epistemic uncertainty, or outright lack of knowledge, about the true risk relationships being quantified. This article describes and quantitatively illustrates sensitivities of PM. risk estimates to several key forms of epistemic uncertainty that pervade those calculations: the risk coefficient, shape of the risk function, and the relative toxicity of individual PM. constituents. It also summarizes findings from a review of U.S.-based epidemiological evidence regarding the PM. risk coefficient for mortality from long-term exposure. That review shows that the set of risk coefficients embedded in BenMAP substantially understates the range in the literature. We conclude that BenMAP would more usefully fulfill its role as a risk analysis support tool if its functions were extended to better enable and prompt its users to characterize the epistemic uncertainties in their risk calculations. This requires expanded automatic sensitivity analysis functions and more recognition of the full range of uncertainty in risk coefficients."		"Smith, AE|Gans, W"	RISK ANALYSIS	benmap epidemiology health risk pm2 5 risk analysis uncertainty	10.1111/risa.12236
246	WOS:000090149600007	2000	An object-oriented structural optimization program	SHAPE OPTIMIZATION	"In this paper, implementation concepts of a structural optimization software using object-oriented programming (OOP) in CS ++ is presented. A brief mathematical formulation of structural optimization and continuum-based sensitivity analysis is presented. The requirements of a computational optimization environment are derived from this formulation. The OOP characteristics are analysed and this paradigm is employed in the implementation of design variables, structural performance functionals, velocity fields, design model and mathematical programming algorithms using CI-Jr. Finally, the program obtained is applied to D linear elastic examples of sizing and shape optimization."		"Silva, CAC|Bittencourt, ML"	STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION	structural optimization sensitivity analysis c plus object-oriented programming linear elasticity	10.1007/s001580050146
248	WOS:000356741300001	2015	Multi-objective model auto-calibration and reduced parameterization: Exploiting gradient-based optimization tool for a hydrologic model	AUTOMATIC CALIBRATION MULTICRITERIA METHODS SENSITIVITY-ANALYSIS GLOBAL OPTIMIZATION SOIL-MOISTURE SWAT MODULE INFORMATION VALIDATION ALGORITHM	"Multi-objective model optimization methods have been extensively studied based on evolutionary algorithms, but less on gradient-based algorithms. This study demonstrates a framework for multi-objective model calibration/optimization using gradient-based optimization tools. Model-independent software Parameter ESTimation (PEST) was used to auto-calibrate ISWAT, a modified version of the distributed hydrologic model Soil and Water Assessment Tool (SWAT), in the Shenandoah River watershed. The time-series processor TSPROC was used to combine multiple objectives into the auto-calibration process. Two sets of roughness coefficients for main channels, one assigned and calibrated according on soil types and one determined via empirical equations, were examined for stream discharge simulation. Five different weighting alternatives were investigated for their effects on ISWAT calibrations. Results showed that using Manning's roughness coefficients obtained from empirical equations improves simulation results and calibration efficiency. Applying a two-step weighting alternative to different observation groups would provide the best calibration results. (C) "	 Elsevier Ltd. All rights reserved.	"Wang, Y|Brubaker, K"	ENVIRONMENTAL MODELLING & SOFTWARE	pest swat tsproc multi-objectives auto-calibration	10.1016/j.envsoft.2015.04.001
251	WOS:000224375900007	2004	A tool for risk-based management of surface water quality	SENSITIVITY-ANALYSIS UNCERTAINTY MODELS EUTROPHICATION PREDICTION PHOSPHORUS SYSTEMS FUTURE	"Water quality Risk Analysis Tool (WaterRAT) is software for supporting decision-making in surface water quality management. The philosophy behind the software is that uncertainty in water quality model predictions is inevitably high due to model equation error, parameter error, and limited definition of boundary conditions and management objectives. Using sensitivity and uncertainty analyses based on Monte Carlo simulation and first order methods, WaterRAT allows the modeller to identify the significant uncertainties, and evaluate the degree to which they control decision-making risk. WaterRAT has a library of river and lake water quality models of varying complexity, and these can be applied at a wide range of temporal and spatial scales, allowing the model design to be responsive to both the modelling task and the data constraints. (C) "	 Elsevier Ltd. All rights reserved.	"McIntyre, NR|Wheater, HS"	ENVIRONMENTAL MODELLING & SOFTWARE	water quality uncertainty risk decision-making	10.1016/j.envsoft.2003.12.003
255	WOS:000303381300019	2012	Chloride migration in groundwater for a tannery belt in Southern India	NATURAL RECHARGE AQUIFER PARAMETERS SOLUTE TRANSPORT TAMIL-NADU TERRAIN BASIN WATER SITE	"Groundwater in a tannery belt in Southern India is being polluted by the discharge of untreated effluents from  operating tanneries. Total dissolved solids and chloride (Cl-) measurements in open wells in the tannery cluster vary from , to , and , to , mg/l, respectively. A mass transport model was constructed using Visual MODFLOW Premium . software to investigate the chloride migration in an area of . km(). Input to the chloride migration model was a groundwater flow model that considered steady and transient conditions. This model was calibrated with field observations; and sensitivity analysis was carried out whereby model parameters, viz., conductivity, dispersivity, and source concentration were altered slightly, and the effect on calibration statistics was evaluated. Results indicated that hydraulic conductivity played a more sensitive role than did dispersivity. The Cl- migration was mainly through advection rather than dispersion. It was found that even if the pollutant load reduced to % of the present level, the Cl- concentration in groundwater, even after  years, would not be reduced to the permissible limit of drinking water in the tannery belt."		"Mondal, NC|Singh, VP"	ENVIRONMENTAL MONITORING AND ASSESSMENT	shallow aquifer tannery industry groundwater pollution chloride migration southern india	10.1007/s10661-011-2156-x
256	WOS:000255415000009	2008	Estimation of process parameter variations in a pre-defined process window using a Latin hypercube method		"The aim of this paper is to present a methodology that provides an analytical tool for estimation of robustness and response variation within a pre-defined process window. To exemplify the developed methodology, the stochastic simulation technique is used for a sheet-metal forming application. A sampling plan based on the Latin hypercube sampling method for variation of design parameters is utilized, and the thickness reduction is specified as the response. Moreover, the response surface methodology is applied for understanding the quantitative relationship between design parameters and response value. The conclusions of this study are that the applied method gives a possibility to illustrate and interpret the variation of the response versus a design parameter variation. Consequently, it gives significant insights into the usefulness of individual design parameters. It has been shown that the method enables us to estimate the admissible design parameter variations and to predict the actual safe margin for given process parameters. Furthermore, the dominating design parameters can be predicated using sensitivity analysis, and this in its turn clarifies how the reliability criteria are met. Finally, the developed software can be used as an additional module for set-up of stochastic finite element simulations and to collect the numerical results from different solvers within different applications."		"Moshfegh, R|Nilsson, L|Larsson, M"	STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION	stochastical analysis sensitivity indicator admissible process parameter variation finite element method sheet-metal forming	10.1007/s00158-007-0136-0
257	WOS:000317749400002	2013	Estimation of uncertainty sources in the projections of Lithuanian river runoff	CLIMATE-CHANGE IMPACTS FLOOD RISK-ASSESSMENT PARAMETER MODELS	"Particular attention is given to the reliability of hydrological modelling results. The accuracy of river runoff projection depends on the selected set of hydrological model parameters, emission scenario and global climate model. The aim of this article is to estimate the uncertainty of hydrological model parameters, to perform sensitivity analysis of the runoff projections, as well as the contribution analysis of uncertainty sources (model parameters, emission scenarios and global climate models) in forecasting Lithuanian river runoff. The impact of model parameters on the runoff modelling results was estimated using a sensitivity analysis for the selected hydrological periods (spring flood, winter and autumn flash floods, and low water). During spring flood the results of runoff modelling depended on the calibration parameters that describe snowmelt and soil moisture storage, while during the low water period-the parameter that determines river underground feeding was the most important. The estimation of climate change impact on hydrological processes in the Merkys and Neris river basins was accomplished through the combination of results from AB, A and B emission scenarios and global climate models (ECHAM and HadCM). The runoff projections of the thirty-year periods (-, -, -) were conducted applying the HBV software. The uncertainties introduced by hydrological model parameters, emission scenarios and global climate models were presented according to the magnitude of the expected changes in Lithuanian rivers runoff. The emission scenarios had much greater influence on the runoff projection than the global climate models. The hydrological model parameters had less impact on the reliability of the modelling results."		"Kriauciuniene, J|Jakimavicius, D|Sarauskiene, D|Kaliatka, T"	STOCHASTIC ENVIRONMENTAL RESEARCH AND RISK ASSESSMENT	lithuanian rivers climate change hbv model calibration sensitivity and uncertainty analysis susa	10.1007/s00477-012-0608-7
259	WOS:000246812000012	2007	Comparing sensitivity analysis methods to advance lumped watershed model identification and evaluation	SURFACE PARAMETERIZATION SCHEMES DISTRIBUTED MODEL MULTIOBJECTIVE OPTIMIZATION ENVIRONMENTAL SYSTEMS HYDROLOGICAL MODELS UNCERTAINTY CALIBRATION EFFICIENT PROJECT OUTPUT	"This study seeks to identify sensitivity tools that will advance our understanding of lumped hydrologic models for the purposes of model improvement, calibration efficiency and improved measurement schemes. Four sensitivity analysis methods were tested: () local analysis using parameter estimation software (PEST), () regional sensitivity analysis (RSA), () analysis of variance (ANOVA), and () Sobol's method. The methods' relative efficiencies and effectiveness have been analyzed and compared. These four sensitivity methods were applied to the lumped Sacramento soil moisture accounting model (SAC-SMA) coupled with SNOW-. Results from this study characterize model sensitivities for two medium sized watersheds within the Juniata River Basin in Pennsylvania, USA. Comparative results for the  sensitivity methods are presented for a -year time series with  h,  h, and  h time intervals. The results of this study show that model parameter sensitivities are heavily impacted by the choice of analysis method as well as the model time interval. Differences between the two adjacent watersheds also suggest strong influences of local physical characteristics on the sensitivity methods' results. This study also contributes a comprehensive assessment of the repeatability, robustness, efficiency, and ease-of-implementation of the four sensitivity methods. Overall ANOVA and Sobol's method were shown to be superior to RSA and PEST. Relative to one another, ANOVA has reduced computational requirements and Sobol's method yielded more robust sensitivity rankings."		"Tang, Y|Reed, P|Wagener, T|van Werkhoven, K"	HYDROLOGY AND EARTH SYSTEM SCIENCES		10.5194/hess-11-793-2007
261	WOS:000263990600027	2009	Life cycle assessment study of a Chinese desktop personal computer	POLYBROMINATED DIPHENYL ETHERS MOBILE PHONE NETWORKS E-WASTE IMPACT ASSESSMENT FLOW	"Associated with the tremendous prosperity in world electronic information and telecommunication industry, there continues to be an increasing awareness of the environmental impacts related to the accelerating mass production, electricity use, and waste management of electronic and electric products (e-products). China's importance as both a consumer and supplier of e-products has grown at an unprecedented pace in recent decade. Hence, this paper aims to describe the application of life cycle assessment (LCA) to investigate the environmental performance of Chinese e-products from a global level. A desktop personal computer system has been selected to carry out a detailed and modular LCA which follows the ISO  series. The LCA is constructed by SimaPro, software version . and expressed with the Eco-indicator' life cycle impact assessment method. For a sensitivity analysis of the overall LCA results, the so-called CML method is used in order to estimate the influence of the choice of the assessment method on the result Life cycle inventory information is complied by ecoinvent . databases, combined with literature and field investigations on the present Chinese situation. The established LCA study shows that that the manufacturing and the use of such devices are of the highest environmental importance. In the manufacturing of such devices, the integrated circuits (Is) and the Liquid Crystal Display (LCD) are those parts contributing most to the impact. As no other aspects are taken into account during the use phase, the impact is due to the way how the electricity is produced. The final process steps - i.e. the end of life phase - lead to a clear environmental benefit if a formal and modem, up-to-date technical system is assumed, like here in this study."	 Crown Copyright (C) 2008 Published by Elsevier B.V. All rights reserved.	"Duan, HB|Eugster, M|Hischier, R|Streicher-Porte, M|Li, JH"	SCIENCE OF THE TOTAL ENVIRONMENT	lca personal computer environmental impact electronics china	10.1016/j.scitotenv.2008.10.063
262	WOS:000230708100004	2005	"Determination of the optimal installation capacity of small hydro-power plants through the use of technical, economic and reliability indices"		"One of the most important issues in planning Small Hydro-Power Plants (SHPPs) of the ""run-off river"" type is to determine the optimal installation capacity of the SHPP and estimate its optimal annual energy value. In this paper, a method to calculate the annual energy is presented, as is the program developed using Excel software. This program analyzes and estimates the most important economic indices of an SHPP using the sensitivity analysis method. Another program, developed by Matlab software, calculates the reliability indices for a number of units of an SHPP with a specified load duration curve using the Monte Carlo method. Ultimately, comparing the technical, economic and reliability indices will determine the optimal installation capacity of an SHPP. By applying the above-mentioned algorithm to a sample SHPP named ""Nari"" (located in the northern part of Iran), the optimal capacity of . MW is obtained. (c) "	 Elsevier Ltd. All rights reserved.	"Hosseini, SMH|Forouzbakhsh, F|Rahimpoor, M"	ENERGY POLICY	installation capacity small hydro-power plant (shpp) economic analysis monte carlo method	10.1016/j.enpol.2004.03.007
263	WOS:000240794000013	2006	Sensitivity analysis of differential-algebraic equations and partial differential equations	ADAPTIVE MESH REFINEMENT SYSTEMS SOFTWARE OPTIMIZATION ALGORITHMS	"Sensitivity analysis generates essential information for model development, design optimization, parameter estimation, optimal control, model reduction and experimental design. In this paper we describe the forward and adjoint methods for sensitivity analysis, and outline some of our recent work on theory, algorithms and software for sensitivity analysis of differential-algebraic equation (DAE) and time-dependent partial differential equation (PDE) systems. (c) "	 Elsevier Ltd. All rights reserved.	"Petzold, L|Li, ST|Cao, Y|Serban, R"	COMPUTERS & CHEMICAL ENGINEERING	sensitivity analysis differential-algebraic equations adjoint method	10.1016/j.compchemeng.2006.05.015
266	WOS:000354547700014	2015	Identification of Critical Erosion Watersheds for Control Management in Data Scarce Condition Using the SWAT Model	LEAST-SQUARES REGRESSION CRITICAL SUBWATERSHEDS SENSITIVITY-ANALYSIS SEDIMENT YIELD SOIL-EROSION PRONE AREAS CLIMATE CATCHMENT INDIA REQUIREMENTS	"Identification of critical watersheds prone to soil erosion has been performed by using a hydrological model in data scarce Damodar River catchment, located in Jharkhand state of India. Model is calibrated and validated for two watersheds, i.e.,()Nagwan, .km; and ()Banikdih, .km, nested within the catchment. The achieved R values of predicted monthly runoff and sediment yield varies, respectively, .-. and .-., for both the watersheds during calibration and validation period. Calibration and validation results revealed that model is predicting monthly runoff and sediment yield satisfactory for the two watersheds of the Damodar River catchment. The validated model parameters were then up-scaled to the whole catchment and model was run from - to identify the critical watersheds. Model was successfully used for prioritization of  watersheds delineated using the computer software model within the catchment. In delineation process, the boundaries of  watersheds matched exactly with the watersheds delineated manually by Damodar Valley Corporation. Out of these  watersheds, erosion classes of  exactly matched with the manually described erosion class. For remaining  watersheds, priority of  watersheds was either one class higher or lower, whereas eight watersheds showed complete mismatch. Overall results showed that the hydrological model used in this paper may be helpful in prioritization of management strategies to manage the resources where availability of data is a big concern. Such approach for managing resources is particularly needed in developing countries for better utilization of limited resources."		"Kumar, S|Mishra, A|Raghuwanshi, NS"	JOURNAL OF HYDROLOGIC ENGINEERING	soil and water assessment tool (swat) calibration validation watershed critical erosion identification prioritization	10.1061/(ASCE)HE.1943-5584.0001093
268	WOS:000408861800155	2017	Improving Thermal Comfort of Low-Income Housing in Thailand through Passive Design Strategies	TROPICAL HUMID REGION BUILDINGS STANDARDS ADAPTATION HOUSES	"In Thailand, the delivery of adequate low-income housing has historically been overshadowed by politics with cost and quantity being prioritised over quality, comfort and resilience. In a country that experiences hot and humid temperatures throughout the year, buildings need to be adaptable to the climate to improve the thermal comfort of inhabitants. This research is focused on identifying areas for improving the thermal performance of these housing designs. Firstly, dynamic thermal simulations were run on a baseline model using the adaptive thermal comfort model CIBSE TM for assessment. The three criteria defined in CIBSE TM were used to assess the frequency and severity of overheating in the buildings. The internal temperature of the apartments was shown to exceed the thermal comfort threshold for these criteria throughout the year. The internal operating daily temperatures of the apartment remain high, ranging from a maximum of . degrees C to a minimum of . degrees C. Based on these findings, five criteria were selected to be analysed for sensitivity to obtain the key parameters that influence the thermal performance and to suggest possible areas for improvement. The computer software package Integrated Environmental SolutionsVirtual Environment (IES-VE) was used to perform building energy simulations. Once the baseline conditions were identified, the software packages SimLab. and RStudio were used to carry out the sensitivity analysis. These results indicated that roof material and the presence of a balcony have the greatest influence on the system. Incorporating insulation into the roof reduced the mean number of days of overheating by .%. Removing the balcony increased the number of days of overheating by .% due to significant reductions in internal ventilation."		"Bhikhoo, N|Hashemi, A|Cruickshank, H"	SUSTAINABILITY	thermal comfort low income housing thailand tropical climates dynamic thermal simulations sensitivity analysis	10.3390/su9081440
269	WOS:000186310600006	2003	Direct and adjoint sensitivity analysis of chemical kinetic systems with KPP: Part I - theory and software tools	VARIATIONAL DATA ASSIMILATION ATMOSPHERIC CHEMISTRY PROBLEMS GREENS-FUNCTION METHOD NON-LINEAR SYSTEMS STIFF ODE SOLVERS ROSENBROCK METHOD MODELS IMPLEMENTATION IMPLICIT EXPLICIT	"The analysis of comprehensive chemical reactions mechanisms, parameter estimation techniques, and variational chemical data assimilation applications require the development of efficient sensitivity methods for chemical kinetics systems. The new release (KPP-.) of the kinetic preprocessor (KPP) contains software tools that facilitate direct and adjoint sensitivity analysis. The direct-decoupled method, built using BDF formulas, has been the method of choice for direct sensitivity studies. In this work, we extend the direct-decoupled approach to Rosenbrock stiff integration methods. The need for Jacobian derivatives prevented Rosenbrock methods to be used extensively in direct sensitivity calculations; however, the new automatic and symbolic differentiation technologies make the computation of these derivatives feasible. The direct-decoupled method is known to be efficient for computing the sensitivities of a large number of output parameters with respect to a small number of input parameters. The adjoint modeling is presented as an efficient tool to evaluate the sensitivity of a scalar response function with respect to the initial conditions and model parameters. In addition, sensitivity with respect to time-dependent model parameters may be obtained through a single backward integration of the adjoint model. KPP software may be used to completely generate the continuous and discrete adjoint models taking full advantage of the sparsity of the chemical mechanism. Flexible direct-decoupled and adjoint sensitivity code implementations are achieved with minimal user intervention. In a companion paper, we present an extensive set of numerical experiments that validate the KPP software tools for several direct/adjoint sensitivity applications, and demonstrate the efficiency of KPP-generated sensitivity code implementations. (C) "	 Elsevier Ltd. All rights reserved.	"Sandu, A|Daescu, DN|Carmichael, GR"	ATMOSPHERIC ENVIRONMENT	chemical kinetics sensitivity analysis direct-decoupled method adjoint model	10.1016/j.atmosenv.2003.08.019
271	WOS:000249895700008	2007	Automatic concept model generation for optimisation and robust design of passenger cars		"A fully automated method of structural optimisation for the body in white structure is presented. The body in white is a technical term for the car body without windows and closures. The iterations in the optimisation loop comprise the following steps: fully parameterised design creation, automated meshing and model assembly, parallel computation and evaluation. For this purpose several free and commercially available software applications were combined, including: SFE concept, Hypermesh, Perl, Matlab, and Radioss. The optimisation was conducted using Genetic Algorithms (GA), which are ideally suited to solve problems with solution spaces that are too large to be exhaustively searched. The viability of the method is demonstrated for a vehicle component model of a front bumper system utilizing both material and geometry related properties as design variables. (c) "	 Elsevier Ltd. and Civil-Comp Ltd. All rights reserved.	"Hilmann, J|Paas, M|Haenschke, A|Vietor, T"	ADVANCES IN ENGINEERING SOFTWARE	vehicle engineering structural optimisation sfe concept genetic algorithms finite element method parametric modelling sensitivity analysis	10.1016/j.advengsoft.2006.08.031
272	WOS:000290190800017	2011	Advances in concrete arch dams shape optimization	DESIGN	"This paper presents an efficient methodology to find the optimum shape of arch dams. In order to create the geometry of arch dams a new algorithm based on Hermit Splines is proposed. A finite element based shape sensitivity analysis for design-dependent loadings involving body force, hydrostatic pressure and earthquake loadings is implemented. The sensitivity analysis is performed using the concept of mesh design velocity. In order to consider the practical requirements in the optimization model such as construction stages, many geometrical and behavioral constrains are included in the model in comparison with previous researches. The optimization problem is solved via the sequential quadratic programming (SQP) method. The proposed methods are applied successfully to an Iranian arch dam, and good results are achieved. By using such methodology, efficient software for shape optimization of concrete arch dams for practical and reliable design now is available."	 (C) 2011 Elsevier Inc. All rights reserved.	"Akbari, J|Ahmadi, MT|Moharrami, H"	APPLIED MATHEMATICAL MODELLING	arch dam shape sensitivity analysis finite element modeling shape optimization	10.1016/j.apm.2011.01.020
274	WOS:000397072800012	2017	On the variational data assimilation problem solving and sensitivity analysis	LAPLACE TRANSFORM INVERSION COVARIANCE MATRICES CONDITION NUMBER REGULARIZATION IMPLEMENTATION	"We consider the Variational Data Assimilation (VarDA) problem in an operational framework, namely, as it results when it is employed for the analysis of temperature and salinity variations of data collected in closed and semi closed seas. We present a computing approach to solve the main computational kernel at the heart of the VarDA problem, which outperforms the technique nowadays employed by the oceanographic operative software. The new approach is obtained by means of Tikhonov regularization. We provide the sensitivity analysis of this approach and we also study its performance in terms of the accuracy gain on the computed solution. We provide validations on two realistic oceanographic data sets."	 (C) 2017 Elsevier Inc. All rights reserved.	"Arcucci, R|D'Amore, L|Pistoia, J|Toumi, R|Murli, A"	JOURNAL OF COMPUTATIONAL PHYSICS	data assimilation sensitivity analysis inverse problem	10.1016/j.jcp.2017.01.034
275	WOS:000255770300011	2008	Tools to support a model-based methodology for emission/immission and benefit/cost/risk analysis of wastewater systems that considers uncertainty	NO. 1 RWQM1 TREATMENT PLANTS WWTP DESIGN QUALITY COSTS BENCHMARKING	"This paper presents a set of tools developed to support an innovative methodology to design and upgrade wastewater treatment systems in a probabilistic way. For the first step, data reconstruction, two different tools were developed, one for situations where data are available and another one where no data are available. The second step, modelling and simulation, implied the development of a new simulation platform and of distributed computation software to deal with the simulation load generated by the third step, uncertainty analysis, with Monte Carlo simulations of the system over one year, important dynamics and stiff behaviour. For the fourth step, evaluation of alternatives, the evaluator tool processes the results of the simulations and plots the relevant information regarding the robustness of the process against input and parameters uncertainties, as well as concentration-duration curves for the risk of non-compliance with effluent and receiving water quality limits. This paper illustrates the merits of these tools to make the innovative methodology of practical interest. The design practice should move from conventional procedures suited for the relatively fixed context of emission limits, to more advanced, transparent and cost-effective procedures appropriate to cope with the flexibility and complexity introduced by integrated water management approaches. (c) "	 Elsevier Ltd. All rights reserved.	"Benedetti, L|Bixio, D|Claeys, F|Vanrolleghem, PA"	ENVIRONMENTAL MODELLING & SOFTWARE	wastewater treatment plant design cost-benefit analysis risk modelling and simulation software tools grid computing	10.1016/j.envsoft.2008.01.001
276	WOS:000289865900008	2011	Automatic differentiation strategy for the local sensitivity analysis of a one-dimensional hydraulic model		"In this paper, automatic differentiation (AD) techniques are introduced and applied in the local sensitivity analysis of the state function handled by the one-dimensional hydraulic model, Mage. We have proposed the different steps to easily compute automatic derivatives of a given numerical model. More specifically, Tapenade software, in the tangent linear mode (TLM), has been used to calculate derivatives of the model outputs (discharge and water level) with respect to the bottom friction expressed in terms of Strickler relation. We have shown the independent contribution of the main stream and floodplain Strickler coefficients on discharges and water levels. Furthermore, numerical comparison has shown that derivatives computed using the AD tool are more accurate than those using the forward divided differences scheme."	" Copyright (C) 2010 John Wiley & Sons, Ltd."	"Souhar, O|Faure, JB"	INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN FLUIDS	automatic differentiation divided differences hydraulic models sensitivity analysis	10.1002/fld.2263
278	WOS:000246956100013	2007	Evaluation of urban stormwater quality models		"The use of urban stormwater quality models necessitates the estimation of their outputs uncertainty level. The results of the application of a Monte Carlo Markov Chain method based on the Bayesian theory for the calibration and uncertainty analysis of a storm water quality model commonly used in available software are presented in this paper. The tested model estimates the accumulation, erosion and transport of pollutants on surfaces and in sewers using a hydrologic/hydrodynamic scheme. The model was calibrated for  different initial conditions of in-sewer deposits. Calibration results showed a large variability in the model outputs in function of the initial conditions and demonstrated that the tested model predictive capacity is very low."		"Kanso, A|Tassin, B|Chebbo, G"	HOUILLE BLANCHE-REVUE INTERNATIONALE DE L EAU		10.1051/lhb:2007025
279	WOS:000318057900004	2013	A long-term sensitivity analysis of the denitrification and decomposition model	NITROUS-OXIDE EMISSIONS DRAINED ILLINOIS AGROECOSYSTEMS GREENHOUSE-GAS EMISSIONS SOIL ORGANIC-CARBON DNDC MODEL AGRICULTURAL SOILS N2O EMISSIONS COMPUTATIONAL EXPERIMENTS MECHANISTIC MODEL RAINFALL EVENTS	"Although sensitivity analysis (SA) was conducted on the DeNitrification-DeComposition (DNDC) model, a global SA over a long period of time is lacking. We used a method of Bayesian analysis of computer code outputs (BACCO) with the Gaussian emulation machine for sensitivity analysis software (GEM-SA) to conduct a long-term SA of DNDC for predicting the annual change of soil organic carbon (dSOC), nitrous oxide emission (NO) and grain yield of spring wheat. Twenty seven non-weather input parameters with wide ranges were selected for SA using weather data recorded from Three Hills, Alberta over  years (-). The SA had two steps: ) a preliminary BACCO GEM-SA was conducted to identify a more accurate emulator sampling method and to screen out parameters with insignificant influence on model outcomes; and ) final BACCO GEM-SA was conducted with optimal input design set for emulator training runs varying only the significant input parameters. Results indicated that the Maximin Latin Hypercube sampling method outperformed the LP-x method with higher emulator accuracy. Most of the  input parameters contributed little to the three outputs by the first step BACCO GEM-SA. In the second step of BACCO GEM-SA there were only three (in the case of dSOC) and six (in the cases of NO and yield) input parameters whose influence contributed to more than % of the total output variances by their total effects. Among the selected parameters, initial soil organic carbon and clay content are very important and were important in determining results for all three outputs. Sensitivities of some parameters, such as clay content and urea fertilizer amount changed dramatically over the years. This indicates that a single year SA may overestimate or underestimate a long-term parameter effect on the model prediction. The two-step procedure with the BACCO GEM-SA method improved the accuracy of SA and provided important information for model validation and parameterization."	 Crown Copyright (C) 2013 Published by Elsevier Ltd. All rights reserved.	"Qin, XB|Wang, H|Li, YE|Li, Y|McConkey, B|Lemke, R|Li, CS|Brandt, K|Gao, QZ|Wan, YF|Liu, S|Liu, YT|Xu, C"	ENVIRONMENTAL MODELLING & SOFTWARE	dndc long-term global sensitivity analysis bacco gem-sa	10.1016/j.envsoft.2013.01.005
281	WOS:000256840200073	2007	"MEDOR, a didactic tool to support interpretation of bioassay data after internal contamination by actinides"	UNCERTAINTIES INHALATION (PUO2)-PU-239 WORKERS MODEL LUNG	"A didactic software, MEthodes DOsimetriques de REference (MEDOR), is being developed to provide help in the interpretation of biological data. Its main purpose is to evaluate the pertinence of the application of different models. This paper describes its first version that is focused on inhalation exposure to actinide aerosols. With this tool, sensitivity analysis on different parameters of the ICRP models can be easily done for aerosol deposition, in terms of activity and particle number, actinide biokinetics and doses. The user can analyse different inhalation cases showing either that dose per unit intake cannot be applied if the aerosol contains a low number of particles or that an inhibition of the late pulmonary clearance by particle transport can occur which contributes to a - fold increase in effective dose as compared with application of default parameters. This underlines the need to estimate systematically the number of deposited particles, as well as to do chest monitoring as long as possible."		"Miele, A|Blanchin, N|Raynaud, P|Quesne, B|Giraud, JM|Fottorino, R|Berard, P|Ansoborlo, E|Franck, D|Blanchardon, E|Vathaire, CCD|Lebaron-Jacobs, L|Poncy, JL|Piechowski, J|Fritsch, P"	RADIATION PROTECTION DOSIMETRY		10.1093/rpd/ncm288
283	WOS:000383298800020	2016	Designing and implementing a multi-core capable integrated urban drainage modelling Toolkit:Lessons from CityDrain3	WASTE-WATER SYSTEM CLIMATE-CHANGE IDENTIFIABILITY ANALYSIS SENSITIVITY-ANALYSIS STORMWATER MODELS STORAGE TANK SEWER SYSTEM CITY DRAIN MANAGEMENT UNCERTAINTY	"Integrated urban drainage modelling combines different aspects of the urban water system into a common framework. With increasing pressures of a changing climate, urban growth and economic constraints, the need for wider spread integration is necessary in the interest of a sustainable future. Greater complexity results in greater computational burden but modelling packages will, likewise, need to be flexible enough to allow incorporation of new algorithms. With advancements in modern information technology, a parallel implementation of such a modelling toolkit is mandatory while still leaving its users the flexibility of extensions. The design and implementation of the integrated modelling framework CityDrain shows that it is possible to write research code that is high-performance and extensible by many research projects. Three use case scenarios are presented to showcase the application of CityDrain. The performance advantage of parallelization (up to  times compared to its predecessor) and the scalability of the framework are also demonstrated. (C) "	 Elsevier Ltd. All rights reserved.	"Burger, G|Bach, PM|Urich, C|Leonhardt, G|Kleidorfer, M|Rauch, W"	ADVANCES IN ENGINEERING SOFTWARE	integrated urban drainage modelling simulation framework object-oriented design multi-core parallel computing	10.1016/j.advengsoft.2016.08.004
284	WOS:000243742700004	2007	The Data Uncertainty Engine (DUE): A software tool for assessing and simulating uncertain environmental variables	MODELING ERROR REJECTION BETA PROPAGATION PREDICTION POISSON GAMMA GIS	"This paper describes a software tool for: () assessing uncertainties in environmental data; and () generating realisations of uncertain data for use in uncertainty propagation analyses: the ""Data Uncertainty Engine (DUE)"". Data may be imported into DUE from file or from a database, and are represented in DUE as objects whose positions and attribute values may be uncertain. Objects supported by DUE include spatial vectors, spatial rasters, time-series of spatial data, simple time-series and objects that are constant in space and time. Attributes supported by DUE include continuous numerical variables (e.g. rainfall), discrete numerical variables (e.g. bird counts) and categorical variables (e.g. land-cover). Once data are imported, an uncertainty model can be developed for the positional and attribute uncertainties of environmental objects. This is currently limited to probability models, but confidence intervals and scenarios will be provided in the future. Using DUE, the spatial and temporal patterns of uncertainty (autocorrelation), as well as cross-correlations between related inputs, can be incorporated into an uncertainty analysis. Alongside expert judgement, sample data may be used to help estimate uncertainties, and to reduce the uncertainty of the simulated output by ensuring each realisation reproduces the sample data. Most importantly, DUE provides a conceptual framework for structuring an uncertainty analysis, allowing users without direct experience of uncertainty methods to develop realistic uncertainty models for their data. (c) "	 Elsevier Ltd. All rights reserved.	"Brown, JD|Heuvelink, GBM"	COMPUTERS & GEOSCIENCES	uncertainty analysis monte carlo uncertainty propagation java	10.1016/j.cageo.2006.06.015
286	WOS:000265341800001	2009	GUI-HDMR - A software tool for global sensitivity analysis of complex models	STREET CANYON MODEL ENVIRONMENTAL-MODELS RS-HDMR REPRESENTATIONS UNCERTAINTY PARAMETERS INDEXES OUTPUT	"The high dimensional model representation (HDMR) method is a set of tools which can be used to construct a fully functional metamodel and to calculate variance based sensitivity indices very efficiently. Extensions to the existing set of random sampling (RS)-HDMR tools have been developed in order to make the method more applicable for complex models with a large number of input parameters as often appear in environmental modelling. The HDMR software described here combines the RS-HDMR tools and its extensions in one Matlab package equipped with a graphical user interface (GUI). This makes the HDMR method easily available for all interested users. The performance of the GUI-HDMR software has been tested in this paper using two analytical test models, the Ishigami function and the Sobol' g-function. In both cases the model is highly non-linear, non-monotonic and has significant parameter interactions. The developed GUI-HDMR software copes very well with the test cases and sensitivity indices of first and second order could be calculated accurately with only low computational effort. The efficiency of the software has also been compared against other recently developed approaches and is shown to be competitive. GUI-HDMR can be applied to a wide range of applications in all fields, because in principle only one random or quasi-random set of input and output values is required to estimate all sensitivity indices up to second order. The size of the set of samples is however dependent on the problem and can be successively increased if additional accuracy is required. A brief description of its application within a range of modelling environments is given. (C) "	 Elsevier Ltd. All rights reserved.	"Ziehn, T|Tomlin, AS"	ENVIRONMENTAL MODELLING & SOFTWARE	global sensitivity analysis high dimensional model representation random sampling matlab software graphical user interface	10.1016/j.envsoft.2008.12.002
288	WOS:000245063400008	2007	Parameter estimation and uncertainty analysis for a watershed model	RAINFALL-RUNOFF MODELS SHUFFLED COMPLEX EVOLUTION GROUNDWATER-FLOW MODEL MONTE-CARLO METHODS BAYESIAN-APPROACH METROPOLIS ALGORITHM PREDICTION INTERVALS CATCHMENT MODELS MARKOV-CHAINS CALIBRATION	"Where numerical models are employed as an aid to environmental management, the uncertainty associated with predictions made by such models must be assessed. A number of different methods are available to make such an assessment. This paper explores the use of three such methods, and compares their performance when used in conjunction with a lumped parameter model for surface water flow (HSPF) in a large watershed. Linear (or first-order) uncertainty analysis has the advantage that it can be implemented with virtually no computational burden. While the results of such an analysis can be extremely useful for assessing parameter uncertainty in a relative sense, and ascertaining the degree of correlation between model parameters, its use in analyzing predictive uncertainty is often limited. Markov Chain Monte Carlo (MCMC) methods are far more robust, and can produce reliable estimates of parameter and predictive uncertainty. As well as this, they can provide the modeler with valuable qualitative information on the shape of parameter and predictive probability distributions; these shapes can be quite complex, especially where local objective function optima lie within those parts of parameter space that are considered probable after calibration has been undertaken. Nonlinear calibration-constrained optimization can also provide good estimates of parameter and predictive uncertainty, even in situations where the objective function surface is complex. Furthermore, they can achieve these estimates using far fewer model runs than MCMC methods. However, they do not provide the same amount of qualitative information on the probability structure of parameter space as do MCMC methods, a situation that can be partially rectified by combining their use with an efficient gradient-based search method that is specifically designed to locate different local optima. All methods of parameter and predictive uncertainty analysis discussed herein are implemented using freely-available software. Hence similar studies, or extensions of the present study, can be easily undertaken in other modeling contexts by other modelers. (c) "	 Elsevier Ltd. All rights reserved.	"Gallagher, M|Doherty, J"	ENVIRONMENTAL MODELLING & SOFTWARE	uncertainty analysis parameter estimation mathematical modeling markov chain monte carlo model calibration	10.1016/j.envsoft.2006.06.007
292	WOS:000358997900004	2015	Sensitivity Analysis for Bayesian Hierarchical Models	LINEAR MIXED MODELS LOCAL INFLUENCE MARGINAL DENSITIES INFERENCE APPROXIMATIONS PERTURBATION	"Prior sensitivity examination plays an important role in applied Bayesian analyses. This is especially true for Bayesian hierarchical models, where interpretability of the parameters within deeper layers in the hierarchy becomes challenging. In addition, lack of information together with identifiability issues may imply that the prior distributions for such models have an undesired influence on the posterior inference. Despite its importance, informal approaches to prior sensitivity analysis are currently used. They require repetitive re-fits of the model with ad-hoc modified base prior parameter values. Other formal approaches to prior sensitivity analysis suffer from a lack of popularity in practice, mainly due to their high computational cost and absence of software implementation. We propose a novel formal approach to prior sensitivity analysis, which is fast and accurate. It quantifies sensitivity without the need for a model re-fit. Through a series of examples we show how our approach can be used to detect high prior sensitivities of some parameters as well as identifiability issues in possibly over-parametrized Bayesian hierarchical models."		"Roos, M|Martins, TG|Held, L|Rue, H"	BAYESIAN ANALYSIS	base prior formal local sensitivity measure bayesian robustness calibration hellinger distance bayesian hierarchical models identifiability overparametrisation	10.1214/14-BA909
297	WOS:000365335000031	2016	A toolbox using the stochastic optimization algorithm MIPT and ChemCAD for the systematic process retrofit of complex chemical processes	MULTIOBJECTIVE OPTIMIZATION EVOLUTIONARY ALGORITHMS FLOWSHEET OPTIMIZATION PROCESS SIMULATORS GENETIC ALGORITHM DESIGN METHODOLOGY PLANTS PERSPECTIVES INTEGRATION	"Global optimization techniques using powerful algorithms have led to a wide range of applications to increase the efficiency of chemical processes. Nevertheless, the performance for optimization of process models is limited by a certain complexity, especially accounting for existing processes (retrofit). Due to the great combinatorial diversity of possible alternatives a systematic approach is essential. The local integration of modifications in the overall process leads to changes in internal streams. Therefore new operating points have to be found and resulting effects on the plant performance have to be evaluated. An optimization framework for the purpose of retrofitting using a rigorous process simulation tool is proposed to fulfill this task. Here, flowsheet simulation software packages are offering a high performance for the prediction of new operation points for following units, and for units affected by recycle streams. An optimization approach using flowsheet simulation software and the stochastic optimization algorithm Molecular-Inspired Parallel Tempering (MIPT) implemented in the programming software Matlab (TM) is presented. Both of these programs are linked via OPC (OLE for process control), a standard communication platform. The toolbox provides a quick evaluation of the process by searching for the global optimum. The MIPT algorithm is suitable for large optimization problems and can handle constraints and infeasibilities. The usage of a rigorous process simulator is providing a high accuracy of the thermodynamic results which is necessary to evaluate the influence of the new process design. Furthermore, a simulation model of the industrial plant can directly be used for the optimization. A complex multicomponent separation process with recycle streams is used to demonstrate the advantages of the proposed toolbox. To simplify the user input a graphical user interface was programmed. The results of a sensitivity analysis and the optimization for different feed compositions are presented. (C) "	 Elsevier Ltd. All rights reserved.	"Otte, D|Lorenz, HM|Repke, JU"	COMPUTERS & CHEMICAL ENGINEERING	optimization molecular-inspired parallel tempering multicomponent separation process retrofit stochastic algorithm chemcad (tm)	10.1016/j.compchemeng.2015.08.023
298	WOS:000302910100010	2012	Robust design in aerodynamics using third-order sensitivity analysis based on discrete adjoint. Application to quasi-1D flows	SHAPE OPTIMIZATION DATA ASSIMILATION 1ST	"In this paper, the second-order second moment approach, coupled with an adjoint-based steepest descent algorithm, for the solution of the so-called robust design problem in aerodynamics is proposed. Because the objective function for the robust design problem comprises first-order and second-order sensitivity derivatives with respect to the environmental parameters, the application of a gradient-based method , which requires the sensitivities of this function with respect to the design variables, calls for the computation of third-order mixed derivatives. To compute these derivatives with the minimum CPU cost, a combination of the direct differentiation and the discrete adjoint variable method is proposed. This is presented for the first time in the relevant literature and is the most efficient among other possible schemes on condition that the design variables are much more than the environmental ones; this is definitely true in most engineering design problems. The proposed approach was used for the robust design of a duct, assuming a quasi-D flow model; the coordinates of the Bezier control points parameterizing the duct shape are used as design variables, whereas the outlet Mach number and the DarcyWeisbach friction coefficient are used as environmental ones. The extension to D and D flow problems, after developing the corresponding direct differentiation and adjoint variable methods and software, is straightforward."	" Copyright (C) 2011 John Wiley & Sons, Ltd."	"Papoutsis-Kiachagias, EM|Papadimitriou, DI|Giannakoglou, KC"	INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN FLUIDS	robust aerodynamic shape optimization discreteadjoint method third-order sensitivity derivatives method of moments	10.1002/fld.2604
299	WOS:000273363700015	2009	BAT - The Bayesian analysis toolkit	MODEL	"We describe the development of a new toolkit for data analysis. The analysis package is based on Bayes' Theorem, and is realized with the use of Markov Chain Monte Carlo. This gives access to the full posterior probability distribution. Parameter estimation, limit setting and uncertainty propagation are implemented in a straightforward manner."	 (C) 2009 Elsevier B.V. All rights reserved.	"Caldwell, A|Kollar, D|Kroninger, K"	COMPUTER PHYSICS COMMUNICATIONS	data analysis markov chain monte carlo	10.1016/j.cpc.2009.06.026
301	WOS:000245766800002	2007	Methods and object-oriented software for FE reliability and sensitivity analysis with application to a bridge structure	OPTIMIZATION	"This paper addresses the growing demand for finite-element software with capabilities to incorporate uncertainty in the input parameters. Reliability and response sensitivity algorithms are implemented in the general-purpose finite-element software OpenSees, which employs an object-oriented programming approach to achieve a sustainable software with focus on maintainability and extensibility. The product is a comprehensive and freely available library of software tools for finite-element reliability and response sensitivity analysis. A numerical example involving a detailed model of a highway bridge with inelastic material behavior and  random variables is presented to demonstrate features of the methodology and the software. Importance vectors are employed to rank the input parameters according to their relative influence on the structural reliability. The required response sensitivities are obtained by an extensive implementation of the direct differentiation method."		"Haukaas, T|Kiureghian, AD"	JOURNAL OF COMPUTING IN CIVIL ENGINEERING		10.1061/(ASCE)0887-3801(2007)21:3(151)
302	WOS:000168416500008	2001	A computational methodology for shape optimization of structures in frictionless contact	SUPERCONVERGENT PATCH RECOVERY SENSITIVITY ANALYSIS FORMULATION ALGORITHMS SOLIDS	"This paper presents a computational methodology for shape optimization of structures in frictionless contact. which provides a basis for developing user-friendly and efficient shape optimization software. For evaluation it has been implemented as a subsystem of a general finite element software. The overall design and main principles of operation of this software are outlined. The parts connected to shape optimization are described in more detail. The key building blocks are: analytic sensitivity analysis, an adaptive finite element method, an accurate contact solver. and a sequential convex programing optimization algorithm. Results for three model application examples are presented, in which the contact pressure and the effective stress are optimized."	 (C) 2001 Elsevier Science B.V. All rights reserved.	"Hilding, D|Torstenfelt, B|Klarbring, A"	COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING	shape optimization frictionless contact finite element method sensitivity analysis adaptive meshing	10.1016/S0045-7825(00)00310-8
304	WOS:000343415200006	2013	UNCERTAINTY IN THE DEVELOPMENT AND USE OF EQUATION OF STATE MODELS		"In this paper we present the results from a series of focus groups on the visualization of uncertainty in equation-of-state (EOS) models. The initial goal was to identify the most effective ways to present EOS uncertainty to analysts, code developers, and material modelers. Four prototype visualizations were developed to present EOS surfaces in a three-dimensional, thermodynamic space. Focus group participants, primarily from Sandia National Laboratories, evaluated particular features of the various techniques for different use cases and discussed their individual workflow processes, experiences with other visualization tools, and the impact of uncertainty on their work. Related to our prototypes, we found the D presentations to be helpful for seeing a large amount of information at once and for a big-picture view; however, participants also desired relatively simple, two-dimensional graphics for better quantitative understanding and because these plots are part of the existing visual language for material models. In addition to feedback on the prototypes, several themes and issues emerged that are as compelling as the original goal and will eventually serve as a starting point for further development of visualization and analysis tools. In particular, a distributed workflow centered around material models was identified. Material model stakeholders contribute and extract information at different points in this workflow depending on their role, but encounter various institutional and technical barriers which restrict the flow of information. An effective software tool for this community must be cognizant of this workflow and alleviate the bottlenecks and barriers within it. Uncertainty in EOS models is defined and interpreted differently at the various stages of the workflow. In this context, uncertainty propagation is difficult to reduce to the mathematical problem of estimating the uncertainty of an output from uncertain inputs."		"Weirs, VG|Fabian, N|Potter, K|McNamara, L|Otahal, T"	INTERNATIONAL JOURNAL FOR UNCERTAINTY QUANTIFICATION	materials uncertainty quantification representation of uncertainty model validation and verification continnum mechanics	10.1615/Int.J.UncertaintyQuantification.2012003960
306	WOS:000344530400002	2014	An SMT Based Method for Optimizing Arithmetic Computations in Embedded Software Code	PROGRAMS EXAMPLES	"We present a new method for optimizing the source code of embedded control software with the objective of minimizing implementation errors in the linear fixed-point arithmetic computations caused by overflow, underflow, and truncation. Our method relies on the use of the satisfiability modulo theory (SMT) solver to search for alternative implementations that are mathematically equivalent but require a smaller bit-width, or implementations that use the same bit-width but have a larger error-free dynamic range. Our systematic search of the bounded implementation space is based on a new inductive synthesis procedure, which is guaranteed to find a valid solution as long as such solution exists. Furthermore, we propose an incremental optimization procedure, which applies the synthesis procedure only to small code regions-one at a time-as opposed to the entire program, which is crucial for scaling the method up to programs of realistic size and complexity. We have implemented our new method in a software tool based on the Clang/LLVM compiler frontend and the Yices SMT solver. Our experiments, conducted on a set of representative benchmarks from embedded control and digital signal processing applications, show that the method is both effective and efficient in optimizing arithmetic computations in embedded software code."		"Eldib, H|Wang, C"	IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS	fixed point arithmetic inductive program synthesis satisfiability modulo theory (smt) solver superoptimization	10.1109/TCAD.2014.2341931
310	WOS:000232093200009	2006	Sensitivity analysis of the strain criterion for multidimensional scaling	SPECTRAL FUNCTIONS OPTIMIZATION	"Multidimensional scaling (MDS) is a collection of data analytic techniques for constructing configurations of points from dissimilarity information about interpoint distances. Classsical MDS assumes a fixed matrix of dissimilarities. However, in some applications, e.g., the problem of inferring -dimensional molecular structure from bounds on interatomic distances, the dissimilarities are free to vary, resulting in optimization problems with a spectral objective function. A perturbation analysis is used to compute first- and second-order directional derivatives of this function. The gradient and Hessian are then inferred as representers of the derivatives. This coordinate-free approach reveals the matrix structure of the objective and facilitates writing customized optimization software. Also analyzed is the spectrum of the Hessian of the objective."	 (c) 2004 Elsevier B.V. All rights reserved.	"Lewis, RM|Trosset, MW"	COMPUTATIONAL STATISTICS & DATA ANALYSIS	classical multidimensional scaling principal coordinate analysis distance matrices distance geometry spectral decomposition perturbation analysis	10.1016/j.csda.2004.07.011
312	WOS:000178643700005	2002	A computer program for a Monte Carlo analysis of sensitivity in equations of environmental modelling obtained from experimental data	SYSTEMS	"In the construction of mathematical models from experimental data, it is possible to determine equations that model relations using several methodologies [Un nuevo algoritmo para la modelizacion de sistemas altamente estructurados (); Env. Model. Software  () ; Guide to Statistics , (); Regression models (); Ecosystems and Sustainable Development II ()]. These methodologies build equations that are in line with the experimental data and they analyse a dimension of adjustment and a dimension of error or distance between the experimental data and the data that is produced by the model. There are studies of sensitivity of the sample of data, as found by Bolado and Alonso [Proceedings SAMO ]. The authors consider that it is useful to obtain new parameters that relate the sensitivity of the equations to the variations that are produced by the experimental data. This will allow the selection of the model according to new criteria. On the one hand, the authors present a theoretical study of sensitivity of the models according to different points of view. On the other hand, they discuss a computing algorithm that allows the analysis of sensitivity (and stability) of the mathematical equations, which are built from any methodology. An interface has been incorporated into this algorithm to allow a graphic visualisation of the effects that are produced when modifications of the model are carried out."	 (C) 2002 Elsevier Science Ltd. All rights reserved.	"Verdu, F|Villacampa, Y"	ADVANCES IN ENGINEERING SOFTWARE	sensitivity analysis environmental modelling monte carlo	10.1016/S0965-9978(02)00023-6
314	WOS:000349876500012	2015	"Understanding the Day Cent model: Calibration, sensitivity, and identifiability through inverse modeling"	NITROUS-OXIDE EMISSIONS EVALUATING PARAMETER IDENTIFIABILITY DAYCENT ECOSYSTEM MODEL SOIL ORGANIC-MATTER ERROR REDUCTION 2 STATISTICS AUTOMATIC CALIBRATION PRODUCTION SYSTEMS N2O EMISSIONS WATER-FLOW	"The ability of biogeochemical ecosystem models to represent agro-ecosystems depends on their correct integration with field observations. We report simultaneous calibration of  DayCent model parameters using multiple observation types through inverse modeling using the PEST parameter estimation software. Parameter estimation reduced the total sum of weighted squared residuals by % and improved model fit to crop productivity, soil carbon, volumetric soil water content, soil temperature, NO, and soil NO- compared to the default simulation. Inverse modeling substantially reduced predictive model error relative to the default model for all model predictions, except for soil NO- and NH+. Post-processing analyses provided insights into parameter-observation relationships based on parameter correlations, sensitivity and identifiability. Inverse modeling tools are shown to be a powerful way to systematize and accelerate the process of biogeochemical model interrogation, improving our understanding of model function and the underlying ecosystem biogeochemical processes that they represent. (C)  The Authors. Published by"	 Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).	"Necpalova, M|Anex, RP|Fienen, MN|Del Grosso, SJ|Castellano, MJ|Sawyer, JE|Iqbal, J|Pantoja, JL|Barker, DW"	ENVIRONMENTAL MODELLING & SOFTWARE	daycent model inverse modeling pest sensitivity analysis parameter identifiability parameter correlations	10.1016/j.envsoft.2014.12.011
315	WOS:000323349500001	2013	Optimal Design of Signal Controlled Road Networks Using Differential Evolution Optimization Algorithm	EQUILIBRIUM TRANSPORTATION NETWORKS VARIATIONAL INEQUALITY CONSTRAINTS SIMULATED ANNEALING APPROACH AREA TRAFFIC CONTROL GENETIC ALGORITHM SENSITIVITY ANALYSIS ASSIGNMENT TIMINGS FLOW	"This study proposes a traffic congestion minimization model in which the traffic signal setting optimization is performed through a combined simulation-optimization model. In this model, the TRANSYT traffic simulation software is combined with Differential Evolution (DE) optimization algorithm, which is based on the natural selection paradigm. In this context, the EQuilibrium Network Design (EQND) problem is formulated as a bilevel programming problem in which the upper level is the minimization of the total network performance index. In the lower level, the traffic assignment problem, which represents the route choice behavior of the road users, is solved using the Path Flow Estimator (PFE) as a stochastic user equilibrium assessment. The solution of the bilevel EQND problem is carried out by the proposed Differential Evolution and TRANSYT with PFE, the so-called DETRANSPFE model, on a well-known signal controlled test network. Performance of the proposed model is compared to that of two previous works where the EQND problem has been solved by Genetic-Algorithms- (GAs-) and Harmony-Search- (HS-) based models. Results show that the DETRANSPFE model outperforms the GA- and HS-based models in terms of the network performance index and the computational time required."		"Ceylan, H"	MATHEMATICAL PROBLEMS IN ENGINEERING		10.1155/2013/696374
319	WOS:000253118500008	2008	"Borehole Optimisation System (BOS) - A case study assessing options for abstraction of urban groundwater in Nottingham, UK"	TRIASSIC SANDSTONE AQUIFER BIODEGRADATION CONTAMINATION BIOTRANSFORMATION TRANSFORMATIONS TRICHLOROETHENE HYDROCARBONS ATTENUATION RECHARGE MTBE	"The recognition that urban groundwater is a potentially valuable resource for potable and industrial uses due to growing pressures on perceived less polluted rural groundwater has led to a requirement to assess the groundwater contamination risk in urban areas from industrial contaminants such as chlorinated solvents. The development of a probabilistic risk based management tool that predicts groundwater quality at potential new urban boreholes is beneficial in determining the best sites for future resource development. The Borehole Optimisation System (BOS) is a custom Geographic Information System (GIs) application that has been developed with the objective of identifying the optimum locations for new abstraction boreholes. BOS can be applied to any aquifer subject to variable contamination risk. The system is described in more detail by Tait et al. [Tait, N.G., Davison, J.J., Whittaker, J.J., Lehame, S.A. Lerner, D.N., a. Borehole Optimisation System (BOS) - a GIs based risk analysis tool for optimising the use of urban groundwater. Environmental Modelling and Software , -]. This paper applies the BOS model to an urban Permo-Triassic Sandstone aquifer in the city centre of Nottingham, UK. The risk of pollution in potential new boreholes from the industrial chlorinated solvent tetrachloroethene (PCE) was assessed for this region. The risk model was validated against contaminant concentrations from  actual field boreholes within the study area. In these studies the model generally underestimated contaminant concentrations. A sensitivity analysis showed that the most responsive model parameters were recharge, effective porosity and contaminant degradation rate. Multiple simulations were undertaken across the study area in order to create surface maps indicating areas of low PCE concentrations, thus indicating the best locations to place new boreholes. Results indicate that northeastern, eastern and central regions have the lowest potential PCE concentrations in abstraction groundwater and therefore are the best sites for locating new boreholes. These locations coincide with aquifer areas that are confined by low permeability Mercia Mudstone deposits. Conversely southern and northwestern areas are unconfined and have shallower depth to groundwater. These areas have the highest potential PCE concentrations. These studies demonstrate the applicability of BOS as a tool for informing decision makers on the development of urban groundwater resources. (c) "	 Elsevier Ltd. All rights reserved.	"Tait, NG|Davison, RM|Leharne, SA|Lerner, DN"	ENVIRONMENTAL MODELLING & SOFTWARE	borehole optimisation system gis pce probabilistic risk modelling urban groundwater	10.1016/j.envsoft.2007.09.001
320	WOS:000256053900010	2008	A comparative life cycle analysis of two different juice packages		"Packaging wastes have a portion of -% in total municipal solid waste (MSW) in Turkey, and they have to be evaluated from production to final disposal from the environmental point of view. The concern about the environmental impacts of packages has been dealt with using several approaches in environmental management, such as risk assessment, environmental impact assessment, environmental auditing, energy analysis, material flow analysis, and life cycle analysis (LCA). The main purpose of this research was to investigate the life cycle environmental impact of glass bottles and beverage cartons. This LCA study was performed by using SimaPro (PRe Consultants, The Netherlands) software. Individual and comparative life cycle analysis of two packages was performed depending on a consumer who lives in Eskisehir city. For that aim, SimaPro was used, and the data to run the software was gathered from package producers, the database of the software, and the literature. Life cycle comparisons of the two juice packages among themselves and also within themselves were carried out by using EcoIndicator  on the basis of climate change, ecotoxicity, acidification/eutrophication, and fossil fuels. Sensitivity analysis was performed to evaluate the effects of transportation. According to comparison figures, the environmental load of glass bottles is higher than beverage carton's load for all the impact categories. This result is also supported by the sensitivity analysis."		"Banar, M|Cokaygil, Z"	ENVIRONMENTAL ENGINEERING SCIENCE	lca glass bottle beverage carton packaging waste simapro7	10.1089/ees.2007.0079
321	WOS:000250159500009	2007	Simiyu River catchment parameterization using SWAT model	INFORMATION-SYSTEM APPROACH RAINFALL-RUNOFF MODELS SURFACE SOIL-MOISTURE MONSOON 90	"The paper presents advances in hydrologic modelling of the Simiyu River catchment using the soil and water assessment tool (SWAT). In this study, the SWAT model set-up and subsequent application to the catchment was based on high-resolution data such as land use from  in LandSat TM Satellite,  in Digital Elevation Model and Soil from Soil and Terrain Database for Southern Africa (SOTERSAF). The land use data were reclassified based on some ground truth maps using IDRISI Kilimanjaro software. The Soil data were also reclassified manually to represent different soil hydrologic groups, which are important for the SWAT model set-up and simulations. The SWAT application first involved analysis of parameter sensitivity, which was then used for model auto-calibration that followed hierarchy of sensitive model parameters. The analysis of sensitive parameters and auto-calibration was achieved by sensitivity analysis and auto-calibration options, which are new in the recent version of SWAT, SWAT . The paper discusses the results of sensitivity and auto -calibration analyses, and present optimum model parameters, which are important for operation and water/land management studies (e.g. rain-fed agriculture and erosion/sediment and pollutant transport) in the catchment using SWAT. The river discharge estimates from this and a previous study were compared so as to evaluate performances of the recent hydrologic simulations in the catchment. Results showed that surface water model parameters are the most sensitive and have more physical meaning especially CN (the most sensitive) and SOL-K. Simulation results showed more or less same estimate of river flow at Ndagalu gauging station. The model efficiencies (R-%) in this and in the pervious study during calibration and validation periods were, respectively, ., . and ., .. The low level of model performance achieved in these studies showed that other factors than the spatial land data are greatly important for improvement of flow estimation by SWAT in Simiyu."	 (c) 2007 Published by Elsevier Ltd.	"Mulungu, DMM|Munishi, SE"	PHYSICS AND CHEMISTRY OF THE EARTH	auto-calibration analysis high-resolution data parameter sensitivity analysis simiyu river catchment swat 2005	10.1016/j.pce.2007.07.053
326	WOS:000287575500001	2011	Laboratory and numerical modeling of water balance in a layered sloped soil cover with channel flow pathway over mine waste rock	WETTING FRONT INSTABILITY ENGINEERED TEST COVERS CHARACTERISTIC CURVE UNSATURATED SOILS OXYGEN BARRIERS WHISTLE MINE TAILINGS INFILTRATION EVAPORATION ONTARIO	"Macropores developed in barrier layers in soil covers overlying acid-generating waste rock may produce preferential flow through the barrier layers and compromise cover performance. However, little has been published on the effects of preferential flow on water balance in soil covers. In the current study, an inclined, layered soil cover with a -cm-wide sand-filled channel pathway in a silty clay barrier layer was built over reactive waste rock in the laboratory. The channel or preferential flow pathway represented the aggregate of cracks or fissures that may occur in the barrier during compaction and/or climate-induced deterioration. Precipitation, runoff, interflow, percolation, and water content were recorded during the test. A commercial software VADOSE/W was used to simulate the measured water balance and to conduct further sensitivity analysis on the effects of the location of the channel and the saturated hydraulic conductivity of the channel material on water balance. The maximum percolation, .% of the total precipitation, was obtained when the distance between the mid-point of the channel pathway and the highest point on the slope accounted for % of the total horizontal length of the soil cover. The modeled percolation increased steadily with an increase in the hydraulic conductivity of the channel material. Percolation was found to be sensitive to the location of the channel and the saturated hydraulic conductivity of the channel material, confirming that proper cover design and construction should aim at minimizing the development of vertical preferential flow in barrier layers. The sum of percolation and interflow was relatively constant when the location of the channel changed along the slope, which may be helpful in locating preferential flow pathways and repairing the barrier."		"Song, Q|Yanful, EK"	ENVIRONMENTAL EARTH SCIENCES	acid rock drainage channel flow laboratory test soil cover vadose/w water balance	10.1007/s12665-010-0488-4
328	WOS:000237124500022	2006	Automatic calibration and predictive uncertainty analysis of a semidistributed watershed model	RAINFALL-RUNOFF MODELS GLOBAL OPTIMIZATION HYDROLOGIC-MODELS PARAMETER-ESTIMATION ALGORITHM MULTIPLE SCHEME	"Semidistributed models are commonly calibrated manually, but software for automatic calibration is now available. We present a two-stage routine for automatic calibration of the semidistributed watershed model Soil and Water Assessment Tool ( SWAT) that finds the best values for the model parameters, preserves spatial variability in essential parameters, and leads to a measure of the model prediction uncertainty. In the first stage, a modified global Shuffled Complex Evolution (SCE-UA) method was employed to find the ""best'' values for the lumped model parameters. In the second stage, the spatial variability of the original model parameters was restored and a local search method ( a variant of Levenberg - Marquart method) was used to find a more distributed set of parameters using the results of the previous stage as starting values. A method called ""regularization'' was adopted to prevent the parameters from taking extreme values. In addition, we applied a nonlinear calibration-constrained method to develop confidence intervals for annual and -d average flow predictions. We calibrated stream flow in the Etowah River measured at Canton, GA ( a watershed area of  km()) for the years  to  and used the years  to  for validation. The Parameter Estimator ( PEST) software was used to conduct the two-stage automatic calibration and prediction uncertainty analysis. Calibration for daily and monthly flow produced a very good fit to the measured data. Nash-Sutcliffe coefficients for daily and monthly flow over the calibration period were . and ., respectively. They were . and ., respectively, for the validation period. The nonlinear prediction uncertainty analysis worked well for long-term ( annual) flow in that our prediction confidence intervals included or were very near to the observed flow for most years. It did not work well for short-term (-d average) flows in that the prediction confidence intervals did not include the observed flow, especially for low and high flow conditions."		"Lin, ZL|Radcliffe, DE"	VADOSE ZONE JOURNAL		10.2136/vzj2005.0025
332	WOS:000187422700003	2004	MULINO-DSS: a computer tool for sustainable use of water resources at the catchment scale	EXAMPLE DESIGN MODELS	"MULINO, an ongoing project financed by the European Commission, has released the prototype of a Decision Support System software (mDSS) for the sustainable management of water resources at the catchment scale. The software integrates socio-economic and environmental modelling, with geo-spatial information and multi-criteria analysis. The policy background refers to the EU Water Framework Directive. The challenging multi-disciplinary context was approached by developing an innovative and dynamic implementation of the DPSIR framework, originally proposed by the European Environmental Agency. In mDSS integrated assessment modelling provides the values of quantitative indicators to be used for transparent and participated decisions, through the application of value functions, weights and decision rules chosen by the end user. Simple routines for the sensitivity analysis and comparison of alternative weight vectors also provides effective decision support by exploring and finding compromises between conflicting interests/perspectives in a multi-stakeholder context."	 (C) 2003 Published by Elsevier B.V on behalf of IMACS.	"Giupponi, C|Mysiak, J|Fassio, A|Cogan, V"	MATHEMATICS AND COMPUTERS IN SIMULATION	dss software water resources sustainable use catchment modelling	10.1016/j.matcom.2003.07.003
333	WOS:000335707200019	2014	FReET: Software for the statistical and reliability analysis of engineering problems and FReET-D: Degradation module	LATIN HYPERCUBE SAMPLES REINFORCEMENT CORROSION INPUT VARIABLES CONCRETE SIZE SIMULATION CARBONATION DURABILITY FRACTURE MODEL	"The objective of the paper is to present methods and software for the efficient statistical, sensitivity and reliability assessment of engineering problems. Attention is given to small-sample techniques which have been developed for the analysis of computationally intensive problems. The paper shows the possibility of ""randomizing"" computationally intensive problems in the manner of the Monte Carlo type of simulation. In order to keep the number of required simulations at an acceptable level, Latin Hypercube Sampling is utilized. The technique is used for both random variables and random fields. Sensitivity analysis is based on non-parametric rank-order correlation coefficients. Statistical correlation is imposed by the stochastic optimization technique - simulated annealing. A hierarchical sampling approach has been developed for the extension of the sample size in Latin Hypercube Sampling, enabling the addition of simulations to a current sample set while maintaining the desired correlation structure. The paper continues with a brief description of the user-friendly implementation of the theory within FReET commercial multipurpose reliability software. FReET-D software is capable of performing degradation modeling, in which a large number of reinforced concrete degradation models can be utilized under the main FReET software engine. Some of the interesting applications of the software are referenced in the paper. (C) "	 Elsevier Ltd. All rights reserved.	"Novak, D|Vorechovsky, M|Teply, B"	ADVANCES IN ENGINEERING SOFTWARE	statistical analysis sensitivity reliability monte carlo simulation latin hypercube sampling simulated annealing random fields material degradation	10.1016/j.advengsoft.2013.06.011
335	WOS:000256840200114	2007	Estimating uncertainty on internal dose assessments	RADON PROGENY UNIT EXPOSURE DOSIMETRY	"The estimation of uncertainty on doses broadly falls into three categories. () Estimating the uncertainty on prospective doses. Here, the intake is known and the uncertainties in individual parameter values must be propagated through the calculated dose. () Estimating the error or uncertainty on dose assessments made from single measurements. Here, intake, model parameter and measurement uncertainties are propagated into the measurement, but default ICRP parameter values are used to estimate the intake and dose from the measurement. ()Estimating the probability distribution of an individual's dose from a set of monitoring data. Here, Bayesian inference methods must be used to estimate the uncertainty on the estimated dose. A computer code is being developed that performs all three types of uncertainty analysis using Monte Carlo simulation. The software samples biokinetic parameters from probability density functions and then calculates doses from these parameters by calling the dosimetry code IMBA Professional Plus. A description of the methodology, together with an example application of the software, is included in this paper."		"Puncher, M|Birchall, A"	RADIATION PROTECTION DOSIMETRY		10.1093/rpd/ncm361
337	WOS:000259895700004	2008	Design of sustainable chemical processes: Systematic retrofit analysis generation and evaluation of alternatives	EFFICIENCY NETWORKS	"The objective of this paper is to present a generic and systematic methodology for identifying the feasible retrofit design alternatives of any chemical process. The methodology determines a set of mass and energy indicators from steady-state process data, establishes the operational and design targets, and through a sensitivity-based analysis, identifies the design alternatives that can match a set of design targets. The significance of this indicator-based method is that it is able to identify alternatives, where one or more performance criteria (factors) move in the same direction thereby eliminating the need to identify tradeoff-based solutions. These indicators are also able to reduce (where feasible) a set of safety indicators. An indicator sensitivity analysis algorithm has been added to the methodology to define design targets and to generate sustainable process alternatives. A computer-aided tool has been developed to facilitate the calculations needed for the application of the methodology. The application of the indicator-based methodology and the developed software are highlighted through a process flowsheet for the production of vinyl chlorine monomer (VCM). (C)  The Institution of Chemical Engineers."	 Published by Elsevier B.V. All rights reserved.	"Carvalho, A|Gani, R|Matos, H"	PROCESS SAFETY AND ENVIRONMENTAL PROTECTION	process design sustainability metrics mass and energy indicators safety index indicator sensitivity algorithm	10.1016/j.psep.2007.11.003
338	WOS:000282320800018	2010	Effect of fracture zone on DNAPL transport and dispersion: a numerical approach	MULTIPHASE FLOW GROUNDWATER DISSOLUTION MIGRATION AQUIFER POOLS MEDIA	"Two numerical simulation techniques have been used to identify a suitable method to assist in the characterization of DNAPL movement within fractured porous rock aquifers. Both MODFLOW and UTCHEM software modeling suites were used to simulate different scenarios in fracture dip and hydraulic conductivities. The complexity and the physical structure of fracture characterization were shown to have a significant effect on modeling results, to the extent that fracture zone should be characterized fully before simulation models are used for DNAPL simulations. Sensitivity analysis was conducted on both the hydraulic conductivity and fracture dip values. DNAPL movement in the subsurface showed a high sensitivity to fracture dip variation."		"Dennis, I|Pretorius, J|Steyl, G"	ENVIRONMENTAL EARTH SCIENCES	dnapl fracture dip numerical simulation hydraulic conductivity	10.1007/s12665-010-0468-8
339	WOS:000275621700002	2010	"Controlling setup cost in (Q, r, L) inventory model with defective items"	LEAD TIME REDUCTION	"This study discusses a mixture inventory model with back orders and lost sales in which the order quantity, reorder point, lead time and setup cost are decision variables. It is assumed that an arrival order lot may contain some defective items and the number of defective items is a random variable. There are two inventory models proposed in this paper, one with normally distributed demand and another with distribution free demand. Finally we develop two computational algorithms to obtain the optimal ordering policy, A computer code using the software Matlab is developed to derive the optimal solution and present numerical examples to illustrate the models. Additionally, sensitivity analysis is conducted with respect to the various system parameters."	 (C) 2009 Elsevier Inc. All rights reserved.	"Annadurai, K|Uthayakumar, R"	APPLIED MATHEMATICAL MODELLING	setup cost lead time defective items minimax distribution-free procedure computational algorithm optimization	10.1016/j.apm.2009.04.010
340	WOS:000361481300001	2015	Multisite Assessment of Hydrologic Processes in Snow-Dominated Mountainous River Basins in Colorado Using a Watershed Model	CONTERMINOUS UNITED-STATES LAND-COVER DATABASE CLIMATE-CHANGE SWAT MODEL AUTOMATIC CALIBRATION SENSITIVITY-ANALYSIS GLOBAL OPTIMIZATION VALIDATION SIMULATIONS COMPLETION	"Hydrologic fluxes in mountainous watersheds are particularly important as these areas often provide a significant source of freshwater for more arid surrounding lowlands. The state of Colorado in the United States comprises a principal snow catchment area, with all major headwater river basins in Colorado providing substantial water flows to surrounding western and midwestern states. The ability to represent and quantify hydrologic processes controlling the generation and movement of water in headwater basins of Colorado therefore has significant implications for effective management of water resources in the western United States under varying climatic and land-use conditions. In the research reported in this paper, hydrologic modeling was applied to four snow-dominated, mountainous basins of Colorado [i.e.,the river basins of ()Cache la Poudre, ()Gunnison, ()San Juan, and ()Yampa] to evaluate the relevance of specific hydrologic components (i.e.,evapotranspiration, snow processes, groundwater processes, surface runoff, and so on) in the complex, high-elevation watersheds. The soil and water assessment tool (SWAT) model was calibrated and tested for multiple river locations within each basin using monthly naturalized flows over the - period. The model was able to adequately simulate streamflows at all locations within the four basins. Monthly patterns of precipitation, snowfall, evapotranspiration (ET), and total water yield were similar for all the basins, while subsurface lateral flow was the dominant hydrologic pathway, contributing between  and % to gross basin water yields on an average annual basis. Overall, results indicated the strong influence of snowmelt and groundwater processes on amounts and timing of streamflows in the study basins. Hence, enhanced representation of these processes may be essential to improve hydrological estimation using computer software in snowmelt-driven mountainous basins. In particular, examination of monthly streamflow residuals indicated that the normality and independence of model residuals, which are often assumed in parameter estimation and uncertainty analysis, were not always satisfied."		"Foy, C|Arabi, M|Yen, H|Gironas, J|Bailey, RT"	JOURNAL OF HYDROLOGIC ENGINEERING	watershed modeling hydrological processes mountainous watersheds snow processes soil and water assessment tool (swat)	10.1061/(ASCE)HE.1943-5584.0001130
341	WOS:000231058700001	2005	A user's guide to the brave new world of designing simulation experiments	COMPUTER EXPERIMENTS SENSITIVITY-ANALYSIS SAMPLING CRITERIA ROBUST DESIGN OUTPUT MODEL OPTIMIZATION METHODOLOGY METAMODELS MANAGEMENT	"Many simulation practitioners can get more from their analyses by using the statistical theory on design of experiments (DOE) developed specifically for exploring computer models. We discuss a toolkit of designs for simulators with limited DOE expertise who want to select a design and an appropriate analysis for their experiments. Furthermore, we provide a research agenda listing problems in the design of simulation experiments-as opposed to real-world experiments-that require more investigation. We consider three types of practical problems: () developing a basic understanding of a particular simulation model or system, () finding robust decisions or policies as opposed to so-called optimal solutions, and () comparing the merits of various decisions or policies. Our discussion emphasizes aspects that are typical for simulation, such as having many more factors than in real-world experiments, and the sequential nature of the data collection. Because the same problem type may be addressed through different design types, we discuss quality attributes of designs, such as the ease of design construction, the flexibility for analysis, and efficiency considerations. Moreover, the selection of the design type depends on the metamodel (response surface) that the analysts tentatively assume; for example, complicated metamodels require more simulation runs. We present several procedures to validate the metamodel estimated from a specific design, and we summarize a case study illustrating several of our major themes. We conclude with a discussion of areas that merit more work to achieve the potential benefits-either via new research or incorporation into standard simulation or statistical packages."		"Kleijnen, JPC|Sanchez, SM|Lucas, TW|Cioppa, TM"	INFORMS JOURNAL ON COMPUTING	simulation design of experiments metamodels latin hypercube sequential bifurcation robust design	10.1287/ijoc.1050.0136
343	WOS:000380760400073	2016	Life Cycle Assessment and Life Cycle Cost Analysis of Magnesia Spinel Brick Production	CEMENT INDUSTRY ENERGY LCA	"Sustainable use of natural resources in the production of construction materials has become a necessity both in Europe and Turkey. Construction products in Europe should have European Conformity (CE) and Environmental Product Declaration (EPD), an independently verified and registered document in line with the European standard EN . An EPD certificate can be created by performing a Life Cycle Assessment (LCA) study. In this particular work, an LCA study was carried out for a refractory brick production for environmental assessment. In addition to the LCA, the Life Cycle Cost (LCC) analysis was also applied for economic assessment. Firstly, a cradle-to-gate LCA was performed for one ton of magnesia spinel refractory brick. The CML IA method included in the licensed SimaPro .. software was chosen to calculate impact categories (namely, abiotic depletion, global warming potential, acidification potential, eutrophication potential, human toxicity, ecotoxicity, ozone depletion potential, and photochemical oxidation potential). The LCC analysis was performed by developing a cost model for internal and external cost categories within the software. The results were supported by a sensitivity analysis. According to the results, the production of raw materials and the firing process in the magnesia spinel brick production were found to have several negative effects on the environment and were costly."		"Ozkan, A|Gunkaya, Z|Tok, G|Karacasulu, L|Metesoy, M|Banar, M|Kara, A"	SUSTAINABILITY	cml method firing process global warming potential life cycle assessment (lca) life cycle cost (lcc) magnesia spinel brick refractory production	10.3390/su8070662
346	WOS:000379138500008	2016	A probabilistic projection of the transient flow equations with random system parameters and internal boundary conditions	FREQUENCY-RESPONSE METHOD POLYNOMIAL CHAOS WATER-HAMMER PIPELINES DESIGN	"This paper presents a novel probabilistic approach based on the polynomial chaos expansion that can model the uncertainty propagation from the beginning of a waterhammer simulation and not as an afterthought. Uncertainties are considered in pipe diameter, friction coefficient, and wave speed, as well as internal boundary conditions of leaks and blockages. The polynomial chaos expansion solver results are in an excellent agreement with those calculated by using a model employing the traditional method of characteristics. The probabilistic polynomial chaos approach has the advantage of being robust and more efficient than other non-intrusive methods such as Monte Carlo simulation, which requires thousands of iterations for sharp solutions. The polynomial chaos approach is further extended to solve for randomness in frequency domain using the transfer matrix method with results of comparable accuracy. With further developments, this probabilistic approach can be integrated within existing network modelling software for practical hydraulic engineering problems."		"Sattar, AMA"	JOURNAL OF HYDRAULIC RESEARCH	blockages leaks pipelines polynomial chaos expansion probabilistic analysis random variable transient flow waterhammer equations	10.1080/00221686.2016.1140682
347	WOS:000235218600050	2005	Comparison of deterministic and Monte Carlo methods in shielding design		"In shielding calculation, deterministic methods have some advantages and also some disadvantages relative to other kind of codes, such as Monte Carlo. The main advantage is the short computer time needed to find solutions while the disadvantages are related to the often-used build-up factor that is extrapolated from high to low energies or with unknown geometrical conditions, which can lead to significant errors in shielding results. The aim of this work is to investigate how good are some deterministic methods to calculating low-energy shielding, using attenuation coefficients and build-up factor corrections. Commercial software MicroShield . has been used as the deterministic code while MCNP has been used as the Monte Carlo code. Point and cylindrical sources with slab shield have been defined allowing comparison between the capability of both Monte Carlo and deterministic methods in a day-by-day shielding calculation using sensitivity analysis of significant parameters, such as energy and geometrical conditions."		"Oliveira, AD|Oliveira, C"	RADIATION PROTECTION DOSIMETRY		10.1093/prd/nci187
351	WOS:000414958700005	2017	Spatial analysis and health risk assessment of heavy metals concentration in drinking water resources	MONTE-CARLO-SIMULATION SURFACE-WATER SENSITIVITY-ANALYSIS GROUNDWATER CONTAMINATION POLLUTION CHINA RIVER GIS EXPOSURE	"The heavy metals available in drinking water can be considered as a threat to human health. Oncogenic risk of such metals is proven in several studies. Present study aimed to investigate concentration of the heavy metals including As, Cd, Cr, Cu, Fe, Hg, Mn, Ni, Pb, and Zn in  water supply wells and  water reservoirs within the cities Ardakan, Meibod, Abarkouh, Bafgh, and Bahabad. The spatial distribution of the concentration was carried out by the software ArcGIS. Such simulations as non-carcinogenic hazard and lifetime cancer risk were conducted for lead and nickel using Monte Carlo technique. The sensitivity analysis was carried out to find the most important and effective parameters on risk assessment. The results indicated that concentration of all metals in  wells (except iron in  cases) reached the levels mentioned in EPA, World Health Organization, and Pollution Control Department standards. Based on the spatial distribution results at all studied regions, the highest concentrations of metals were derived, respectively, for iron and zinc. Calculated HQ values for non-carcinogenic hazard indicated a reasonable risk. Average lifetime cancer risks for the lead in Ardakan and nickel in Meibod and Bahabad were shown to be . x (-), . x (-), and  x (-), respectively, demonstrating high carcinogenic risk compared to similar standards and studies. The sensitivity analysis suggests high impact of concentration and BW in carcinogenic risk."		"Fallahzadeh, RA|Ghaneian, MT|Miri, M|Dashti, MM"	ENVIRONMENTAL SCIENCE AND POLLUTION RESEARCH	groundwater metals health risk assessment monte carlo simulation sensitivity analysis geographic information systems	10.1007/s11356-017-0102-3
353	WOS:000086420500002	2000	Assessing the impact of managed-care on the distribution of length-of-stay using Bayesian hierarchical models	CANCER CLINICAL-TRIAL SURVIVAL-DATA FRAILTY	"Hierarchical models provide a useful framework for the complexities encountered in policy-relevant research in which the impact of social programs is being assessed. Such complexities include multi-site data, censored data and over-dispersion. In this paper, Bayesian inference through Markov Chain Monte Carlo methods is used for the analysis of a complex hierarchical log-normal model that shows the impact of a managed care strategy aimed at limiting length of hospital stays. Parameters in this model allow for variability in baseline length-of-stay as well as the program effect across hospitals. The authors demonstrate elicitation and sensitivity analysis with respect to prior distributions. All calculations for the posterior and predictive distributions were obtained using the software BUGS."		"Stangl, D|Huerta, G"	LIFETIME DATA ANALYSIS	hierarchical log-normal model prior elicitation gibbs sampling predictive distribution health policy	10.1023/A:1009691326989
354	WOS:000336668600061	2014	A New Algorithm for Small-Signal Analysis of DC-DC Converters	POWER CONVERTERS STABILITY ANALYSIS PWM CONVERTERS A SURVEY IMPLEMENTATION SYSTEMS	"This paper presents a new approach for small-signal analysis of all types of dc-dc converters with any number of topological modes within a switching cycle. So far, sampled-data modeling and sensitivity analysis are mostly used for such a purpose. In both cases, the switching conditions implicitly appear in the small-signal matrices which increases the complexity of the computation for the system with a larger number of topological modes. Here, we propose an alternative approach based on Filippov's method, which studies the effects of each switching separately. It uses a shooting method with an event detector to locate a periodic steady-state, and, when the Newton-Raphson search process of the shooting method converges, it also gives the Jacobian matrix. The algorithm can be easily implemented in a software program as an analytical tool which is expected to be useful for fast and accurate frequency-domain analysis (by small-signal transfer functions) to facilitate controller design. Moreover, since it uses the Filippov's method, this algorithm can also predict the slow-and fast-timescale instabilities."		"Mandal, K|Banerjee, S|Chakraborty, C"	IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS	dc-dc converters filippov's method resonant converters small-signal analysis	10.1109/TII.2013.2277942
358	WOS:000175645300003	2002	Automatic versus manual model differentiation to compute sensitivities and solve non-linear inverse problems		"Emerging tools for automatic differentiation (AD) of computer programs should be of great benefit for the implementation of many derivative-based numerical methods such as those used for inverse modeling. The Odyssee software, one such tool for Fortran  codes, has been tested on a sample model that solves a D non-linear diffusion-type equation. Odyssee offers both the forward and the reverse differentiation modes, that produce the tangent and the cotangent models, respectively. The two modes have been implemented on the sample application. A comparison is made with a manually-produced differentiated code for this model (MD), obtained by solving the adjoint equations associated with the model's discrete state equations. Following a presentation of the methods and tools and of their relative advantages and drawbacks, the performances of the codes produced by the manual and automatic methods are compared, in terms of accuracy and of computing efficiency (CPU and memory needs). The perturbation method (finite-difference approximation of derivatives) is also used as a reference. Based on the test of Taylor, the accuracy of the two AD modes proves to be excellent and as high as machine precision permits, a good indication of Odyssee's capability to produce error-free codes. In comparison, the manually-produced derivatives (MD) sometimes appear to be slightly biased, which is likely due to the fact that a theoretical model (state equations) and a practical model (computer program) do not exactly coincide, while the accuracy of the perturbation method is very uncertain. The MD code largely outperforms all other methods in computing efficiency, a subject of current research for the improvement of AD tools. Yet these tools can already be of considerable help for the computer implementation of many numerical methods, avoiding the tedious task of hand-coding the differentiation of complex algorithms."	 (C) 2002 Published by Elsevier Science Ltd.	"Elizondo, D|Cappelaere, B|Faure, C"	COMPUTERS & GEOSCIENCES	code differentiation optimization adjoint state data assimilation sensitivity analysis odyssee	10.1016/S0098-3004(01)00048-6
360	WOS:000419231500134	2017	Passive Optimization Design Based on Particle Swarm Optimization in Rural Buildings of the Hot Summer and Warm Winter Zone of China	ENERGY PERFORMANCE RESIDENTIAL BUILDINGS SENSITIVITY-ANALYSIS MULTIOBJECTIVE OPTIMIZATION HIGHRISE BUILDINGS THERMAL COMFORT SIMULATION CONSUMPTION ENVELOPE SYSTEM	"The development of green building is an important way to solve the environmental problems of China's construction industry. Energy conservation and energy utilization are important for the green building evaluation criteria (GBEC). The objective of this study is to evaluate the quantitative relationship between building shape parameter, envelope parameters, shading system, courtyard and the energy consumption (EC) as well as the impact on indoor thermal comfort of rural residential buildings in the hot summer and warm winter zone (HWWZ). Taking Quanzhou (Fujian Province of China) as an example, based on the field investigation, EnergyPlus is used to build the building performance model. In addition, the classical particle swarm optimization algorithm in GenOpt software is used to optimize the various factors affecting the EC. Single-objective optimization has provided guidance to the multi-dimensional optimization and regression analysis is used to find the effects of a single input variable on an output variable. Results shows that the energy saving rate of an optimized rural residence is about -% corresponding to the existing rural residence. Moreover, the payback period is about  years. A simple case study is used to demonstrate the accuracy of the proposed optimization analysis. The optimization can be used to guide the design of new rural construction in the area and the energy saving transformation of the existing rural houses, which can help to achieve the purpose of energy saving and comfort."		"Lu, SL|Wang, R|Zheng, SQ"	SUSTAINABILITY	rural residence green building energy consumption multidimensional optimization particle swarm optimization regression analysis	10.3390/su9122288
361	WOS:000352040700012	2015	A comparative life cycle assessment of conventional hand dryer and roll paper towel as hand drying methods	UNCERTAINTY HYGIENE	"A comparative life cycle assessment, under a cradle to gate scope, was carried out between two hand drying methods namely conventional hand dryer use and dispenser issued roll paper towel use. The inventory analysis for this study was aided by the deconstruction of a hand dryer and dispenser unit besides additional data provided by the Physical Resources department, from the product system manufacturers and information from literature. The LCA software SimaPro, supported by the ecoinvent and US-EI databases, was used towards establishing the environmental impacts associated with the lifecycle stages of both the compared product systems. The Impact + method was used for classification and characterization of these environmental impacts. An uncertainty analysis addressing key input data and assumptions made, a sensitivity analysis covering the use intensity of the product systems and a scenario analysis looking at a US based use phase for the hand dryer were also conducted. Per functional unit, which is to achieve a pair of dried hands, the dispenser product system has a greater life cycle impact than the dryer product system across three of four endpoint impact categories. The use group of lifecycle stages for the dispenser product system, which represents the cradle to gate lifecycle stages associated with the paper towels, constitutes the major portion of this impact. For the dryer product system, the use group of lifecycle stages, which essentially covers the electricity consumption during dryer operation, constitutes the major stake in the impact categories. It is evident from the results of this study that per dry, for a use phase supplied by Ontario's grid ( grid mix scenario) and a United States based manufacturing scenario, the use of a conventional hand dryer (rated at  W and under a  s use intensity) has a lesser environmental impact than with using two paper towels (% recycled content, unbleached and weighing  g) issued from a roll dispenser."	 (C) 2015 Elsevier B.V. All rights reserved.	"Joseph, T|Baah, K|Jahanfar, A|Dubey, B"	SCIENCE OF THE TOTAL ENVIRONMENT	hand dryer paper towel life cycle assessment impact 2002+ uncertainty analysis	10.1016/j.scitotenv.2015.01.112
362	WOS:000329561100024	2014	A comprehensive evaluation of various sensitivity analysis methods: A case study with a hydrological model	RAINFALL-RUNOFF MODELS IMPROVED CALIBRATION COMPUTER EXPERIMENTS GLOBAL OPTIMIZATION REGRESSION INDEXES SYSTEMS OUTPUT	"Sensitivity analysis (SA) is a commonly used approach for identifying important parameters that dominate model behaviors. We use a newly developed software package, a Problem Solving environment for Uncertainty Analysis and Design Exploration (PSUADE), to evaluate the effectiveness and efficiency of ten widely used SA methods, including seven qualitative and three quantitative ones. All SA methods are tested using a variety of sampling techniques to screen out the most sensitive (i.e., important) parameters from the insensitive ones. The Sacramento Soil Moisture Accounting (SAC-SMA) model, which has thirteen tunable parameters, is used for illustration. The South Branch Potomac River basin near Springfield, West Virginia in the U.S. is chosen as the study area. The key findings from this study are: () For qualitative SA methods, Correlation Analysis (CA), Regression Analysis (RA), and Gaussian Process (GP) screening methods are shown to be not effective in this example. Morris One-At-a-Time (MOAT) screening is the most efficient, needing only  samples to identify the most important parameters, but it is the least robust method. Multivariate Adaptive Regression Splines (MARS), Delta Test (DT) and Sum-Of-Trees (SOT) screening methods need about - samples for the same purpose. Monte Carlo (MC), Orthogonal Array (OA) and Orthogonal Array based Latin Hypercube (OALH) are appropriate sampling techniques for them; () For quantitative SA methods, at least  samples are needed for Fourier Amplitude Sensitivity Test (FAST) to identity parameter main effect. McKay method needs about  samples to evaluate the main effect, more than  samples to assess the two-way interaction effect. OALH and LP tau (LPTAU) sampling techniques are more appropriate for McKay method. For the Sobol' method, the minimum samples needed are  to compute the first-order and total sensitivity indices correctly. These comparisons show that qualitative SA methods are more efficient but less accurate and robust than quantitative ones. (C)  The Authors. Published by"	 Elsevier Ltd. All rights reserved.	"Gan, YJ|Duan, QY|Gong, W|Tong, C|Sun, YW|Chu, W|Ye, AZ|Miao, CY|Di, ZH"	ENVIRONMENTAL MODELLING & SOFTWARE	uncertainty quantification sensitivity analysis parameter screening space-filling sampling psuade	10.1016/j.envsoft.2013.09.031
363	WOS:000283094800003	2010	Uncertainty analysis for estimation of landfill emissions and data sensitivity for the input variation		"Results of research and practical experience confirm that stabilization of GHG concentrations will require a tremendous effort. One of the sectors identified as a significant source of methane (CH()) emissions are solid waste disposal sites (SWDS). Landfills are the key source of CH() emissions in the emissions inventory of Slovakia, and the actual emission factors are estimated with a high uncertainty level. The calculation of emission uncertainty of the landfills using the more sophisticated Tier  Monte Carlo method is evaluated in this article. The software package that works with the probabilistic distributions and their combination was developed with this purpose in mind. The results, sensitivity analysis, and computational methodology of the CH() emissions from SWDS are presented in this paper."		"Szemesova, J|Gera, M"	CLIMATIC CHANGE		10.1007/s10584-010-9919-1
366	WOS:000252516900010	2008	Self-adjoint sensitivity analysis of lossy dielectric structures with electromagnetic time-domain simulators	MICROWAVE IMAGE-RECONSTRUCTION OPTIMAL-DESIGN METHOD GRIDS	"We present an efficient self-adjoint approach for the computation of response derivatives in lossy inhomogencous structures with time-domain electromagnetic solvers. Our approach yields the responses and their derivatives with only one system analysis regardless of the number of optimizable parameters. The only requirement is to access the field solution at the perturbation grid points. The computation is performed as an independent post-process outside the solver. This makes our approach easy to implement as stand-alone software, which aids microwave design based on commercial computer-aided design packages. We show that our sensitivity analysis approach yields Jacobians of second-order accuracy for lossy dielectric structures. The approach is verified through -D, -D and -D examples using the time-domain field solutions obtained with solvers based on the finite-difference time-domain (FDTD) and transmission line modeling (TLM) methods."	" Copyright (c) 2007 John Wiley & Sons, Ltd."	"Song, YP|Li, Y|Nikolova, NK|Bakr, MH"	INTERNATIONAL JOURNAL OF NUMERICAL MODELLING-ELECTRONIC NETWORKS DEVICES AND FIELDS	time-domain analysis sensitivity analysis adjoint-variable methods tlm method fdtd method	10.1002/jnm.659
367	WOS:000264171200019	2009	A Life Cycle Comparison of Alternative Cheese Packages		"A comparative life cycle assessment (LCA) between three different cheese packages (P: completely polypropylene (PP), P: tin and polyethylene (PE), and P: carton and PE) has been carried out for the production, distribution and waste disposal (% landfill) phase. A package for  kg of cheese was selected as the functional unit. SimaPro software (PReConsultants, The Netherlands) was used for the LCA study. The EcoIndicator  method was selected for comparison of the packages. The comparisons show that the total environmental performance of the cheese package types in order from worst to best is P, P, and P. This conclusion was supported by a sensitivity analysis, which was conducted by using different impact assessment methods."		"Banar, M|Cokaygil, Z"	CLEAN-SOIL AIR WATER	ecoindicator 99 food technology landfilling life cycle assessment packaging simapro7	10.1002/clen.200700185
369	WOS:000395529100004	2017	Monte Carlo Approach for Uncertainty Analysis of Acoustic Doppler Current Profiler Discharge Measurement by Moving Boat	ADCP VELOCITY FRAMEWORK VARIANCE	"This paper presents a method using Monte Carlo simulations for assessing uncertainty of moving-boat acoustic Doppler current profiler (ADCP) discharge measurements using a software tool known as QUant, which was developed for this purpose. Analysis was performed on  data sets from four Water Survey of Canada gauging stations in order to evaluate the relative contribution of a range of error sources to the total estimated uncertainty. The factors that differed among data sets included the fraction of unmeasured discharge relative to the total discharge, flow nonuniformity, and operator decisions about instrument programming and measurement cross section. As anticipated, it was found that the estimated uncertainty is dominated by uncertainty of the discharge in the unmeasured areas, highlighting the importance of appropriate selection of the site, the instrument, and the user inputs required to estimate the unmeasured discharge. The main contributor to uncertainty was invalid data, but spatial inhomogeneity in water velocity and bottom-track velocity also contributed, as did variation in the edge velocity, uncertainty in the edge distances, edge coefficients, and the top and bottom extrapolation methods. To a lesser extent, spatial inhomogeneity in the bottom depth also contributed to the total uncertainty, as did uncertainty in the ADCP draft at shallow sites. The estimated uncertainties from QUant can be used to assess the adequacy of standard operating procedures. They also provide quantitative feedback to the ADCP operators about the quality of their measurements, indicating which parameters are contributing most to uncertainty, and perhaps even highlighting ways in which uncertainty can be reduced. Additionally, QUant can be used to account for self-dependent error sources such as heading errors, which are a function of heading. The results demonstrate the importance of a Monte Carlo method tool such as QUant for quantifying random and bias errors when evaluating the uncertainty of moving-boat ADCP measurements."		"Moore, SA|Jamieson, EC|Rainville, F|Rennie, CD|Mueller, DS"	JOURNAL OF HYDRAULIC ENGINEERING	moving-boat acoustic doppler current profiler (adcp) uncertainty monte carlo probabilistic stream gauging procedures	10.1061/(ASCE)HY.1943-7900.0001249
