AU,year,title,journal,citations,Avg. Citations,abstract,DOI,DOI link
"Yang, J | Reichert, P | Abbaspour, KC | Xia, J | Yang, H",2008,Comparing uncertainty analysis techniques for a SWAT application to the Chaohe Basin in China,Applications_JOURNAL OF HYDROLOGY,316,31.6,"Distributed watershed models are increasingly being used to support decisions about alternative management strategies in the areas of land use change, climate change, water allocation, and pollution control. For this reason it is important that these models pass through a careful calibration and uncertainty analysis. To fulfil this demand, in recent years, scientists have come up with various uncertainty analysis techniques for watershed models. To determine the differences and similarities of these techniques we compared five uncertainty analysis procedures: Generalized Likelihood Uncertainty Estimation (GLUE), Parameter Solution (ParaSol), Sequential Uncertainty Fitting algorithm (SUFI-), and a Bayesian framework implemented using Markov chain Monte Carlo (MCMC) and Importance Sampling (IS) techniques. As these techniques are different in their philosophies and leave the user some freedom in formulating the generalized likelihood measure, objective function, or likelihood function, a literal comparison between these techniques is not possible. As there is a small spectrum of different applications in hydrology for the first three techniques, we made this choice according to their typical, use in hydrology. For Bayesian inference, we used a recently developed likelihood function that does not obviously violate the statistical assumptions, namely a continuous-time autoregressive error model. We implemented all these techniques for the soil and water assessment toot (SWAT) and applied them to the Chaohe Basin in China. We compared the results with respect to the posterior parameter distributions, performances of their best estimates, prediction uncertainty, conceptual bases, computational efficiency, and difficulty of implementation. The comparison results for these categories are listed and the advantages and disadvantages are analyzed. From the point of view of the authors, if computationally feasible, Bayesian-based approaches are most recommendable because of their solid conceptual basis, but construction and test of the likelihood function requires critical attention.",10.1016/j.jhydrol.2008.05.012,https://dx.doi.org/10.1016/j.jhydrol.2008.05.012
"Wang, YQ | Luo, Z | Kang, Z | Zhang, N",2015,A multi-material level set-based topology and shape optimization method,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,69,23.0,"This paper proposes a new Multi-Material Level Set (MM-LS) topology description model for topology and shape optimization of structures involving multiple materials. Each phase is represented by a combined formulation of different level set functions. With a total number of M level set functions, this approach provides a representation of M materials and one void phase (totally M +  phases). The advantages of the proposed method include: () it can guarantee that each point contains exactly one phase, without overlaps between each two phases and redundant regions within the design domain; () it possesses an explicit mathematical expression, which greatly facilitates the design sensitivity analysis; and () it retains the merits of the level set method, including smooth boundary and distinct interface. A parametric level set method is applied to evolve the topology and shape of multi-material structures, with a high computational efficiency. Several numerical examples are presented to demonstrate the effectiveness of the proposed method.",10.1016/j.cma.2014.11.002,https://dx.doi.org/10.1016/j.cma.2014.11.002
"McVicar, TR | Van Niel, TG | Li, LT | Hutchinson, MF | Mu, XM | Liu, ZH",2007,Spatially distributing monthly reference evapotranspiration and pan evaporation considering topographic influences,Applications_JOURNAL OF HYDROLOGY,205,18.64,"Many hydrological models engage spatially distributed measures of 'potential evapotranspiration' (ETpot). The reliability and utility of the physically based Penman-Monteith approach to generate ETpot has been recently advocated. Assuming land-surface conditions, spatial surfaces of reference evapotranspiration (ETO) can be generated taking into account the topographic influence of forcing meteorological variables. This was performed in this paper by spatially interpolating maximum (T-max) and minimum (T-min) air temperatures, wind speed (u) and vapor pressure (ea), using a spline model with a linear sub-model dependency on elevation, and modelling the radiation environment, taking topography (i.e., elevation, slope and aspect) into account, prior to calculating ETO at each grid-cell. In accordance with previous research, resultant lapse rates showed a strong seasonal pattern; values were steeper in summer than winter and those for Tma, were steeper than for T-min. Monthly mean T-max lapse rates varied from -. degrees C km(-) in winter to -. degrees C km(-) in summer, with T-min lapse rates ranging from -. degrees C km(-) in winter, to -. degrees C km(-) in summer. Monthly climatotogies of the near-surface elevation-dependence (NSED) for u and ea also showed strong seasonal values. NSED of u varied from . ms(-) km(-) in winter reducing to . ms(-) km(-) in summer. The NSED for e(a) ranged from -. kPa km(-) in winter to -. kPa km(-) in summer. For a -month sequence from  through , spatial surfaces of ET with a  m resolution for the , km() study site located in the Loess Plateau, China were generated using an 'interpolate-then-catculate' approach. Resultant ET values varied from about  mm month(-) in winter to over  mm month(-) in summer. In order to assess the reliability of these ETO surfaces, pan evaporation (Ep,,,) was also spatially interpolated and from these a set of pan coefficient (K-pan - a unitless ratio defined as ET/E-pan) surfaces were calculated. Spatio-temporally averaged K-pan values for the study site varied from . in April to . in late summer. K-pan values were in agreement with another study using a Chinese  cm diameter micro-pan, and, as expected, were tower than other values documented using a Class A pan. The influence of topography, especially aspect, was seen on the resultant ETO and K-pan, but not E-pan, surfaces. Sensitivity analysis showed that results were particularly stable in the hydrologically active portion of the year extending from March to October, inclusive. This study demonstrated that high spatial resolution monthly surfaces of ETO can be spatially modelled while taking into account the influence of topography on the forcing variables.",10.1016/j.jhydrol.2007.02.018,https://dx.doi.org/10.1016/j.jhydrol.2007.02.018
"Gong, LB | Xu, CY | Chen, DL | Halldin, S | Chen, YQD",2006,Sensitivity of the Penman-Monteith reference evapotranspiration to key climatic variables in the Changjiang (Yangtze River) basin,Applications_JOURNAL OF HYDROLOGY,200,16.67,"Sensitivity analysis is important in understanding the relative importance of climatic variables to the variation of reference evapotranspiration (ETref). In this study, a non-dimensional relative sensitivity coefficient was employed to predict responses of ETref to perturbations of four climatic variables in the Changjiang (Yangtze River) basin. ETref was estimated with the FAO- Penman-Monteith equation. A -year historical dataset of daily air temperature, wind speed, relative humidity and daily sunshine duration at  national meteorological observatory stations was used in the analysis. Results show that the response of ETref can be precisely predicted under perturbation of relative humidity or shortwave radiation by their sensitivity coefficients; the predictive power under perturbations of air temperature and wind speed depended on the magnitude of the perturbation, season and region. The prediction errors were much smaller than the seasonal and regional variation of their sensitivity coefficients. The sensitivity coefficient could also be used to predict the response of ETref to co-perturbation of several variables. The accuracy of the prediction increases from the lower to the upper region. Spatial variations of long-term average monthly and yearly sensitivity coefficients were obtained by interpolation of station estimates. In general, relative humidity was the most sensitive variable, followed by shortwave radiation, air temperature and wind speed. The actual rank of the four climatic variables in terms of their sensitivity varied with season and region. The large spatial variability of the sensitivity coefficients of all the climatic variables in the middle and tower regions of the basin was to a large extent determined by the distinct wind-speed patterns in those two regions.",10.1016/j.jhydrol.2006.03.027,https://dx.doi.org/10.1016/j.jhydrol.2006.03.027
"Oh, HJ | Kim, YS | Choi, JK | Park, E | Lee, S",2011,"GIS mapping of regional probabilistic groundwater potential in the area of Pohang City, Korea",Applications_JOURNAL OF HYDROLOGY,109,15.57,"This study analyzed the relationships between groundwater specific capacity (SPC) and its related hydrological factors to assess the sensitivity of each factor and map the regional groundwater potential for the area of Pohang City, Korea, using a geographic information system (GIS) and a probability model. All related factors including topography, geology, lineament, and soil data were collected and entered into a spatial database. SPC data were collected from well locations, and SPC values of >= . m()/d/m, corresponding to a yield of  m()/d, were input to a spatial database. SPC data were then randomly selected in a / ratio to train and validate the model. A frequency-ratio model and sensitivity analysis were used to determine the relationships between SPC and its related factors and the importance of SPC-related factors. Sensitivity analysis allows for comparison of the combined effects of all factors except for one. The validation of the groundwater potential map overlain by all factors showed .% accuracy. In the sensitivity analysis, the best accuracy was obtained by omitting ground elevation data (.%), and the worst accuracy resulted when soil texture was not included (.%). The results show that soil texture had the greatest effect on the groundwater potential and ground elevation had the least effect. Such information and the maps generated from it can be applied to groundwater management and groundwater resource exploration.",10.1016/j.jhydrol.2010.12.027,https://dx.doi.org/10.1016/j.jhydrol.2010.12.027
"Pappenberger, F | Beven, K | Horritt, M | Blazkova, S",2005,Uncertainty in the calibration of effective roughness parameters in HEC-RAS using inundation and downstream level observations,Applications_JOURNAL OF HYDROLOGY,201,15.46,"An uncertainty analysis of the unsteady flow component (UNET) of the one-dimensional model HEC-RAS within the generalised likelihood uncertainty estimation (GLUE) is presented. For this, the model performance of runs with different sets of Manning roughness coefficients, chosen from a range between . and ., are compared to inundation data and an outflow hydrograph. The influence of variation in the weighting coefficient of the numerical scheme is also investigated. For the latter, the empirical results show no advantage of using values below  and suggest the use of a fully implicit scheme (weighting parameter equals ). The results of varying the reach scale roughnesses shows that many parameter sets can perform equally well (problem of equifinality) even with extreme values. However, this depends on the model region and boundary conditions. The necessity to distinguish between effective parameters and real physical parameters is emphasised. The study demonstrates that this analysis can be used to produce dynamic probability maps of flooding during an event and can be linked to a stopping criterion for GLUE.",10.1016/j.jhydrol.2004.06.036,https://dx.doi.org/10.1016/j.jhydrol.2004.06.036
"Sattar, AMA | Gharabaghi, B",2015,Gene expression models for prediction of longitudinal dispersion coefficient in streams,Applications_JOURNAL OF HYDROLOGY,46,15.33,"Longitudinal dispersion is the key hydrologic 'process that governs transport of pollutants in natural streams. It is critical for spill action centers to be able to predict the pollutant travel time and breakthrough curves accurately following accidental spills in urban streams. This study presents a novel gene expression model for longitudinal dispersion developed using  published data sets of geometric and hydraulic parameters in natural streams in the United States, Canada, Europe, and New Zealand. The training and testing of the model were accomplished using randomly-selected % ( data sets) and % ( data sets) of the data sets, respectively. Gene expression programming (GEP) is used to develop empirical relations between the longitudinal dispersion coefficient and various control variables, including the Froude number which reflects the effect of reach slope, aspect ratio, and the bed material roughness on the dispersion coefficient. Two GEP models have been developed, and the prediction uncertainties of the developed GEP models are quantified and compared with those of existing models, showing improved prediction accuracy in favor of GEP models. Finally, a parametric analysis is performed for further verification of the developed GEP models. The main reason for the higher accuracy of the GEP models compared to the existing regression models is that exponents of the key variables (aspect ratio and bed material roughness) are not constants but a function of the Froude number. The proposed relations are both simple and accurate and can be effectively used to predict the longitudinal dispersion coefficients in natural streams.",10.1016/j.jhydrol.2015.03.016,https://dx.doi.org/10.1016/j.jhydrol.2015.03.016
"Brunetti, G | Simunek, J | Turco, M | Piro, P",2017,On the use of surrogate-based modeling for the numerical analysis of Low Impact Development techniques,Applications_JOURNAL OF HYDROLOGY,15,15.0,"Mechanistic models have proven to be accurate tools for the numerical analysis of the hydraulic behavior of Low Impact Development (LIDs) techniques. However, their widespread adoption has been limited by their computational cost. In this view, surrogate modeling is focused on developing and using a computationally inexpensive surrogate of the original model. While having been previously applied to various water-related and environmental modeling problems, no studies have used surrogate models for the analysis of LIDs. The aim of this research thus was to investigate the benefit of surrogate-based modeling in the numerical analysis of LIDs. The kriging technique was used to approximate the deterministic response of the widely used mechanistic model HYDRUS-D, which was employed to simulate the variably-saturated hydraulic behavior of a contained stormwater filter. The Nash-Sutcliffe efficiency (NSE) index was used to compare the simulated and measured outflows and as the variable of interest for the construction of the response surface. The validated kriging model was first used to carry out a Global Sensitivity Analysis of the unknown soil hydraulic parameters of the filter layer, revealing that only the shape parameter a and the saturated hydraulic conductivity Ks significantly affected the model response. Next, the Particle Swarm Optimization algorithm was used to estimate their values. The NSE value of . indicated a good accuracy of estimated parameters. Finally, the calibrated model was validated against an independent set of measured outflows with a NSE value of ., which again corroborated the reliability of the surrogate-based optimized parameters.",10.1016/j.jhydrol.2017.03.013,https://dx.doi.org/10.1016/j.jhydrol.2017.03.013
"Wang, ZP | Poh, LH | Dirrenberger, J | Zhu, YL | Forest, S",2017,Isogeometric shape optimization of smoothed petal auxetic structures via computational periodic homogenization,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,15,15.0,"An important feature that drives the auxetic behaviour of the star-shaped auxetic structures is the hinge-functional connection at the vertex connections. This feature poses a great challenge for manufacturing and may lead to significant stress concentrations. To overcome these problems, we introduced smoothed petal-shaped auxetic structures, where the hinges are replaced by smoothed connections. To accommodate the curved features of the petal-shaped auxetics, a parametrisation modelling scheme using multiple NURBS patches is proposed. Next, an integrated shape design frame work using isogeometric analysis is adopted to improve the structural performance. To ensure a minimum thickness for each member, a geometry sizing constraint is imposed via piece-wise bounding polynomials. This geometry sizing constraint, in the context of isogeometric shape optimization, is particularly interesting due to the non-interpolatory nature of NURBS basis. The effective Poisson ratio is used directly as the objective function, and an adjoint sensitivity analysis is carried out. The optimized designs smoothed petal auxetic structures are shown to achieve low negative Poisson's ratios, while the difficulties of manufacturing the hinges are avoided. For the case with six petals, an in-plane isotropy is achieved.",10.1016/j.cma.2017.05.013,https://dx.doi.org/10.1016/j.cma.2017.05.013
"Le Maitre, OP | Knio, OM | Najm, HN | Ghanem, RG",2004,Uncertainty propagation using Wiener-Haar expansions,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,209,14.93,"An uncertainty quantification scheme is constructed based on generalized Polynomial Chaos (PC) representations. Two such representations are considered, based on the orthogonal projection of uncertain data and solution variables using either a Haar or a Legendre basis. Governing equations for the unknown coefficients in the resulting representations are derived using a Galerkin procedure and then integrated in order to determine the behavior of the stochastic process. The schemes are applied to a model problem involving a simplified dynamical system and to the classical problem of Rayleigh-Benard instability. For situations involving random parameters close to a critical point, the computational implementations show that the Wiener-Haar (WHa) representation provides more robust predictions that those based on a Wiener-Legendre (WLe) decomposition. However, when the solution depends smoothly on the random data, the WLe scheme exhibits superior convergence. Suggestions regarding future extensions are finally drawn based on these experiences.",10.1016/j.jcp.2003.11.033,https://dx.doi.org/10.1016/j.jcp.2003.11.033
"Oberkampf, WL | Barone, MF",2006,Measures of agreement between computation and experiment: Validation metrics,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,176,14.67,"With the increasing role of computational modeling in engineering design, performance estimation, and safety assessment, improved methods are needed for comparing computational results and experimental measurements. Traditional methods of graphically comparing computational and experimental results, though valuable, are essentially qualitative. Computable measures are needed that can quantitatively compare computational and experimental results over a range of input, or control, variables to sharpen assessment of computational accuracy. This type of measure has been recently referred to as a validation metric. We discuss various features that we believe should be incorporated in a validation metric, as well as features that we believe should be excluded. We develop a new validation metric that is based on the statistical concept of confidence intervals. Using this fundamental concept, we construct two specific metrics: one that requires interpolation of experimental data and one that requires regression (curve fitting) of experimental data. We apply the metrics to three example problems: thermal decomposition of a polyurethane foam, a turbulent buoyant plume of helium, and compressibility effects on the growth rate of a turbulent free-shear layer. We discuss how the present metrics are easily interpretable for assessing computational model accuracy, as well as the impact of experimental measurement uncertainty on the accuracy assessment.",10.1016/j.jcp.2006.03.037,https://dx.doi.org/10.1016/j.jcp.2006.03.037
"Chen, YF | Zhou, JQ | Hu, SH | Hu, R | Zhou, CB",2015,Evaluation of Forchheimer equation coefficients for non-Darcy flow in deformable rough-walled fractures,Applications_JOURNAL OF HYDROLOGY,44,14.67,"This study focuses on experimental evaluation of the Forchheimer equation coefficients for non-Darcy flow in deformable rough-walled fractures. Water flow tests through twelve granite fracture samples with different roughness were conducted in a triaxial cell under confining stresses varying from . MPa to . MPa. A total of  experimental data in the form of pressure gradient versus discharge were collected. Three representative types of nonlinear flow behaviors induced by inertial effect, fracture dilation and solid water interaction, respectively, were observed. Regression analyses of the experimental data show that the Forchheimer equation adequately describes the non-Darcy flow behavior induced by significant inertial effect. Based on the experimental observations, two empirical equations were proposed for parametric expression of the Forchheimer's nonlinear coefficient, one as a power function of hydraulic aperture and the other dependent on both hydraulic aperture and peak asperity of the fracture surface. A new criterion was presented for assessing the applicability of Darcy's law, which relies on the ratio of discharge or pressure gradient predicted by the Forchheimer's law incorporated with the single-parameter equation to that predicted by the Darcy's law. A sensitivity analysis was performed using the double-parameter equation for examining the dependence of the Forchheimer's nonlinear coefficient on peak asperity, demonstrating the importance of incorporating the fracture roughness in the development of non-Darcy flow models. The experimental results and the proposed models are useful for understanding and numerical modeling of the nonlinear flow behaviors in fractured aquifers.",10.1016/j.jhydrol.2015.09.021,https://dx.doi.org/10.1016/j.jhydrol.2015.09.021
"Pedersen, CBW | Buhl, T | Sigmund, O",2001,Topology synthesis of large-displacement compliant mechanisms,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,237,13.94,"This paper describes the use of topology optimization as a synthesis tool for the design of large-displacement compliant mechanisms. An objective function for the synthesis of large-displacement mechanisms is proposed together with a formulation for synthesis of path-generating compliant mechanisms. The responses of the compliant mechanisms are modelled using a total Lagrangian finite element formulation, the sensitivity analysis is performed using the adjoint method and the optimization problem is solved using the method of moving asymptotes. Procedures to circumvent some numerical problems are discussed.",10.1002/nme.148,https://dx.doi.org/10.1002/nme.148
"Kersaudy, P | Sudret, B | Varsier, N | Picon, O | Wiart, J",2015,A new surrogate modeling technique combining Kriging and polynomial chaos expansions - Application to uncertainty analysis in computational dosimetry,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,40,13.33,"In numerical dosimetry, the recent advances in high performance computing led to a strong reduction of the required computational time to assess the specific absorption rate(SAR) characterizing the human exposure to electromagnetic waves. However, this procedure remains time-consuming and a single simulation can request several hours. As a consequence, the influence of uncertain input parameters on the SAR cannot be analyzed using crude Monte Carlo simulation. The solution presented here to perform such an analysis is surrogate modeling. This paper proposes a novel approach to build such a surrogate model from a design of experiments. Considering a sparse representation of the polynomial chaos expansions using least-angle regression as a selection algorithm to retain the most influential polynomials, this paper proposes to use the selected polynomials as regression functions for the universal Kriging model. The leave-one-outcross validation is used to select the optimal number of polynomials in the deterministic part of the Kriging model. The proposed approach, called LARS-Kriging-PC modeling, is applied to three benchmark examples and then to a full-scale metamodeling problem involving the exposure of a numerical fetus model to a femtocell device. The performances of the LARS-Kriging-PC are compared to an ordinary Kriging model and to a classical sparse polynomial chaos expansion. The LARS-Kriging-PC appears to have better performances than the two other approaches. A significant accuracy improvement is observed compared to the ordinary Kriging or to the sparse polynomial chaos depending on the studied case. This approach seems to be an optimal solution between the two other classical approaches. A global sensitivity analysis is finally performed on the LARS-Kriging-PC model of the fetus exposure problem.",10.1016/j.jcp.2015.01.034,https://dx.doi.org/10.1016/j.jcp.2015.01.034
"Stegmann, J | Lund, E",2005,Discrete material optimization of general composite shell structures,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,169,13.0,A novel method for doing material optimization of general composite laminate shell structures is presented and its capabilities are illustrated with three examples. The method is labelled Discrete Material Optimization (DMO) but uses gradient information combined with mathematical programming to solve a discrete optimization problem. The method can be used to solve the orientation problem of orthotropic materials and the material selection problem as well as problems involving both. The method relies on ideas from multiphase topology optimization to achieve a parametrization which is very general and reduces the risk of obtaining a local optimum solution for the tested configurations. The applicability of the DMO method is demonstrated for fibre angle optimization of a cantilever beam and combined fibre angle and material selection optimization of a four-point beam bending problem and a doubly curved laminated shell.,10.1002/nme.1259,https://dx.doi.org/10.1002/nme.1259
"Le Maitre, OP | Najm, HN | Ghanem, RG | Knio, OM",2004,Multi-resolution analysis of Wiener-type uncertainty propagation schemes,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,181,12.93,"A multi-resolution analysis (MRA) is applied to an uncertainty propagation scheme based on a generalized polynomial chaos (PC) representation. The MRA relies on an orthogonal projection of uncertain data and solution variables onto a multi-wavelet basis, consisting of compact piecewise-smooth polynomial functions. The coefficients of the expansion are computed through a Galerkin procedure. The MRA scheme is applied to the simulation of the Lorenz system having a single random parameter. The convergence of the solution with respect to the resolution level and expansion order is investigated. In particular, results are compared to two Monte-Carlo sampling strategies, demonstrating the superiority of the MRA. For more complex problems, however, the MRA approach may require excessive CPU times. Adaptive methods are consequently developed in order to overcome this drawback. Two approaches are explored: the first is based on adaptive refinement of the multi-wavelet basis, while the second is based on adaptive block-partitioning of the space of random variables. Computational tests indicate that the latter approach is better suited for large problems, leading to a more efficient, flexible and parallelizable scheme.",10.1016/j.jcp.2003.12.020,https://dx.doi.org/10.1016/j.jcp.2003.12.020
"Ma, X | Zabaras, N",2010,An adaptive high-dimensional stochastic model representation technique for the solution of stochastic partial differential equations,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,103,12.88,"A computational methodology is developed to address the solution of high-dimensional stochastic problems. It utilizes high-dimensional model representation (HDMR) technique in the stochastic space to represent the model output as a finite hierarchical correlated function expansion in terms of the stochastic inputs starting from lower-order to higher-order component functions HDMR is efficient at capturing the high-dimensional input-output relationship such that the behavior for many physical systems can be modeled to good accuracy only by the first few lower-order terms An adaptive version of HDMR is also developed to automatically detect the important dimensions and construct higher-order terms using only the important dimensions The newly developed adaptive sparse grid collocation (ASGC) method is incorporated into HDMR to solve the resulting sub-problems By integrating HDMR and ASGC, it is computationally possible to construct a low-dimensional stochastic reduced-order model of the high-dimensional stochastic problem and easily perform various statistic analysis on the output. Several numerical examples involving elementary mathematical functions and fluid mechanics problems are considered to illustrate the proposed method The cases examined show that the method provides accurate results for stochastic dimensionality as high as  even with large-input variability The efficiency of the proposed method is examined by comparing with Monte Carlo (MC) simulation (C)  Elsevier Inc All rights reserved",10.1016/j.jcp.2010.01.033,https://dx.doi.org/10.1016/j.jcp.2010.01.033
"Zhang, XS | Srinivasan, R | Bosch, D",2009,Calibration and uncertainty analysis of the SWAT model using Genetic Algorithms and Bayesian Model Averaging,Applications_JOURNAL OF HYDROLOGY,113,12.56,"In this paper, the Genetic Algorithms (GA) and Bayesian Model Averaging (BMA) were used to simultaneously conduct calibration and uncertainty analysis for the Soil and Water Assessment Tool (SWAT), In this combined method, several SWAT models with different structures are first selected; next GA is used to calibrate each model using observed streamflow data; finally, BMA is applied to combine the ensemble predictions and provide uncertainty interval estimation. This method was tested in two contrasting basins, the Little River Experimental Basin in Georgia, USA, and the Yellow River Headwater Basin in China. The results obtained in the two case studies show that this combined method can provide deterministic predictions better than or comparable to the best calibrated model using GA. The .% and % uncertainty intervals estimated by this method were analyzed. The differences between the percentage of coverage of observations and the corresponding expected coverage percentage are within % for both calibration and validation periods in these two test basins. This combined methodology provides a practical and flexible tool to attain reliable deterministic simulation and uncertainty analysis of SWAT.",10.1016/j.jhydrol.2009.06.023,https://dx.doi.org/10.1016/j.jhydrol.2009.06.023
"Takezawa, A | Nishiwaki, S | Kitamura, M",2010,Shape and topology optimization based on the phase field method and sensitivity analysis,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,100,12.5,"This paper discusses a structural optimization method that optimizes shape and topology based oil the phase field method. The proposed method has the same functional capabilities as a structural optimization method based oil the level set method incorporating perimeter control functions. The advantage of the method is the simplicity of computation, since extra operations such as re-initialization of functions are not required Structural shapes are represented by the phase field function defined in the design domain, and optimization of this function is performed by solving a time-dependent reaction diffusion equation. The artificial double Well potential function used in the equation is derived from sensitivity analysis The proposed method is applied to two-dimensional linear elastic and vibration optimization problems Such as the minimum compliance problem, a compliant mechanism design problem and the eigenfrequency maximization problem. The numerical examples provided Illustrate the convergence of the various objective functions and the effect that perimeter control has oil the optimal configurations (C)  Elsevier Inc All rights reserved.",10.1016/j.jcp.2009.12.017,https://dx.doi.org/10.1016/j.jcp.2009.12.017
"Le Coz, J | Renard, B | Bonnifait, L | Branger, F | Le Boursicaud, R",2014,Combining hydraulic knowledge and uncertain gaugings in the estimation of hydrometric rating curves: A Bayesian approach,Applications_JOURNAL OF HYDROLOGY,50,12.5,"Discharge time series in rivers and streams are usually based on simple stage-discharge relations calibrated using a set of direct stage-discharge measurements called gaugings. Bayesian inference recently emerged as a most promising framework to build such hydrometric rating curves accurately and to estimate the associated uncertainty. In addition to providing the rigorous statistical framework necessary to uncertainty analysis, the main advantage of the Bayesian analysis of rating curves arises from the quantitative assessment of (i) the hydraulic controls that govern the stage-discharge relation, and of (ii) the individual uncertainties of available gaugings, which often differ according to the discharge measurement procedure and the flow conditions. In this paper, we introduce the BaRatin method for the Bayesian analysis of stationary rating curves and we apply it to three typical cases of hydrometric stations with contrasted flow conditions and variable abundance of hydraulic knowledge and gauging data. The results exemplify that the thorough analysis of hydraulic controls and the quantification of gauging uncertainties are required to obtain reliable and physically sound results.",10.1016/j.jhydrol.2013.11.016,https://dx.doi.org/10.1016/j.jhydrol.2013.11.016
"Jakeman, JD | Eldred, MS | Sargsyan, K",2015,Enhancing l(1)-minimization estimates of polynomial chaos expansions using basis selection,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,37,12.33,"In this paper we present a basis selection method that can be used with l()-minimization to adaptively determine the large coefficients of polynomial chaos expansions (PCE). The adaptive construction produces anisotropic basis sets that have more terms in important dimensions and limits the number of unimportant terms that increase mutual coherence and thus degrade the performance of l()-minimization. The important features and the accuracy of basis selection are demonstrated with a number of numerical examples. Specifically, we show that for a given computational budget, basis selection produces a more accurate PCE than would be obtained if the basis were fixed a priori. We also demonstrate that basis selection can be applied with non-uniform random variables and can leverage gradient information.",10.1016/j.jcp.2015.02.025,https://dx.doi.org/10.1016/j.jcp.2015.02.025
"Diez, M | Campana, EF | Stern, F",2015,Design-space dimensionality reduction in shape optimization by Karhunen-Loeve expansion,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,37,12.33,"The paper presents a methodology to reduce the dimension of design spaces in shape optimization problems, while retaining a desired level of geometric variance. The method is based on a generalized Karhunen-Loeve expansion (KLE). Arbitrary shape modification spaces are assessed in terms of Karhunen-Loeve modes (eigenvectors) and associated geometric variance (eigenvalues). The former are used as a basis in order to build a reduced-dimensionality representation of the shape modification. The method is demonstrated for the shape optimization of a high-speed catamaran, based on CFD simulations and aimed at the reduction of the wave component of calm-water resistance. KLE is applied to three design spaces with large dimensionality (>= ), based on a free form deformation technique. The space with the largest geometric variance is selected for dimensionality reduction and design optimization. N-dimensional design spaces are used, with N = , , , and , retaining up to the % of the geometric variance associated to the original space. The correlation between the objective reduction achieved, the dimension N and the geometric variance of the reduced-dimensionality space is shown and found significant.",10.1016/j.cma.2014.10.042,https://dx.doi.org/10.1016/j.cma.2014.10.042
"Kiendl, J | Schmidt, R | Wuchner, R | Bletzinger, KU",2014,Isogeometric shape optimization of shells using semi-analytical sensitivity analysis and sensitivity weighting,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,49,12.25,"We present isogeometric shape optimization for shell structures applying sensitivity weighting and semi-analytical analysis. We use a rotation-free shell formulation and all involved geometry models, i.e., initial design, analysis model, optimization model, and final design use the same geometric basis, in particular NURBS. A sensitivity weighting scheme is presented which eliminates certain effects of the chosen discretization on the design update. A multilevel design approach is applied such that the design space can be chosen independently from the analysis space. The use of semi-analytical sensitivities allows having different polynomial degrees for design and analysis model. Different numerical examples are performed which confirm the applicability of the proposed method. Furthermore, a shape optimization example with an exact solution is presented which can serve as general benchmark for shape optimization methods.",10.1016/j.cma.2014.02.001,https://dx.doi.org/10.1016/j.cma.2014.02.001
"Shao, Q | Younes, A | Fahs, M | Mara, TA",2017,Bayesian sparse polynomial chaos expansion for global sensitivity analysis,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,12,12.0,"Polynomial chaos expansions are frequently used by engineers and modellers for uncertainty and sensitivity analyses of computer models. They allow representing the input/output relations of computer models. Usually only a few terms are really relevant in such a representation. It is a challenge to infer the best sparse polynomial chaos expansion of a given model input/output data set. In the present article, sparse polynomial chaos expansions are investigated for global sensitivity analysis of computer model responses. A new Bayesian approach is proposed to perform this task, based on the Kashyap information criterion for model selection. The efficiency of the proposed algorithm is assessed on several benchmarks before applying the algorithm to identify the most relevant inputs of a double-diffusive convection model.",10.1016/j.cma.2017.01.033,https://dx.doi.org/10.1016/j.cma.2017.01.033
"Chakraborty, S | Chowdhury, R",2017,Towards 'h-p adaptive' generalized ANOVA,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,12,12.0,"This paper presents a novel 'h-p adaptive' generalized analysis of variance decomposition (G-ANOVA) for high dimensional stochastic computations. Firstly, an iterative scheme based on variance based global sensitivity analysis is proposed to determine the optimal polynomial order and important component functions (p adaptivity) of G-ANOVA. Then, a homotopy algorithm is employed to determine the unknown expansion coefficients. Moreover, a novel distribution adaptive sequential experimental design (h-adaptivity) is utilized at each step and the accuracy of the G-ANOVA based surrogate model is computed using leave one out statistical test. It is observed that significantly less number of component functions are retained. As a consequence, the proposed approach is highly efficient and is applicable for solving problems involving large number of stochastic dimensions. Implementation of the proposed approach has been illustrated with four high dimensional applied mechanics problems. The proposed approach is found to yield accurate and efficient results. Finally, the proposed approach has been applied for structural reliability analysis of a large scale real life problem.",10.1016/j.cma.2017.03.028,https://dx.doi.org/10.1016/j.cma.2017.03.028
"Yin, SW | Yu, DJ | Yin, H | Xia, BZ",2017,A new evidence-theory-based method for response analysis of acoustic system with epistemic uncertainty by using Jacobi expansion,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,11,11.0,"Evidence theory has strong ability to handle epistemic uncertainties whose precise probability distributions cannot be obtained due to limited information. However, the excessive computational cost produced by repetitively extreme value analysis severely influences the practical application of evidence theory. This paper aims to develop an efficient algorithm for epistemic uncertainty analysis of acoustic problem under evidence theory. Based on the orthogonal polynomial approximation theory, a numerical approach named as the evidence-theory-based Jacobi expansion method (ETJEM) is proposed. In ETJEM, the response of acoustic system with evidence variables is approximated by Jacobi expansion, through which the repetitively extreme value analysis needed in evidence theory can be efficiently performed. The parametric Jacobi polynomial of Jacobi expansion holds a large number of polynomials as special cases, such as the Legendre polynomial and Chebyshev polynomial. Thus, the ETJEM permits a much wider choice of polynomial bases to control the error of approximation than the traditional evidence-theory-based orthogonal polynomial approximation method, in which only the Legendre polynomial is used for approximation. Three numerical examples are employed to demonstrate the effectiveness of the proposed methodology, including a mathematic problem with explicit expression and two engineering applications in acoustic field. In these three numerical examples, efficiency and accuracy are fully studied by comparing with Legendre expansion method as well as Monte Carlo simulations.",10.1016/j.cma.2017.04.020,https://dx.doi.org/10.1016/j.cma.2017.04.020
"Kharmanda, G | Olhoff, N | Mohamed, A | Lemaire, M",2004,Reliability-based topology optimization,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,154,11.0,"The objective of this work is to integrate reliability analysis into topology optimization problems. The new model, in which we introduce reliability constraints into a deterministic topology optimization formulation, is called Reliability-Based Topology Optimization (RBTO). Several applications show the importance of this integration. The application of the RBTO model gives a different topology relative to deterministic topology optimization. We also find that the RBTO model yields structures that are more reliable than those produced by deterministic topology optimization (for the same weight).",10.1007/s00158-003-0322-7,https://dx.doi.org/10.1007/s00158-003-0322-7
"Blasone, RS | Madsen, H | Rosbjerg, D",2008,Uncertainty assessment of integrated distributed hydrological models using GLUE with Markov chain Monte Carlo sampling,Applications_JOURNAL OF HYDROLOGY,108,10.8,"In recent years, there has been an increase in the application of distributed, physically-based and integrated hydrological models. Many questions regarding how to property calibrate and validate distributed models and assess the uncertainty of the estimated parameters and the spatially-distributed responses are, however, still quite unexplored. Especially for complex models, rigorous parameterization, reduction of the parameter space and use of efficient and effective algorithms are essential to facilitate the calibration process and make it more robust. Moreover, for these models multi-site validation must complement the usual time validation. In this study, we develop, through an application, a comprehensive framework for multi-criteria calibration and uncertainty assessment of distributed physically-based, integrated hydrological, models. A revised version of the generalized likelihood uncertainty estimation (GLUE) procedure based on Markov chain Monte Carlo sampling is applied in order to improve the performance of the methodology in estimating parameters and posterior output distributions. The description of the spatial variations of the hydrological. processes is accounted for by defining a measure of model performance that includes multiple criteria and spatially-distributed information. An initial sensitivity analysis is conducted on the model to avoid overparameterisation and to increase the robustness of the approach. It is demonstrated that the employed methodology increases the identifiability of the parameters and results in satisfactory multi-variable simulations and uncertainty estimates. However, the parameter uncertainty atone cannot explain the total uncertainty at all the sites, due to limitations in the distributed data included in the model. calibration. The study also indicates that property distributed information of discharge is particularly crucial in model calibration and validation.",10.1016/j.jhydrol.2007.12.026,https://dx.doi.org/10.1016/j.jhydrol.2007.12.026
"Huang, XD | Zhou, SW | Sun, GY | Li, GY | Xie, YM",2015,Topology optimization for microstructures of viscoelastic composite materials,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,32,10.67,"The viscoelastic response of materials is often utilized for wide applications such as vibration reduction devices. This paper extends the bi-directional evolutionary structural optimization (BESO) method to the design of composite microstructure with optimal viscoelastic characteristics. Both storage and loss moduli of composite materials are calculated through the homogenization theory using complex variables. Then, the BESO method is established based on the sensitivity analysis. Through iteratively redistributing the base material phases within the unit cell, optimized microstructures of composites with the desirable viscoelastic properties will be achieved. Numerical examples demonstrate the effectiveness of the proposed optimization method for the design of viscoelastic composite materials. Various microstructures of optimized composites are presented and discussed. Meanwhile, the storage and loss moduli of the optimized viscoelastic composites are compared with available theoretical bounds.",10.1016/j.cma.2014.10.007,https://dx.doi.org/10.1016/j.cma.2014.10.007
"Gersborg-Hansen, A | Sigmund, O | Haber, RB",2005,Topology optimization of channel flow problems,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,137,10.54,"This paper describes a topology design method for simple two-dimensional flow problems. We consider steady, incompressible laminar viscous flows at low-to-moderate Reynolds numbers. This makes the flow problem nonlinear and hence a nontrivial extension of the work of Borrvall and Petersson (). Further, the inclusion of inertia effects significantly alters the physics, enabling solutions of new classes of optimization problems, such as velocity-driven switches, that are not addressed by the earlier method. Specifically, we determine optimal layouts of channel flows that extremize a cost function which measures either some local aspect of the velocity field or a global quantity, such as the rate of energy dissipation. We use the finite element method to model the flow, and we solve the optimization problem with a gradient-based math-programming algorithm that is driven by analytical sensitivities. Our target application is optimal layout design of channels in fluid network systems. Using concepts borrowed from topology optimization of compliant mechanisms in solid mechanics, we introduce a method for the synthesis of fluidic components, such as switches, diodes, etc.",10.1007/s00158-004-0508-7,https://dx.doi.org/10.1007/s00158-004-0508-7
"Yaji, K | Yamada, T | Yoshino, M | Matsumoto, T | Izui, K | Nishiwaki, S",2016,Topology optimization in thermal-fluid flow using the lattice Boltzmann method,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,21,10.5,"This paper proposes a topology optimization method for thermal-fluid flow problems using the lattice Boltzmann method (LBM). The design sensitivities are derived based on the adjoint lattice Boltzmann method (ALBM), whose basic idea is that the adjoint problem is first formulated using a continuous adjoint approach, and the adjoint problem is then solved using the LBM. In this paper, the discrete velocity Boltzmann equation, in which only the particle velocities are discretized, is introduced to the ALBM to deal with the various boundary conditions in the LBM. The novel sensitivity analysis is applied in two flow channel topology optimization problems: ) a pressure drop minimization problem, and ) a heat exchange maximization problem. Several numerical examples are provided to confirm the utility of the proposed method.",10.1016/j.jcp.2015.12.008,https://dx.doi.org/10.1016/j.jcp.2015.12.008
"Kang, Z | Luo, YJ",2009,Non-probabilistic reliability-based topology optimization of geometrically nonlinear structures using convex models,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,94,10.44,"This paper describes a non-probabilistic reliability-based topology optimization method for the design of continuum structures undergoing large deformations. The variation of the structural system is treated with the multi-ellipsoid convex model, which is a realistic description of the parameters being inherently uncertain-but-bounded or lacking sufficient probabilistic data. The formulation of the optimal design is established as a volume minimization problem with non-probabilistic reliability constraints on the geometrically nonlinear structural behaviour. In order to circumvent numerical difficulties in solving the nested double-loop optimization problem, a performance measure-based approach is employed to transform the constraint on the reliability index into one on the concerned performance. In conjunction with an efficient adjoint variable scheme for the sensitivity analysis of reliability constraints, the optimization problem is solved by gradient-based mathematical programming methods. Three numerical examples for the optimization design of planar structures are presented to illustrate the validity and applicability of the proposed method. The obtained optimal solutions show the importance of incorporating various uncertainties in the design problem. Moreover, it is also revealed that the geometrical nonlinearity needs to be accounted for to ensure satisfaction of the reliability constraints in the optimal design of structures with large deformation.",10.1016/j.cma.2009.06.001,https://dx.doi.org/10.1016/j.cma.2009.06.001
"Gersborg-Hansen, A | Bendsoe, MP | Sigmund, O",2006,Topology optimization of heat conduction problems using the finite volume method,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,122,10.17,"This note addresses the use of the finite volume method (FVM) for topology optimization of a heat conduction problem. Issues pertaining to the proper choice of cost functions, sensitivity analysis, and example test problems are used to illustrate the effect of applying the FVM as an analysis tool for design optimization. This involves an application of the FVM to problems with nonhomogeneous material distributions, and the arithmetic and harmonic averages have here been used to provide a unique value for the conductivity at element boundaries. It is observed that when using the harmonic average, checkerboards do not form during the topology optimization process.",10.1007/s00158-005-0584-3,https://dx.doi.org/10.1007/s00158-005-0584-3
"Lee, SH | Chen, W",2009,A comparative study of uncertainty propagation methods for black-box-type problems,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,91,10.11,"A wide variety of uncertainty propagation methods exist in literature; however, there is a lack of good understanding of their relative merits. In this paper, a comparative study on the performances of several representative uncertainty propagation methods, including a few newly developed methods that have received growing attention, is performed. The full factorial numerical integration, the univariate dimension reduction method, and the polynomial chaos expansion method are implemented and applied to several test problems. They are tested under different settings of the performance nonlinearity, distribution types of input random variables, and the magnitude of input uncertainty. The performances of those methods are compared in moment estimation, tail probability calculation, and the probability density function construction, corresponding to a wide variety of scenarios of design under uncertainty, such as robust design, and reliability-based design optimization. The insights gained are expected to direct designers for choosing the most applicable uncertainty propagation technique in design under uncertainty.",10.1007/s00158-008-0234-7,https://dx.doi.org/10.1007/s00158-008-0234-7
"Yang, X | Lei, H | Baker, NA | Lin, G",2016,Enhancing sparsity of Hermite polynomial expansions by iterative rotations,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,20,10.0,"Compressive sensing has become a powerful addition to uncertainty quantification in recent years. This paper identifies new bases for random variables through linear mappings such that the representation of the quantity of interest is more sparse with new basis functions associated with the new random variables. This sparsity increases both the efficiency and accuracy of the compressive sensing-based uncertainty quantification method. Specifically, we consider rotation-based linear mappings which are determined iteratively for Hermite polynomial expansions. We demonstrate the effectiveness of the new method with applications in solving stochastic partial differential equations and high-dimensional (O()) problems.",10.1016/j.jcp.2015.11.038,https://dx.doi.org/10.1016/j.jcp.2015.11.038
"Brunetti, G | Simunek, J | Piro, P",2016,A comprehensive numerical analysis of the hydraulic behavior of a permeable pavement,Applications_JOURNAL OF HYDROLOGY,19,9.5,"The increasing frequency of flooding events in urban catchments related to an increase in impervious surfaces highlights the inadequacy of traditional urban drainage systems. Low Impact Development (LID) techniques have proven to be a viable and effective alternative by reducing stormwater runoff and increasing the infiltration and evapotranspiration capacity of urban areas. However, the lack of adequate modeling tools represents a barrier in designing and constructing such systems. This paper investigates the suitability of a mechanistic model, HYDRUS-D, to correctly describe the hydraulic behavior of permeable pavement installed at the University of Calabria. Two different scenarios of describing the hydraulic behavior of the permeable pavement system were analyzed: the first one uses a single porosity model for all layers of the permeable pavement; the second one uses a dual-porosity model for the base and sub-base layers. Measured and modeled month-long hydrographs were compared using the Nash-Sutcliffe efficiency (NSE) index. A Global Sensitivity Analysis (GSA) followed by a Monte Carlo filtering highlighted the influence of the wear layer on the hydraulic behavior of the pavement and identified the ranges of parameters generating behavioral solutions. Reduced ranges were then used in the calibration procedure conducted with the metaheuristic Particle swarm optimization (PSO) algorithm for the estimation of hydraulic parameters. The best fit value for the first scenario was NSE = .; for the second scenario, it was NSE = ., indicating that the dual-porosity approach is more appropriate for describing the variably-saturated flow in the base and sub-base layers. Estimated parameters were validated using an independent, month-long set of measurements, resulting in NSE values of . and . for the first and second scenarios, respectively. The improvement in correspondence between measured and modeled hydrographs confirmed the reliability of the combination of GSA and PSO in dealing with highly dimensional optimization problems. Obtained results have demonstrated that PSO, due to its easiness of implementation and effectiveness, can represent a new and viable alternative to traditional optimization algorithms for the inverse estimation of unsaturated hydraulic properties. Finally, the results confirmed the suitability and the accuracy of HYDRUS-D in correctly describing the hydraulic behavior of permeable pavements.",10.1016/j.jhydrol.2016.07.030,https://dx.doi.org/10.1016/j.jhydrol.2016.07.030
"Yang, XF | Liu, YS | Gao, Y",2016,Unified reliability analysis by active learning Kriging model combining with random-set based Monte Carlo simulation method,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,19,9.5,"Reliability analysis with both aleatory and epistemic uncertainties is investigated in this paper. The aleatory uncertainties are described with random variables, and epistemic uncertainties are tackled with evidence theory. To estimate the bounds of failure probability, several methods have been proposed. However, the existing methods suffer the dimensionality challenge of epistemic variables. To get rid of this challenge, a so-called random-set based Monte Carlo simulation (RS-MCS) method derived from the theory of random sets is offered. Nevertheless, RS-MCS is also computational expensive. So an active learning Kriging (ALK) model that only rightly predicts the sign of performance function is introduced and closely integrated with RS-MCS. The proposed method is termed as ALK-RS-MCS. ALK-RS-MCS accurately predicts the bounds of failure probability using as few function calls as possible. Moreover, in ALK-RS-MCS, an optimization method based on Karush-Kuhn-Tucker conditions is proposed to make the estimation of failure probability interval more efficient based on the Kriging model. The efficiency and accuracy of the proposed approach are demonstrated with four examples.",10.1002/nme.5255,https://dx.doi.org/10.1002/nme.5255
"Yang, XF | Liu, YS | Gao, Y | Zhang, YS | Gao, ZZ",2015,An active learning kriging model for hybrid reliability analysis with both random and interval variables,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,28,9.33,"Hybrid reliability analysis (HRA) with both random and interval variables is investigated in this paper. Firstly, it is figured out that a surrogate model just rightly predicting the sign of performance function can meet the requirement of HRA in accuracy. According to this idea, a methodology based on active learning Kriging (ALK) model named ALK-HRA is proposed. When constructing the Kriging model, the presented method only finely approximates the performance function in the region of interest: the region where the sign tends to be wrongly predicted. Based on the constructed Kriging model, Monte Carlo Simulation (MCS) is carried out to estimate both the lower and upper bounds of failure probability. ALK-HRA is accurate enough with calling the performance function as few times as possible. Four numerical examples and one engineering application are investigated to demonstrate the performance of the proposed method.",10.1007/s00158-014-1189-5,https://dx.doi.org/10.1007/s00158-014-1189-5
"Elsheikh, AH | Hoteit, I | Wheeler, MF",2014,Efficient Bayesian inference of subsurface flow models using nested sampling and sparse polynomial chaos surrogates,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,37,9.25,"An efficient Bayesian calibration method based on the nested sampling (NS) algorithm and non-intrusive polynomial chaos method is presented. Nested sampling is a Bayesian sampling algorithm that builds a discrete representation of the posterior distributions by iteratively re-focusing a set of samples to high likelihood regions. NS allows representing the posterior probability density function (PDF) with a smaller number of samples and reduces the curse of dimensionality effects. The main difficulty of the NS algorithm is in the constrained sampling step which is commonly performed using a random walk Markov Chain Monte-Carlo (MCMC) algorithm. In this work, we perform a two-stage sampling using a polynomial chaos response surface to filter out rejected samples in the Markov Chain Monte-Carlo method. The combined use of nested sampling and the two-stage MCMC based on approximate response surfaces provides significant computational gains in terms of the number of simulation runs. The proposed algorithm is applied for calibration and model selection of subsurface flow models.",10.1016/j.cma.2013.11.001,https://dx.doi.org/10.1016/j.cma.2013.11.001
"Smith, T | Marshall, L | Sharma, A",2015,Modeling residual hydrologic errors with Bayesian inference,Applications_JOURNAL OF HYDROLOGY,27,9.0,"Hydrologic modelers are confronted with the challenge of producing estimates of the uncertainty associated with model predictions across an array of catchments and hydrologic flow regimes. Formal Bayesian approaches are commonly employed for parameter calibration and uncertainty analysis, but are often criticized for making strong assumptions about the nature of model residuals via the likelihood function that may not be well satisfied (or even checked). This technical note outlines a residual error model (likelihood function) specification framework that aims to provide guidance for the application of more appropriate residual error models through a nested approach that is both flexible and extendible. The framework synthesizes many previously employed residual error models and has been applied to four synthetic datasets (of differing error structure) and a real dataset from the Black River catchment in Queensland, Australia. Each residual error model was investigated and assessed under a top-down approach focused on its ability to properly characterize the errors. The results of these test applications indicate that a multifaceted assessment strategy is necessary to determine the adequacy of an individual likelihood function.",10.1016/j.jhydrol.2015.05.051,https://dx.doi.org/10.1016/j.jhydrol.2015.05.051
"Raissi, M | Perdikaris, P | Karniadakis, GE",2017,Machine learning of linear differential equations using Gaussian processes,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,9,9.0,"This work leverages recent advances in probabilistic machine learning to discover governing equations expressed by parametric linear operators. Such equations involve, but are not limited to, ordinary and partial differential, integro-differential, and fractional order operators. Here, Gaussian process priors are modified according to the particular form of such operators and are employed to infer parameters of the linear equations from scarce and possibly noisy observations. Such observations may come from experiments or ""blackbox"" computer simulations, as demonstrated in several synthetic examples and a realistic application in functional genomics.",10.1016/j.jcp.2017.07.050,https://dx.doi.org/10.1016/j.jcp.2017.07.050
"Edeling, WN | Cinnella, P | Dwight, RP | Bijl, H",2014,Bayesian estimates of parameter variability in the k-epsilon turbulence model,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,36,9.0,"In this paper we are concerned with obtaining estimates for the error in Reynolds-averaged Navier-Stokes (RANS) simulations based on the Launder-Sharma k-epsilon turbulence closure model, for a limited class of flows. In particular we search for estimates grounded in uncertainties in the space of model closure coefficients, for wall-bounded flows at a variety of favorable and adverse pressure gradients. In order to estimate the spread of closure coefficients which reproduces these flows accurately, we perform  separate Bayesian calibrations - each at a different pressure gradient - using measured boundary-layer velocity profiles, and a statistical model containing a multiplicative model-inadequacy term in the solution space. The results are  joint posterior distributions over coefficients and hyper-parameters. To summarize this information we compute Highest Posterior-Density (HPD) intervals, and subsequently represent the total solution uncertainty with a probability-box (p-box). This p-box represents both parameter variability across flows, and epistemic uncertainty within each calibration. A prediction of a new boundary-layer flow is made with uncertainty bars generated from this uncertainty information, and the resulting error estimate is shown to be consistent with measurement data.",10.1016/j.jcp.2013.10.027,https://dx.doi.org/10.1016/j.jcp.2013.10.027
"Li, ZL | Shao, QX | Xu, ZX | Cai, XT",2010,Analysis of parameter uncertainty in semi-distributed hydrological models using bootstrap method: A case study of SWAT model applied to Yingluoxia watershed in northwest China,Applications_JOURNAL OF HYDROLOGY,71,8.88,"Much attention has been paid to uncertainty issues in hydrological modelling due to their great effects on prediction and further on decision-making. The uncertainty of model parameters is one of the major uncertainty sources in hydrological modelling. The aim of this study is to quantify the parameter uncertainty in Soil and Water Assessment Tool (SWAT) model using bootstrap method with application to Yingluoxia watershed located in the upper reaches of Heihe River basin. Bootstrap method is a nonparametric technique for simulating the parameter distribution. Nine sensitive aggregate parameters are investigated. The results from bootstrap method show that six of the nine marginal distributions are not normally distributed and each parameter has its own uncertainty range. Further investigation about the effects of parameter uncertainty on simulation results shows that although the parameter uncertainty is one of the important sources of uncertainties, its contribution to simulation uncertainty is relatively small. Only -% of the observed runoff data fall inside the % simulation confidence intervals in the calibration and validation periods. For a better understanding of the applicability of bootstrap method, the commonly used Bayesian approach is also investigated for comparison. Results show that the approximate results are obtained from both methods, not only in the percentage of observations falling inside the % confidence interval of simulations, but also in the uncertainty range of parameters, although the range obtained from Bayesian method is slightly narrower than that from bootstrap method, possibly due to the correlation structure amongst parameters in the MCMC (Markov Chain Monte Carlo) simulation employed in Bayesian method. The computational efficiencies of both methods presented are comparable as well.",10.1016/j.jhydrol.2010.01.025,https://dx.doi.org/10.1016/j.jhydrol.2010.01.025
"Hu, C | Youn, BD",2011,Adaptive-sparse polynomial chaos expansion for reliability analysis and design of complex engineering systems,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,62,8.86,"This paper presents an adaptive-sparse polynomial chaos expansion (adaptive-sparse PCE) method for performing engineering reliability analysis and design. The proposed method combines three ideas: (i) an adaptive-sparse scheme to build sparse PCE with the minimum number of bivariate basis functions, (ii) a new projection method using dimension reduction techniques to effectively compute the expansion coefficients of system responses, and (iii) an integration of copula to handle nonlinear correlation of input random variables. The proposed method thus has three positive features for reliability analysis and design: (a) there is no need for response sensitivity analysis, (b) it is highly efficient and accurate for reliability analysis and its sensitivity analysis, and (c) it is capable of handling a nonlinear correlation. In addition to the features, an error decomposition scheme for the proposed method is presented to help analyze error sources in probability analysis. Several engineering problems are used to demonstrate the three positive features of the adaptive-sparse PCE method.",10.1007/s00158-010-0568-9,https://dx.doi.org/10.1007/s00158-010-0568-9
"Choi, HT | Beven, K",2007,Multi-period and multi-criteria model conditioning to reduce prediction uncertainty in an application of TOPMODEL within the GLUE framework,Applications_JOURNAL OF HYDROLOGY,97,8.82,"A new approach to multi-criteria model evaluation is presented. The approach is consistent with the equifinality thesis and is developed within the Generalised Likelihood Uncertainty Estimation (GLUE) framework. The predictions of Monte Carlo realisations of TOP-MODEL parameter sets are evaluated using a number of performance measures calibrated for both global (annual) and seasonal ( day) periods. The seasonal periods were clustered using a Fuzzy C-means algorithm, into  types representing different hydrological conditions. The model shows good performance on a classical efficiency measure at the global level, but no model realizations were found that were behavioural over all multi-period clusters and all performance measures, raising questions about what should be considered as an acceptable model performance. Prediction uncertainties can still be calculated by allowing that different clusters require different parameter sets. Variations in parameter distributions between clusters, as well as examination of where observed discharges depart from model prediction bounds, give some indication of model structure deficiencies.",10.1016/j.jhydrol.2006.07.012,https://dx.doi.org/10.1016/j.jhydrol.2006.07.012
"Doostan, A | Ghanem, RG | Red-Horse, J",2007,Stochastic model reduction for chaos representations,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,97,8.82,"This paper addresses issues of model reduction of stochastic representations and computational efficiency of spectral stochastic Galerkin schemes for the solution of partial differential equations with stochastic coefficients. In particular, an algorithm is developed for the efficient characterization of a lower dimensional manifold occupied by the solution to a stochastic partial differential equation (SPDE) in the Hilbert space spanned by Wiener chaos. A description of the stochastic aspect of the problem on two well-separated scales is developed to enable the stochastic characterization on the fine scale using algebraic operations on the coarse scale. With such algorithms at hand, the solution of SPDE's becomes both computationally manageable and efficient. Moreover, a solid foundation is thus provided for the adaptive error control in stochastic Galerkin procedures. Different aspects of the proposed methodology are clarified through its application to an example problem from solid mechanics.",10.1016/j.cma.2006.10.047,https://dx.doi.org/10.1016/j.cma.2006.10.047
"Kang, Z | Zhang, XP | Jiang, SG | Cheng, GD",2012,On topology optimization of damping layer in shell structures under harmonic excitations,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,52,8.67,"This paper investigates the optimal distribution of damping material in vibrating structures subject to harmonic excitations by using topology optimization method. Therein, the design objective is to minimize the structural vibration level at specified positions by distributing a given amount of damping material. An artificial damping material model that has a similar form as in the SIMP approach is suggested and the relative densities of the damping material are taken as design variables. The vibration equation of the structure has a non-proportional damping matrix. A system reduction procedure is first performed by using the eigenmodes of the undamped system. The complex mode superposition method in the state space, which can deal with the non-proportional damping, is then employed to calculate the steady-state response of the vibrating structure. In this context, an adjoint variable scheme for the response sensitivity analysis is developed. Numerical examples are presented for illustrating validity and efficiency of this approach. Impacts of the excitation frequency as well as the damping coefficients on topology optimization results are also discussed.",10.1007/s00158-011-0746-4,https://dx.doi.org/10.1007/s00158-011-0746-4
"Galbally, D | Fidkowski, K | Willcox, K | Ghattas, O",2010,Non-linear model reduction for uncertainty quantification in large-scale inverse problems,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,69,8.62,"We present a model reduction approach to the Solution of large-scale statistical inverse problems ill a Bayesian inference setting. A key to the model reduction is an efficient representation of the non-linear terms in the reduced model. To achieve this, we present a formulation that employs masked projection of the discrete equations, that is, We Compute all approximation of the non-linear term using a select subset of interpolation points. Further, through this formulation we show similarities among the existing techniques of gappy proper orthogonal decomposition, missing point estimation, and empirical interpolation via coefficient-function approximation. The resulting model reduction methodology is applied to a highly non-linear combustion problem governed by all advection-diffusion-reaction partial differential equation (PDE). Our reduced model is used as a surrogate for a finite element discretization of the non-linear PDE within the Markov chain Monte Carlo sampling employed by the Bayesian inference approach. In two spatial dimensions, we show that this approach yields accurate results while reducing the computational cost by several orders of magnitude. For the full three-dimensional problem, a forward solve using a reduced model that has high fidelity over the input parameter space is more than two million times faster than the full-order finite element model, making tractable the Solution of the statistical inverse problem that would otherwise require many years of CPU time.",10.1002/nme.2746,https://dx.doi.org/10.1002/nme.2746
"Coffin, P | Maute, K",2016,A level-set method for steady-state and transient natural convection problems,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,17,8.5,"This paper introduces a topology optimization method for D and D, steady-state and transient heat transfer problems that are dominated by natural convection in the fluid phase and diffusion in the solid phase. The geometry of the fluid-solid interface is described by an explicit level set method which allows for both shape and topological changes in the optimization process. The heat transfer in the fluid is modeled by an advection-diffusion equation. The fluid velocity is described by the incompressible Navier-Stokes equations augmented by a Boussinesq approximation of the buoyancy forces. The temperature field in the solid is predicted by a linear diffusion model. The governing equations in both the fluid and solid phases are discretized in space by a generalized formulation of the extended finite element method which preserves the crisp geometry definition of the level set method. The interface conditions at the fluid-solid boundary are enforced by Nitsche's method. The proposed method is studied for problems optimizing the geometry of cooling devices. The numerical results demonstrate the applicability of the proposed method for a wide spectrum of problems. As the flow may exhibit dynamic instabilities, transient phenomena need to be considered when optimizing the geometry. However, the computational burden increases significantly when the time evolution of the flow fields needs to be resolved.",10.1007/s00158-015-1377-y,https://dx.doi.org/10.1007/s00158-015-1377-y
"Liu, P | Li, LP | Chen, GJ | Rheinheimer, DE",2014,Parameter uncertainty analysis of reservoir operating rules based on implicit stochastic optimization,Applications_JOURNAL OF HYDROLOGY,34,8.5,"Reservoir operating rules are often derived using either a fitting or a simulation-based optimization method in the context of implicit stochastic optimization. Analysis of the parameter uncertainty in reservoir operating rules and their impact is necessary for robust solutions. In the present study, parameter uncertainty for reservoir operating rules is analyzed using two statistical methods, linear regression (LR) and Bayesian simulation (BS). LR estimates the confidence interval based on fitting the operating rules to the optimal deterministic solution. BS deals with the operating rule parameters as stochastic variables and treats the goodness-of-fit to the optimal deterministic solution or the operation profits as the likelihood measure. Two alternative techniques, the generalized likelihood uncertainty estimation (GLUE) and Markov Chain Monte Carlo method (MCMC), are implemented for the BS uncertainty analysis. These methods were applied to the operating rules of China's Three Gorges Reservoir. The LR performed less than the BS, and the MCMC outperformed the GLUE. Even for the BS methods, the operation profits criterion was better than the goodness-of-fit criterion for deriving the reservoir operating rules.",10.1016/j.jhydrol.2014.04.012,https://dx.doi.org/10.1016/j.jhydrol.2014.04.012
"Vanrolleghem, PA | Mannina, G | Cosenza, A | Neumann, MB",2015,"Global sensitivity analysis for urban water quality modelling: Terminology, convergence and comparison of different methods",Applications_JOURNAL OF HYDROLOGY,25,8.33,"Sensitivity analysis represents an important step in improving the understanding and use of environmental models. Indeed, by means of global sensitivity analysis (GSA), modellers may identify both important (factor prioritisation) and non-influential (factor fixing) model factors. No general rule has yet been defined for verifying the convergence of the GSA methods. In order to fill this gap this paper presents a convergence analysis of three widely used GSA methods (SRC, Extended FAST and Morris screening) for an urban drainage stormwater quality-quantity model. After the convergence was achieved the results of each method were compared. In particular, a discussion on peculiarities, applicability, and reliability of the three methods is presented. Moreover, a graphical Venn diagram based classification scheme and a precise terminology for better identifying important, interacting and non-influential factors for each method is proposed. In terms of convergence, it was shown that sensitivity indices related to factors of the quantity model achieve convergence faster. Results for the Morris screening method deviated considerably from the other methods. Factors related to the quality model require a much higher number of simulations than the number suggested in literature for achieving convergence with this method. In fact, the results have shown that the term ""screening"" is improperly used as the method may exclude important factors from further analysis. Moreover, for the presented application the convergence analysis shows more stable sensitivity coefficients for the Extended-FAST method compared to SRC and Morris screening. Substantial agreement in terms of factor fixing was found between the Morris screening and Extended FAST methods. In general, the water quality related factors exhibited more important interactions than factors related to water quantity. Furthermore, in contrast to water quantity model outputs, water quality model outputs were found to be characterised by high non-linearity.",10.1016/j.jhydrol.2014.12.056,https://dx.doi.org/10.1016/j.jhydrol.2014.12.056
"Zhang, XJ | Xu, YP | Fu, GT",2014,Uncertainties in SWAT extreme flow simulation under climate change,Applications_JOURNAL OF HYDROLOGY,33,8.25,"Uncertainty in climate change impact analysis has been widely recognized. Analyzing it becomes an important task particularly when impact analysis results are used for adaptation purposes. A methodology aiming to investigate the impact of climate change and the separate and combined impacts of several uncertainty sources on future extreme flows in the Lanjiang catchment, East China is proposed. A regional climate model PRECIS (Providing REgional Climates for Impacts Studies) is applied to downscale the General Circulation Model (GCM) outputs and the extreme flows are simulated by the SWAT (Soil and Water Assessment Tool) model. Besides emission scenarios and extreme value models, the main uncertainty source, namely SWAT parameters is taken into account and the sequential uncertainty analysis method is employed to analyze the parameter uncertainties. The SWAT model calibration and validation results indicate that the model has a good performance in Lanjiang catchment. The projected future extreme flows show that the design discharges of small return periods are likely larger than those in the baseline period, while those of large return periods will be likely smaller than those in the baseline period at Misai, Quzhou and Lanxi stations. The uncertainty analysis results show that for small return periods such as  years, the uncertainty introduced from the SWAT model parameters is much larger than those from emission scenarios and extreme value models at Misai and Quzhou stations while for large return periods such as  years, the uncertainty introduced from all three sources are substantial. However, the already large uncertainty due to SWAT model parameters, emissions scenarios and extreme flow distributions might be dwarfed by GCM uncertainty, which is not concerned in this study.",10.1016/j.jhydrol.2014.04.064,https://dx.doi.org/10.1016/j.jhydrol.2014.04.064
"Guo, J | Du, XP",2009,Reliability sensitivity analysis with random and interval variables,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,74,8.22,"In reliability analysis and reliability-based design, sensitivity analysis identifies the relationship between the change in reliability and the change in the characteristics of uncertain variables. Sensitivity analysis is also used to identify the most significant uncertain variables that have the highest contributions to reliability. Most of the current sensitivity analysis methods are applicable for only random variables. In many engineering applications, however, some of uncertain variables are intervals. In this work, a sensitivity analysis method is proposed for the mixture of random and interval variables. Six sensitivity indices are defined for the sensitivity of the average reliability and reliability bounds with respect to the averages and widths of intervals, as well as with respect to the distribution parameters of random variables. The equations of these sensitivity indices are derived based on the first-order reliability method (FORM). The proposed reliability sensitivity analysis is a byproduct of FORM without any extra function calls after reliability is found. Once FORM is performed, the sensitivity information is obtained automatically. Two examples are used for demonstration.",10.1002/nme.2543,https://dx.doi.org/10.1002/nme.2543
"Graham, IG | Kuo, FY | Nuyens, D | Scheichl, R | Sloan, IH",2011,Quasi-Monte Carlo methods for elliptic PDEs with random coefficients and applications,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,57,8.14,"We devise and implement quasi-Monte Carlo methods for computing the expectations of nonlinear functionals of solutions of a class of elliptic partial differential equations with random coefficients. Our motivation comes from fluid flow in random porous media, where relevant functionals include the fluid pressure/velocity at any point in space or the breakthrough time of a pollution plume being transported by the velocity field. Our emphasis is on situations where a very large number of random variables is needed to model the coefficient field. As an alternative to classical Monte Carlo, we here employ quasi-Monte Carlo methods, which use deterministically chosen sample points in an appropriate (usually high-dimensional) parameter space. Each realization of the PDE solution requires a finite element (FE) approximation in space, and this is done using a realization of the coefficient field restricted to a suitable regular spatial grid (not necessarily the same as the FE grid). In the statistically homogeneous case the corresponding covariance matrix can be diagonalized and the required coefficient realizations can be computed efficiently using FFT. In this way we avoid the use of a truncated Karhunen-Loeve expansion, but introduce high nominal dimension in parameter space. Numerical experiments with -dimensional rough random fields, high variance and small length scale are reported, showing that the quasi-Monte Carlo method consistently outperforms the Monte Carlo method, with a smaller error and a noticeably better than O(N-/) convergence rate, where N is the number of samples. Moreover, the rate of convergence of the quasi-Monte Carlo method does not appear to degrade as the nominal dimension increases. Examples with dimension as high as  are reported.",10.1016/j.jcp.2011.01.023,https://dx.doi.org/10.1016/j.jcp.2011.01.023
"Doostan, A | Iaccarino, G",2009,A least-squares approximation of partial differential equations with high-dimensional random inputs,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,73,8.11,"Uncertainty quantification schemes based on stochastic Galerkin projections, with global or local basis functions, and also stochastic collocation methods in their conventional form, suffer from the so called curse of dimensionality: the associated computational cost grows exponentially as a function of the number of random variables defining the underlying probability space of the problem. In this paper, to overcome the curse of dimensionality, a low-rank separated approximation of the solution of a stochastic partial differential (SPDE) with high-dimensional random input data is obtained using an alternating least-squares (ALS) scheme. It will be shown that, in theory, the computational cost of the proposed algorithm grows linearly with respect to the dimension of the underlying probability space of the system. For the case of an elliptic SPDE, an a priori error analysis of the algorithm is derived. Finally, different aspects of the proposed methodology are explored through its application to some numerical experiments.",10.1016/j.jcp.2009.03.006,https://dx.doi.org/10.1016/j.jcp.2009.03.006
"Changizi, N | Kaboodanian, H | Jalalpour, M",2017,Stress-based topology optimization of frame structures under geometric uncertainty,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,8,8.0,"Probabilistic topology optimization has gained significant research attention recently. This interest stems from the realization that the achieved high-performance designs resulted from deterministic topology optimization algorithms may become suboptimal under real-world conditions that are often accompanied with uncertainties. Among sources of these uncertainties, the ones that define structural characteristics, such as geometry, are numerically challenging to treat as they lead to stochastic structural stiffness. To date, research on developing efficient probabilistic topology optimization under stochastic stiffness is mainly focused on displacement-based objectives. However, in the design of structures, stress is also a primary design criterion that needs to be directly controlled for. A robust stress-based topology optimization methodology for frame structures under geometric uncertainty is proposed in this work. Assuming that such uncertainties are small relative to frame member lengths, the proposed methodology uses stochastic perturbation method to propagate these uncertainties up to the response level, which is expressed by the maximum of expected values of von Mises stresses throughout the domain. Sensitivities of the response with respect to design variables are derived analytically, which allows using efficient gradient-based optimizers. The proposed algorithm is examined with stress-based design of three frame structures under geometric uncertainty. Changes in the topology of these new designs are discussed, and they are shown to outperform deterministic designs when subjected to geometric uncertainties. Moreover, predictions and the resulting designs from the proposed methodology are found to be in excellent agreement with Monte Carlo simulation results.",10.1016/j.cma.2016.10.039,https://dx.doi.org/10.1016/j.cma.2016.10.039
"Ahlfeld, R | Belkouchi, B | Montomoli, F",2016,SAMBA: Sparse Approximation of Moment-Based Arbitrary Polynomial Chaos,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,16,8.0,"A new arbitrary Polynomial Chaos (aPC) method is presented for moderately high-dimensional problems characterised by limited input data availability. The proposed methodology improves the algorithm of aPC and extends the method, that was previously only introduced as tensor product expansion, to moderately high-dimensional stochastic problems. The fundamental idea of aPC is to use the statistical moments of the input random variables to develop the polynomial chaos expansion. This approach provides the possibility to propagate continuous or discrete probability density functions and also histograms (data sets) as long as their moments exist, are finite and the determinant of the moment matrix is strictly positive. For cases with limited data availability, this approach avoids bias and fitting errors caused by wrong assumptions. In this work, an alternative way to calculate the aPC is suggested, which provides the optimal polynomials, Gaussian quadrature collocation points and weights from the moments using only a handful of matrix operations on the Hankel matrix of moments. It can therefore be implemented without requiring prior knowledge about statistical data analysis or a detailed understanding of the mathematics of polynomial chaos expansions. The extension to more input variables suggested in this work, is an anisotropic and adaptive version of Smolyak's algorithm that is solely based on the moments of the input probability distributions. It is referred to as SAMBA (PC), which is short for Sparse Approximation of Moment-Based Arbitrary Polynomial Chaos. It is illustrated that for moderately high-dimensional problems (up to  different input variables or histograms) SAMBA can significantly simplify the calculation of sparse Gaussian quadrature rules. SAMBA's efficiency for multivariate functions with regard to data availability is further demonstrated by analysing higher order convergence and accuracy for a set of nonlinear test functions with ,  and  different input distributions or histograms.",10.1016/j.jcp.2016.05.014,https://dx.doi.org/10.1016/j.jcp.2016.05.014
"Faes, M | Cerneels, J | Vandepitte, D | Moens, D",2017,Identification and quantification of multivariate interval uncertainty in finite element models,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,8,8.0,"The objective of this work is to develop and validate a methodology for the identification and quantification of multivariate interval uncertainty in finite element models. The principal idea is to find a solution to an inverse problem, where the variability on the output side of the model is known from measurement data, but the multivariate uncertainty on the input parameters is unknown. For this purpose, the uncertain simulation results set created by propagating interval uncertainty through the model is represented by its convex hull. The same concept is used to model the uncertainty in the measurements. A metric to describe the discrepancy between these convex hulls is defined based on the difference between their volumes and their mutual intersection. By minimisation of this metric, the interval uncertainty on the input side of the model is identified. It is further shown how the procedure can be optimised with respect to output quantity selection. Validation of the methodology is done using simulated measurement data in two case studies. Numerically exact identification of multiple, coupled parameters having interval uncertainty is possible following the proposed methodology. Furthermore, the robustness of the method with respect to the analyst's initial estimate of the input uncertainty is illustrated. The method presented in this work in se is generic, but for the examples in this paper, it is specifically applied to dynamic models, using eigenfrequencies as output quantities, as commonly applied in modal updating procedures.",10.1016/j.cma.2016.11.023,https://dx.doi.org/10.1016/j.cma.2016.11.023
"Gunawan, S | Azarm, S",2005,Multi-objective robust optimization using a sensitivity region concept,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,104,8.0,"In multi-objective design optimization, it is quite desirable to obtain solutions that are ""multi-objectively"" optimum and insensitive to uncontrollable (noisy) parameter variations. We call such solutions robust Pareto solutions. In this paper we present a method to measure the multi-objective sensitivity of a design alternative, and an approach to use such a measure to obtain multi-objectively robust Pareto optimum solutions. Our sensitivity measure does not require a presumed probability distribution of uncontrollable parameters and does not utilize gradient information; therefore, it is applicable to multi-objective optimization problems that have non-differentiable and/or discontinuous objective functions, and also to problems with large parameter variations. As a demonstration, we apply our robust optimization method to an engineering example, the design of a vibrating platform. We show that the solutions obtained for this example are indeed robust.",10.1007/s00158-004-0450-8,https://dx.doi.org/10.1007/s00158-004-0450-8
"Constantine, PG | Emory, M | Larsson, J | Iaccarino, G",2015,Exploiting active subspaces to quantify uncertainty in the numerical simulation of the HyShot II scramjet,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,23,7.67,"We present a computational analysis of the reactive flow in a hypersonic scramjet engine with focus on effects of uncertainties in the operating conditions. We employ a novel methodology based on active subspaces to characterize the effects of the input uncertainty on the scramjet performance. The active subspace identifies one-dimensional structure in the map from simulation inputs to quantity of interest that allows us to reparameterize the operating conditions; instead of seven physical parameters, we can use a single derived active variable. This dimension reduction enables otherwise infeasible uncertainty quantification, considering the simulation cost of roughly  CPU-hours per run. For two values of the fuel injection rate, we use a total of  simulations to (i) identify the parameters that contribute the most to the variation in the output quantity of interest, (ii) estimate upper and lower bounds on the quantity of interest, (iii) classify sets of operating conditions as safe or unsafe corresponding to a threshold on the output quantity of interest, and (iv) estimate a cumulative distribution function for the quantity of interest.",10.1016/j.jcp.2015.09.001,https://dx.doi.org/10.1016/j.jcp.2015.09.001
"Zou, T | Mahadevan, S",2006,A direct decoupling approach for efficient reliability-based design optimization,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,89,7.42,"This paper develops an efficient methodology to perform reliability-based design optimization (RBDO) by decoupling the optimization and reliability analysis iterations that are nested in traditional formulations. This is achieved by approximating the reliability constraints based on the reliability analysis results. The proposed approach does not use inverse first-order reliability analysis as other existing decoupled approaches, but uses direct reliability analysis. This strategy allows a modular approach and the use of more accurate methods, including Monte-Carlo-simulation (MCS)-based methods for highly nonlinear reliability constraints where first-order reliability approximation may not be accurate. The use of simulation-based methods also enables system-level reliability estimates to be included in the RBDO formulation. The efficiency of the proposed RBDO approach is further improved by identifying the potentially active reliability constraints at the beginning of each reliability analysis. A vehicle side impact problem is used to examine the proposed method, and the results show the usefulness of the proposed method.",10.1007/s00158-005-0572-7,https://dx.doi.org/10.1007/s00158-005-0572-7
"Sieber, A | Uhlenbrook, S",2005,Sensitivity analyses of a distributed catchment model to verify the model structure,Applications_JOURNAL OF HYDROLOGY,95,7.31,"Sensitivity analyses are valuable tools for identifying important model parameters, testing the model conceptualization, and improving the model structure. They help to apply the model efficiently and to enable a focussed planning of future research and field measurement. Two different methods were used for sensitivity analyses of the complex process-oriented model TACD (tracer aided catchment model, distributed) that was applied to the meso-scale Brugga basin ( km()) and the sub-basin St Wilhelmer Talbach (. km()). Five simulations periods were investigated: two summer events, two snow melt induced events and one summer low flow period. The model was applied using  different parameter sets, which were generated by Monte Carlo simulations using latin hypercube sampling. The regional sensitivity analysis (RSA) allowed determining the most significant parameters for the complete simulation periods using a graphical method. The results of the regression-based sensitivity analysis were more detailed and complex. The temporal variability of the simulation sensitivity could be observed continuously and the significance of the parameters could be determined in a quantitative way. A dependency of the simulation sensitivity on initial- and boundary conditions and the temporal and spatial variability of the sensitivity to some model parameters was revealed by the regression-based sensitivity analysis. Thus, the difficulty of transferring the results to different time periods or model applications in other catchments became obvious. The analysis of the temporal course of the simulation sensitivity to parameter values in conjunction with simulated and measured additional data sets (precipitation, temperature, reservoir volumes etc.) gave further insight into the internal model behaviour and demonstrated the plausibility of the model structure and process conceptionalizations.",10.1016/j.jhydrol.2005.01.004,https://dx.doi.org/10.1016/j.jhydrol.2005.01.004
"Freni, G | Mannina, G",2010,Bayesian approach for uncertainty quantification in water quality modelling: The influence of prior distribution,Applications_JOURNAL OF HYDROLOGY,57,7.12,"Mathematical models are of common use in urban drainage, and they are increasingly being applied to support decisions about design and alternative management strategies. In this context, uncertainty analysis is of undoubted necessity in urban drainage modelling. However, despite the crucial role played by uncertainty quantification, several methodological aspects need to be clarified and deserve further investigation, especially in water quality modelling. One of them is related to the ""a priori"" hypotheses involved in the uncertainty analysis. Such hypotheses are usually condensed in ""a priori"" distributions assessing the most likely values for model parameters. This paper explores Bayesian uncertainty estimation methods investigating the influence of the choice of these prior distributions. The research aims at gaining insights in the selection of the prior distribution and the effect the user-defined choice has on the reliability of the uncertainty analysis results. To accomplish this, an urban stormwater quality model developed in previous studies has been employed. The model has been applied to the Fossolo catchment (Italy), for which both quantity and quality data were available. The results show that a uniform distribution should be applied whenever no information is available for specific parameters describing the case study. The use of weak information (mostly coming from literature or other model applications) should be avoided because it can lead to wrong estimations of uncertainty in modelling results. Model parameter related hypotheses would be better dropped in these cases.",10.1016/j.jhydrol.2010.07.043,https://dx.doi.org/10.1016/j.jhydrol.2010.07.043
"Mishra, S | Schwab, C | Sukys, J",2012,Multi-level Monte Carlo finite volume methods for nonlinear systems of conservation laws in multi-dimensions,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,42,7.0,"We extend the multi-level Monte Carlo (MLMC) in order to quantify uncertainty in the solutions of multi-dimensional hyperbolic systems of conservation laws with uncertain initial data. The algorithm is presented and several issues arising in the massively parallel numerical implementation are addressed. In particular, we present a novel load balancing procedure that ensures scalability of the MLMC algorithm on massively parallel hardware. A new code is described and applied to simulate uncertain solutions of the Euler equations and ideal magnetohydrodynamics (MHD) equations. Numerical experiments showing the robustness, efficiency and scalability of the proposed algorithm are presented.",10.1016/j.jcp.2012.01.011,https://dx.doi.org/10.1016/j.jcp.2012.01.011
"Noori, R | Yeh, HD | Abbasi, M | Kachoosangi, FT | Moazami, S",2015,Uncertainty analysis of support vector machine for online prediction of five-day biochemical oxygen demand,Applications_JOURNAL OF HYDROLOGY,21,7.0,"Uncertainty is considered as one of the most important limitations for applying the results of artificial intelligence techniques (AI) in water quality management to obtain appropriate control strategies. In this research, a proper methodology was proposed to determine the uncertainty of support vector machine (SVM) for the prediction of five-day biochemical oxygen demand (BOD). In this regard, the SVM model was calibrated using different records for many times (here,  times), to investigate model performance according to calibration pattern changes. Therefore, to implement the random selection of calibration patterns for several times, an alternative database was required. By this methodology, the parameters of SVM model will be obtained  times, giving various predicted BOD values each time. To evaluate the SVM model's uncertainty, the percentage of observed data bracketed by  percent predicted uncertainties (PPU) and the band width of  percent confidence intervals (d-factor) were selected. Findings indicated that the SVM model was more sensitive to capacity parameter (C) than to kernel parameter (Gamma) and error tolerance (Epsilon). Besides, results showed that the SVM model had acceptable uncertainty in BOD prediction. It is notified that the novelty of the presented methodology is beyond a mere application to water resources, and can also be used in other fields of sciences and engineering.",10.1016/j.jhydrol.2015.05.046,https://dx.doi.org/10.1016/j.jhydrol.2015.05.046
"Zuo, WJ | Bai, JT | Yu, JF",2016,Sensitivity reanalysis of static displacement using Taylor series expansion and combined approximate method,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,14,7.0,"This paper presents a new sensitivity reanalysis of static displacement for arbitrary changes of design variables. The current displacement of modified sensitivity equations is approximately calculated by using Taylor series expansion and then the direct sensitivity equations are solved by combined approximate method. Two types of numerical examples, including size sensitivity and shape sensitivity, are used to verify the accuracy and efficiency of the proposed method, which is more efficient than the existed methods. Although the Kirsch and Papalambros reanalysis method is more accurate, the proposed method can also acquire the accurate solution for the case of large modification. Therefore, this new method will have great potential application in the gradient-based structural optimization.",10.1007/s00158-015-1368-z,https://dx.doi.org/10.1007/s00158-015-1368-z
"Brindha, K | Elango, L",2015,Cross comparison of five popular groundwater pollution vulnerability index approaches,Applications_JOURNAL OF HYDROLOGY,21,7.0,"Identification of a suitable overlay and index method to map vulnerable zones for pollution in weathered rock aquifers was carried out in this study. DRASTIC and four models derived from it, namely Pesticide DRASTIC, modified DRASTIC, modified Pesticide DRASTIC and Susceptibility Index (SI) were compared by applying them to a weathered rock aquifer in southern India. The results were validated with the measured geochemical data. This study also introduces the use of temporal variation in the groundwater level and nitrate concentration in groundwater as input and for validation respectively to obtain more reliable and meaningful results. Sensitivity analysis of the vulnerability index maps highlight the importance of one parameter over another for a given hydrogeological setting, which will help to plan the field investigations based on the most or the least influential parameter. It is recommended to use modified Pesticide DRASTIC for weathered rock regions with irrigation practises and shallow aquifers (< m bgl). The crucial input due to land use should not be neglected and to be considered in any hydrogeological setting. It is better to estimate the specific vulnerability wherever possible rather than the intrinsic vulnerability as overlay and index methods are more suited for this purpose. It is also necessary to consider the maximum and minimum values of input parameters measured during a normal year in the models used for decision making.",10.1016/j.jhydrol.2015.03.003,https://dx.doi.org/10.1016/j.jhydrol.2015.03.003
"Yun, KS | Youn, SK",2017,Design sensitivity analysis for transient response of non-viscously damped dynamic systems,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,7,7.0,A design sensitivity analysis for the transient response of the non-viscously damped dynamic systems is presented. The non-viscously (viscoelastically) damped system is widely used in structural vibration control. The damping forces in the system depend on the past history of motion via convolution integrals. The non-viscos damping is modeled by the generalized Maxwell model. The transient response is calculated with the implicit Newmark time integration scheme. The design sensitivity analysis method of the history dependent system is developed using the adjoint variable method. The discretize-then-differentiate approach is adopted for deriving discrete adjoint equations. The accuracy and the consistency of the proposed method are demonstrated through a single dof system. The proposed method is also applied to a multi-dof system. The validity and accuracy of the sensitivities from the proposed method are confirmed by finite difference results.,10.1007/s00158-016-1636-6,https://dx.doi.org/10.1007/s00158-016-1636-6
"Jin, S | Xiu, DB | Zhu, XY",2015,Asymptotic-preserving methods for hyperbolic and transport equations with random inputs and diffusive scalings,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,21,7.0,"In this paper we develop a set of stochastic numerical schemes for hyperbolic and transport equations with diffusive scalings and subject to random inputs. The schemes are asymptotic preserving (AP), in the sense that they preserve the diffusive limits of the equations in discrete setting, without requiring excessive refinement of the discretization. Our stochastic AP schemes are extensions of the well-developed deterministic AP schemes. To handle the random inputs, we employ generalized polynomial chaos (gPC) expansion and combine it with stochastic Galerkin procedure. We apply the gPC Galerkin scheme to a set of representative hyperbolic and transport equations and establish the AP property in the stochastic setting. We then provide several numerical examples to illustrate the accuracy and effectiveness of the stochastic AP schemes.",10.1016/j.jcp.2015.02.023,https://dx.doi.org/10.1016/j.jcp.2015.02.023
"Doostan, A | Validi, A | Iaccarino, G",2013,Non-intrusive low-rank separated approximation of high-dimensional stochastic models,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,34,6.8,"This work proposes a sampling-based (non-intrusive) approach within the context of low-rank separated representations to tackle the issue of curse-of-dimensionality associated with the solution of models, e.g., PDEs/ODEs, with high-dimensional random inputs. Under some conditions discussed in details, the number of random realizations of the solution, required for a successful approximation, grows linearly with respect to the number of random inputs. The construction of the separated representation is achieved via a regularized alternating least-squares regression, together with an error indicator to estimate model parameters. The computational complexity of such a construction is quadratic in the number of random inputs. The performance of the method is investigated through its application to three numerical examples including two ODE problems with high-dimensional random inputs.",10.1016/j.cma.2013.04.003,https://dx.doi.org/10.1016/j.cma.2013.04.003
"Youn, BD | Choi, KK | Du, L",2005,Adaptive probability analysis using an enhanced hybrid mean value method,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,88,6.77,"This paper proposes an adaptive probability analysis method that can effectively generate the probability distribution of the output performance function by identifying the propagation of input uncertainty to output uncertainty. The method is based on an enhanced hybrid mean value (HMV+) analysis in the performance measure approach (PMA) for numerical stability and efficiency in search of the most probable point (MPP). The HMV+ method improves numerical stability and efficiency especially for highly nonlinear output performance functions by providing steady convergent behavior in the MPP search. The proposed adaptive probability analysis method approximates the MPP locus, and then adaptively refines this locus using an a posteriori error estimator. Using the fact that probability levels can be easily set a priori in PMA, the MPP locus is approximated using the interpolated moving least-squares method. For refinement of the approximated MPP locus, additional probability levels are adaptively determined through an a posteriori error estimator. The adaptive probability analysis method will determine the minimum number of necessary probability levels, while ensuring accuracy of the approximated MPP locus. Several examples are used to show the effectiveness of the proposed adaptive probability analysis method using the enhanced HMV+ method.",10.1007/s00158-004-0452-6,https://dx.doi.org/10.1007/s00158-004-0452-6
"Huang, X | Xie, YM | Jia, B | Li, Q | Zhou, SW",2012,Evolutionary topology optimization of periodic composites for extremal magnetic permeability and electrical permittivity,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,39,6.5,"This paper presents a bidirectional evolutionary structural optimization (BESO) method for designing periodic microstructures of two-phase composites with extremal electromagnetic permeability and permittivity. The effective permeability and effective permittivity of the composite are obtained by applying the homogenization technique to the representative periodic base cell (PBC). Single or multiple objectives are defined to maximize or minimize the electromagnetic properties separately or simultaneously. The sensitivity analysis of the objective function is conducted using the adjoint method. Based on the established sensitivity number, BESO gradually evolves the topology of the PBC to an optimum. Numerical examples demonstrate that the electromagnetic properties of the resulting D and D microstructures are very close to the theoretical Hashin-Shtrikman (HS) bounds. The proposed BESO algorithm is computationally efficient as the solution usually converges in less than  iterations. The proposed BESO method can be implemented easily as a post-processor to standard commercial finite element analysis software packages, e.g. ANSYS which has been used in this study. The resulting topologies are clear black-and-white solutions (with no grey areas). Some interesting topological patterns such as Vigdergauz-type structure and Schwarz primitive structure have been found which will be useful for the design of electromagnetic materials.",10.1007/s00158-012-0766-8,https://dx.doi.org/10.1007/s00158-012-0766-8
"Fox, GA | Munoz-Carpena, R | Sabbagh, GJ",2010,Influence of flow concentration on parameter importance and prediction uncertainty of pesticide trapping by vegetative filter strips,Applications_JOURNAL OF HYDROLOGY,51,6.38,"Flow concentration is a key hydrologic factor limiting the effectiveness of vegetated filter strips (VFS) in removing pesticides from surface runoff. Numerical models, such as VFSMOD-W, offer a mechanistic approach for evaluating VFS effectiveness under various hydrological conditions including concentrated flow. This research hypothesizes that the presence of concentrated flow drastically alters the importance of various hydrological, sedimentological, and pesticide input factors and the prediction uncertainty of pesticide reduction. Using data from a VFS experimental field study investigating chlorpyrifos and atrazine transport, a two-step global sensitivity and uncertainty analysis framework was used with VFSMOD-W based on () a screening method (Morris) and () a variance-based method (extended Fourier Analysis Sensitivity Test, FAST). The vertical, saturated hydraulic conductivity was consistently the most important input factor for predicting infiltration, explaining % of total output variance for uniform sheet flow, but only % for concentrated flow. Sedimentation was governed by both hydrologic (vertical, saturated hydraulic conductivity and initial and saturated water content) and sediment characteristics (average particle diameter). The vertical, saturated hydraulic conductivity was the most important input factor for atrazine or chlorpyrifos trapping under uniform sheet flow (explained more than % of the total output variance) and concentrated flow (although only explained % of the total variance in this case). The % confidence intervals for atrazine and chlorpyrifos reduction ranged between % and % for uniform sheet flow and decreased to between % and % under concentrated flow. Concentrated flow increased interactions among the system components, enhancing the relative importance of processes that were latent under shallow flow conditions. This complex behavior warrants the need for processbased modeling to be able to predict the performance of VFS under a wide range of specific hydrological conditions.",10.1016/j.jhydrol.2010.01.020,https://dx.doi.org/10.1016/j.jhydrol.2010.01.020
"Isaac, T | Petra, N | Stadler, G | Ghattas, O",2015,"Scalable and efficient algorithms for the propagation of uncertainty from data through inference to prediction for large-scale problems, with application to flow of the Antarctic ice sheet",Applications_JOURNAL OF COMPUTATIONAL PHYSICS,19,6.33,"The majority of research on efficient and scalable algorithms in computational science and engineering has focused on the forward problem: given parameter inputs, solve the governing equations to determine output quantities of interest. In contrast, here we consider the broader question: given a (large-scale) model containing uncertain parameters, (possibly) noisy observational data, and a prediction quantity of interest, how do we construct efficient and scalable algorithms to () infer the model parameters from the data (the deterministic inverse problem), () quantify the uncertainty in the inferred parameters (the Bayesian inference problem), and () propagate the resulting uncertain parameters through the model to issue predictions with quantified uncertainties (the forward uncertainty propagation problem)? We present efficient and scalable algorithms for this end-to-end, data-to-prediction process under the Gaussian approximation and in the context of modeling the flow of the Antarctic ice sheet and its effect on loss of grounded ice to the ocean. The ice is modeled as a viscous, incompressible, creeping, shear-thinning fluid. The observational data come from satellite measurements of surface ice flow velocity, and the uncertain parameter field to be inferred is the basal sliding parameter, represented by a heterogeneous coefficient in a Robin boundary condition at the base of the ice sheet. The prediction quantity of interest is the present-day ice mass flux from the Antarctic continent to the ocean. We show that the work required for executing this data-to-prediction process-measured in number of forward (and adjoint) ice sheet model solves-is independent of the state dimension, parameter dimension, data dimension, and the number of processor cores. The key to achieving this dimension independence is to exploit the fact that, despite their large size, the observational data typically provide only sparse information on model parameters. This property can be exploited to construct a low rank approximation of the linearized parameter-to-observable map via randomized SVD methods and adjoint-based actions of Hessians of the data misfit functional.",10.1016/j.jcp.2015.04.047,https://dx.doi.org/10.1016/j.jcp.2015.04.047
"Cacuci, DG",2015,Second-order adjoint sensitivity analysis methodology (2nd-ASAM) for computing exactly and efficiently first- and second-order sensitivities in large-scale linear systems: I. Computational methodology,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,19,6.33,"This work presents the second-order forward and adjoint sensitivity analysis methodologies( nd-FSAMand nd-ASAM) for computing exactly and efficiently the second-order functional derivatives of physical (engineering, biological, etc.) system responses (i.e., ""system performance parameters"") to the system's model parameters. The definition of ""system parameters"" used in this work includes all computational input data, correlations, initial and/or boundary conditions, etc. For a physical system comprising N-alpha parameters and N-r responses, we note that the nd-FSAM requires a total of (N-alpha()/ + N(alpha)/) large-scale computations for obtaining all of the first- and second-order sensitivities, for all N-r system responses. On the other hand, for one functional-type system response, the nd-ASAM requires one large-scale computation using the first-level adjoint sensitivity system for obtaining all of the first-order sensitivities, followed by at most N-alpha large-scale computations using the second-level adjoint sensitivity systems for obtaining exactly all of the second-order sensitivities. Therefore, the nd-FSAM should be used when N-r >> N-alpha, while the nd-ASAM should be used when N-alpha >> N-r. The original nd-ASAM presented in this work should enable the hitherto very difficult, if not intractable, exact computation of all of the second-order response sensitivities (i.e., functional Gateaux-derivatives) for large-systems involving many parameters, as usually encountered in practice. Very importantly, the implementation of the nd-ASAM requires very little additional effort beyond the construction of the adjoint sensitivity system needed for computing the first-order sensitivities.",10.1016/j.jcp.2014.12.042,https://dx.doi.org/10.1016/j.jcp.2014.12.042
"Mukhopadhyay, T | Dey, TK | Chowdhury, R | Chakrabarti, A | Adhikari, S",2015,Optimum design of FRP bridge deck: an efficient RS-HDMR based approach,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,19,6.33,"A novel efficient hybrid method based on random sampling-high dimensional model representations (RS-HDMR) and genetic algorithm coupled with a local unconstrained multivariable minimization function is proposed in this study for optimization of FRP composite web core bridge deck panels. The optimization is performed for lightweight design of FRP composite bridge deck panels based on deflection limit, stresses, buckling and failure criteria and subsequently the representative design curves are developed considering normal as well as skew configurations of FRP bridge decks. Sensitivity analysis is also performed to study the effect of variation in geometry of the bridge deck to its deflection, stress and buckling behaviours. High level of computational efficiency can be achieved without compromising the accuracy of results for optimization of high dimensional systems following the proposed approach.",10.1007/s00158-015-1251-y,https://dx.doi.org/10.1007/s00158-015-1251-y
"Yadav, V | Rahman, S",2014,Adaptive-sparse polynomial dimensional decomposition methods for high-dimensional stochastic computing,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,25,6.25,"This article presents two novel adaptive-sparse polynomial dimensional decomposition (PDD) methods for solving high-dimensional uncertainty quantification problems in computational science and engineering. The methods entail global sensitivity analysis for retaining important PDD component functions, and a full- or sparse-grid dimension-reduction integration or quasi Monte Carlo simulation for estimating the PDD expansion coefficients. A unified algorithm, endowed with two distinct ranking schemes for grading component functions, was created for their numerical implementation. The fully adaptive-sparse PDD method is comprehensive and rigorous, leading to the second-moment statistics of a stochastic response that converges to the exact solution when the tolerances vanish. A partially adaptive-sparse PDD method, obtained through regulated adaptivity and sparsity, is economical and is, therefore, expected to solve practical problems with numerous variables. Compared with past developments, the adaptive-sparse PDD methods do not require their truncation parameter(s) to be assigned a priori or arbitrarily. The numerical results reveal that an adaptive-sparse PDD method achieves a desired level of accuracy with considerably fewer coefficients compared with existing PDD approximations. For a required accuracy in calculating the probabilistic response characteristics, the new bivariate adaptive-sparse PDD method is more efficient than the existing bivariately truncated PDD method by almost an order of magnitude. Finally, stochastic dynamic analysis of a disk brake system was performed, demonstrating the ability of the new methods to tackle practical engineering problems.",10.1016/j.cma.2014.01.027,https://dx.doi.org/10.1016/j.cma.2014.01.027
"Mannina, G | Viviani, G",2010,An urban drainage stormwater quality model: Model development and uncertainty quantification,Applications_JOURNAL OF HYDROLOGY,50,6.25,"The evaluation of urban stormwater quality is of relevant importance for urban drainage, and mathematical models may be of great interest in this respect. To date, several detailed mathematical models are available to predict stormwater quantity-quality characteristics in urban drainage systems. However, only a few models take sewer sediments into account, considering their cohesive-like properties that influence the build-up process of the pollutant load. Furthermore, the model data requirements, especially for the quality aspects, are extensive, which limit their applicability and affect model results with large uncertainty. Uncertainty analysis provides a measure or index regarding the significance and the accuracy of the results obtained by mathematical modelling and is therefore of high interest. Nevertheless, only few studies have been carried out in the urban drainage field, and very few deal with water quality issues. One of the main reasons for this lack of research is the computational burden required by detailed models that preserve this analysis and generally require several Monte Carlo simulation runs. A possible to this problem may be the adoption of simplified parsimonious models that generally require shorter computational times. In this context, this paper presents a parsimonious conceptual model for the evaluation of the pollutant load in-sewers. The model contains two modules: a hydrological and hydraulic module that calculates the hydrographs at the inlet and at the outlet of the sewer system, and a solid transfer module that calculates the pollutographs. The cohesive properties of sewer sediments were carefully considered. Further, the effectiveness of the innovative sewer sediment modelling approach has been verified by taking into account the uncertainty assessed according to the GLUE methodology. The model has been tested using experimental quantity-quality data gathered in two Italian catchments, Fossolo (Bologna) and Parco d'Orleans (Palermo).",10.1016/j.jhydrol.2009.11.047,https://dx.doi.org/10.1016/j.jhydrol.2009.11.047
"Amir, O | Sigmund, O",2013,Reinforcement layout design for concrete structures based on continuum damage and truss topology optimization,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,31,6.2,"This article presents a new procedure for the layout design of reinforcement in concrete structures. Concrete is represented by a gradient-enhanced continuum damage model with strain-softening and reinforcement is modeled as elastic bars that are embedded into the concrete domain. Adjoint sensitivity analysis is derived in complete consistency with respect to path-dependency and the nonlocal model. Classical truss topology optimization based on the ground structure approach is applied to determine the optimal topology and cross-sections of the reinforcement bars. This approach facilitates a fully digital work flow that can be highly effective, especially for the design of complex structures. Several test cases involving two- and three-dimensional concrete structures illustrate the capabilities of the proposed procedure.",10.1007/s00158-012-0817-1,https://dx.doi.org/10.1007/s00158-012-0817-1
"Eckhardt, K | Breuer, L | Frede, HG",2003,Parameter uncertainty and the significance of simulated land use change effects,Applications_JOURNAL OF HYDROLOGY,92,6.13,"Uncertainty in parameters characterising different land covers leads to uncertainty in model predictions of land use change effects. In this study, a new approach is presented which allows a model to be assessed to see whether it is suitable for investigating land use change scenarios in the sense that different land covers can be significantly distinguished in their effects on model output. It consists of the following steps: (a) The uncertainty in land cover-dependent parameters is quantified. (b) The model of an artificial catchment with representative characteristics and uniform land cover is established.",10.1016/S0022-1694(02)00395-5,https://dx.doi.org/10.1016/S0022-1694(02)00395-5
"Xiong, FF | Greene, S | Chen, W | Xiong, Y | Yang, SX",2010,A new sparse grid based method for uncertainty propagation,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,49,6.12,"Current methods for uncertainty propagation suffer from their limitations in providing accurate and efficient solutions to high-dimension problems with interactions of random variables. The sparse grid technique, originally invented for numerical integration and interpolation, is extended to uncertainty propagation in this work to overcome the difficulty. The concept of Sparse Grid Numerical Integration (SGNI) is extended for estimating the first two moments of performance in robust design, while the Sparse Grid Interpolation (SGI) is employed to determine failure probability by interpolating the limit-state function at the Most Probable Point (MPP) in reliability analysis. The proposed methods are demonstrated by high-dimension mathematical examples with notable variate interactions and one multidisciplinary rocket design problem. Results show that the use of sparse grid methods works better than popular counterparts. Furthermore, the automatic sampling, special interpolation process, and dimension-adaptivity feature make SGI more flexible and efficient than using the uniform sample based metamodeling techniques.",10.1007/s00158-009-0441-x,https://dx.doi.org/10.1007/s00158-009-0441-x
"Najm, HN | Debusschere, BJ | Marzouk, YA | Widmer, S | Le Maitre, OP",2009,Uncertainty quantification in chemical systems,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,55,6.11,"We demonstrate the use of multiwavelet spectral polynomial chaos techniques for uncertainty quantification in non-isothermal ignition of a methane-air system We employ Bayesian inference for identifying the probabilistic representation of the uncertain parameters and propagate tills uncertainty through the ignition process We analyze the time evolution of moments and probability density functions of the, solution We also examine the role and significance of dependence among the uncertain parameters We finish with a discussion of the role of non-linearity and the performance of the algorithm.",10.1002/nme.2551,https://dx.doi.org/10.1002/nme.2551
"Thorndahl, S | Beven, KJ | Jensen, JB | Schaarup-Jensen, K",2008,"Event based uncertainty assessment in urban drainage modelling, applying the GLUE methodology",Applications_JOURNAL OF HYDROLOGY,61,6.1,"In the present paper an uncertainty analysis on an application of the commercial urban drainage model. MOUSE is conducted. Applying the Generalized Likelihood Uncertainty Estimation (GLUE) methodology the model. is conditioned on observation time series from two flow gauges as well as the occurrence of combined sewer overflow. The GLUE methodology is used to test different conceptual setups in order to determine if one model, setup gives a better goodness of fit conditional on the observations than the other. Moreover, different methodological investigations of GLUE are conducted in order to test if the uncertainty analysis is unambiguous. It is shown that the GLUE methodology is very applicable in uncertainty analysis of this application of an urban drainage model, although it was shown to be quite difficult to get good fits of the whole time series.",10.1016/j.jhydrol.2008.05.027,https://dx.doi.org/10.1016/j.jhydrol.2008.05.027
"Li, H | Li, PG | Gao, L | Zhang, L | Wu, T",2015,A level set method for topological shape optimization of 3D structures with extrusion constraints,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,18,6.0,"This paper proposes a new level set method for topological shape optimization of D structures considering manufacturing constraints. First, the boundary of structure is implicitly represented as the zero level set of a higher-dimensional level set function, and the implicit surface is parameterized through the interpolation of a given set of compactly supported radial basis functions. In this way, the original Hamilton-Jacobi partial differential equation is transformed into a system of algebraic equations. Correspondingly, the topological shape optimization is changed to the easiest size optimization in structural optimization. Many more efficient gradient-based optimization algorithms can be directly applied to the size optimization. Second, to save the expensive computational cost in the D large-scale optimization problems, the discrete wavelet transform is introduced into the level set method to compress the size of the coefficient matrix in compactly supported radial basis function interpolant. The discrete wavelet transform converts the original matrix into a set of wavelet basis and therefore eliminates the ""noise"" elements in the new matrix, so that the linear system can be replaced by a sparser one. Finally, a cross section projection strategy is utilized to ensure the satisfaction of the extrusion constraint and reduce the number of the design variables, simultaneously. Several numerical examples in D structures are employed to demonstrate the effectiveness of the proposed method.",10.1016/j.cma.2014.10.006,https://dx.doi.org/10.1016/j.cma.2014.10.006
"Soize, C",2010,Identification of high-dimension polynomial chaos expansions with random coefficients for non-Gaussian tensor-valued random fields using partial and limited experimental data,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,48,6.0,"This paper is devoted to the identification of high-dimension polynomial chaos expansions with random coefficients for non-Gaussian tensor-valued random fields using partial and limited experimental data. The experimental data sets correspond to partial experimental data made up of an observation vector which is the response of a stochastic boundary value problem depending on the tensor-valued random field which has to be identified. So an inverse stochastic problem has to be solved to carry out the identification of the random field. A complete methodology is proposed to solve this challenging problem and consists in introducing a family of prior probability models, in identifying an optimal prior model in the constructed family using the experimental data, in constructing a statistical reduced order optimal prior model, in constructing the polynomial chaos expansion with deterministic vector-valued coefficients of the reduced order optimal prior model and finally, in constructing the probability distribution of random coefficients of the polynomial chaos expansion and in identifying the parameters using experimental data. An application is presented for which several millions of random coefficients are identified solving an inverse stochastic problem.",10.1016/j.cma.2010.03.013,https://dx.doi.org/10.1016/j.cma.2010.03.013
"Micovic, Z | Schaefer, MG | Taylor, GH",2015,Uncertainty analysis for Probable Maximum Precipitation estimates,Applications_JOURNAL OF HYDROLOGY,18,6.0,"An analysis of uncertainty associated with Probable Maximum Precipitation (PMP) estimates is presented. The focus of the study is firmly on PMP estimates derived through meteorological analyses and not on statistically derived PMPs. Theoretical PMP cannot be computed directly and operational PMP estimates are developed through a stepwise procedure using a significant degree of subjective professional judgment. This paper presents a methodology for portraying the uncertain nature of PMP estimation by analyzing individual steps within the PMP derivation procedure whereby for each parameter requiring judgment, a set of possible values is specified and accompanied by expected probabilities. The resulting range of possible PMP values can be compared with the previously derived operational single-value PMP, providing measures of the conservatism and variability of the original estimate. To our knowledge, this is the first uncertainty analysis conducted for a PMP derived through meteorological analyses. The methodology was tested on the La Joie Dam watershed in British Columbia. The results indicate that the commonly used single-value PMP estimate could be more than % higher when possible changes in various meteorological variables used to derive the PMP are considered. The findings of this study imply that PMP estimates should always be characterized as a range of values recognizing the significant uncertainties involved in PMP estimation. In fact, we do not know at this time whether precipitation is actually upper-bounded, and if precipitation is upper-bounded, how closely PMP estimates approach the theoretical limit.",10.1016/j.jhydrol.2014.12.033,https://dx.doi.org/10.1016/j.jhydrol.2014.12.033
"Chen, P | Schwa, C",2016,"Sparse-grid, reduced-basis Bayesian inversion: Nonaffine-parametric nonlinear equations",Applications_JOURNAL OF COMPUTATIONAL PHYSICS,12,6.0,"We extend the reduced basis (RB) accelerated Bayesian inversion methods for affine-parametric, linear operator equations which are considered in [,] to non-affine, nonlinear parametric operator equations. We generalize the analysis of sparsity of parametric forward solution maps in [] and of Bayesian inversion in [,] to the fully discrete setting, including Petrov-Galerkin high-fidelity (""HiFi"") discretization of the forward maps. We develop adaptive, stochastic collocation based reduction methods for the efficient computation of reduced bases on the parametric solution manifold. The nonaffinity and nonlinearity with respect to (w.r.t.) the distributed, uncertain parameters and the unknown solution is collocated; specifically, by the so-called Empirical Interpolation Method (EIM). For the corresponding Bayesian inversion problems, computational efficiency is enhanced in two ways: first, expectations w.r.t. the posterior are computed by adaptive quadratures with dimension-independent convergence rates proposed in []; the present work generalizes [] to account for the impact of the PG discretization in the forward maps on the convergence rates of the Quantities of Interest (QoI for short). Second, we propose to perform the Bayesian estimation only w.r.t. a parsimonious, RB approximation of the posterior density. Based on the approximation results in [], the infinite-dimensional parametric, deterministic forward map and operator admit N-term RB and EIM approximations which converge at rates which depend only on the sparsity of the parametric forward map. In several numerical experiments, the proposed algorithms exhibit dimension-independent convergence rates which equal, at least, the currently known rate estimates for N-term approximation. We propose to accelerate Bayesian estimation by first offline construction of reduced basis surrogates of the Bayesian posterior density. The parsimonious surrogates can then be employed for online data assimilation and for Bayesian estimation. They also open a perspective for optimal experimental design.",10.1016/j.jcp.2016.02.055,https://dx.doi.org/10.1016/j.jcp.2016.02.055
"Poette, G | Despres, B | Lucor, D",2009,Uncertainty quantification for systems of conservation laws,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,54,6.0,"Uncertainty quantification through stochastic spectral methods has been recently applied to several kinds of non-linear stochastic PDEs. In this paper, we introduce a formalism based on kinetic theory to tackle uncertain hyperbolic systems of conservation laws with Polynomial Chaos (PC) methods. The idea is to introduce a new variable, the entropic variable, in bijection with our vector of unknowns, which we develop on the polynomial basis: by performing a Galerkin projection, we obtain a deterministic system of conservation laws. We state several properties of this deterministic system in the case of a general uncertain system of conservation laws. We then apply the method to the case of the inviscid Burgers' equation with random initial conditions and we present some preliminary results for the Euler system. We systematically compare results from our new approach to results from the stochastic Galerkin method. In the vicinity of discontinuities, the new method bounds the oscillations due to Gibbs phenomenon to a certain range through the entropy of the system without the use of any adaptative random space discretizations. It is found to be more precise than the stochastic Galerkin method for smooth cases but above all for discontinuous cases.",10.1016/j.jcp.2008.12.018,https://dx.doi.org/10.1016/j.jcp.2008.12.018
"Bishop, JE | Emery, JM | Field, RV | Weinberger, CR | Littlewood, DJ",2015,Direct numerical simulations in solid mechanics for understanding the macroscale effects of microscale material variability,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,18,6.0,"A fundamental challenge for the quantification of uncertainty in solid mechanics is understanding how microscale material variability is manifested at the macroscale. In an era of petascale computing and future exascale computing, it is now possible to perform direct numerical simulations (DNS) in solid mechanics where the microstructure is modeled directly in a macroscale structure. Using this DNS capability, we investigate the macroscale response of polycrystalline microstructures and the accuracy of homogenization theory for upscaling the microscale response. Using a massively parallel finite-element code, we perform an ensemble of direct numerical simulations in which polycrystalline microstructures are embedded throughout a macroscale structure. The largest simulations model approximately  thousand grains within an I-beam. The inherently random DNS results are compared with corresponding simulations based on the deterministic governing equations and material properties obtained from homogenization theory. Evidence is sought for both surface effects and other higher-order effects as predicted by homogenization theory for macroscale structures containing finite microstructures.",10.1016/j.cma.2015.01.017,https://dx.doi.org/10.1016/j.cma.2015.01.017
"Wang, HR | Wang, C | Wang, Y | Gao, X | Yu, C",2017,Bayesian forecasting and uncertainty quantifying of stream flows using Metropolis-Hastings Markov Chain Monte Carlo algorithm,Applications_JOURNAL OF HYDROLOGY,6,6.0,"This paper presents a Bayesian approach using Metropolis-Hastings Markov Chain Monte Carlo algorithm and applies this method for daily river flow rate forecast and uncertainty quantification for Zhujiachuan River using data collected from Qiaotoubao Gage Station and other  gage stations in Zhujiachuan watershed in China. The proposed method is also compared with the conventional maximum likelihood estimation (MLE) for parameter estimation and quantification of associated uncertainties. While the Bayesian method performs similarly in estimating the mean value of daily flow rate, it performs over the conventional MLE method on uncertainty quantification, providing relatively narrower reliable interval than the MLE confidence interval and thus more precise estimation by using the related information from regional gage stations. The Bayesian MCMC method might be more favorable in the uncertainty analysis and risk management.",10.1016/j.jhydrol.2017.03.073,https://dx.doi.org/10.1016/j.jhydrol.2017.03.073
"Hu, Z | Mahadevan, S",2017,A surrogate modeling approach for reliability analysis of a multidisciplinary system with spatio-temporal output,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,6,6.0,"Reliability analysis of a multidisciplinary system is computationally intensive due to the involvement of multiple disciplinary models and coupling between the individual models. When the system inputs and outputs are varying over time and space, the reliability analysis is even more challenging. This paper proposes a surrogate model-based method for the reliability analysis of a multidisciplinary system with spatio-temporal output. The transient characteristics of the multidisciplinary system output under time-dependent variability are analyzed first. Based on the transient analysis, surrogate models are built for individual disciplinary analyses instead of a single surrogate model for the fully coupled analysis. To address the challenge introduced by the high-dimensionality of spatially varying inter-disciplinary coupling variables, a data compression method is first employed to convert the high-dimensional coupling variables into low-dimensional latent space. Kriging surrogate modeling is then used to build surrogates for the individual disciplinary models in the latent space. Based on the individual disciplinary surrogate models, reliability analysis of the coupled multidisciplinary system under time-dependent uncertainty is investigated. Further, epistemic uncertainty sources, such as data uncertainty and model uncertainty, lead to uncertainty in the reliability estimate. Therefore, an auxiliary variable approach is used to efficiently include the epistemic uncertainty sources within the reliability analysis. An aircraft panel subjected to hypersonic flow conditions is used to demonstrate the proposed method. The analysis involves four interacting disciplinary models, namely, aerodynamics, aerothermal analysis, heat transfer, and structural analysis. The results show that the proposed method is able to effectively perform reliability analysis of a multidisciplinary system with spatio-temporal output.",10.1007/s00158-017-1737-x,https://dx.doi.org/10.1007/s00158-017-1737-x
"Xia, L | Fritzen, F | Breitkopf, P",2017,Evolutionary topology optimization of elastoplastic structures,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,6,6.0,"We have recently proposed in (Fritzen et al., Int J Numer Methods Eng ():-, ) an evolutionary topology optimization model for the design of multiscale elastoplastic structures, which is in general independent of the applied material law. Facing the variability of the final design for minor parameter changes when dealing with plastic structural designs, we further improve the robustness and the effectiveness of the BESO optimization procedure in this work by introducing a damping scheme on sensitivity numbers and by progressively reducing the sensitivity filtering radius. The damping scheme constraining the variance of the sensitivity numbers stabilizes the topological evolution process in particular for dissipative structural designs. By setting initially a large filter radius value and reducing it gradually, the emergence of the redundant structural branches, which are to be eliminated afterwards and are the main reasons deteriorating the design process, could be avoided. The robustness and the effectiveness of the improved model has been validated by means of benchmark numerical examples of conventional homogeneous structures.",10.1007/s00158-016-1523-1,https://dx.doi.org/10.1007/s00158-016-1523-1
"Walther, M | Graf, T | Kolditz, O | Liedl, R | Post, V",2017,How significant is the slope of the sea-side boundary for modelling seawater intrusion in coastal aquifers?,Applications_JOURNAL OF HYDROLOGY,6,6.0,"Application of numerical models is a common method to assess groundwater resources. The versatility of these models allows consideration of different levels of complexity, but the accuracy of the outcomes hinges upon a proper description of the system behaviour. In seawater intrusion assessment, the implementation of the sea-side boundary condition is of particular importance. We evaluate the influence of the slope of the sea-side boundary on the simulation results of seawater intrusion in a freshwater aquifer by employing a series of slope variations together with a sensitivity analysis by varying additional sensitive parameters (freshwater inflow and longitudinal and transverse dispersivities). Model results reveal a multi-dimensional dependence of the investigated variables with an increasing relevance of the seaside boundary slope for seawater intrusion (decrease of up to %), submarine groundwater discharge zone (reduction of up to %), and turnover times (increase of up to %) with increasing freshwater inflow or dispersivity values.",10.1016/j.jhydrol.2017.02.031,https://dx.doi.org/10.1016/j.jhydrol.2017.02.031
"Jensen, HA | Mayorga, F | Papadimitriou, C",2015,Reliability sensitivity analysis of stochastic finite element models,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,18,6.0,This contribution presents a scheme for integrating a model reduction technique into a simulation-based method for reliability sensitivity analysis of a class of medium/large nonlinear finite element models under stochastic excitation. The solution of this type of problems requires a large number of finite element model re-analyses to be performed over the space of system parameters. A component mode synthesis technique is implemented to carry out the sensitivity analysis in a reduced space of generalized coordinates. The reliability sensitivity analysis is performed by an approach based on a simple post-processing of an advanced sampling-based reliability analysis. The feasibility and effectiveness of the proposed scheme is demonstrated on a bridge finite element model under stochastic ground excitation.,10.1016/j.cma.2015.08.007,https://dx.doi.org/10.1016/j.cma.2015.08.007
"Rahman, S",2008,A polynomial dimensional decomposition for stochastic computing,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,60,6.0,"This article presents a new polynomial dimensional decomposition method for solving stochastic problems commonly encountered in engineering disciplines and applied sciences. The method involves a hierarchical decomposition of a multivariate response function in terms of variables with increasing dimensions, abroad range of orthonormal polynomial bases consistent with the probability measure for Fourier-polynomial expansion of component functions, and an innovative dimension-reduction integration for calculating the coefficients of the expansion. The new decomposition method does not require sample points as in the previous version; yet, it generates a convergent sequence of lower-variate estimates of the probabilistic characteristics of a generic stochastic response. The results of five numerical examples indicate that the proposed decomposition method provide accurate, convergent, and computationally efficient estimates of the tail probability of random mathematical functions or the reliability of mechanical system.",10.1002/nme.2394,https://dx.doi.org/10.1002/nme.2394
"Wang, S | Huang, GH | Baetz, BW | Ancell, BC",2017,Towards robust quantification and reduction of uncertainty in hydrologic predictions: Integration of particle Markov chain Monte Carlo and factorial polynomial chaos expansion,Applications_JOURNAL OF HYDROLOGY,6,6.0,"The particle filtering techniques have been receiving increasing attention from the hydrologic community due to its ability to properly estimate model parameters and states of nonlinear and non-Gaussian systems. To facilitate a robust quantification of uncertainty in hydrologic predictions, it is necessary to explicitly examine the forward propagation and evolution of parameter uncertainties and their interactions that affect the predictive performance. This paper presents a unified probabilistic framework that merges the strengths of particle Markov chain Monte Carlo (PMCMC) and factorial polynomial chaos expansion (FPCE) algorithms to robustly quantify and reduce uncertainties in hydrologic predictions. A Gaussian anamorphosis technique is used to establish a seamless bridge between the data assimilation using the PMCMC and the uncertainty propagation using the FPCE through a straightforward transformation of posterior distributions of model parameters. The unified probabilistic framework is applied to the Xiangxi River watershed of the Three Gorges Reservoir (TGR) region in China to demonstrate its validity and applicability. Results reveal that the degree of spatial variability of soil moisture capacity is the most identifiable model parameter with the fastest convergence through the streamflow assimilation process. The potential interaction between the spatial variability in soil moisture conditions and the maximum soil moisture capacity has the most significant effect on the performance of streamflow predictions. In addition, parameter sensitivities and interactions vary in magnitude and direction over time due to temporal and spatial dynamics of hydrologic processes.",10.1016/j.jhydrol.2017.03.027,https://dx.doi.org/10.1016/j.jhydrol.2017.03.027
"Hiriyur, B | Waisman, H | Deodatis, G",2011,Uncertainty quantification in homogenization of heterogeneous microstructures modeled by XFEM,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,42,6.0,"An extended finite element method (XFEM) coupled with a Monte Carlo approach is proposed to quantify the uncertainty in the homogenized effective elastic properties of multiphase materials. The methodology allows for an arbitrary number, aspect ratio, location and orientation of elliptic inclusions within a matrix, without the need for fine meshes in the vicinity of tightly packed inclusions and especially without the need to remesh for every different generated realization of the microstructure. Moreover, the number of degrees of freedom in the enriched elements is dynamically reallocated for each Monte Carlo sample run based on the given volume fraction. The main advantage of the proposed XFEM-based methodology is a major reduction in the computational effort in extensive Monte Carlo simulations compared with the standard FEM approach. Monte Carlo and XFEM appear to work extremely efficiently together. The Monte Carlo approach allows for the modeling of the size, aspect ratios, orientations, and spatial distribution of the elliptical inclusions as random variables with any prescribed probability distributions. Numerical results are presented and the uncertainty of the homogenized elastic properties is discussed.",10.1002/nme.3174,https://dx.doi.org/10.1002/nme.3174
"Cheng, M | Hou, TY | Zhang, Z",2013,A dynamically bi-orthogonal method for time-dependent stochastic partial differential equations I: Derivation and algorithms,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,29,5.8,"We propose a dynamically bi-orthogonal method (DyBO) to solve time dependent stochastic partial differential equations (SPDEs). The objective of our method is to exploit some intrinsic sparse structure in the stochastic solution by constructing the sparsest representation of the stochastic solution via a bi-orthogonal basis. It is well-known that the Karhunen-Loeve expansion (KLE) minimizes the total mean squared error and gives the sparsest representation of stochastic solutions. However, the computation of the KL expansion could be quite expensive since we need to form a covariance matrix and solve a large-scale eigenvalue problem. The main contribution of this paper is that we derive an equivalent system that governs the evolution of the spatial and stochastic basis in the KL expansion. Unlike other reduced model methods, our method constructs the reduced basis on-the-fly without the need to form the covariance matrix or to compute its eigendecomposition. In the first part of our paper, we introduce the derivation of the dynamically bi-orthogonal formulation for SPDEs, discuss several theoretical issues, such as the dynamic bi-orthogonality preservation and some preliminary error analysis of the DyBO method. We also give some numerical implementation details of the DyBO methods, including the representation of stochastic basis and techniques to deal with eigenvalue crossing. In the second part of our paper [], we will present an adaptive strategy to dynamically remove or add modes, perform a detailed complexity analysis, and discuss various generalizations of this approach. An extensive range of numerical experiments will be provided in both parts to demonstrate the effectiveness of the DyBO method.",10.1016/j.jcp.2013.02.033,https://dx.doi.org/10.1016/j.jcp.2013.02.033
"Kim, YI | Park, GJ",2010,Nonlinear dynamic response structural optimization using equivalent static loads,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,46,5.75,"It is well known that nonlinear dynamic response optimization using a conventional optimization algorithm is fairly difficult and expensive for the gradient or non-gradient based optimization methods because many nonlinear dynamic analyses are required. Therefore, it is quite difficult to find practical large scale examples with many design variables and constraints for nonlinear dynamic response structural optimization. The equivalent static loads (ESLs) method is newly proposed and applied to nonlinear dynamic response optimization. The equivalent static loads are defined as the linear static load sets which generate the same response field in linear static analysis as that from nonlinear dynamic analysis. The ESLs are made from the results of nonlinear dynamic analysis and used as external forces in linear static response optimization. Then the same response from nonlinear dynamic analysis can be considered throughout linear static response optimization. The updated design from linear response optimization is used again in nonlinear dynamic analysis and the process proceeds in a cyclic manner until the convergence criteria are satisfied. Several examples are solved to validate the method. The results are compared to those of the conventional method with sensitivity analysis using the finite difference method.",10.1016/j.cma.2009.10.014,https://dx.doi.org/10.1016/j.cma.2009.10.014
"Edeling, WN | Cinnella, P | Dwight, RP",2014,Predictive RANS simulations via Bayesian Model-Scenario Averaging,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,23,5.75,"The turbulence closure model is the dominant source of error in most Reynolds-Averaged Navier-Stokes simulations, yet no reliable estimators for this error component currently exist. Here we develop a stochastic, a posteriori error estimate, calibrated to specific classes of flow. It is based on variability in model closure coefficients across multiple flow scenarios, for multiple closure models. The variability is estimated using Bayesian calibration against experimental data for each scenario, and Bayesian Model-Scenario Averaging (BMSA) is used to collate the resulting posteriors, to obtain a stochastic estimate of a Quantity of Interest (QoI) in an unmeasured (prediction) scenario. The scenario probabilities in BMSA are chosen using a sensor which automatically weights those scenarios in the calibration set which are similar to the prediction scenario. The methodology is applied to the class of turbulent boundary-layers subject to various pressure gradients. For all considered prediction scenarios the standard-deviation of the stochastic estimate is consistent with the measurement ground truth. Furthermore, the mean of the estimate is more consistently accurate than the individual model predictions.",10.1016/j.jcp.2014.06.0520021,https://dx.doi.org/10.1016/j.jcp.2014.06.0520021
"Pushpalatha, R | Perrin, C | Le Moine, N | Mathevet, T | Andreassian, V",2011,A downward structural sensitivity analysis of hydrological models to improve low-flow simulation,Applications_JOURNAL OF HYDROLOGY,40,5.71,"Better simulation and earlier prediction of river low flows are needed for improved water management Here, a top-down structural analysis to improve a hydrological model in a low-flow simulation perspective is presented. Starting from a simple but efficient rainfall-runoff model (GRJ), we analyse the sensitivity of low-flow simulations to progressive modifications of the model's structure. These modifications correspond to the introduction of more complex routing schemes and/or the addition of simple representations of groundwater-surface water exchanges. In these tests, we wished to improve low-flow simulation while avoiding performance losses in high-flow conditions, i.e. keeping a general model. In a typical downward modelling perspective, over  versions of the model were tested on a large set of French catchments corresponding to various low-flow conditions, and performance was evaluated using criteria emphasising errors in low-flow conditions. The results indicate that several best performing structures yielded quite similar levels of efficiency. The addition of a new flow component to the routing part of the model yielded the most significant improvement. In spite of the close performance of several model structures, we conclude by proposing a modified model version of GRJ with a single additional parameter.",10.1016/j.jhydrol.2011.09.034,https://dx.doi.org/10.1016/j.jhydrol.2011.09.034
"Mignolet, MP | Soize, C",2008,Stochastic reduced order models for uncertain geometrically nonlinear dynamical systems,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,57,5.7,"A general methodology is presented for the consideration of both parameter and model uncertainty in the determination of the response of geometrically nonlinear structural dynamic systems. The approach is rooted in the availability of reduced order models of these nonlinear Systems with a deterministic basis extracted from a reference model (the mean model). Uncertainty, both from parameters and model, is introduced by randomizing the coefficients of the reduced order model in a manner that guarantees the physical appropriateness of every realization of the reduced order model, i.e. while maintaining the fundamental properties of symmetry and positive definiteness of every such reduced order model. This randomization is achieved not by postulating a specific joint statistical distribution of the reduced order model coefficients but rather by deriving this distribution through the principle of maximization of the entropy constrained to satisfy the necessary symmetry and positive definiteness properties. Several desirable features of this approach are that the uncertainty can be characterized by a single measure of dispersion, affects all coefficients of the reduced order model, and is computationally easily achieved. The reduced order modeling strategy and this stochastic modeling of its coefficients are presented in details and several applications to a beam undergoing large displacement are presented. These applications demonstrate the appropriateness and computational efficiency of the method to the broad class of uncertain geometrically nonlinear dynamic systems.",10.1016/j.cma.2008.03.032,https://dx.doi.org/10.1016/j.cma.2008.03.032
"Klarbring, A | Stromberg, N",2013,Topology optimization of hyperelastic bodies including non-zero prescribed displacements,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,28,5.6,"Stiffness topology optimization is usually based on a state problem of linear elasticity, and there seems to be little discussion on what is the limit for such a small rotation-displacement assumption. We show that even for gross rotations that are in all practical aspects small (<  deg), topology optimization based on a large deformation theory might generate different design concepts compared to what is obtained when small displacement linear elasticity is used. Furthermore, in large rotations, the choice of stiffness objective (potential energy or compliance), can be crucial for the optimal design concept. The paper considers topology optimization of hyperelastic bodies subjected simultaneously to external forces and prescribed non-zero displacements. In that respect it generalizes a recent contribution of ours to large deformations, but we note that the objectives of potential energy and compliance are no longer equivalent in the non-linear case. We use seven different hyperelastic strain energy functions and find that the numerical performance of the Kirchhoff-St.Venant model is in general significantly worse than the performance of the other six models, which are all modifications of this classical law that are equivalent in the limit of infinitesimal strains, but do not contain the well-known collapse in compression. Numerical results are presented for two different problem settings.",10.1007/s00158-012-0819-z,https://dx.doi.org/10.1007/s00158-012-0819-z
"Ueckermann, MP | Lermusiaux, PFJ | Sapsis, TP",2013,Numerical schemes for dynamically orthogonal equations of stochastic fluid and ocean flows,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,28,5.6,"The quantification of uncertainties is critical when systems are nonlinear and have uncertain terms in their governing equations or are constrained by limited knowledge of initial and boundary conditions. Such situations are common in multiscale, intermittent and non-homogeneous fluid and ocean flows. The dynamically orthogonal (DO) field equations provide an adaptive methodology to predict the probability density functions of such flows. The present work derives efficient computational schemes for the DO methodology applied to unsteady stochastic Navier-Stokes and Boussinesq equations, and illustrates and studies the numerical aspects of these schemes. Semi-implicit projection methods are developed for the mean and for the DO modes, and time-marching schemes of first to fourth order are used for the stochastic coefficients. Conservative second-order finite-volumes are employed in physical space with new advection schemes based on total variation diminishing methods. Other results include: (i) the definition of pseudo-stochastic pressures to obtain a number of pressure equations that is linear in the subspace size instead of quadratic; (ii) symmetric advection schemes for the stochastic velocities; (iii) the use of generalized inversion to deal with singular subspace covariances or deterministic modes; and (iv) schemes to maintain orthonormal modes at the numerical level. To verify our implementation and study the properties of our schemes and their variations, a set of stochastic flow benchmarks are defined including asymmetric Dirac and symmetric lock-exchange flows, lid-driven cavity flows, and flows past objects in a confined channel. Different Reynolds number and Grashof number regimes are employed to illustrate robustness. Optimal convergence under both time and space refinements is shown as well as the convergence of the probability density functions with the number of stochastic realizations.",10.1016/j.jcp.2012.08.041,https://dx.doi.org/10.1016/j.jcp.2012.08.041
"Jin, BT | Marin, L",2007,The method of fundamental solutions for inverse source problems associated with the steady-state heat conduction,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,61,5.55,"This paper presents the use of the method of fundamental solutions (MFS) for recovering the heat source in steady-state heat conduction problems from boundary temperature and heat flux measurements. It is well known that boundary data alone do not determine uniquely a general heat source and hence some a priori knowledge is assumed in order to guarantee the uniqueness of the solution. In the present study, the heat source is assumed to satisfy a second-order partial differential equation on a physical basis, thereby transforming the problem into a fourth-order partial differential equation, which can be conveniently solved using the MFS. Since the matrix arising from the MFS discretization is severely ill-conditioned, a regularized solution is obtained by employing the truncated singular value decomposition, whilst the optimal regularization parameter is determined by the L-curve criterion. Numerical results are presented for several two-dimensional problems with both exact and noisy data. The sensitivity analysis with respect to two solution parameters, i.e. the number of source points and the distance between the fictitious and physical boundaries, and one problem parameter, i.e. the measure of the accessible part of the boundary, is also performed. The stability of the scheme with respect to the amount of noise added into the data is analysed. The numerical results obtained show that the proposed numerical algorithm is accurate, convergent, stable and computationally efficient for solving inverse source problems in steady-state heat conduction.",10.1002/nme.1826,https://dx.doi.org/10.1002/nme.1826
"Magri, L | Bauerheim, M | Juniper, MP",2016,Stability analysis of thermo-acoustic nonlinear eigenproblems in annular combustors. Part I. Sensitivity,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,11,5.5,"We present an adjoint-based method for the calculation of eigenvalue perturbations in nonlinear, degenerate and non-self-adjointeigenproblems. This method is applied to a thermo-acoustic annular combustor network, the stability of which is governed by a nonlinear eigenproblem. We calculate the first-and second-order sensitivities of the growth rate and frequency to geometric, flow and flame parameters. Three different configurations are analysed. The benchmark sensitivities are obtained by finite difference, which involves solving the nonlinear eigenproblem at least as many times as the number of parameters. By solving only one adjoint eigenproblem, we obtain the sensitivities to any thermo-acoustic parameter, which match the finite-difference solutions at much lower computational cost.",10.1016/j.jcp.2016.07.032,https://dx.doi.org/10.1016/j.jcp.2016.07.032
"Yan, K | Cheng, GD | Wang, BP",2016,Topology optimization of plate structures subject to initial excitations for minimum dynamic performance index,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,11,5.5,"This paper studies optimal topology design of damped vibrating plate structures subject to initial excitations. The design objective is to minimize an integrated square performance measure. The artificial density of the plate element is the topology design variable and the material volume is given. The Lyapunov's second method is applied to reduce the calculation of performance measure to the solution of the Lyapunov equation. An adjoint sensitivity analysis method is used, which only needs to solve the Lyapunov equation twice. However, when the problem has a large number of degrees of freedom, the solution process of Lyapunov equation is computational costly. Thus, the full model is transformed to a reduced space by mode reduction method. To further reduce the scale of reduced model, we propose a mode screen method to decrease the number of eigenmodes. Numerical examples of optimum topology design of bending plates are presented for illustrating validity and efficiency of our new algorithm.",10.1007/s00158-015-1350-9,https://dx.doi.org/10.1007/s00158-015-1350-9
"Mehr, AD | Kahya, E | Ozger, M",2014,A gene-wavelet model for long lead time drought forecasting,Applications_JOURNAL OF HYDROLOGY,22,5.5,"Drought forecasting is an essential ingredient for drought risk and sustainable water resources management. Due to increasing water demand and looming climate change, precise drought forecasting models have recently been receiving much attention. Beginning with a brief discussion of different drought forecasting models, this study presents a new hybrid gene-wavelet model, namely wavelet-linear genetic programing (WLGP), for long lead-time drought forecasting. The idea of WLGP is to detect and optimize the number of significant spectral bands of predictors in order to forecast the original predictand (drought index) directly. Using the observed El Nino-Southern Oscillation indicator (NINO . index) and Palmer's modified drought index (PMDI) as predictors and future PMDI as predictand, we proposed the WLGP model to forecast drought conditions in the State of Texas with , , and -month lead times. We compared the efficiency of the model with those of a classic linear genetic programing model developed in this study, a neuro-wavelet (WANN), and a fuzzy-wavelet (WFL) drought forecasting models formerly presented in the relevant literature. Our results demonstrated that the classic linear genetic programing model is unable to learn the non-linearity of drought phenomenon in the lead times longer than  months; however, the WLGP can be effectively used to forecast drought conditions having , , and -month lead times. Genetic-based sensitivity analysis among the input spectral bands showed that NINO . index has strong potential effect in drought forecasting of the study area with --month lead times.",10.1016/j.jhydrol.2014.06.012,https://dx.doi.org/10.1016/j.jhydrol.2014.06.012
"Arnst, M | Ghanem, R | Soize, C",2010,Identification of Bayesian posteriors for coefficients of chaos expansions,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,43,5.38,"This article is concerned with the identification of probabilistic characterizations of random variables and fields from experimental data. The data used for the identification consist of measurements of several realizations of the uncertain quantities that must be characterized. The random variables and fields are approximated by a polynomial chaos expansion, and the coefficients of this expansion are viewed as unknown parameters to be identified. It is shown how the Bayesian paradigm can be applied to formulate and solve the inverse problem. The estimated polynomial chaos coefficients are hereby themselves characterized as random variables whose probability density function is the Bayesian posterior. This allows to quantify the impact of missing experimental information on the accuracy of the identified coefficients, as well as on subsequent predictions. An illustration in stochastic aeroelastic stability analysis is provided to demonstrate the proposed methodology.",10.1016/j.jcp.2009.12.033,https://dx.doi.org/10.1016/j.jcp.2009.12.033
"Van Miegroet, L | Duysinx, P",2007,Stress concentration minimization of 2D filets using X-FEM and level set description,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,59,5.36,"This paper presents and applies a novel shape optimization approach based on the level set description of the geometry and the extended finite element method (X-FEM). The method benefits from the fixed mesh work using X-FEM and from the curves smoothness of the level set description. Design variables are shape parameters of basic geometric features that are described with a level set representation. The number of design variables of this formulation remains small, whereas global (i.e. compliance) and local constraints (i.e. stresses) can be considered. To illustrate the capability of the method to handle stress constraints, numerical applications revisit the minimization of stress concentration in a D filet in tension, which has been previously studied in Pedersen (). Our results illustrate the great interest of using X-FEM and level set description together. A special attention is also paid to stress computation and accuracy with the X-FEM.",10.1007/s00158-006-0091-1,https://dx.doi.org/10.1007/s00158-006-0091-1
"Raisee, M | Kumar, D | Lacor, C",2015,A non-intrusive model reduction approach for polynomial chaos expansion using proper orthogonal decomposition,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,16,5.33,"In this paper, a non-intrusive stochastic model reduction scheme is developed for polynomial chaos representation using proper orthogonal decomposition. The main idea is to extract the optimal orthogonal basis via inexpensive calculations on a coarse mesh and then use them for the fine-scale analysis. To validate the developed reduced-order model, the method is implemented to: () the stochastic steady-state heat diffusion in a square slab; () the incompressible, two-dimensional laminar boundary-layer over a flat plate with uncertainties in free-stream velocity and physical properties; and () the highly nonlinear Ackley function with uncertain coefficients. For the heat diffusion problem, the thermal conductivity of the slab is assumed to be a stochastic field with known exponential covariance function and approximated via the Karhunen-Loeve expansion. In all three test cases, the input random parameters are assumed to be uniformly distributed, and a polynomial chaos expansion is found using the regression method. The Sobol's quasi-random sequence is used to generate the sample points. The numerical results of the three test cases show that the non-intrusive model reduction scheme is able to produce satisfactory results for the statistical quantities of interest. It is found that the developed non-intrusive model reduction scheme is computationally more efficient than the classical polynomial chaos expansion for uncertainty quantification of stochastic problems. The performance of the developed scheme becomes more apparent for the problems with larger stochastic dimensions and those requiring higher polynomial order for the stochastic discretization.",10.1002/nme.4900,https://dx.doi.org/10.1002/nme.4900
"Agarwal, N | Aluru, NR",2009,A domain adaptive stochastic collocation approach for analysis of MEMS under uncertainties,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,48,5.33,"This work proposes a domain adaptive stochastic collocation approach for uncertainty quantification, suitable for effective handling of discontinuities or sharp variations in the random domain. The basic idea of the proposed methodology is to adaptively decompose the random domain into subdomains. Within each subdomain, a sparse grid interpolant is constructed using the classical Smolyak construction [S. Smolyak, Quadrature and interpolation formulas for tensor products of certain classes of functions, Soviet Math. Dokl.  () -], to approximate the stochastic solution locally. The adaptive strategy is governed by the hierarchical surpluses, which are computed as part of the interpolation procedure. These hierarchical surpluses then serve as an error indicator for each subdomain, and lead to subdivision whenever it becomes greater than a threshold value. The hierarchical surpluses also provide information about the more important dimensions, and accordingly the random elements can be split along those dimensions. The proposed adaptive approach is employed to quantify the effect of uncertainty in input parameters on the performance of micro-electromechanical systems (MEMS). Specifically, we study the effect of uncertain material properties and geometrical parameters on the pull-in behavior and actuation properties of a MEMS switch. Using the adaptive approach, we resolve the pull-in instability in MEMS switches. The results from the proposed approach are verified using Monte Carlo simulations and it is demonstrated that it computes the required statistics effectively.",10.1016/j.jcp.2009.07.014,https://dx.doi.org/10.1016/j.jcp.2009.07.014
"Beran, PS | Pettit, CL | Millman, DR",2006,Uncertainty quantification of limit-cycle oscillations,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,64,5.33,"Different computational methodologies have been developed to quantify the uncertain response of a relatively simple aeroelastic system in limit-cycle oscillation, subject to parametric variability. The aeroelastic system is that of a rigid airfoil, supported by pitch and plunge structural coupling, with nonlinearities in the component in pitch. The nonlinearities are adjusted to permit the formation of a either a subcritical or supercritical branch of limit-cycle oscillations. Uncertainties are specified in the cubic coefficient of the torsional spring and in the initial pitch angle of the airfoil. Stochastic projections of the time-domain and cyclic equations governing system response are carried out, leading to both intrusive and non-intrusive computational formulations. Non-intrusive formulations are examined using stochastic projections derived from Wiener expansions involving Haar wavelet and B-spline bases, while Wiener-Hermite expansions of the cyclic equations are employed intrusively and non-intrusively. Application of the B-spline stochastic projection is extended to the treatment of aerodynamic nonlinearities, as modeled through the discrete Euler equations. The methodologies are compared in terms of computational cost, convergence properties, ease of implementation, and potential for application to complex aeroelastic systems.",10.1016/j.jcp.2006.03.038,https://dx.doi.org/10.1016/j.jcp.2006.03.038
"Sinha, K",2007,Reliability-based multiobjective optimization for automotive crashworthiness and occupant safety,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,58,5.27,"This paper presents a methodology for reliability-based multiobjective optimization of large-scale engineering systems. This methodology is applied to the vehicle crashworthiness design optimization for side impact, considering both structural crashworthiness and occupant safety, with structural weight and front door velocity under side impact as objectives. Uncertainty quantification is performed using two first order reliability method-based techniques: approximate moment approach and reliability index approach. Genetic algorithm-based multiobjective optimization software GDOT, developed in-house, is used to come up with an optimal pareto front in all cases. The technique employed in this study treats multiple objective functions separately without combining them in any form. It shows that the vehicle weight can be reduced significantly from the baseline design and at the same time reduce the door velocity. The obtained pareto front brings out useful inferences about optimal design regions. A decision-making criterion is subsequently invoked to select the ""best"" subset of solutions from the obtained nondominated pareto optimal solutions. The reliability, thus computed, is also checked with Monte Carlo simulations. The optimal solution indicated by knee point on the optimal pareto front is verified with LS-DYNA simulation results.",10.1007/s00158-006-0050-x,https://dx.doi.org/10.1007/s00158-006-0050-x
"Xia, Q | Shi, TL | Wang, MY | Liu, SY",2010,A level set based method for the optimization of cast part,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,42,5.25,"A cast part is formed via casting process in which molten liquid is poured into and solidifies in a cavity enclosed by molds. Then, one obtains the cast part when the molds are removed. An important issue in the casting process is that a cast part should have a proper geometry so that the molds can actually be removed. Accordingly, in the optimization of a cast part one not only needs to optimize the performance of the cast part but also needs to ensure the cast part have a proper geometry. With these goals, a level set based method is proposed for the optimization of cast part. A molding condition and a performance condition on the design velocity are derived for the optimization. Numerical examples are provided in D and D.",10.1007/s00158-009-0444-7,https://dx.doi.org/10.1007/s00158-009-0444-7
"Yamasaki, S | Nomura, T | Kawamoto, A | Sato, K | Nishiwaki, S",2011,A level set-based topology optimization method targeting metallic waveguide design problems,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,36,5.14,"In this paper, we propose a level set-based topology optimization method targeting metallic waveguide design problems, where the skin effect must be taken into account since the metallic waveguides are generally used in the high-frequency range where this effect critically affects performance. One of the most reasonable approaches to represent the skin effect is to impose an electric field constraint condition on the surface of the metal. To implement this approach, we develop a boundary-tracking scheme for the arbitrary Lagrangian Eulerian (ALE) mesh pertaining to the zero iso-contour of the level set function that is given in an Eulerian mesh, and impose Dirichlet boundary conditions at the nodes on the zero iso-contour in the ALE mesh to compute the electric field. Since the ALE mesh accurately tracks the zero iso-contour at every optimization iteration, the electric field is always appropriately computed during optimization. For the sensitivity analysis, we compute the nodal coordinate sensitivities in the ALE mesh and smooth them by solving a Helmholtz-type partial differential equation. The obtained smoothed sensitivities are used to compute the normal velocity in the level set equation that is solved using the Eulerian mesh, and the level set function is updated based on the computed normal velocity. Finally, the utility of the proposed method is discussed through several numerical examples.",10.1002/nme.3135,https://dx.doi.org/10.1002/nme.3135
"Jensen, HA | Valdebenito, MA | Schueller, GI | Kusanovic, DS",2009,Reliability-based optimization of stochastic systems using line search,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,46,5.11,This contribution presents an approach for solving reliability-based optimization problems involving structural systems under stochastic loading. The associated reliability problems to be solved during the optimization process are high-dimensional ( or more random variables). A standard gradient-based algorithm with line search is used in this work. Subset simulation is adopted for the purpose of estimating the corresponding failure probabilities. The gradients of the failure probability functions are estimated by an approach based on the local behavior of the performance functions that define the failure domains. Numerical results show that only a moderate number of reliability estimates has to be performed during the entire design process. Two numerical examples showing the effectiveness of the approach reported herein are presented.,10.1016/j.cma.2009.08.016,https://dx.doi.org/10.1016/j.cma.2009.08.016
"Luo, Z | Tong, LY",2008,A level set method for shape and topology optimization of large-displacement compliant mechanisms,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,51,5.1,"A parameterization level set method is presented for structural shape and topology optimization of compliant mechanisms involving large displacements. A level set model is established mathematically as the Hamilton-Jacobi equation to capture the motion of the free boundary of a continuum structure. The structural design boundary is thus described implicitly as the zero level set of a level set scalar function of higher dimension. The radial basis function with compact support is then applied to interpolate the level set function, leading to a relaxation and separation of the temporal and spatial discretizations related to the original partial differential equation. In doing so, the more difficult shape and topology optimization problem is now fully parameterized into a relatively easier size optimization of generalized expansion coefficients. As a result, the optimization is changed into a numerical process of implementing a series of motions of the implicit level set function via an existing efficient convex programming method. With the concept of the shape derivative, the geometrical non-linearity is included in the rigorous design sensitivity analysis to appropriately capture the large displacements of compliant mechanisms. Several numerical benchmark examples illustrate the effectiveness of the present level set method, in particular, its capability of generating new holes inside the material domain. The proposed method not only retains the favorable features of the implicit free boundary representation but also overcomes several unfavorable numerical considerations relevant to the explicit scheme, the reinitialization procedure, and the velocity extension algorithm in the conventional level set method.",10.1002/nme.2352,https://dx.doi.org/10.1002/nme.2352
"Linhoss, AC | Siegert, CM",2016,A comparison of five forest interception models using global sensitivity and uncertainty analysis,Applications_JOURNAL OF HYDROLOGY,10,5.0,"Interception by the forest canopy plays a critical role in the hydrologic cycle by removing a significant portion of incoming precipitation from the terrestrial component. While there are a number of existing physical models of forest interception, few studies have summarized or compared these models. The objective of this work is to use global sensitivity and uncertainty analysis to compare five mechanistic interception models including the Rutter, Rutter Sparse, Gash, Sparse Gash, and Liu models. Using parameter probability distribution functions of values from the literature, our results show that on average storm duration [Dur], gross precipitation [PG], canopy storage [S] and solar radiation [Rn] are the most important model parameters. On the other hand, empirical parameters used in calculating evaporation and drip (i.e. trunk evaporation as a proportion of evaporation from the saturated canopy [e], the empirical drainage parameter [b], the drainage partitioning coefficient [pd], and the rate of water dripping from the canopy when canopy storage has been reached [Ds]) have relatively low levels of importance in interception modeling. As such, future modeling efforts should aim to decompose parameters that are the most influential in determining model outputs into easily measurable physical components. Because this study compares models, the choices regarding the parameter probability distribution functions are applied across models, which enables a more definitive ranking of model uncertainty.",10.1016/j.jhydrol.2016.04.011,https://dx.doi.org/10.1016/j.jhydrol.2016.04.011
"Zhang, WH | Zhou, Y | Zhu, JH",2017,A comprehensive study of feature definitions with solids and voids for topology optimization,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,5,5.0,"In this paper, we present a CAD-oriented and feature-driven topology optimization methodology for engineering structures with freeform design domains. Mixed solid-void features are adopted as basic design primitives to build a unified formulation of feature definition, while pure solid or pure void features are considered as two special cases. Topology optimization is achieved as a Boolean intersection process between the so-called topology variation modeler (TVM) and freeform design domain modeler (FDDM) in terms of level-set functions (LSFs). The TVMs constructed with solid and/or void features are applied to clip the FDDM through Boolean operations. It is shown that sensitivity analysis with boundary integral scheme is independent of the mathematical expressions of the LSF. Numerical problems with freeform and periodic design domains are dealt with to demonstrate the proposed topology optimization method.",10.1016/j.cma.2017.07.004,https://dx.doi.org/10.1016/j.cma.2017.07.004
"Ni, AX | Wang, QQ",2017,Sensitivity analysis on chaotic dynamical systems by Non-Intrusive Least Squares Shadowing (NILSS),Applications_JOURNAL OF COMPUTATIONAL PHYSICS,5,5.0,"This paper develops the Non-Intrusive Least Squares Shadowing (NILSS) method, which computes the sensitivity for long-time averaged objectives in chaotic dynamical systems. In NILSS, we represent a tangent solution by a linear combination of one inhomogeneous tangent solution and several homogeneous tangent solutions. Next, we solve a least squares problem using this representation; thus, the resulting solution can be used for computing sensitivities. NILSS is easy to implement with existing solvers. In addition, for chaotic systems with many degrees of freedom but few unstable modes, NILSS has a low computational cost. NILSS is applied to two chaotic PDE systems: the Lorenz  system and a CFD simulation of flow over a backward-facing step. In both cases, the sensitivities computed by NILSS reflect the trends in the long-time averaged objectives of dynamical systems.",10.1016/j.jcp.2017.06.033,https://dx.doi.org/10.1016/j.jcp.2017.06.033
"de Figueiredo, LP | Grana, D | Santos, M | Figueiredo, W | Roisenberg, M | Neto, GS",2017,"Bayesian seismic inversion based on rock-physics prior modeling for the joint estimation of acoustic impedance, porosity and lithofacies",Applications_JOURNAL OF COMPUTATIONAL PHYSICS,5,5.0,"We propose a Bayesian approach for seismic inversion to estimate acoustic impedance, porosity and lithofacies within the reservoir conditioned to post-stack seismic and well data. The link between elastic and petrophysical properties is given by a joint prior distribution for the logarithm of impedance and porosity, based on a rock-physics model. The well conditioning is performed through a background model obtained by well log interpolation. Two different approaches are presented: in the first approach, the prior is defined by a single Gaussian distribution, whereas in the second approach it is defined by a Gaussian mixture to represent the well data multimodal distribution and link the Gaussian components to different geological lithofacies. The forward model is based on a linearized convolutional model. For the single Gaussian case, we obtain an analytical expression for the posterior distribution, resulting in a fast algorithm to compute the solution of the inverse problem, i.e. the posterior distribution of acoustic impedance and porosity as well as the facies probability given the observed data. For the Gaussian mixture prior, it is not possible to obtain the distributions analytically, hence we propose a Gibbs algorithm to perform the posterior sampling and obtain several reservoir model realizations, allowing an uncertainty analysis of the estimated properties and lithofacies. Both methodologies are applied to a real seismic dataset with three wells to obtain D models of acoustic impedance, porosity and lithofacies. The methodologies are validated through a blind well test and compared to a standard Bayesian inversion approach. Using the probability of the reservoir lithofacies, we also compute a D isosurface probability model of the main oil reservoir in the studied field.",10.1016/j.jcp.2017.02.013,https://dx.doi.org/10.1016/j.jcp.2017.02.013
"Pang, G | Perdikaris, P | Cai, W | Karniadakis, GE",2017,Discovering variable fractional orders of advection-dispersion equations from field data using multi-fidelity Bayesian optimization,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,5,5.0,"The fractional advection-dispersion equation (FADE) can describe accurately the solute transport in groundwater but its fractional order has to be determined a priori. Here, we employ multi-fidelity Bayesian optimization to obtain the fractional order under various conditions, and we obtain more accurate results compared to previously published data. Moreover, the present method is very efficient as we use different levels of resolution to construct a stochastic surrogate model and quantify its uncertainty. We consider two different problem set ups. In the first set up, we obtain variable fractional orders of one-dimensional FADE, considering both synthetic and field data. In the second set up, we identify constant fractional orders of two-dimensional FADE using synthetic data. We employ multi-resolution simulations using two-level and three-level Gaussian process regression models to construct the surrogates.",10.1016/j.jcp.2017.07.052,https://dx.doi.org/10.1016/j.jcp.2017.07.052
"Soize, C | Farhat, C",2017,A nonparametric probabilistic approach for quantifying uncertainties in low-dimensional and high-dimensional nonlinear models,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,5,5.0,"A nonparametric probabilistic approach for modeling uncertainties in projection-based, nonlinear, reduced-order models is presented. When experimental data are available, this approach can also quantify uncertainties in the associated high-dimensional models. The main underlying idea is twofold. First, to substitute the deterministic reduced-order basis (ROB) with a stochastic counterpart. Second, to construct the probability measure of the stochastic reduced-order basis (SROB) on a subset of a compact Stiefel manifold in order to preserve some important properties of a ROB. The stochastic modeling is performed so that the probability distribution of the constructed SROB depends on a small number of hyperparameters. These are determined by solving a reduced-order statistical inverse problem. The mathematical properties of this novel approach for quantifying model uncertainties are analyzed through theoretical developments and numerical simulations. Its potential is demonstrated through several example problems from computational structural dynamics.",10.1002/nme.5312,https://dx.doi.org/10.1002/nme.5312
"Garcea, G | Liguori, FS | Leonetti, L | Magisano, D | Madeo, A",2017,Accurate and efficient a posteriori account of geometrical imperfections in Koiter finite element analysis,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,5,5.0,"The Koiter method recovers the equilibrium path of an elastic structure using a reduced model, obtained by means of a quadratic asymptotic expansion of the finite element model. Its main feature is the possibility of efficiently performing sensitivity analysis by including a posteriori the effects of the imperfections in the reduced nonlinear equations. The state-of-art treatment of geometrical imperfections is accurate only for small imperfection amplitudes and linear pre-critical behaviour. This work enlarges the validity of the method to a wider range of practical problems through a new approach, which accurately takes into account the imperfection without losing the benefits of the a posteriori treatment. A mixed solid-shell finite element is used to build the discrete model. A large number of numerical tests, regarding nonlinear buckling problems, modal interaction, unstable post-critical and imperfection sensitive structures, validates the proposal.",10.1002/nme.5550,https://dx.doi.org/10.1002/nme.5550
"Oxberry, GM | Kostova-Vassilevska, T | Arrighi, W | Chand, K",2017,Limited-memory adaptive snapshot selection for proper orthogonal decomposition,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,5,5.0,"Reduced order models are useful for accelerating simulations in many-query contexts, such as optimization, uncertainty quantification, and sensitivity analysis. However, offline training of reduced order models (ROMs) can have prohibitively expensive memory and floating-point operation costs in high-performance computing applications, where memory per core is limited. To overcome this limitation for proper orthogonal decomposition, we propose a novel adaptive selection method for snapshots in time that limits offline training costs by selecting snapshots according an error control mechanism similar to that found in adaptive time-stepping ordinary differential equation solvers. The error estimator used in this work is related to theory bounding the approximation error in time of proper orthogonal decomposition-based ROMs, and memory usage is minimized by computing the singular value decomposition using a single-pass incremental algorithm. Results for a viscous Burgers' test problem demonstrate convergence in the limit as the algorithm error tolerances go to zero; in this limit, the full-order model is recovered to within discretization error. A parallel version of the resulting method can be used on supercomputers to generate proper orthogonal decomposition-based ROMs, or as a subroutine within hyperreduction algorithms that require taking snapshots in time, or within greedy algorithms for sampling parameter space.",10.1002/nme.5283,https://dx.doi.org/10.1002/nme.5283
"Mohamad, MA | Cousins, W | Sapsis, TP",2016,A probabilistic decomposition-synthesis method for the quantification of rare events due to internal instabilities,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,10,5.0,"We consider the problem of the probabilistic quantification of dynamical systems that have heavy-tailed characteristics. These heavy-tailed features are associated with rare transient responses due to the occurrence of internal instabilities. Systems with these properties can be found in a variety of areas including mechanics, fluids, and waves. Here we develop a computational method, a probabilistic decomposition-synthesis technique, that takes into account the nature of internal instabilities to inexpensively determine the non-Gaussian probability density function for any arbitrary quantity of interest. Our approach relies on the decomposition of the statistics into a 'non-extreme core', typically Gaussian, and a heavy-tailed component. This decomposition is in full correspondence with a partition of the phase space into a 'stable' region where we have no internal instabilities, and a region where non-linear instabilities lead to rare transitions with high probability. We quantify the statistics in the stable region using a Gaussian approximation approach, while the non-Gaussian distribution associated with the intermittently unstable regions of phase space is inexpensively computed through order-reduction methods that take into account the strongly nonlinear character of the dynamics. The probabilistic information in the two domains is analytically synthesized through a total probability argument. The proposed approach allows for the accurate quantification of non-Gaussian tails at more than  standard deviations, at a fraction of the cost associated with the direct Monte-Carlo simulations. We demonstrate the probabilistic decomposition-synthesis method for rare events for two dynamical systems exhibiting extreme events: a two-degree-of-freedom system of nonlinearly coupled oscillators, and in a nonlinear envelope equation characterizing the propagation of unidirectional water waves.",10.1016/j.jcp.2016.06.047,https://dx.doi.org/10.1016/j.jcp.2016.06.047
"Espinosa, HD | Dwivedi, S | Lu, HC",2000,Modeling impact induced delamination of woven fiber reinforced composites with contact/cohesive laws,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,88,4.89,"The dynamic delamination in woven glass fiber reinforced plastic (GRP) composite is studied with a D finite deformation anisotropic viscoplastic model in conjunction with contact/cohesive laws. Thr large deformation of the material during impact loading is described through an anisotropic plasticity model in total Lagrangian co-ordinates whose coefficients are determined experimentally. The interaction between lamina is analyzed through a contact/interface model. The tensile and shear tractions in zero thickness interface elements, embedded between lamina, are calculated from interface cohesive law. The interface cohesive law describes the evolution of these tractions in terms of normal and tangential displacement jumps and other interface parameters. The compressive traction at the interface is calculated through the impenetrability condition employed in the contact module. Once the effective dis placement jump exceeds a specified critical value, the interface elements are assumed to have failed, i.e., delamination is said to have taken place. Three interface cohesive laws are proposed to describe the delamination process. It is assumed that loading of interface takes place reversibly up to a specified value of the displacement jump Followed by irreversible loading beyond this value. This feature represents a partial damage of the interface in the event of unloading. Dynamic delamination in the woven GRP composite is studied through analyses of plate-on-plate impact experiments. The heterogeneity of composite materials leading to wave dispersion and scattering is modeled by considering a layered composition of the GRP plate. Each lamina is assumed to be made of three layers of materials. The middle layer of half the thickness of lamina is considered as GRP and the two end layers of equal thicknesses are considered to be of matrix material, i.e., polyester resin. The possible delamination of the composite material under compressive shock loading is shown to occur due to local shear effects. This is modeled by considering waviness of the interface between lamina, Interfaces with Aat as well as two types of wavy structures are analyzed. Analyses are carried out to establish the effect of critical displacement jump, mixed mode coupling parameter employed in interface laws, interface waviness and the effect of interface laws. The response of GRP composite during impact is characterized in terms of the free surface velocity, delamination event vs, time and interface normal and shear stresses, The interface normal and shear stresses are obtained directly from the interface cohesive laws, as well as, by extrapolating continuum stresses From integration points of the neighboring triangular elements to the interface.:It is shown that the finite element model predicts the response of the material in confirmation with the available experimental results. The wave dispersion and scattering effects are obtained in the form of attenuation of shock stress and free surface velocity. The model predicts partial delamination during compressive shock loading above a certain threshold due to local shear and mode coupling.",10.1016/S0045-7825(99)00222-4,https://dx.doi.org/10.1016/S0045-7825(99)00222-4
"Massmann, C | Holzmann, H",2012,Analysis of the behavior of a rainfall-runoff model using three global sensitivity analysis methods evaluated at different temporal scales,Applications_JOURNAL OF HYDROLOGY,29,4.83,"The effect of  parameters on the discharge of a conceptual rainfall-runoff model was analyzed for a small Austrian catchment. The sensitivities were computed using three methods: Sobol's indices, the mutual entropy and regional sensitivity analysis (RSA). The calculations were carried out for different temporal scales of evaluation ranging from daily to a multiannual period. A comparison of the methods shows that the mutual entropy and the RSA methods give more robust results than Sobol's method, which shows a higher variability in the sensitivities when they are calculated using different data sets. While all sensitivity methods are suitable for identifying the most sensitive parameters of a model, there are increasing differences in the results when the parameters become less important and also when shorter temporal scales are considered. A correlation analysis further indicated that the periods in which the parameter sensitivity rankings did not agree between the different methods are characterized by a higher impact of the parameters interactions on the modeled discharge. An analysis of the parameter sensitivity across the scales showed that the number of important parameter decreases when longer evaluation periods are considered. For instance, it was observed that all parameters were important at least during  day a daily scale, while at a yearly scale only the parameters characterizing the soil storage and the recession constants for interflow and percolation had high sensitivities. With respect to the impact of the interactions between parameters on the model results, it was observed that the largest effect is related to the parameters describing the size of the soil storage, the interflow and the percolation flow recession constants. Further, it was observed that there is a positive correlation between the importance of the interactions and the measured discharge. While the study focuses on quantitative sensitivity measures, it is also highlighted that graphical RSA analyses provide additional information regarding the parameter ranges associated with different discharge levels. This information can be used for increasing the understanding of the mechanisms by which the parameters affect the model results.",10.1016/j.jhydrol.2012.09.026,https://dx.doi.org/10.1016/j.jhydrol.2012.09.026
"Xiao, NC | Huang, HZ | Wang, ZL | Liu, Y | Zhang, XL",2012,Unified uncertainty analysis by the mean value first order saddlepoint approximation,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,29,4.83,"Uncertainties exist in products or systems widely. In general, uncertainties are classified as epistemic uncertainty or aleatory uncertainty. This paper proposes a unified uncertainty analysis (UUA) method based on the mean value first order saddlepoint approximation (MVFOSPA), denoted as MVFOSPA-UUA, to estimate the systems probabilities of failure considering both epistemic and aleatory uncertainties simultaneously. In this method, the input parameters with epistemic uncertainty are modeled using interval variables while input parameters with aleatory uncertainty are modeled using probability distribution or random variables. In order to calculate the lower and upper bounds of system probabilities of failure, both the best case and the worst case scenarios of the system performance function need to be considered, and the proposed MVFOSPA-UUA method can handle these two cases easily. The proposed method is demonstrated to be more efficient, robust and in some situations more accurate than the existing methods such as uncertainty analysis based on the first order reliability method. The proposed method is demonstrated using several examples.",10.1007/s00158-012-0794-4,https://dx.doi.org/10.1007/s00158-012-0794-4
"Firl, M | Wuchner, R | Bletzinger, KU",2013,Regularization of shape optimization problems using FE-based parametrization,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,24,4.8,"This paper introduces a general fully stabilized mesh based shape optimization strategy, which allows for shape optimization of mechanical problems on FE-based parametrization. The well-known mesh dependent results are avoided by application of filter methods and mesh regularization strategies. Filter methods are successfully applied to SIMP (Solid Isotropic Material with Penalization) based topology optimization for many years. The filter method presented here uses a specific formulation that is based on convolution integrals. It is shown that the filter methods ensure mesh independency of the optimal designs. Furthermore they provide an easy and robust tool to explore the whole design space with respect to optimal designs with similar mechanical properties. A successful application of optimization strategies with FE-based parametrization requires the combination of filter methods with mesh regularization strategies. The latter ones ensure reliable results of the finite element solutions that are crucial for the sensitivity analysis. This presentation introduces a new mesh regularization strategy that is based on the Updated Reference Strategy (URS). It is shown that the methods formulated on this mechanical basis result in fast and robust mesh regularization methods. The resulting grids show a minimum mesh distortion even for large movements of the mesh boundary. The performance of the proposed regularization methods is demonstrated by several illustrative examples.",10.1007/s00158-012-0843-z,https://dx.doi.org/10.1007/s00158-012-0843-z
"Keaveny, EE | Maxey, MR",2008,Modeling the magnetic interactions between paramagnetic beads in magnetorheological fluids,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,48,4.8,"In this study, we develop and compare new and existing methods for computing the magnetic interactions between paramagnetic particles in magneto rheological (MR) fluids. The commonly employed point-dipole methods are outlined and the inter-particle magnetic forces, given by these representations, are compared with exact values. An alternative finite-dipole model, where the magnetization of a particle is represented as a distribution of current density, is described and the associated computational effort is shown to scale as O(N). As the dipole moments and forces given by this model depend on the length scale of the current distribution, a sensitivity analysis is performed to reveal a proper choice of this length scale. While the dipole models give a good estimation of the far-field interactions, as two particles come into contact, higher order multipoles are needed to properly resolve their interaction. We present the exact two-body calculation and describe a procedure to include the higher multipoles arising in a pairwise interaction into a dipole model. This inclusion procedure can be integrated with any dipole or higher-multipole calculation. Results from relevant three-body problems are compared to exact solutions to provide information as to how well the inclusion procedure performs in simulations of self-assembly and estimating the yield strength of structures.",10.1016/j.jcp.2008.07.008,https://dx.doi.org/10.1016/j.jcp.2008.07.008
"Zhu, GF | Su, YH | Li, X | Zhang, K | Li, CB",2013,Estimating actual evapotranspiration from an alpine grassland on Qinghai-Tibetan plateau using a two-source model and parameter uncertainty analysis by Bayesian approach,Applications_JOURNAL OF HYDROLOGY,24,4.8,"A Bayesian method was used to fit the Shuttleworth-Wallace model to half-hourly measurements of evapotranspiration (ET) with the eddy covariance technique from an alpine grassland on the Qinghai-Tibetan plateau during the main growing season in , and probabilistically estimated its parameters and predication uncertainties using both dataset-by-dataset and multi-data procedures. This enabled us to reveal the seasonal variations of some physiology-related parameters and the impacts of constant parameters on the performances of the model. Results indicated that the S-W model using the posterior mean parameter values obtained by different procedures all successfully reproduced the observed responses in ET. However, the seasonal variations in the canopy conductance parameter (g(max)) should be counted in long-term ET estimating. From simulated results, the daily mean partitioning [i.e. the ratio of the estimated daily soil evaporation (E) over total evapotranspiration (ET); E/ET] was relative low (.-. with a mean of .) for the alpine grassland when leaf area index (LAI) was more than  m() m(-), and was closed related to LAI and vegetation condition. At the diurnal timescale, the canopy conductance was the main factor control the partitioning of ET.",10.1016/j.jhydrol.2012.10.006,https://dx.doi.org/10.1016/j.jhydrol.2012.10.006
"Srivastava, PK | Han, DW | Rico-Ramirez, MA | O'Neill, P | Islam, T | Gupta, M",2014,Assessment of SMOS soil moisture retrieval parameters using tau-omega algorithms for soil moisture deficit estimation,Applications_JOURNAL OF HYDROLOGY,19,4.75,"Soil Moisture and Ocean Salinity (SMOS) is the latest mission which provides flow of coarse resolution soil moisture data for land applications. However, the efficient retrieval of soil moisture for hydrological applications depends on optimally choosing the soil and vegetation parameters. The first stage of this work involves the evaluation of SMOS Level  products and then several approaches for soil moisture retrieval from SMOS brightness temperature are performed to estimate Soil Moisture Deficit (SMD). The most widely applied algorithm i.e. Single channel algorithm (SCA), based on tau-omega is used in this study for the soil moisture retrieval. In tau-omega, the soil moisture is retrieved using the Horizontal (H) polarisation following Hallikainen dielectric model, roughness parameters, Fresnel's equation and estimated Vegetation Optical Depth (tau). The roughness parameters are empirically calibrated using the numerical optimization techniques. Further to explore the improvement in retrieval models, modifications have been incorporated in the algorithms with respect to the sources of the parameters, which include effective temperatures derived from the European Center for Medium-Range Weather Forecasts (ECMWF) downscaled using the Weather Research and Forecasting (WRF)-NOAH Land Surface Model and Moderate Resolution Imaging Spectroradiometer (MODIS) land surface temperature (LST) while the tau is derived from MODIS Leaf Area Index (LAI). All the evaluations are performed against SMD, which is estimated using the Probability Distributed Model following a careful calibration and validation integrated with sensitivity and uncertainty analysis. The performance obtained after all those changes indicate that SCA-H using WRF-NOAH LSM downscaled ECMWF LST produces an improved performance for SMD estimation at a catchment scale.",10.1016/j.jhydrol.2014.07.056,https://dx.doi.org/10.1016/j.jhydrol.2014.07.056
"Du, JB | Olhoff, N",2010,Topological design of vibrating structures with respect to optimum sound pressure characteristics in a surrounding acoustic medium,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,38,4.75,"This paper deals with topological design optimization of vibrating bi-material elastic structures placed in an acoustic medium. The structural vibrations are excited by a time-harmonic external mechanical surface loading with prescribed excitation frequency, amplitude and spatial distribution. The design objective is minimization of the sound pressure generated by the vibrating structures on a prescribed reference plane or surface in the acoustic medium. The design variables are the volumetric densities of material in the admissible design domain for the structure. A high frequency boundary integral equation is employed to calculate the sound pressure in the acoustic field. This way the acoustic analysis and the corresponding sensitivity analysis can be carried out in a very efficient manner. The structural damping is considered as Rayleigh damping. Penalization models with respect to the acoustic transformation matrix and/or the damping matrix are proposed in order to eliminate intermediate material volume densities, which have been found to appear obstinately in some of the high frequency designs. The influences of the excitation frequency and the structural damping on optimum topologies are investigated by numerical examples. Also, the problem of maximizing (rather than minimizing) sound pressures in points on a reference plane in the acoustic medium is treated. Many interesting features of the examples are revealed and discussed.",10.1007/s00158-009-0477-y,https://dx.doi.org/10.1007/s00158-009-0477-y
"Shimoda, M | Liu, Y",2014,A non-parametric free-form optimization method for shell structures,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,19,4.75,"This paper presents a numerical shape optimization method for the optimum free-form design of shell structures. It is assumed that the shell is varied in the out-of-plane direction to the surface to determine the optimal free-form. A compliance minimization problem subject to a volume constraint is treated here as an example of free-form design problem of shell structures. This problem is formulated as a distributed-parameter, or non-parametric, shape optimization problem. The shape gradient function and the optimality conditions are theoretically derived using the material derivative formulae, the Lagrange multiplier method and the adjoint variable method. The negative shape gradient function is applied to the shell surface as a fictitious distributed traction force to vary the shell. Mathematically, this method is a gradient method with a Laplacian smoother in the Hilbert space. Therefore, this shape variation makes it possible both to reduce the objective functional and to maintain the mesh regularity simultaneously. With this method, the optimal smooth curvature distribution of a shell structure can be determined without shape parameterization. The calculated results show the effectiveness of the proposed method for the optimum free-form design of shell structures.",10.1007/s00158-014-1059-1,https://dx.doi.org/10.1007/s00158-014-1059-1
"van Dijk, NP | Langelaar, M | van Keulen, F",2012,Explicit level-set-based topology optimization using an exact Heaviside function and consistent sensitivity analysis,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,28,4.67,"This paper presents a level-set-based topology optimization method based on numerically consistent sensitivity analysis. The proposed method uses a direct steepest-descent update of the design variables in a level-set method; the level-set nodal values. An exact Heaviside formulation is used to relate the level-set function to element densities. The level-set function is not required to be a signed-distance function, and reinitialization is not necessary. Using this approach, level-set-based topology optimization problems can be solved consistently and multiple constraints treated simultaneously. The proposed method leads to more insight in the nature of level-set-based topology optimization problems. The level-set-based design parametrization can describe gray areas and numerical hinges. Consistency causes results to contain these numerical artifacts. We demonstrate that alternative parameterizations, level-set-based or density-based regularization can be used to avoid artifacts in the final results. The effectiveness of the proposed method is demonstrated using several benchmark problems. The capability to treat multiple constraints shows the potential of the method. Furthermore, due to the consistency, the optimizer can run into local minima; a fundamental difficulty of level-set-based topology optimization. More advanced optimization strategies and more efficient optimizers may increase the performance in the future.",10.1002/nme.4258,https://dx.doi.org/10.1002/nme.4258
"Dai, H | Ye, M",2015,Variance-based global sensitivity analysis for multiple scenarios and models with implementation using sparse grid collocation,Applications_JOURNAL OF HYDROLOGY,14,4.67,"Sensitivity analysis is a vital tool in hydrological modeling to identify influential parameters for inverse modeling and uncertainty analysis, and variance-based global sensitivity analysis has gained popularity. However, the conventional global sensitivity indices are defined with consideration of only parametric uncertainty. Based on a hierarchical structure of parameter, model, and scenario uncertainties and on recently developed techniques of model- and scenario-averaging, this study derives new global sensitivity indices for multiple models and multiple scenarios. To reduce computational cost of variance-based global sensitivity analysis, sparse grid collocation method is used to evaluate the mean and variance terms involved in the variance-based global sensitivity analysis. In a simple synthetic case of groundwater flow and reactive transport, it is demonstrated that the global sensitivity indices vary substantially between the four models and three scenarios. Not considering the model and scenario uncertainties, might result in biased identification of important model parameters. This problem is resolved by using the new indices defined for multiple models and/or multiple scenarios. This is particularly true when the sensitivity indices and model/scenario probabilities vary substantially. The sparse grid collocation method dramatically reduces the computational cost, in comparison with the popular quasi-random sampling method. The new framework of global sensitivity analysis is mathematically general, and can be applied to a wide range of hydrologic and environmental problems.",10.1016/j.jhydrol.2015.06.034,https://dx.doi.org/10.1016/j.jhydrol.2015.06.034
"Herckenrath, D | Doherty, J | Panday, S",2015,Incorporating the effect of gas in modelling the impact of CBM extraction on regional groundwater systems,Applications_JOURNAL OF HYDROLOGY,14,4.67,"Production of Coalbed Methane (CBM) requires extraction of large quantities of groundwater. To date, standard groundwater flow simulators have mostly been used to assess the impact of this extraction on regional groundwater systems. Recent research has demonstrated that predictions of regional impact assessment made by such models may be seriously compromised unless account is taken of the presence of a gas phase near extraction wells. At the same time, CBM impact assessment must accommodate the traditional requirements of regional groundwater modelling. These include representation of surficial groundwater processes and up-scaled rock properties as well as the need for calibration and predictive uncertainty quantification. The study documented herein () quantifies errors in regional drawdown predictions incurred through neglect of the presence of a gas phase near CBM extraction centres, and () evaluates the extent to which these errors can be mitigated by simulating near-well desaturation using a modified Richards equation formulation within a standard groundwater flow simulator. Two synthetic examples are provided to quantify the impact of the gas phase and verify the proposed modelling approach (implemented in MODFLOW-USG) against rigorous multiphase flow simulations (undertaken using ECLIPSE*). ECLIPSE simulations demonstrate convergence towards a time-asymptotic relationship between water saturation and pressure. This relationship can be approximated using a slightly modified van Genuchten function. Where this function is employed in combination with the modified Richards equation strategy to accommodate near-well desaturation, errors in predicted drawdown are reduced significantly, including in cases where complexities such as sloping coal layers are introduced to the model domain (the latter promoting buoyancy-driven movement of gas). Sensitivity analyses further indicate that only the general properties of the employed desaturation function need to be respected to significantly reduce errors in regional drawdown predictions that would arise if the presence of the near-well gas phase was ignored. These properties can be inferred from reservoir properties and from the outcomes of reservoir model simulations that are available at local CBM operation sites.",10.1016/j.jhydrol.2015.02.012,https://dx.doi.org/10.1016/j.jhydrol.2015.02.012
"Ridler, ME | Sandholt, I | Butts, M | Lerer, S | Mougin, E | Timouk, F | Kergoat, L | Madsen, H",2012,Calibrating a soil-vegetation-atmosphere transfer model with remote sensing estimates of surface temperature and soil surface moisture in a semi arid environment,Applications_JOURNAL OF HYDROLOGY,28,4.67,"A series of numerical experiments has been designed to investigate how effective satellite estimates of radiometric surface temperatures and soil surface moisture are for calibrating a Soil-Vegetation-Atmosphere Transfer (SVAT) model. Multi-objective calibration based on error minimization of temperature and soil moisture model outputs is performed in a semi-arid environment. Model accuracy when calibrated using in situ versus satellite objectives is explored in detail. Observational meteorological datasets from the African Monsoon Multidisciplinary Analysis (AMMA) were used to force a column model during a growing season in Mali. Fourier Amplitude Sensitivity Test (FAST) revealed the most sensitive parameters to model outputs. Parameters found sensitive were subsequently optimized in a series of model calibrations to reveal trade-offs between model objectives. Our main findings are () the SVAT model performs well in the semi-arid environment, but underestimates peak growing season evapotranspiration and overestimates soil moisture, () most of the parameters important for flux estimates can be constrained using surface temperature and soil surface moisture with the three exceptions: root depth, the extinction coefficient and unstressed stomatal resistance, () flux simulations are improved when the model is calibrated using in situ surface temperature and soil surface moisture versus satellite estimates.",10.1016/j.jhydrol.2012.01.047,https://dx.doi.org/10.1016/j.jhydrol.2012.01.047
"Cacuci, DG",2015,Second-order adjoint sensitivity analysis methodology (2nd-ASAM) for computing exactly and efficiently first- and second-order sensitivities in large-scale linear systems: II. Illustrative application to a paradigm particle diffusion problem,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,14,4.67,"This work presents an illustrative application of the second-order adjoint sensitivity analysis methodology(nd-ASAM) to a paradigm neutron diffusion problem, which is sufficiently simple to admit an exact solution, thereby making transparent the underlying mathematical derivations. The general theory underlying nd-ASAM indicates that, for a physical system comprising N-alpha parameters, the computation of all of the first-and second-order response sensitivities requires (per response) at most (N(alpha) + )""large-scale"" computations using the first-level and, respectively, second-level adjoint sensitivity systems(st-LASS and nd-LASS). Very importantly, however, the illustrative application presented in this work shows that the actual number of adjoint computations needed for computing all of the first-and second-order response sensitivities may be significantly less than (N(alpha) + ) per response. For this illustrative problem, four ""large-scale"" adjoint computations sufficed for the complete and exact computations of all  first-and  distinct second-order derivatives. Furthermore, the construction and solution of the nd-LASS requires very little additional effort beyond the construction of the adjoint sensitivity system needed for computing the first-order sensitivities. Very significantly, only the sources on the right-sides of the diffusion (differential) operator needed to be modified; the left-side of the differential equations (and hence the ""solver"" in large-scale practical applications) remained unchanged. All of the first-order relative response sensitivities to the model parameters have significantly large values, of order unity. Also importantly, most of the second-order relative sensitivities are just as large, and some even up to twice as large as the first-order sensitivities. In the illustrative example presented in this work, the second-order sensitivities contribute little to the response variances and covariances. However, they have the following major impacts on the computed moments of the response distribution: (a) they cause the ""expected value of the response"" to differ from the ""computed nominal value of the response""; and (b) they contribute decisively to causing asymmetries in the response distribution. Indeed, neglecting the second-order sensitivities would nullify the third-order response correlations, and hence would nullify the skewness of the response. Consequently, any events occurring in a response's long and/or short tails, which are characteristic of rare but decisive events (e. g., major accidents, catastrophes), would likely be missed. The nd-ASAM is expected to affect significantly other fields that need efficiently computed order response sensitivities, e.g., optimization, data assimilation/adjustment, model calibration, and predictive modeling.",10.1016/j.jcp.2014.11.030,https://dx.doi.org/10.1016/j.jcp.2014.11.030
"Hupet, F | Vanclooster, M",2001,Effect of the sampling frequency of meteorological variables on the estimation of the reference evapotranspiration,Applications_JOURNAL OF HYDROLOGY,79,4.65,"In this paper, we quantify the effect of the temporal sampling frequency of commonly measured climatic variables on the estimation of the reference evapotranspiration. Using a set of data sampled on an intensive basis (i.e. one measurement each minute) during a period of  months, we first analyse the effect of the temporal sampling frequency on the estimation of the daily means of the shortwave solar radiation, the wind speed, the dry and wet temperatures, and on the estimation of the daily maximum and minimum dry temperature. Subsequently, a sensitivity analysis of a reference evapotranspiration model is carried out to determine the most sensible meteorological variables. The sensitivity coefficients were then combined with the errors due to the temporal sampling to quantify for each variable the impact of the sampling frequency on the estimation of daily ETo. The results showed that the solar radiation and the wind speed are the most sensitive to bias induced by inadequate temporal sampling frequency. Daily errors of . MJ m(-) d(-) or .% for the solar radiation, and . m s(-) or % for the wind speed may be obtained if these variables are inappropriately sampled. Moreover, the impact of inappropriate temporal sampling on the estimation of ETo can be significant with respective maximum bias of . mm d(-) due to inappropriate solar radiation sampling and . mm d(-) due to inappropriate maximum temperature sampling. A non-intensive hourly temporal sampling schedule of all meteorological variables may induce errors on the daily ETo so high as -. mm d(-) or -%. Fortunately, the errors generated on the estimation of the long-term integrated evapotranspiration are clearly lower (.%). Our study clearly demonstrates the importance of scheduling appropriately the sampling frequency of climatic variables to correctly estimate land surfaces fluxes as well in fundamental as in more practically oriented research studies.",10.1016/S0022-1694(00)00413-3,https://dx.doi.org/10.1016/S0022-1694(00)00413-3
"Poon, NMK | Martins, JRRA",2007,An adaptive approach to constraint aggregation using adjoint sensitivity analysis,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,50,4.55,"Constraint aggregation is the key for efficient structural optimization when using a gradient-based optimizer and an adjoint method for sensitivity analysis. We explore different methods of constraint aggregation for numerical optimization. We analyze existing approaches, such as considering all constraints individually, taking the maximum of the constraints and using the Kreisselmeier-Steinhauser (KS) function. A new adaptive approach based on the KS function is proposed that updates the aggregation parameter by taking into account the constraint sensitivity. This adaptive approach is shown to significantly increase the accuracy of the results without additional computational cost especially when a large number of constraints are active at the optimum. The characteristics of each aggregation method and the performance of the proposed adaptive approach are shown by solving a wing structure weight minimization problem.",10.1007/s00158-006-0061-7,https://dx.doi.org/10.1007/s00158-006-0061-7
"Novotny, AA | Feijoo, RA | TaroCo, E | Padra, C",2007,Topological sensitivity analysis for three-dimensional linear elasticity problem,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,50,4.55,"In this work we use the Topological-Shape Sensitivity Method to obtain the topological derivative for three-dimensional linear elasticity problems, adopting the total potential energy as cost function and the equilibrium equation as constraint. This method, based on classical shape sensitivity analysis, leads to a systematic procedure to calculate the topological derivative. In particular, firstly we present the mechanical model, later we perform the shape derivative of the corresponding cost function and, finally, we calculate the final expression for the topological derivative using the Topological-Shape Sensitivity Method and results from classical asymptotic analysis around spherical cavities. In order to point out the applicability of the topological derivative in the context of topology optimization problems, we use this information as a descent direction to solve a three-dimensional topology design problem. Furthermore, through this example we also show that the topological derivative together with an appropriate mesh refinement strategy are able to capture high quality shapes even using a very simple topology algorithm.",10.1016/j.cma.2007.05.006,https://dx.doi.org/10.1016/j.cma.2007.05.006
"Long, XY | Jiang, C | Yang, C | Han, X | Gao, W | Liu, J",2016,A stochastic scaled boundary finite element method,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,9,4.5,"By extending the existing scaled boundary finite element method (SBFEM) into the random field problem, a stochastic scaled boundary finite element method (SSBFEM) is developed in this paper to predict the structural responses with randomly distributed material properties. The analyzed domain is discretized into a number of polygons modeled as sub-domains of the SBFEM so that the non-homogeneous material properties are possible to be considered in establishment of the constitutive equation. The random field represented by the Karhunen-Loeve (KL) expansion is then directly incorporated into both the equilibrium equation of the SBFEM and its sensitivity analysis formulation. The perturbation method is employed to calculate the statistical moments of the structural responses and a semi-analytical sensitivity analysis method is used to efficiently calculate the gradients involved. Particularly, the proposed method is successfully applied to the stochastic fracture analysis with spatially varying random material properties. Four numerical examples are investigated to demonstrate the validity of the proposed method.",10.1016/j.cma.2016.04.037,https://dx.doi.org/10.1016/j.cma.2016.04.037
"Bonnet, M | Guzina, BB",2004,Sounding of finite solid bodies by way of topological derivative,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,63,4.5,"This paper is concerned with an application of the concept of topological derivative to elastic-wave imaging of finite solid bodies containing cavities. Building on the approach originally proposed in the (elastostatic) theory of shape optimization, the topological derivative, which quantifies the sensitivity of a featured cost functional due to the creation of an infinitesimal hole in the cavity-free (reference) body, is used as a void indicator through an assembly of sampling points where it attains negative values. The computation of topological derivative is shown to involve an elastodynamic solution to a set of supplementary boundary-value problems for the reference body, which are here formulated as boundary integral equations. For a comprehensive treatment of the subject, formulas for topological sensitivity are obtained using three alternative methodologies, namely (i) direct differentiation approach, (ii) adjoint field method, and (iii) limiting form of the shape sensitivity analysis. The competing techniques are further shown to lead to distinct computational procedures. Methodologies (i) and (ii) are implemented within a BEM-based platform and validated against an analytical solution. A set of numerical results is included to illustrate the utility of topological derivative for D elastic-wave sounding of solid bodies; an approach that may perform best when used as a pre-conditioning tool for more accurate, gradient-based imaging algorithms. Despite the fact that the formulation and results presented in this investigation are established on the basis of a boundary integral solution, the proposed methodology is readily applicable to other computational platforms such as the finite element and finite difference techniques.",10.1002/nme.1153,https://dx.doi.org/10.1002/nme.1153
"Zhang, XP | Kang, Z | Li, M",2014,Topology optimization of electrode coverage of piezoelectric thin-walled structures with CGVF control for minimizing sound radiation,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,18,4.5,"It is impractical to implement arbitrary-shaped piezoelectric patches from the view point of manufacturability of fragile piezoelectric ceramics, thus using designable electrode layers to deliver desired actuation forces provides a more realistic option in engineering applications. This study develops a topological design method of surface electrode distribution over piezoelectric sensors/actuators attached to a thin-walled shell structure for reducing the sound radiation in an unbounded acoustic domain. In the optimization model, the sound pressure norm at specific reference points under excitations at a certain excitation frequency or in a given frequency range is taken as the objective function. The pseudo densities for indicating absence and presence of surface electrodes at each element are taken as the design variables, and a penalized relationship between the densities and the active damping effect is employed. The vibrating structure is discretized with finite element model for the frequency response analysis and the sound radiation analysis in the unbounded acoustic domain is treated by boundary element method. The applied voltage on each actuator is determined by the constant gain velocity feedback (CGVF) control law. The technique of the complex mode superposition in the state space, in conjunction with a model reduction transformation, is adopted in the response analysis of the system characterized by a non-proportional active damping property. In this context, the adjoint-variable sensitivity analysis scheme is derived. The effectiveness and efficiency of the proposed method are demonstrated by numerical examples, and several key factors on the optimal designs are also discussed.",10.1007/s00158-014-1082-2,https://dx.doi.org/10.1007/s00158-014-1082-2
"Kreissl, S | Pingen, G | Evgrafov, A | Maute, K",2010,Topology optimization of flexible micro-fluidic devices,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,36,4.5,"A multi-objective topology optimization formulation for the design of dynamically tunable fluidic devices is presented. The flow is manipulated via external and internal mechanical actuation, leading to elastic deformations of flow channels. The design objectives characterize the performance in the undeformed and deformed configurations. The layout of fluid channels is determined by material topology optimization. In addition, the thickness distribution, the distribution of active material for internal actuation, and the support conditions are optimized. The coupled fluid-structure response is predicted by a non-linear finite element model and a hydrodynamic lattice Boltzmann method. Focusing on applications with low flow velocities and pressures, structural deformations due to fluidforces are neglected. A mapping scheme is presented that couples the material distributions in the structural and fluid mesh. The governing and the adjoint equations of the resulting fluid-structure interaction problem are derived. The proposed method is illustrated with the design of tunable manifolds.",10.1007/s00158-010-0526-6,https://dx.doi.org/10.1007/s00158-010-0526-6
"Rajabi, MM | Ataie-Ashtiani, B",2016,Efficient fuzzy Bayesian inference algorithms for incorporating expert knowledge in parameter estimation,Applications_JOURNAL OF HYDROLOGY,9,4.5,"Bayesian inference has traditionally been conceived as the proper framework for the formal incorporation of expert knowledge in parameter estimation of groundwater models. However, conventional Bayesian inference is incapable of taking into account the imprecision essentially embedded in expert provided information. In order to solve this problem, a number of extensions to conventional Bayesian inference have been introduced in recent years. One of these extensions is 'fuzzy Bayesian inference' which is the result of integrating fuzzy techniques into Bayesian statistics. Fuzzy Bayesian inference has a number of desirable features which makes it an attractive approach for incorporating expert knowledge in the parameter estimation process of groundwater models: () it is well adapted to the nature of expert provided information, () it allows to distinguishably model both uncertainty and imprecision, and () it presents a framework for fusing expert provided information regarding the various inputs of the Bayesian inference algorithm. However an important obstacle in employing fuzzy Bayesian inference in groundwater numerical modeling applications is the computational burden, as the required number of numerical model simulations often becomes extremely exhaustive and often computationally infeasible. In this paper, a novel approach of accelerating the fuzzy Bayesian inference algorithm is proposed which is based on using approximate posterior distributions derived from surrogate modeling, as a screening tool in the computations. The proposed approach is first applied to a synthetic test case of seawater intrusion (SWI) in a coastal aquifer. It is shown that for this synthetic test case, the proposed approach decreases the number of required numerical simulations by an order of magnitude. Then the proposed approach is applied to a real-world test case involving three-dimensional numerical modeling of SWI in Kish Island, located in the Persian Gulf. An expert elicitation methodology is developed and applied to the real-world test case in order to provide a road map for the use of fuzzy Bayesian inference in groundwater modeling applications.",10.1016/j.jhydrol.2016.02.029,https://dx.doi.org/10.1016/j.jhydrol.2016.02.029
"Sun, SA | Fu, GT | Djordjevic, S | Khu, ST",2012,Separating aleatory and epistemic uncertainties: Probabilistic sewer flooding evaluation using probability box,Applications_JOURNAL OF HYDROLOGY,27,4.5,"Uncertainty is generally present in flood evaluation and can be divided into aleatory and epistemic categories. It is not uncommon that flood evaluation has to consider both aleatory and epistemic uncertainties when a simulation model is used. This paper presents a probability box methodology that enables various representations of uncertainty to be simultaneously propagated through a model while separation of aleatory and epistemic uncertainties is preserved in the model output. The proposed methodology is applied to a sewer flood evaluation problem, in which rainfall variables are characterized by probability boxes and two model parameters are respectively described by fuzzy sets and random sets. Consequently, the probabilistic flood evaluation is expressed by probability boxes. Simulation results demonstrate the critical importance of separating aleatory and epistemic uncertainties and of maintaining the uncertainty type (either aleatory or epistemic) in uncertainty propagation. It is suggested that the pooling of aleatory and epistemic uncertainties may lead to incoherent information in the model output.",10.1016/j.jhydrol.2011.12.027,https://dx.doi.org/10.1016/j.jhydrol.2011.12.027
"Tang, KK | Congedo, PM | Abgrall, R",2015,Sensitivity analysis using anchored ANOVA expansion and high-order moments computation,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,13,4.33,"An anchored analysis of variance (ANOVA) method is proposed in this paper to decompose the statistical moments. Compared to the standard ANOVA with mutually orthogonal component functions, the anchored ANOVA, with an arbitrary choice of the anchor point, loses the orthogonality if employing the same measure. However, an advantage of the anchored ANOVA consists in the considerably reduced number of deterministic solver's computations, which renders the uncertainty quantification of real engineering problems much easier. Different from existing methods, the covariance decomposition of the output variance is used in this work to take account of the interactions between non-orthogonal components, yielding an exact variance expansion and thus, with a suitable numerical integration method, provides a strategy that converges. This convergence is verified by studying academic tests. In particular, the sensitivity problem of existing methods to the choice of anchor point is analyzed via the Ishigami case, and we point out that covariance decomposition survives from this issue. Also, with a truncated anchored ANOVA expansion, numerical results prove that the proposed approach is less sensitive to the anchor point. The covariance-based sensitivity indices (SI) are also used, compared to the variance-based SI. Furthermore, we emphasize that the covariance decomposition can be generalized in a straightforward way to decompose higher-order moments. For academic problems, results show the method converges to exact solution regarding both the skewness and kurtosis. Finally, the proposed method is applied on a realistic case, that is, estimating the chemical reactions uncertainties in a hypersonic flow around a space vehicle during an atmospheric reentry.",10.1002/nme.4856,https://dx.doi.org/10.1002/nme.4856
"Fan, YR | Huang, WW | Li, YP | Huang, GH | Huang, K",2015,A coupled ensemble filtering and probabilistic collocation approach for uncertainty quantification of hydrological models,Applications_JOURNAL OF HYDROLOGY,13,4.33,"In this study, a coupled ensemble filtering and probabilistic collocation (EFPC) approach is proposed for uncertainty quantification of hydrologic models. This approach combines the capabilities of the ensemble Kalman filter (EnKF) and the probabilistic collocation method (PCM) to provide a better treatment of uncertainties in hydrologic models. The EnKF method would be employed to approximate the posterior probabilities of model parameters and improve the forecasting accuracy based on the observed measurements; the PCM approach is proposed to construct a model response surface in terms of the posterior probabilities of model parameters to reveal uncertainty propagation from model parameters to model outputs. The proposed method is applied to the Xiangxi River, located in the Three Gorges Reservoir area of China. The results indicate that the proposed EFPC approach can effectively quantify the uncertainty of hydrologic models. Even for a simple conceptual hydrological model, the efficiency of EFPC approach is about  times faster than traditional Monte Carlo method without obvious decrease in prediction accuracy. Finally, the results can explicitly reveal the contributions of model parameters to the total variance of model predictions during the simulation period.",10.1016/j.jhydrol.2015.09.035,https://dx.doi.org/10.1016/j.jhydrol.2015.09.035
"Vandenbohede, A | Hermans, T | Nguyen, F | Lebbe, L",2011,Shallow heat injection and storage experiment: Heat transport simulation and sensitivity analysis,Applications_JOURNAL OF HYDROLOGY,30,4.29,"Interest in heat transport in porous media has increased because of its many applications such use as tracer or in geotechnical engineering solutions. Understanding of the physical processes and parameters determining heat transport is therefore important. In this paper, heat transport is studied during a shallow heat injection and storage field test. The test is simulated using SEAWAT. Sensitivity analyses and collinear diagnostics are used to derive which parameters can be derived from the test and how reliable these values are. Heat transport during the test is compared with heat transport in the surficial zone at the same field site to compare parameter values. The most sensitive parameter is the thermal conductivity of the solid followed by the porosity, heat capacity of the solid and the longitudinal dispersivity. This indicates the predominance of conductive transport during the storage phase over the convective transport during the injection phase. Whereas heat transport in the surficial zone is insensitive to the longitudinal dispersivity, this parameter must be included to simulate the field test. This indicates that dispersivity cannot be ignored simulating convective heat transport in aquifers.",10.1016/j.jhydrol.2011.08.024,https://dx.doi.org/10.1016/j.jhydrol.2011.08.024
"Guillas, S | Glover, N | Malki-Epshtein, L",2014,Bayesian calibration of the constants of the k-epsilon turbulence model for a CFD model of street canyon flow,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,17,4.25,"In this paper we carry out a Bayesian calibration for uncertainty analysis in Computational Fluid Dynamics modelling of urban flows. Taking the case of airflow in a regular street canyon, and choosing turbulent kinetic energy (TKE) as our quantity of interest, we calibrate -D CFD simulations against wind tunnel observations. We focus our calibration on the model constants contained within the standard RANS k-epsilon turbulence model and the uncertainties relating to these values. Thus we are able to narrow down the space of k-epsilon model constants which provide the best match with experimental data and quantify the uncertainty relating to both the k-epsilon model constants in the case of street canyon flow and the TKE outputs of the CFD simulation. Furthermore, we are able to construct a statistical emulator of the CFD model. Finally, we provide predictions of TKE based on the emulator and the estimated bias between model and observations, accompanied with uncertainties in these predictions. (C)  The Authors.",10.1016/j.cma.2014.06.008,https://dx.doi.org/10.1016/j.cma.2014.06.008
"Dahl, J | Jensen, JS | Sigmund, O",2008,Topology optimization for transient wave propagation problems in one dimension Design of filters and pulse modulators,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,42,4.2,"Structures exhibiting band gap properties, i.e., having frequency ranges for which the structure attenuates propagating waves, have applications in damping of acoustic and elastic wave propagation and in optical communication. A topology optimization method for synthesis of such structures, employing a time domain formulation, is developed. The method is extended to synthesis of pulse converting structures with possible applications in optical communication.",10.1007/s00158-007-0192-5,https://dx.doi.org/10.1007/s00158-007-0192-5
"Kang, Z | Bai, S",2013,On robust design optimization of truss structures with bounded uncertainties,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,21,4.2,"This paper investigates robust design optimization of truss structures with uncertain-but-bounded parameters and loads. The variations of the cross-sectional areas, Young's moduli and applied loads are treated with non-probabilistic ellipsoid convex models. A robustness index for quantitatively measuring the maximal allowable magnitude of system variations is presented, and the design problem is then formulated as to maximize the minimum of the robustness indices for all the concerned design requirements under a given material volume constraint. For circumventing the difficulty associated with the max-min type problem, an aggregate function technique is employed to construct a smooth objective function. The computational scheme for the sensitivity of the robustness index is derived on the basis of optimum sensitivity analysis. The optimization problem is then solved by using the GCMMA optimizer. Numerical examples illustrate the validity and effectiveness of the present formulation and solution techniques.",10.1007/s00158-012-0868-3,https://dx.doi.org/10.1007/s00158-012-0868-3
"Kirsch, U",2010,Reanalysis and sensitivity reanalysis by combined approximations,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,33,4.12,"One of the main obstacles in the solution of structural optimization problems is the need to repeat solutions of the analysis and sensitivity analysis equations. In large-scale structures, having complex analysis models, the computational effort may become prohibitive. To alleviate this difficulty a general approach for repeated analysis and repeated sensitivity analysis, called combined approximations, was developed during the last  years. The solution is based on the integration of several algorithms and methods. As a result, accurate results can be achieved efficiently. In previous studies, solution procedures for various particular problems were developed. This article summarizes the various formulations and solution procedures for reanalysis and sensitivity reanalysis of linear, nonlinear, static and dynamic systems. It is shown that the various solution procedures are based on applications of similar basic algorithms. Numerical examples demonstrate the efficiency of the calculations and the accuracy of the results.",10.1007/s00158-009-0369-1,https://dx.doi.org/10.1007/s00158-009-0369-1
"Joerin, C | Beven, KJ | Iorgulescu, I | Musy, A",2002,Uncertainty in hydrograph separations based on geochemical mixing models,Applications_JOURNAL OF HYDROLOGY,66,4.12,"A detailed uncertainty analysis of three-component mixing models based on the Haute-Mentue watershed (Switzerland) is presented. Two types of uncertainty are distinguished: the 'model uncertainty', which is affected by model assumptions, and the 'statistical uncertainty', which is due to temporal and spatial variability of chemical tracer concentrations of components. The statistical uncertainty is studied using a Monte Carlo procedure. The model uncertainty is investigated by the comparison of four different mixing models all based on the same tracers but considering for each component alternative hypotheses about their concentration and their spatio-temporal variability. This analysis indicates that despite the uncertainty, the flow sources, which generate the stream flow are clearly identified at the catchments scale by the application of the mixing model. However, the precision and the coherence of hydrograph separations can be improved by taking into account any available information about the temporal and spatial variability of component chemical concentrations.",10.1016/S0022-1694(01)00509-1,https://dx.doi.org/10.1016/S0022-1694(01)00509-1
"Freni, G | Mannina, G",2010,Uncertainty in water quality modelling The applicability of Variance Decomposition Approach,Applications_JOURNAL OF HYDROLOGY,33,4.12,Quantification of uncertainty is of paramount interest in integrated urban drainage water quality modelling Indeed the assessment of the reliability of the results of complex water quality models is crucial in understanding their significance However the state of knowledge regarding uncertainties in urban drainage models is poor In the case of integrated urban drainage water quality models due to the fact that integrated approaches are basically a cascade of sub-models (simulating the sewer system waste-water treatment plant and receiving water body) uncertainty produced in one sub-model propagates to the following ones in a manner dependent on the model structure the estimation of parameters and the availability and uncertainty of measurements in the different parts of the system Uncertainty basically propagates throughout a chain of models in which the simulation output from upstream models is transferred to the downstream ones as input The Variance Decomposition Approach tracks uncertainty propagation commonly assuming that the correlation among error sources is negligible In complex environmental models the overall uncertainty can differ significantly from the simple sum of uncertainties generated in each sub-model showing the well-known uncertainty accumulation problems due to non-linearity in the model and correlation among the sources of uncertainty This work discusses the importance of such issues in the application of a complex Integrated urban drainage model with the aim of evaluating the applicability of Variance Decomposition Approach The integrated model and the methodology for the uncertainty decomposition were then applied to a complex integrated catchment the Nocella basin (Italy) The results showed that the Variance Decomposition Approach can be a powerful tool for uncertainty analysis but a possible correlation among uncertainty sources should be considered because it can greatly affect the analysis (C)  Elsevier B V All rights reserved,10.1016/j.jhydrol.2010.09.006,https://dx.doi.org/10.1016/j.jhydrol.2010.09.006
"Stanford, B | Ifju, P",2009,Aeroelastic topology optimization of membrane structures for micro air vehicles,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,37,4.11,"This work considers the aeroelastic optimization of a membrane micro air vehicle wing through topology optimization. The low aspect ratio wing is discretized into panels: a two material formulation on the wetted surface is used, where each panel can be membrane (wing skin) or carbon fiber (laminate reinforcement). An analytical sensitivity analysis of the aeroelastic system is used for the gradient-based optimization of aerodynamic objective functions. An explicit penalty is added, as needed, to force the structure to a - distribution. The dependence of the solution upon initial design, angle of attack, mesh density, and objective function are presented. Deformation and pressure distributions along the wing are studied for various load-augmenting and load-alleviating designs (both baseline and optimized), in order to establish a link between stiffness distribution and aerodynamic performance of membrane micro air vehicle wings. The work concludes with an experimental validation of the superiority of select optimal designs.",10.1007/s00158-008-0292-x,https://dx.doi.org/10.1007/s00158-008-0292-x
"Yoon, GH | Kim, YY | Bendsoe, MP | Sigmund, O",2004,Hinge-free topology optimization with embedded translation-invariant differentiable wavelet shrinkage,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,57,4.07,"In topology optimization applications for the design of compliant mechanisms, the formation of hinges is typically encountered. Often such hinges are unphysical artifacts that appear due to the choice of discretization spaces for design and analysis. The objective of this work is to present a new method to find hinge-free designs using multiscale wavelet-based topology optimization formulation. The specific method developed in this work does not require refinement of the analysis model and it consists of a translation-invariant wavelet shrinkage method where a hinge-free condition is imposed in the multiscale design space. To imbed the shrinkage method implicitly in the optimization formulation and thus facilitate sensitivity analysis, the shrinkage method is made differentiable by means of differentiable versions of logical operators. The validity of the present method is confirmed by solving typical two-dimensional compliant mechanism design problems.",10.1007/s00158-004-0378-z,https://dx.doi.org/10.1007/s00158-004-0378-z
"Lu, D | Ye, M | Curtis, GP",2015,Maximum likelihood Bayesian model averaging and its predictive analysis for groundwater reactive transport models,Applications_JOURNAL OF HYDROLOGY,12,4.0,"While Bayesian model averaging (BMA) has been widely used in groundwater modeling, it is infrequently applied to groundwater reactive transport modeling because of multiple sources of uncertainty in the coupled hydrogeochemical processes and because of the long execution time of each model run. To resolve these problems, this study analyzed different levels of uncertainty in a hierarchical way, and used the maximum likelihood version of BMA, i.e., MLBMA, to improve the computational efficiency. This study demonstrates the applicability of MLBMA to groundwater reactive transport modeling in a synthetic case in which twenty-seven reactive transport models were designed to predict the reactive transport of hexavalent uranium (U(VI)) based on observations at a former uranium mill site near Naturita, CO. These reactive transport models contain three uncertain model components, i.e., parameterization of hydraulic conductivity, configuration of model boundary, and surface complexation reactions that simulate U(VI) adsorption. These uncertain model components were aggregated into the alternative models by integrating a hierarchical structure into MLBMA. The modeling results of the individual models and MLBMA were analyzed to investigate their predictive performance. The predictive logscore results show that MLBMA generally outperforms the best model, suggesting that using MLBMA is a sound strategy to achieve more robust model predictions relative to a single model. MLBMA works best when the alternative models are structurally distinct and have diverse model predictions. When correlation in model structure exists, two strategies were used to improve predictive performance by retaining structurally distinct models or assigning smaller prior model probabilities to correlated models. Since the synthetic models were designed using data from the Naturita site, the results of this study are expected to provide guidance for real-world modeling. Limitations of applying MLBMA to the synthetic study and future real-world modeling are discussed.",10.1016/j.jhydrol.2015.07.029,https://dx.doi.org/10.1016/j.jhydrol.2015.07.029
"Takalloozadeh, M | Yoon, GH",2017,Development of Pareto topology optimization considering thermal loads,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,4,4.0,"In this research, we developed a level-set based topology optimization with a topological derivative formulation considering thermal load. Thermo-elasticity equations were utilized to obtain the sensitivity of the objective function after inserting a small hole in the domain. Total strain energy and the maximum stress in the design domain were taken as the objective functions. After taking the thermal loading effect into account, the total strain energy density function became a nonhomogeneous function of the strain. In addition, temperature variation changed Hooke's law from a linear homogeneous to a linear nonhomogeneous expression including a zero order term. We derived the sensitivity value of the selected objective functions with respect to a perturbation in the structural domain under mechanical and thermal loads while considering these changes in the governing equations. We performed several numerical optimization problems to demonstrate the validity of the present level-set based Pareto topology optimization. Two types of examples (compliance and stress minimization) were solved based on the chosen objective functions. Furthermore, in the stress minimization examples, the derived formula was extended to consider thermal effects in the failure theories for pressure independent and dependent materials.",10.1016/j.cma.2016.12.030,https://dx.doi.org/10.1016/j.cma.2016.12.030
"Goderniaux, P | Brouyere, S | Wildemeersch, S | Therrien, R | Dassargues, A",2015,Uncertainty of climate change impact on groundwater reserves - Application to a chalk aquifer,Applications_JOURNAL OF HYDROLOGY,12,4.0,"Recent studies have evaluated the impact of climate change on groundwater resources for different geographical and climatic contexts. However, most studies have either not estimated the uncertainty around projected impacts or have limited the analysis to the uncertainty related to climate models. In this study, the uncertainties around impact projections from several sources (climate models, natural variability of the weather, hydrological model calibration) are calculated and compared for the Geer catchment ( km()) in Belgium. We use a surface-subsurface integrated model implemented using the finite element code HydroGeoSphere, coupled with climate change scenarios (-) and the UCODE_ inverse model, to assess the uncertainty related to the calibration of the hydrological model. This integrated model provides a more realistic representation of the water exchanges between surface and subsurface domains and constrains more the calibration with the use of both surface and subsurface observed data. Sensitivity and uncertainty analyses were performed on predictions. The linear uncertainty analysis is approximate for this nonlinear system, but it provides some measure of uncertainty for computationally demanding models. Results show that, for the Geer catchment, the most important uncertainty is related to calibration of the hydrological model. The total uncertainty associated with the prediction of groundwater levels remains large. By the end of the century, however, the uncertainty becomes smaller than the predicted decline in groundwater levels.",10.1016/j.jhydrol.2015.06.018,https://dx.doi.org/10.1016/j.jhydrol.2015.06.018
"Lan, SW | Bui-Thanh, T | Christie, M | Girolami, M",2016,Emulation of higher-order tensors in manifold Monte Carlo methods for Bayesian Inverse Problems,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,8,4.0,"The Bayesian approach to Inverse Problems relies predominantly on Markov Chain Monte Carlo methods for posterior inference. The typical nonlinear concentration of posterior measure observed in many such Inverse Problems presents severe challenges to existing simulation based inference methods. Motivated by these challenges the exploitation of local geometric information in the form of covariant gradients, metric tensors, Levi-Civita connections, and local geodesic flows have been introduced to more effectively locally explore the configuration space of the posterior measure. However, obtaining such geometric quantities usually requires extensive computational effort and despite their effectiveness affects the applicability of these geometrically-based Monte Carlo methods. In this paper we explore one way to address this issue by the construction of an emulator of the model from which all geometric objects can be obtained in a much more computationally feasible manner. The main concept is to approximate the geometric quantities using a Gaussian Process emulator which is conditioned on a carefully chosen design set of configuration points, which also determines the quality of the emulator. To this end we propose the use of statistical experiment design methods to refine a potentially arbitrarily initialized design online without destroying the convergence of the resulting Markov chain to the desired invariant measure. The practical examples considered in this paper provide a demonstration of the significant improvement possible in terms of computational loading suggesting this is a promising avenue of further development.",10.1016/j.jcp.2015.12.032,https://dx.doi.org/10.1016/j.jcp.2015.12.032
"Reeve, ST | Strachan, A",2017,Error correction in multi-fidelity molecular dynamics simulations using functional uncertainty quantification,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,4,4.0,"We use functional, Frechet, derivatives to quantify how thermodynamic outputs of a molecular dynamics (MD) simulation depend on the potential used to compute atomic interactions. Our approach quantifies the sensitivity of the quantities of interest with respect to the input functions as opposed to its parameters as is done in typical uncertainty quantification methods. We show that the functional sensitivity of the average potential energy and pressure in isothermal, isochoric MD simulations using Lennard Jones two body interactions can be used to accurately predict those properties for other interatomic potentials (with different functional forms) without re-running the simulations. This is demonstrated under three different thermodynamic conditions, namely a crystal at room temperature, a liquid at ambient pressure, and a high pressure liquid. The method provides accurate predictions as long as the change in potential can be reasonably described to first order and does not significantly affect the region in phase space explored by the simulation. The functional uncertainty quantification approach can be used to estimate the uncertainties associated with constitutive models used in the simulation and to correct predictions if a more accurate representation becomes available.",10.1016/j.jcp.2016.12.039,https://dx.doi.org/10.1016/j.jcp.2016.12.039
"Chen, MS | Senay, GB | Singh, RK | Verdin, JP",2016,Uncertainty analysis of the Operational Simplified Surface Energy Balance (SSEBop) model at multiple flux tower sites,Applications_JOURNAL OF HYDROLOGY,8,4.0,"Evapotranspiration (ET) is an important component of the water cycle - ET from the land surface returns approximately % of the global precipitation back to the atmosphere. ET also plays an important role in energy transport among the biosphere, atmosphere, and hydrosphere. Current regional to global and daily to annual ET estimation relies mainly on surface energy balance (SEB) ET models or statistical and empirical methods driven by remote sensing data and various climatological databases. These models have uncertainties due to inevitable input errors, poorly defined parameters, and inadequate model structures. The eddy covariance measurements on water, energy, and carbon fluxes at the AmeriFlux tower sites provide an opportunity to assess the ET modeling uncertainties. In this study, we focused on uncertainty analysis of the Operational Simplified Surface Energy Balance (SSEBop) model for ET estimation at multiple AmeriFlux tower sites with diverse land cover characteristics and climatic conditions. The -day composite -km MODerate resolution Imaging Spectroradiometer (MODIS) land surface temperature (LST) was used as input land surface temperature for the SSEBop algorithms. The other input data were taken from the AmeriFlux database. Results of statistical analysis indicated that the SSEBop model performed well in estimating ET with an R of . between estimated ET and eddy covariance measurements at  AmeriFlux tower sites during -. It was encouraging to see that the best performance was observed for croplands, where R was . with a root mean square error of  mm/month. The uncertainties or random errors from input variables and parameters of the SSEBop model led to monthly ET estimates with relative errors less than % across multiple flux tower sites distributed across different biomes. This uncertainty of the SSEBop model lies within the error range of other SEB models, suggesting systematic error or bias of the SSEBop model is within the normal range. This finding implies that the simplified parameterization of the SSEBop model did not significantly affect the accuracy of the ET estimate while increasing the ease of model setup for operational applications. The sensitivity analysis indicated that the SSEBop model is most sensitive to input variables, land surface temperature (LST) and reference ET (ET); and parameters, differential temperature (dT), and maximum ET scalar (K-max), particularly during the non-growing season and in dry areas. In summary, the uncertainty assessment verifies that the SSEBop model is a reliable and robust method for large-area ET estimation. The SSEBop model estimates can be further improved by reducing errors in two input variables (ET and LST) and two key parameters (Kmax and dT). (C)  The Authors.",10.1016/j.jhydrol.2016.02.026,https://dx.doi.org/10.1016/j.jhydrol.2016.02.026
"Resmini, A | Peter, J | Lucor, D",2016,Sparse grids-based stochastic approximations with applications to aerodynamics sensitivity analysis,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,8,4.0,"This work compares sample-based polynomial surrogates, well suited for moderately high-dimensional stochastic problems. In particular, generalized polynomial chaos in its sparse pseudospectral form and stochastic collocation methods based on both isotropic and dimension-adapted sparse grids are considered. Both classes of approximations are compared, and an improved version of a stochastic collocation with dimension adaptivity driven by global sensitivity analysis is proposed. The stochastic approximations efficiency is assessed on multivariate test function and airfoil aerodynamics simulations. The latter study addresses the probabilistic characterization of global aerodynamic coefficients derived from viscous subsonic steady flow about a NACA airfoil in the presence of geometrical and operational uncertainties with both simplified aerodynamics model and Reynolds-Averaged Navier-Stokes (RANS) simulation. Sparse pseudospectral and collocation approximations exhibit similar level of performance for isotropic sparse simulation ensembles. Computational savings and accuracy gain of the proposed adaptive stochastic collocation driven by Sobol' indices are patent but remain problem-dependent.",10.1002/nme.5005,https://dx.doi.org/10.1002/nme.5005
"Lin, JZ | Luo, Z | Tong, LY",2010,A new multi-objective programming scheme for topology optimization of compliant mechanisms,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,32,4.0,"This paper presents an alternative method in implementing multi-objective optimization of compliant mechanisms in the field of continuum-type topology optimization. The method is designated as ""SIMP-PP"" and it achieves multi-objective topology optimization by merging what is already a mature topology optimization method-solid isotropic material with penalization (SIMP) with a variation of the robust multi-objective optimization method-physical programming (PP). By taking advantages of both sides, the combination causes minimal variation in computation algorithm and numerical scheme, yet yields improvements in the multi-objective handling capability of topology optimization. The SIMP-PP multi-objective scheme is introduced into the systematic design of compliant mechanisms. The final optimization problem is formulated mathematically using the aggregate objective function which is derived from the original individual design objectives with PP, subjected to the specified constraints. A sequential convex programming method, the method of moving asymptotes (MMA) is then utilized to process the optimization evolvement based on the design sensitivity analysis. The main findings in this study include distinct advantages of the SIMP-PP method in various aspects such as computation efficiency, adaptability in convex and non-convex multi-criteria environment, and flexibility in problem formulation. Observations are made regarding its performance and the effect of multi-objective optimization on the final topologies. In general, the proposed SIMP-PP method is an appealing multi-objective topology optimization scheme suitable for ""real world"" problems, and it bridges the gap between standard topological design and multi-criteria optimization. The feasibility of the proposed topology optimization method is exhibited by benchmark examples.",10.1007/s00158-008-0355-z,https://dx.doi.org/10.1007/s00158-008-0355-z
"Adinehvand, R | Raeisi, E | Hartmann, A",2017,A step-wise semi-distributed simulation approach to characterize a karst aquifer and to support dam construction in a data-scarce environment,Applications_JOURNAL OF HYDROLOGY,4,4.0,"Karst systems provide significant volumes of drinking water for large parts of the world population. Due to chemical weathering, karst systems are characterized by strong heterogeneity resulting in a complex flow and storage behaviour. Presently available karst modelling strategies account for the karstic heterogeneity but often a lack of data limits their applicability in data-scarce regions. In this study, a step-wise simulation approach with a semi-distributed karst model is proposed to characterize a karst aquifer at a data-scarce region in Southwest Iran and to evaluate the leakage potential related to a future dam construction project at a river that cuts through the aquifer. Observed groundwater level time series were applied to calibrate and validate the model. In order to avoid over-parameterization, the karst aquifer was split into three sections down the hydraulic gradient. At each section, groundwater level observations were used to iteratively calibrate the model from the first to the last section. A spatial split sample test and sensitivity analysis served to evaluate the prediction performance and the identifiability of the model parameters. Finally, simple scenarios of the river infiltration into the aquifer were applied to evaluate the leakage potential of the aquifer for future dam constructions. The spatial split-sample test showed that the semi-distributed model provided reliable predictions but prediction performance and parameter identifiability decreased from the first towards the last aquifer section, most probably due to increased aquifer complexity and propagation of uncertainty from the up-gradient model section. Using sensitivity analysis, we also show that parameter sensitivities increase significantly if parameter estimation was applied simultaneously to all three aquifer subsections. Using the model to assess the leakage potential indicated that, without further technical measures, the all river flow would be able infiltrate into the aquifer and the dam would never be filled up completely.",10.1016/j.jhydrol.2017.08.056,https://dx.doi.org/10.1016/j.jhydrol.2017.08.056
"Saloranta, TM",2016,Operational snow mapping with simplified data assimilation using the seNorge snow model,Applications_JOURNAL OF HYDROLOGY,8,4.0,"Frequently updated maps of snow conditions are useful for many applications, e.g., for avalanche and flood forecasting services, hydropower energy situation analysis, as well as for the general public. Numerical snow models are often applied in snow map production for operational hydrological services. However, inaccuracies in the simulated snow maps due to model uncertainties and the lack of suitable data assimilation techniques to correct them in near-real time may often reduce the usefulness of the snow maps in operational use. In this paper the revised seNorge snow model (v...) for snow mapping is described, and a simplified data assimilation procedure is introduced to correct detected snow model biases in near real-time. The data assimilation procedure is theoretically based on the Bayesian updating paradigm and is meant to be pragmatic with modest computational and input data requirements. Moreover, it is flexible and can utilize both point-based snow depth and satellite-based areal snow-covered area observations, which are generally the most common data-sources of snow observations. The model and analysis codes as well as the ""R"" statistical software are freely available. All these features should help to lower the challenges and hurdles hampering the application of data-assimilation techniques in operational hydrological modeling. The steps of the data assimilation procedure (evaluation, sensitivity analysis, optimization) and their contribution to significantly increased accuracy of the snow maps are demonstrated with a case from eastern Norway in winter /.",10.1016/j.jhydrol.2016.03.061,https://dx.doi.org/10.1016/j.jhydrol.2016.03.061
"Ren, HY | Hou, ZS | Huang, MY | Bao, J | Sun, Y | Tesfa, T | Leung, LR",2016,Classification of hydrological parameter sensitivity and evaluation of parameter transferability across 431 US MOPEX basins,Applications_JOURNAL OF HYDROLOGY,8,4.0,"The Community Land Model (CLM) represents physical, chemical, and biological processes of the terrestrial ecosystems that interact with climate across a range of spatial and temporal scales. As CLM includes numerous sub-models and associated parameters, the high-dimensional parameter space presents a formidable challenge for quantifying uncertainty and improving Earth system predictions needed to assess environmental changes and risks. This study aims to evaluate the potential of transferring hydrologic model parameters in CLM through sensitivity analyses and classification across watersheds from the Model Parameter Estimation Experiment (MOPEX) in the United States. The sensitivity of CLM-simulated water and energy fluxes to hydrological parameters across  MOPEX basins are first examined using an efficient stochastic sampling-based sensitivity analysis approach. Linear, interaction, and high-order nonlinear impacts are all identified via statistical tests and stepwise backward removal parameter screening. The basins are then classified according to their parameter sensitivity patterns (internal attributes), as well as their hydrologic indices attributes (external hydrologic factors) separately, using Principal component analysis (PCA) and expectation-maximization (EM) - based clustering approach. Similarities and differences among the parameter sensitivity-based classification system (S-Class), the hydrologic indices-based classification (H-Class), and the Koppen climate classification systems (K-Class) are discussed. Within each parameter sensitivity-based classification system (S-Class) with similar parameter sensitivity characteristics, similar inversion modeling setups can be used for parameter calibration, and the parameters and their contribution or significance to water and energy cycling may also be more transferrable. This classification study provides guidance on identifiable parameters, and on parameterization and inverse model design for CLM but the methodology is applicable to other models. A set of experiments of model calibration were conducted to evaluate the transferability of model calibration strategies and parameter values within and between the classes. It was demonstrated that inverting parameters at representative sites belonging to the same class can significantly reduce parameter calibration efforts.",10.1016/j.jhydrol.2016.02.042,https://dx.doi.org/10.1016/j.jhydrol.2016.02.042
"Chen, LL | Zheng, CJ | Chen, HB",2014,FEM/wideband FMBEM coupling for structural-acoustic design sensitivity analysis,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,16,4.0,"A coupling algorithm based on the finite element method (FEM) and the wideband fast multipole boundary element method (wideband FMBEM) is proposed for acoustic fluid-structure interaction simulation and structural-acoustic design sensitivity analysis by using the direct differentiation method. The wideband fast multipole method (FMM), which is developed by combining the original FMM and the diagonal form FMM, is used to accelerate the calculation of the matrix vector products in boundary element analysis. The iterative solver generalized minimal residual method is applied to accelerate the calculation of the solution to the linear system of equations. The FEM/wideband FMBEM algorithm makes it possible to predict the effects of arbitrarily shaped vibrating structures on the sound field numerically. Numerical examples are presented to demonstrate the validity and efficiency of the proposed algorithm.",10.1016/j.cma.2014.03.016,https://dx.doi.org/10.1016/j.cma.2014.03.016
"Sankaran, S | Grady, L | Taylor, CA",2015,Impact of geometric uncertainty on hemodynamic simulations using machine learning,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,12,4.0,"In the cardiovascular system, blood flow rates, blood velocities and blood pressures can be modeled using the Navier-Stokes equations. Inputs to the system are typically uncertain, such as (a) the geometry of the arterial tree, (b) clinically measured blood pressure and viscosity, (c) boundary resistances, among others. Due to a large number of such parameters, efficient quantification of uncertainty in solution fields in this multi-parameter space is challenging. We use an adaptive stochastic collocation method to quantify the impact of uncertainty in geometry in patient-specific models. We develop a novel subdivision method to define the stochastic space of geometries. To accelerate convergence and make the problem tractable, we use a machine learning approach to approximate the simulation-based solution. Towards this, a reduced order model of the Navier-Stokes equations is developed using a segmental resistance analog boundary conditions (ratio of pressure to flow). Using an offline database of pre-computed solutions, we compute a map (rule) from the features to solution fields. We achieve significant speed-up (of a few orders of magnitude) by approximating the simulation-based solution using a machine learning predictor. A bootstrap aggregated decision tree was found to be the best predictor among many candidate regressors (correlation coefficient of training set was .). We demonstrate stochastic space convergence using the adaptive stochastic collocation method, and also show robustness to the choice of geometry parameterization. The sensitivities to geometry obtained using machine learning had a correlation coefficient of . with the values obtained using finite element simulations. Segments with significant disease in the larger arteries had the highest sensitivities. Terminal segments are more sensitive to dilation and proximal healthy segments are more sensitive to erosion. Sensitivity to geometry is highest when geometric resistance is comparable to net downstream resistance.",10.1016/j.cma.2015.08.014,https://dx.doi.org/10.1016/j.cma.2015.08.014
"Sun, ZG | Wang, QX | Matsushita, B | Fukushima, T | Ouyang, Z | Watanabe, M",2009,Development of a Simple Remote Sensing EvapoTranspiration model (Sim-ReSET): Algorithm and model test,Applications_JOURNAL OF HYDROLOGY,36,4.0,"Remote sensing (RS) has been considered as the most promising tool for evapotranspiration (ET) estimations from local, regional to global scales. Many studies have been conducted to estimated ET using RS data, however, most of them are based partially on ground observations. In this study, we developed a new dual-source Simple Remote Sensing EvapoTranspiration model (Sim-ReSET) based only on RS data. One merit of this model is that the calculation of aerodynamic resistance can be avoided by means of a reference dry bare soil and an assumption that wind speed at the upper boundary of atmospheric surface layer is homogenous, but the aerodynamic characters are still considered by means of canopy height. The other merit is that all inputs (net radiation, soil heat flux, canopy height, variables related to land surface temperature) can be potentially obtained from remote sensing data, which allows obtaining regular RS-driven ET product. For the purposes of sensitivity analysis and performance evaluation of the Sim-ReSET model without the effect of potential uncertainties and errors from remote sensing data, the Sim-ReSET model was tested only using intensive ground observations at the Yucheng ecological station in the North China Plain from  to . Results show that the model has a good performance for instantaneous ET estimations with a mean absolute difference (MAD) of . W/m() and a root mean square error (RMSE) of . W/m() under neutral or near-neutral atmospheric conditions. On  cloudless days, the MAD of daily ET accumulated from instantaneous estimations is . mm/day, and the RMSE is . mm/day.",10.1016/j.jhydrol.2009.07.054,https://dx.doi.org/10.1016/j.jhydrol.2009.07.054
"Rafieeinasab, A | Seo, DJ | Lee, H | Kim, S",2014,Comparative evaluation of maximum likelihood ensemble filter and ensemble Kalman filter for real-time assimilation of streamflow data into operational hydrologic models,Applications_JOURNAL OF HYDROLOGY,16,4.0,"Various data assimilation (DA) methods have been used and are being explored for use in operational streamflow forecasting. For ensemble forecasting, ensemble Kalman filter (EnKF) is an appealing candidate for familiarity and relative simplicity. EnKF, however, is optimal in the second-order sense, only if the observation equation is linear. As such, without an iterative approach, EnKF may not be appropriate for assimilating streamflow data for updating soil moisture states due to the strong nonlinear relationships between the two. Maximum likelihood ensemble filter (MLEF), on the other hand, is not subject to the above limitation. Being an ensemble extension of variational assimilation (VAR), MLEF also offers a strong connection with the traditional single-valued forecast process through the control, or the maximum likelihood, solution. In this work, we apply MLEF and EnKF as a fixed lag smoother to the Sacramento (SAC) soil moisture accounting model and unit hydrograph (UH) for assimilation of streamflow, mean areal precipitation (MAP) and potential evaporation (MAPE) data for updating soil moisture states. For comparative evaluation, three experiments were carried out. Comparison between homoscedastic vs. heteroscedastic modeling of selected statistical parameters for DA indicates that heteroscedastic modeling does not improve over homoscedastic modeling, and that homoscedastic error modeling with sensitivity analysis may suffice for application of MLEF for soil moisture updating using streamflow data. Comparative evaluation with respect to the model errors associated with soil moisture dynamics, the ensemble size and the number of streamflow observations assimilated per cycle showed that, in general, MLEF outperformed EnKF under varying conditions of observation and model errors, and ensemble size, and that MLEF performed well with an ensemble size as small as  while EnKF required a much larger ensemble size to perform closely to MLEF. Also, MLEF was not very sensitive to the uncertainty parameters and performed reasonably well over relatively wide ranges of parameter settings, an attribute desirable for operational applications where accurate estimation of such parameters is often difficult.",10.1016/j.jhydrol.2014.06.052,https://dx.doi.org/10.1016/j.jhydrol.2014.06.052
"Gu, XG | Sun, GY | Li, GY | Huang, XD | Li, YC | Li, Q",2013,Multiobjective optimization design for vehicle occupant restraint system under frontal impact,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,20,4.0,"Occupant Restraint System (ORS) can effectively protect passengers from severe injury in vehicle collision, thus its design signifies a key issue in automobile engineering. To ensure a high safety rating, e.g. five or at least four stars in the European New Car Assessment Program (Euro-NCAP) rating system, which has been widely used to rate the different vehicles from different manufacturers, design optimization becomes essential. Nevertheless, the effectiveness of conventional mathematical programming methods directly integrated with numerical simulation and sensitivity analysis for optimization is of limited practical value, due to high complexity of structures, nonlinearity of materials and deformation involved. To address the issue, this paper combines a Kriging (KRG) model with Non-dominated Sorting Genetic Algorithm II (NSGA-II) for vehicle ORS design. The ORS design of a % Offset Deformable Barrier (ODB) frontal impact test with the collision speed of  km/h is exemplified for the presented method. The results show that the KRG model can well predict the ORS responses for the design. Finally, the optimum result is verified by using sled physical tests. It is found that the ORS performance can be substantially improved for meeting product development requirements through the proposed approach.",10.1007/s00158-012-0811-7,https://dx.doi.org/10.1007/s00158-012-0811-7
"Elham, A | van Tooren, MJL",2016,Coupled adjoint aerostructural wing optimization using quasi-three-dimensional aerodynamic analysis,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,8,4.0,"This paper presents a method for wing aerostructural analysis and optimization, which needs much lower computational costs, while computes the wing drag and structural deformation with a level of accuracy comparable to the higher fidelity CFD and FEM tools. A quasi-three-dimensional aerodynamic solver is developed and connected to a finite beam element model for wing aerostructural optimization. In a quasi-three-dimensional approach an inviscid incompressible vortex lattice method is coupled with a viscous compressible airfoil analysis code for drag prediction of a three dimensional wing. The accuracy of the proposed method for wing drag prediction is validated by comparing its results with the results of a higher fidelity CFD analysis. The wing structural deformation as well as the stress distribution in the wingbox structure is computed using a finite beam element model. The Newton method is used to solve the coupled system. The sensitivities of the outputs, for example the wing drag, with respect to the inputs, for example the wing geometry, is computed by a combined use of the coupled adjoint method, automatic differentiation and the chain rule of differentiation. A gradient based optimization is performed using the proposed tool for minimizing the fuel weight of an A class aircraft. The optimization resulted in more than  % reduction in the aircraft fuel weight by optimizing the wing planform and airfoils shape as well as the wing internal structure.",10.1007/s00158-016-1447-9,https://dx.doi.org/10.1007/s00158-016-1447-9
"Geraci, G | Congedo, PM | Abgrall, R | Iaccarino, G",2016,High-order statistics in global sensitivity analysis: Decomposition and model reduction,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,8,4.0,"ANalysis Of VAriance (ANOVA) is a common technique for computing a ranking of the input parameters in terms of their contribution to the output variance. Nevertheless, the variance is not a universal criterion for ranking variables, since non symmetric outputs could require higher order statistics for their description and analysis. In this work, we illustrate how third and fourth-order moments, i.e. skewness and kurtosis, respectively, can be decomposed mimicking the ANOVA approach. It is also shown how this decomposition is correlated to a Polynomial Chaos (PC) expansion leading to a simple strategy to compute each term. New sensitivity indices, based on the contribution to the skewness and kurtosis, are proposed. The outcome of the proposed analysis is depicted by considering several test functions. Moreover, the ranking of the sensitivity indices is shown to vary according to their statistics order. Furthermore, the problem of formulating a truncated polynomial representation of the original function is treated. Both the reduction of the number of dimensions and the reduction of the order of interaction between parameters are considered. In both cases, the impact on the reduction is assessed in terms of statistics, namely the probability density function. Feasibility of the proposed analysis in a real-case is then demonstrated by presenting the sensitivity analysis of the performances of a turbine cascade in an Organic Rankine Cycles (ORCs), in the presence of complex thermodynamic models and multiple sources of uncertainty.",10.1016/j.cma.2015.12.022,https://dx.doi.org/10.1016/j.cma.2015.12.022
"Giusti, SM | Ferrer, A | Oliver, J",2016,Topological sensitivity analysis in heterogeneous anisotropic elasticity problem. Theoretical and computational aspects,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,8,4.0,"The topological sensitivity analysis for the heterogeneous and anisotropic elasticity problem in two-dimensions is performed in this work. The main result of the paper is an analytical closed-form of the topological derivative for the total potential energy of the problem. This derivative displays the sensitivity of the cost functional (the energy in this case) when a small singular perturbation is introduced in an arbitrary point of the domain. In this case, we consider a small disc with a completely different elastic material. Full mathematical justification for the derived formula, and derivations of precise estimates for the remainders of the topological asymptotic expansion are provided. Finally, the influence of the heterogeneity and anisotropy is shown through some numerical examples of structural topology optimization.",10.1016/j.cma.2016.08.004,https://dx.doi.org/10.1016/j.cma.2016.08.004
"Huang, YC | Yeh, HD",2007,The use of sensitivity analysis in on-line aquifer parameter estimation,Applications_JOURNAL OF HYDROLOGY,44,4.0,"Generally, a pumping test requires a lot of effort and expense to perform the test and the drawdown is measured and analyzed for determining the aquifer parameters. The estimated aquifer parameters obtained from graphical approaches may not be in good accuracy if the pumping time is too short to give a good visual fit to the type curve. Yet, the problems of tong pumping time and required efforts can be significantly reduced if the drawdown data are measured and the parameters are simultaneously estimated on-Line. However, the drawdown behavior of the Leaky and unconfined aquifers in response to the pumping may have a time tag and the time to terminate the estimation may not be easily and quickly to decide when applying a parameter estimation model (PEM) on-Line to analyze the parameters. This study uses the sensitivity analysis to explore the influence period of each aquifer parameter to the pumping drawdown and the influence period is used as a guide in terminating the estimation when applying the PEM for on-line parameter identification. In addition, the sensitivity analysis is also used to study the effects of different value of S-y and the distance between pumping well and observation well on the influence time of S-y during the pumping.",10.1016/j.jhydrol.12.007,https://dx.doi.org/10.1016/j.jhydrol.12.007
"Babuska, I | Nobile, F | Tempone, R",2008,A systematic approach to model validation based on Bayesian updates and prediction related rejection criteria,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,39,3.9,"This work describes a solution to the validation challenge problem posed at the SANDIA Validation Challenge Workshop, May -, , NM. It presents and applies a general methodology to it. The solution entails several standard steps, namely selecting and fitting several models to the available prior information and then sequentially rejecting those which do not perform satisfactorily in the validation and accreditation experiments. The rejection procedures are based on Bayesian updates, where the prior density is related to the current candidate model and the posterior density is obtained by conditioning on the validation and accreditation experiments. The result of the analysis is the computation of the failure probability as well as a quantification of the confidence in the computation, depending on the amount of available experimental data.",10.1016/j.cma.2007.08.031,https://dx.doi.org/10.1016/j.cma.2007.08.031
"Hvejsel, CF | Lund, E | Stolpe, M",2011,Optimization strategies for discrete multi-material stiffness optimization,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,27,3.86,"Design of composite laminated lay-ups are formulated as discrete multi-material selection problems. The design problem can be modeled as a non-convex mixed-integer optimization problem. Such problems are in general only solvable to global optimality for small to moderate sized problems. To attack larger problem instances we formulate convex and non-convex continuous relaxations which can be solved using gradient based optimization algorithms. The convex relaxation yields a lower bound on the attainable performance. The optimal solution to the convex relaxation is used as a starting guess in a continuation approach where the convex relaxation is changed to a non-convex relaxation by introduction of a quadratic penalty constraint whereby intermediate-valued designs are prevented. The minimum compliance, mass constrained multiple load case problem is formulated and solved for a number of examples which numerically confirm the sought properties of the new scheme in terms of convergence to a discrete solution.",10.1007/s00158-011-0648-5,https://dx.doi.org/10.1007/s00158-011-0648-5
"Dotto, CBS | Kleidorfer, M | Deletic, A | Rauch, W | McCarthy, DT",2014,Impacts of measured data uncertainty on urban stormwater models,Applications_JOURNAL OF HYDROLOGY,15,3.75,"Assessing uncertainties in models due to different sources of errors is crucial for advancing urban drainage modelling practice. This paper explores the impact of input and calibration data errors on the parameter sensitivity and predictive uncertainty by propagating these errors through an urban stormwater model (rainfall runoff model KAREN coupled with a build-up/wash-off water quality model). Error models were developed to disturb the measured input and calibration data to reflect common systematic and random uncertainties found in these types of datasets. A Bayesian approach was used for model sensitivity and uncertainty analysis. It was found that random errors in measured data had minor impact on the model performance and sensitivity. In general, systematic errors in input and calibration data impacted the parameter distributions (e.g. changed their shapes and location of peaks). In most of the systematic error scenarios (especially those where uncertainty in input and calibration data was represented using 'best-case' assumptions), the errors in measured data were fully compensated by the parameters. Parameters were unable to compensate in some of the scenarios where the systematic uncertainty in the input and calibration data were represented using extreme worst-case scenarios. As such, in these few worst case scenarios, the model's performance was reduced considerably.",10.1016/j.jhydrol.2013.10.025,https://dx.doi.org/10.1016/j.jhydrol.2013.10.025
"Wei, LY | Tang, TB | Xie, XH | Shen, WJ",2011,Truss optimization on shape and sizing with frequency constraints based on parallel genetic algorithm,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,26,3.71,"Truss shape and sizing optimization under frequency constraints is extremely useful when improving the dynamic performance of structures. However, coupling of two different types of design variables, nodal coordinates and cross-sectional areas, often lead to slow convergence or even divergence. Because shape and sizing variables coupled increase the number of design variables and the changes of shape and sizing variables are of widely different orders of magnitude. Otherwise, multiple frequency constraints often cause difficult dynamic sensitivity analysis. Thus optimal criteria and mathematical programming methods have considerable limitations on solving the problems because of needing complex dynamic sensitivity analysis and being easily trapped into the local optima. Genetic Algorithms (GAs) show great potentials to solve the truss shape and sizing optimization problems. Since GAs adopt global probabilistic population search techniques and require no gradient information. The improved genetic algorithms can effectively increase the solution quality. However, the serial GA is computationally expensive and is limited on gaining higher quality solutions. To solve the truss shape and sizing optimization problems with frequency constraints more effectively and efficiently, a Niche Hybrid Parallel Genetic Algorithm (NHPGA) is proposed to significantly reduce the computational cost and to further improve solution quality. The NHPGA is to blend the advantages of parallel computing, simplex search and genetic algorithm with niche technique. Several typical truss optimization examples demonstrate that NHPGA can significantly reduce computing time and attain higher quality solutions. It also suggests that the NHPGA provide a potential algorithm architecture, which effectively combines the robust and global search characteristics of genetic algorithm, strong exploitation ability of simplex search and computational speedup property of parallel computing.",10.1007/s00158-010-0600-0,https://dx.doi.org/10.1007/s00158-010-0600-0
"McIntyre, N | Jackson, B | Wade, AJ | Butterfield, D | Wheater, HS",2005,Sensitivity analysis of a catchment-scale nitrogen model,Applications_JOURNAL OF HYDROLOGY,48,3.69,"There are now considerable expectations that semi-distributed models are useful tools for supporting catchment water quality management. However, insufficient attention has been given to evaluating the uncertainties inherent to this type of model, especially those associated with the spatial disaggregation of the catchment. The Integrated Nitrogen in Catchments model (INCA) is subjected to an extensive regionalised sensitivity analysis in application to the River Kennet, part of the groundwater-dominated upper Thames catchment, UK The main results are: () model output was generally insensitive to land-phase parameters, very sensitive to groundwater parameters, including initial conditions, and significantly sensitive to in-river parameters; () INCA was able to produce good fits simultaneously to the available flow, nitrate and ammonium in-river data sets; () representing parameters as heterogeneous over the catchment ( calibrated parameters) rather than homogeneous ( calibrated parameters) produced a significant improvement in fit to nitrate but no significant improvement to flow and caused a deterioration in ammonium performance; () the analysis indicated that calibrating the flow-related parameters first, then calibrating the remaining parameters (as opposed to calibrating all parameters together) was not a sensible strategy in this case; () even the parameters to which the model output was most sensitive suffered from high uncertainty due to spatial inconsistencies in the estimated optimum values, parameter equifinality and the sampling error associated with the calibration method; () soil and groundwater nutrient and flow data are needed to reduce. uncertainty in initial conditions, residence times and nitrogen transformation parameters, and long-term historic data are needed so that key responses to changes in land-use management can be assimilated. The results indicate the general, difficulty of reconciling the questions which catchment nutrient models are expected to answer with typically limited data sets and limited knowledge about suitable model structures. The results demonstrate the importance of analysing semi-distributed model uncertainties prior to model application, and illustrate the value and limitations of using Monte Carlo-based methods for doing so.",10.1016/j.jhydrol.2005.04.010,https://dx.doi.org/10.1016/j.jhydrol.2005.04.010
"Tromme, E | Tortorelli, D | Bruls, O | Duysinx, P",2015,Structural optimization of multibody system components described using level set techniques,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,11,3.67,"The structural optimization of the components in multibody systems is performed using a fully coupled optimization method. The design's predicted response is obtained from a flexible multibody system simulation under various service conditions. In this way, the resulting optimization process enhances most existing studies which are limited to weakly coupled (quasi-) static or frequency domain loading conditions. A level set description of the component geometry is used to formulate a generalized shape optimization problem which is solved via efficient gradient-based optimization methods. Gradients of cost and constraint functions are obtained from a sensitivity analysis which is revisited in order to facilitate its implementation and retain its computational efficiency. The optimizations of a slider-crank mechanism and a -dof robot are provided to exemplify the procedure.",10.1007/s00158-015-1280-6,https://dx.doi.org/10.1007/s00158-015-1280-6
"Huang, MY | Liang, X",2006,On the assessment of the impact of reducing parameters and identification of parameter uncertainties for a hydrologic model with applications to ungauged basins,Applications_JOURNAL OF HYDROLOGY,44,3.67,"In this paper, we investigate model parameter uncertainties associated with hydrological process parameterizations and their impacts on model simulation, in the Three-Layer Variable Infiltration Capacity (VIC-L) land surface model. We introduce an alternative subsurface flow parameterization into VIC-L to reduce the impacts of model parameter uncertainties on model simulations by reducing the number of model parameters that need to be estimated through a calibration process. The new subsurface flow parameterization is based on the concepts of kinematic wave and hydrologic similarity, and has one parameter for calibration. Results from the  MOPEX (Model Parameter Estimation Experiment) basins obtained by applying the VIC-L model with the new subsurface flow formulation show that the performance of the new parameterization is comparable to the original subsurface flow formulation, which has three parameters for calibration. In addition, a probabilistic approach based on Monte Carlo simulations is used to evaluate model performance and uncertainties associated with model parameters over different ranges of streamflow. Studies based on the  MOPEX watersheds show that compared to the parameter associated with the new subsurface flow parameterization, the VIC shape parameter (i.e. the b parameter that represents the shape of the heterogeneity distribution of effective soil moisture capacity over a study area) has a larger impact on model simulations and could introduce more uncertainty if not estimated appropriately. Furthermore, investigations on the b parameter suggest that the ensembles (i.e. the mean response and its bounds) from the Monte Carlo simulations could provide reasonable predictions and uncertainty estimates of streamflows, which have important implications for applications to ungauged basins. The study also shows that appropriate reduction of the number of model parameters is an effective approach to reduce the impacts of parameter uncertainties on model simulations. This is more so for applications to ungauged basins or basins with limited data available for calibration. The new subsurface flow parameterization and the probabilistic uncertainty analysis approach are general and can be applied to other modeling studies. (c) ",10.1016/j.jhydrol.2005.07.010,https://dx.doi.org/10.1016/j.jhydrol.2005.07.010
"Sikorska, AE | Del Giudice, D | Banasik, K | Rieckermann, J",2015,The value of streamflow data in improving TSS predictions - Bayesian multi-objective calibration,Applications_JOURNAL OF HYDROLOGY,11,3.67,"The concentration of total suspended solids (TSS) in surface waters is a commonly used indicator of water quality impairments. Its accurate prediction remains, however, problematic because: (i) TSS build-up, erosion, and wash-off are not easily identifiable; (ii) calibrating a TSS model requires observations of sediment loads, which are rare, and streamflow observations to calculate concentrations; and (iii) predicted TSS usually deviate systematically from observations, an effect which is commonly neglected. Ignoring systematic errors during calibration can lead to overconfident (i.e. unreliable) uncertainty estimates during predictions. In this paper, we therefore investigate whether a statistical description of systematic model errors makes it possible to generate reliable predictions for TSS. In addition, we explore how the reliability of TSS predictions increases when streamflow data are additionally used in model calibration. A key aspect of our study is that we use a Bayesian multi-output calibration and a novel autoregressive error model, which describes the model predictive error as a sum of independent random noise and autocorrelated bias. Our results show that using a statistical description of model bias provides more reliable uncertainty estimates of TSS than before and including streamflow data into calibration makes TSS predictions more precise. For a case study of a small ungauged catchment, this improvement was as much as %. Our approach can be easily implemented for other water quality variables which are dependent on streamflow.",10.1016/j.jhydrol.2015.09.051,https://dx.doi.org/10.1016/j.jhydrol.2015.09.051
"Xiang, YJ | Arora, JS | Abdel-Malek, K",2009,Optimization-based motion prediction of mechanical systems: sensitivity analysis,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,33,3.67,"In this study, we derive sensitivity equations for the problem of optimization-based motion prediction of a mechanical system using the inverse recursive Lagrangian formulation. The simulation and sensitivity formulations are based on Denavit-Hartenberg transformation matrices. External forces and moments are taken into account in the formulation. The sensitivity information is needed in the optimization-based simulation process. The proposed formulation is demonstrated by calculating sensitivities for the optimal time trajectory planning problem of a two-link manipulator. In addition, sensitivities obtained using the proposed algorithm are compared to those obtained using the closed-form equations of motion. The two sensitivities match quite closely. The lifting motion of the two-link manipulator with external loads is also optimized by using the algorithm developed in this paper. More complex applications of the proposed formulation to digital human motion prediction are presented elsewhere.",10.1007/s00158-008-0247-2,https://dx.doi.org/10.1007/s00158-008-0247-2
"Lee, I | Choi, KK | Gorsich, D",2010,System reliability-based design optimization using the MPP-based dimension reduction method,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,29,3.62,"The system probability of failure calculation of the series system entails multi-dimensional integration, which is very difficult and numerically expensive. To resolve the computational burden, the narrow bound method, which accounts for the component failures and joint failures between two failure modes, has been widely used. For the analytic calculation of the component probability of failure, this paper proposes to use the most probable point (MPP)-based dimension reduction method (DRM). For the joint probability of failure calculation, three cases are considered based on the convexity or concavity of the performance functions. Design sensitivity analysis for the system reliability-based design optimization (RBDO), which is the major contribution of this paper, is carried out as well. Based on the results of numerical examples, the system probability of failure and its sensitivity calculation show very good agreement with the results obtained by Monte Carlo simulation (MCS) and the finite difference method (FDM).",10.1007/s00158-009-0459-0,https://dx.doi.org/10.1007/s00158-009-0459-0
"Jakeman, JD | Narayan, A | Xiu, DB",2013,Minimal multi-element stochastic collocation for uncertainty quantification of discontinuous functions,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,18,3.6,"We propose a multi-element stochastic collocation method that can be applied in high-dimensional parameter space for functions with discontinuities lying along manifolds of general geometries. The key feature of the method is that the parameter space is decomposed into multiple elements defined by the discontinuities and thus only the minimal number of elements are utilized. On each of the resulting elements the function is smooth and can be approximated using high-order methods with fast convergence properties. The decomposition strategy is in direct contrast to the traditional multi-element approaches which define the sub-domains by repeated splitting of the axes in the parameter space. Such methods are more prone to the curse-of-dimensionality because of the fast growth of the number of elements caused by the axis based splitting. The present method is a two-step approach. Firstly a discontinuity detector is used to partition parameter space into disjoint elements in each of which the function is smooth. The detector uses an efficient combination of the high-order polynomial annihilation technique along with adaptive sparse grids, and this allows resolution of general discontinuities with a smaller number of points when the discontinuity manifold is low-dimensional. After partitioning, an adaptive technique based on the least orthogonal interpolant is used to construct a generalized Polynomial Chaos surrogate on each element. The adaptive technique reuses all information from the partitioning and is variance-suppressing. We present numerous numerical examples that illustrate the accuracy, efficiency, and generality of the method. When compared against standard locally-adaptive sparse grid methods, the present method uses many fewer number of collocation samples and is more accurate.",10.1016/j.jcp.2013.02.035,https://dx.doi.org/10.1016/j.jcp.2013.02.035
"Congedo, PM | Corre, C | Martinez, JM",2011,Shape optimization of an airfoil in a BZT flow with multiple-source uncertainties,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,25,3.57,"Bethe-Zel'dovich-Thompson fluids (BZT) are characterized by negative values of the fundamental derivative of gasdynamics for a range of temperatures and pressures in the vapor phase, which leads to non-classical gasdynamic behaviors such as the disintegration of compression shocks. These non-classical phenomena can be exploited, when using these fluids in Organic Rankine Cycles (ORCs), to increase isentropic efficiency. A predictive numerical simulation of these flows must account for two main sources of physical uncertainties: the BZT fluid properties often difficult to measure accurately and the usually fluctuating turbine inlet conditions. For taking full advantage of the BZT properties, the turbine geometry must also be specifically designed, keeping in mind the geometry achieved in practice after machining always slightly differs from the theoretical shape. This paper investigates some efficient procedures to perform shape optimization in a D BZT flow with multiple-source uncertainties (thermodynamic model, operating conditions and geometry). To demonstrate the feasibility of the proposed efficient strategies for shape optimization in the presence of multiple-source uncertainties, a zero incidence symmetric airfoil wave-drag minimization problem is retained as a case-study. This simplified configuration encompasses most of the features associated with a turbine design problem, as far the uncertainty quantification is concerned. A preliminary analysis of the contributions to the variance of the wave-drag allows to select the most significant sources of uncertainties using a reduced number of flow computations. The resulting mean value and variance of the objective are next turned into rnetamodels. The optimal Pareto sets corresponding to the minimization of various substitute functions are obtained using a genetic algorithm as optimizer and their differences are discussed.",10.1016/j.cma.2010.08.006,https://dx.doi.org/10.1016/j.cma.2010.08.006
"Kang, Z | Wang, R | Tong, LY",2011,Combined optimization of bi-material structural layout and voltage distribution for in-plane piezoelectric actuation,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,25,3.57,"This paper investigates the combined optimization of bi-material structural layout and actuation voltage distribution of structures with embedded in-plane piezoelectric actuators. The maximization of the nodal displacement at a selected output port is considered as the design objective. A two-phase material model with power-law penalization is employed in the topology optimization of the actuator elements and the coupled surrounding structure. In order to incorporate the actuation voltage directly into the design for achieving the best overall actuation performance, element-wise voltage design variables are also included in the optimization. For the purpose of easy implementation of the electric system, the allowable voltage levels at an individual element are confined to three discrete values, namely zero and two prescribed values with opposite signs. To this end, a special interpolation scheme between the tri-level voltage values and the design variables is used in the optimization model. Based on the design sensitivity analysis of the objective function, the combined optimization problem is solved with the MMA algorithm. Numerical examples are presented to demonstrate the applicability of the proposed optimization model and numerical techniques. The optimal solutions also confirmed that larger output displacement can be achieved by introducing voltage design variables into the design problem.",10.1016/j.cma.2011.01.005,https://dx.doi.org/10.1016/j.cma.2011.01.005
"Audouze, C | De Vuyst, F | Nair, PB",2009,Reduced-order modeling of parameterized PDEs using time-space-parameter principal component analysis,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,32,3.56,"This paper presents a methodology for constructing low-order surrogate models of finite element/finite volume discrete solutions of parameterized steady-state partial differential equations. The construction of proper orthogonal decomposition modes in both physical space and parameter space allows us to represent high-dimensional discrete solutions using only a few coefficients. An incremental greedy approach is developed for efficiently tackling problems with high-dimensional parameter spaces. For numerical experiments and validation, several non-linear steady-state convection-diffusion-reaction problems are considered: first in one spatial dimension with two parameters, and then in two spatial dimensions with two and five parameters. In the two-dimensional spatial case with two parameters, it is shown that a  x  coefficient matrix is sufficient to accurately reproduce the expected solution, while in the five parameters problem, a  x  coefficient matrix is shown to reproduce the Solution with sufficient accuracy. The proposed methodology is expected to find applications to parameter variation Studies, uncertainty analysis, inverse problems and optimal design.",10.1002/nme.2540,https://dx.doi.org/10.1002/nme.2540
"Jensen, HA | Munoz, A | Papadimitriou, C | Vergara, C",2016,An enhanced substructure coupling technique for dynamic re-analyses: Application to simulation-based problems,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,7,3.5,This work presents the implementation of an enhanced substructure coupling technique in the context of complex simulation-based problems. Attention is focused to problems requiring a large number of dynamic re-analyses of involved finite element models. The enhanced technique is derived by first considering explicitly the effect of higher order substructural modes in the definition of reduced-order models. The formulation is then combined with an effective finite element model parametrization scheme. The performance of the proposed implementation is demonstrated through numerical examples involving the estimation of the dynamic characteristics of a bridge structural model and the Bayesian model updating of a high fidelity finite element model.,10.1016/j.cma.2016.04.011,https://dx.doi.org/10.1016/j.cma.2016.04.011
"Lopes, CG | Novotny, AA",2016,Topology design of compliant mechanisms with stress constraints based on the topological derivative concept,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,7,3.5,"Compliant mechanisms are mechanical devices composed by one single piece that transforms simple inputs into complex movements. This kind of multi-flexible structure can be manufactured at a very small scale. Therefore, the spectrum of applications of such microtools has become broader in recent years including microsurgery, nanotechnology processing, among others. In this paper, we deal with topology design of compliant mechanisms under von Mises stress constraints. The topology optimization problem is addressed with an efficient approach based on the topological derivative concept and a level-set domain representation method. The resulting topology optimization algorithm is remarkably efficient and of simple computational implementation. Finally, some numerical experiments are presented, showing that the proposed approach naturally avoids the undesirable flexible joints (hinges) by keeping the stress level under control.",10.1007/s00158-016-1436-z,https://dx.doi.org/10.1007/s00158-016-1436-z
"Zhou, RR | Li, Y | Lu, D | Liu, HX | Zhou, HC",2016,An optimization based sampling approach for multiple metrics uncertainty analysis using generalized likelihood uncertainty estimation,Applications_JOURNAL OF HYDROLOGY,7,3.5,"This paper investigates the use of an epsilon-dominance non-dominated sorted genetic algorithm II (epsilon-NSGAII) as a sampling approach with an aim to improving sampling efficiency for multiple metrics uncertainty analysis using Generalized Likelihood Uncertainty Estimation (GLUE). The effectiveness of epsilon-NSGAII based sampling is demonstrated compared with Latin hypercube sampling (LHS) through analyzing sampling efficiency, multiple metrics performance, parameter uncertainty and flood forecasting uncertainty with a case study of flood forecasting uncertainty evaluation based on Xinanjiang model (XAJ) for Qing River reservoir, China. Results obtained demonstrate the following advantages of the epsilon-NSGAII based sampling approach in comparison to LHS: () The former performs more effective and efficient than LHS, for example the simulation time required to generate  behavioral parameter sets is shorter by  times; () The Pareto tradeoffs between metrics are demonstrated clearly with the solutions from epsilon-NSGAII based sampling, also their Pareto optimal values are better than those of LHS, which means better forecasting accuracy of epsilon-NSGAII parameter sets; () The parameter posterior distributions from epsilon-NSGAII based sampling are concentrated in the appropriate ranges rather than uniform, which accords with their physical significance, also parameter uncertainties are reduced significantly; () The forecasted floods are close to the observations as evaluated by three measures: the normalized total flow outside the uncertainty intervals (FOUI), average relative band-width (RB) and average deviation amplitude (D). The flood forecasting uncertainty is also reduced a lot with epsilon-NSGAII based sampling. This study provides a new sampling approach to improve multiple metrics uncertainty analysis under the framework of GLUE, and could be used to reveal the underlying mechanisms of parameter sets under multiple conflicting metrics in the uncertainty analysis process.",10.1016/j.jhydrol.2016.06.030,https://dx.doi.org/10.1016/j.jhydrol.2016.06.030
"Zhu, YH | Zhan, HB | Jin, MG",2016,Analytical solutions of solute transport in a fracture-matrix system with different reaction rates for fracture and matrix,Applications_JOURNAL OF HYDROLOGY,7,3.5,"This study deals with the problem of reactive solute transport in a fracture-matrix system using both analytical and numerical modeling methods. The groundwater flow velocity in the fracture is assumed to be high enough (no less than . m/day) to ensure the advection-dominant transport in the fracture. The problem includes advection along the fracture, transverse diffusion in the matrix, with linear sorption as well as first-order reactions operative in both the fracture and the matrix. A constant concentration boundary condition and a decay source boundary condition in the fracture are considered. With a constant-concentration source, we obtain closed-form analytical solutions that account for the transport without reaction as well as steady-state solutions with different first-order reactions in the two media. With a decay source, a semi-analytical solution is obtained. The analytical and semi analytical solutions are in excellent agreement with the numerical simulation results obtained using COMSOL Multiphysics. Sensitivity analysis is conducted to assess the relative importance of matrix diffusion coefficient, fracture aperture, and matrix porosity. We conclude that the first-order reaction as well as the matrix diffusion in the fractured rock would decrease the solute peak concentration and shorten the penetration distance into the fracture. The solutions can be applied to assess the spatial-temporal distribution of concentrations in the fracture and the matrix as well as to assess the contaminant mass stored in the rock matrix. All of these are useful for designing remediation plans for contaminated fractured rocks or for risk assessment of contaminated fracture-matrix systems.",10.1016/j.jhydrol.2016.05.056,https://dx.doi.org/10.1016/j.jhydrol.2016.05.056
"Hughes, DA | Kapangaziwiri, E | Sawunyama, T",2010,Hydrological model uncertainty assessment in southern Africa,Applications_JOURNAL OF HYDROLOGY,28,3.5,"The importance of hydrological uncertainty analysis has been emphasized in recent years and there is an urgent need to incorporate uncertainty estimation into water resources assessment procedures used in the southern Africa region. The region is characterized by a paucity of accurate data and limited human resources, but the need for informed development decisions is critical to social and economic development. One of the main sources of uncertainty is related to the estimation of the parameters of hydrological models. This paper proposes a framework for establishing parameter values, exploring parameter inter-dependencies and setting parameter uncertainty bounds for a monthly time-step rainfall-runoff model (Pitman model) that is widely used in the region. The method is based on well-documented principles of sensitivity and uncertainty analysis, but recognizes the limitations that exist within the region (data scarcity and accuracy, model user attitudes, etc.). Four example applications taken from different climate and physiographic regions of South Africa illustrate that the methods are appropriate for generating behavioural stream flow simulations which include parameter uncertainty. The parameters that dominate the model response and their degree of uncertainty vary between regions. Some of the results suggest that the uncertainty bounds will be too wide for effective water resources decision making. Further work is required to reduce some of the subjectivity in the methods and to investigate other approaches for constraining the uncertainty. The paper recognizes that probability estimates of uncertainty and methods to include input climate data uncertainties need to be incorporated into the framework in the future.",10.1016/j.jhydrol.2010.04.010,https://dx.doi.org/10.1016/j.jhydrol.2010.04.010
"Lin, YC | Yang, SY | Fen, CS | Yeh, HD",2016,A general analytical model for pumping tests in radial finite two-zone confined aquifers with Robin-type outer boundary,Applications_JOURNAL OF HYDROLOGY,7,3.5,"This study develops a general analytical model for describing transient drawdown distribution induced by pumping at a finite-radius well in a radial two-zone confined aquifer of finite areal extent with Robin-type condition at both inner and outer boundaries. This model is also applicable to heat conduction problems for a composite hollow cylinder on the basis of the analogy between heat flow and groundwater flow. The time-domain solution of the model is derived by the methods of Laplace transform, Bromwich integral, and residue theorem. This new solution can reduce to the solution for constant-head test (CHT) or constant-rate test (CRT) problem by specifying appropriate coefficients at the Robin inner boundary condition. The solution describing the flow rate across the wellbore due to CHT is further developed by applying Darcy's law to the new solution. In addition, steady-state solutions for both CHT and CRT are also developed based on the approximation for Bessel functions with very small argument values. Many existing solutions for transient flow in homogeneous or two-zone finite aquifers with Dirichlet or no-flow condition at the outer boundary are shown to be special cases of the present solution. Furthermore, the sensitivity analysis is also performed to investigate the behaviors of the wellbore flow due to CHT and the aquifer drawdown induced by CRT in response to the change in each of aquifer parameters.",10.1016/j.jhydrol.2016.07.028,https://dx.doi.org/10.1016/j.jhydrol.2016.07.028
"Leven, C | Dietrich, P",2006,What information can we get from pumping tests? - comparing pumping test configurations using sensitivity coefficients,Applications_JOURNAL OF HYDROLOGY,42,3.5,"In this paper we present a comparison of two different pumping test configurations by means of sensitivity coefficients. Sensitivity coefficients, which are a measure for the relationship between a change in drawdown and a change in the parameter distribution, allow the analysis of the intrinsic characteristics of pumping tests in order to provide a better understanding of their response to aquifer heterogeneity. By considering the evolution of changes in the sensitivity distribution, a direct link between temporal and spatial information is given. Thus, any deviation in the parameter distribution in a particular area around the wells of the considered test configuration can be assigned to deviations in the drawdown Curve indicated by a changing slope. Consequently, sensitivity coefficients allow the assessment of temporal information from the drawdown curve. By means of sensitivity coefficients it can be shown that the spatial assignment of estimated parameters is much simpler for single-well than for two-well pumping tests. Based on a numerical example, difficulties and consequences arising from pumping test evaluation are illustrated as result of the intrinsic characteristics of the test configuration due to the particularities of the sensitivity distribution. In this context, misinterpretations resulting from the application of interpretation methods-such as the Theis Solution for the classical two-well pumping test-are demonstrated that might lead to inaccurate estimates of hydraulic parameters. For the reduction of non-uniqueness, a modified concept is proposed for an improved characterization of aquifer heterogeneity.",10.1016/j.jhydrol.2005.06.030,https://dx.doi.org/10.1016/j.jhydrol.2005.06.030
"Luo, YJ | Kang, Z",2013,Layout design of reinforced concrete structures using two-material topology optimization with Drucker-Prager yield constraints,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,17,3.4,"This paper aims to develop a method that can automatically generate the optimal layout of reinforced concrete structures by incorporating concrete strength constraints into the two-material topology optimization formulation. The Drucker-Prager yield criterion is applied to predict the failure behavior of concrete. By using the power-law interpolation, the proposed optimization model is stated as a minimum compliance problem under the yield stress constraints on concrete elements and the material volume constraint of steel. The epsilon-relaxation technique is employed to prevent the stress singularity. A hybrid constraint-reduction strategy, in conjunction with the adjoint-variable sensitivity information, is integrated into a gradient-based optimization algorithm to overcome the numerical difficulties that arise from large-scale constraints. It can be concluded from numerical investigations that the proposed model is suitable for obtaining a reasonable layout which makes the best uses of the compressive strength of concrete and the tensile strength of steel. Numerical results also reveal that the hybrid constraint-reduction strategy is effective in solving the topology optimization problems involving a large number of constraints.",10.1007/s00158-012-0809-1,https://dx.doi.org/10.1007/s00158-012-0809-1
"Hauet, A | Creutin, JD | Belleudy, P",2008,Sensitivity study of large-scale particle image velocimetry measurement of river discharge using numerical simulation,Applications_JOURNAL OF HYDROLOGY,34,3.4,"This study deals with the uncertainty of large-scale particle image velocimetry (LSPIV) measurements in rivers. LSPIV belongs to the methods of local remote sensing of rivers, like Radar- and Lidar-based techniques. These methods have many potential advantages, in comparison with classical river gauging, but they have a fundamental drawback: they are indirect measurements. As such they need to be assessed in reference to direct measurements. A first validation method consists in the comparison of LSPIV measurements with classic gauging results, in field and laboratory experiments. Unfortunately, in both cases, it is impossible in practice to control, all the parameters and to distinguish the impact of the various error sources. In the present study we propose a more theoretical assessment of LSPIV potential through numerical simulation. The idea is simply to mathematically formulate the present state of knowledge of the measurement including both the physics of the phenomenon (the illuminated river) and the physics of the sensor (the camera and the PIV tracking). The dilemma about when to start this type of simulation is the following: The simulation is satisfactory if we can validate it which means to be able to compare simulations and observations over a wide range of conditions. The simulation is useful to get preliminary insights about the most important measurement conditions to organize validation studies. Our simulator is composed of three blocks: () The river block represents the unidirectional river flow by the association of the EDM model and a theoretical vertical velocity profile giving a D velocity distribution. This hydraulic model is complemented by features representing free surface tracers, the illumination of the free-surface (shadows and sun reflection) and the effect of the wind. () The camera block transforms the river state parameters into raster images according to the intrinsic and extrinsic parameters of the camera. () The LSPIV analysis block performs a classical LSPIV analysis, including geometric transformation of the images, PIV analysis to obtain a surface velocity field, and discharge computation. We tried to keep a good balance between the different blocks of the simulator (i.e. not to make one component much more sophisticated than the others). The simulator was partly tested during the development of its different blocks, and then globally validated. It reproduced well the variability observed in the field LSPIV experiments conducted with the real-time continuous system of Hauet et al. [Hauet, A., Kruger, A., Krajewski, W., Bradley, A., Muste, M., Creutin, J.D., Wilson, M., . Experimental system for real-time discharge estimation using an image-based method. Journal of Hydrologic Engineering]. The simulator can also be used to check different scenarios and to assess relative importance of the different sources of error. With two examples, we illustrate this capability of the simulator to assess the relative weight of a given error source and to test a new configuration of measurement.",10.1016/j.jhydrol.2007.10.062,https://dx.doi.org/10.1016/j.jhydrol.2007.10.062
"Chaudhuri, A | Haftka, RT | Ifju, P | Chang, K | Tyler, C | Schmitz, T",2015,Experimental flapping wing optimization and uncertainty quantification using limited samples,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,10,3.33,"Flapping wing micro air vehicles are capable of hover and forward flight with high maneuverability. However, flapping wing flight is difficult to simulate accurately because it is a more complex phenomenon than fixed wing or rotorcraft flight. Consequently, the optimization of flapping wing behavior based on simulation is limited and, therefore, we have elected to optimize a wing experimentally. Specifically, we use experimental data to optimize the flapping wing structure for maximum thrust production in hover mode. We point out the similarities or otherwise between experimental optimization and the more common simulation-based optimization. Experimental optimization is hampered by noisy data, which is due to manufacturing variability and testing/measurement uncertainty in this study. These uncertainties must be reduced to an acceptable level and this requires their quantification. Therefore, improvements in manufacturing and testing procedures were implemented to reduce the noise. Another challenge is to limit the number of experiments for reducing time and cost. This is realized by using surrogates, or meta-models, to approximate the response (in this case, thrust) of the wing. In order to take into account the uncertainty, or noise, in the response, we use a Gaussian Process surrogate with noise and a nd order polynomial response surface. We apply a surrogate-based optimization algorithm called Efficient Global Optimization with different sampling criteria and multiple surrogates. This enables us to select multiple points per optimization cycle, which is especially useful in this case as it is more time efficient to manufacture multiple wings at once and this also serves as insurance against failed designs.",10.1007/s00158-014-1184-x,https://dx.doi.org/10.1007/s00158-014-1184-x
"Bonte, MHA | van den Boogaard, AH | Huetink, J",2008,An optimisation strategy for industrial metal forming processes,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,33,3.3,"Product improvement and cost reduction have always been important goals in the metal forming industry. The rise of finite element (FEM) simulations for processes has contributed to these goals in a major way. More recently, coupling FEM simulations to mathematical optimisation techniques has shown the potential to make a further giant contribution to product improvement and cost reduction. Much research on the optimisation of metal forming processes has been published during the last couple of years. Although the results are impressive, the optimisation techniques are generally only applicable to specific optimisation problems for specific products and specific metal forming processes. As a consequence, applying optimisation techniques to other metal forming problems requires a lot of optimisation expertise, which forms a barrier for more general industrial application of these techniques. In this paper, we overcome this barrier by proposing a generally applicable optimisation strategy that makes use of FEM simulations of metal forming processes. It consists of a structured methodology for modelling optimisation problems related to metal forming. Subsequently, screening is applied to reduce the size of the optimisation problem by selecting only the most important design variables. Finally, the reduced optimisation problem is solved by an efficient optimisation algorithm. The strategy is generally applicable in a sense that it is not constrained to a certain type of metal forming problem, product or process. Also, any FEM code may be included in the strategy. Furthermore, the structured approach for modelling and solving optimisation problems should enable non-optimisation specialists to apply optimisation techniques to improve their products and processes. The optimisation strategy has been successfully applied to a hydroforming process, which demonstrates the potential of the optimisation of metal forming processes in general and more specific the proposed optimisation strategy.",10.1007/s00158-007-0206-3,https://dx.doi.org/10.1007/s00158-007-0206-3
"Dostert, P | Efendiev, Y | Hou, TY",2008,Multiscale finite element methods for stochastic porous media flow equations and application to uncertainty quantification,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,33,3.3,"In this paper. we study multiscale finite element methods for stochastic porous media flow equations as well as applications to uncertainty quantification. We assume that the permeability field (the diffusion coefficient) is stochastic and can be described in a finite dimensional stochastic space. This is common in applications where the coefficients are expanded using chaos approximations. The proposed multiscale method constructs multiscale basis functions corresponding to sparse realizations, and these basis functions are used to approximate the solution on the coarse-grid for any realization. Furthermore, we apply our coarse-scale model to uncertainty quantification problem where the goal is to sample the porous media properties given an integrated response such as production data. Our algorithm employs pre-computed posterior response surface obtained via the proposed coarse-scale model. Using fast analytical computations of the gradients of this posterior, we propose approximate Langevin samples. These samples are further screened through the coarse-scale simulation and, finally, used as a proposal in Metropolis-Hasting Markov chain Monte Carlo method. Numerical results are presented which demonstrate the efficiency of the proposed approach.",10.1016/j.cma.2008.02.030,https://dx.doi.org/10.1016/j.cma.2008.02.030
"Li, L | Xu, CY | Xia, J | Engeland, K | Reggiani, P",2011,Uncertainty estimates by Bayesian method with likelihood of AR (1) plus Normal model and AR (1) plus Multi-Normal model in different time-scales hydrological models,Applications_JOURNAL OF HYDROLOGY,23,3.29,"Bayesian revision is widely used in hydrological model uncertainty assessment. With respect to model calibration, parameter estimation and analysis of uncertainty sources, various regression and probabilistic approaches have been used in different models calibrated for either daily or monthly time step. None of these applications however includes a comparison of uncertainty analysis in hydrological models with respect to the time periods, at which the models are operated. This study pursues a comprehensive inter-comparison and evaluation of uncertainty assessments by Bayesian revision using the Metropolis Hasting (MH) algorithm with the hydrological model WASMOD with daily and monthly time step. In the daily step model three likelihood functions are used in combination with Bayesian revision: (i) the AR () plus Normal time period independent model (Model ), (ii) the AR () plus Multi-Normal model (Model ), and (iii) the AR () plus Normal time period dependent model (Model ). In addition an index called the percentage of observations bracketed by the Unit Confidence Interval (PUCI) was used for uncertainty evaluation. The results reveal that it is more important to consider the autocorrelation in daily WASMOD rather than monthly WASMOD. Firstly, the resulting goodness of fit of the daily model vs. observations as measured by the Nash-Sutcliffe efficiency value is comparable with that calculated by the optimization algorithm in monthly WASMOD. Secondly, the AR () model is not sufficiently adequate to estimate the distribution of residuals in daily WASMOD since PUCI shows that Model  outperforms Model . Furthermore, the maximum Nash-Sutcliffe efficiency value of Model  is the largest. Thirdly, Model  performs best over the entire flow range, while Model  outperforms Model  for high flows. This shows that additional statistical parameters reflect the statistical characters of the residuals more efficiently and accurately. Fourthly, by considering the difference in terms of application and computational efficiency it becomes evident that Model  performs best for daily WASMOD. Model  on the other hand is superior for daily time step WASMOD if the auto-correlation of parameters is considered.",10.1016/j.jhydrol.2011.05.052,https://dx.doi.org/10.1016/j.jhydrol.2011.05.052
"Shin, MK | Park, KJ | Park, GJ",2007,Optimization of structures with nonlinear behavior using equivalent loads,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,36,3.27,"Nonlinear Response Optimization using the Equivalent Loads (NROEL) method/algorithm is proposed to perform optimization of non-linear response structures. The conventional method spends most of the design time on nonlinear analysis. The NROEL algorithm makes the equivalent load cases for each response and repeatedly performs linear response optimization and uses them as multiple loading conditions. The equivalent loads are defined as the loads in linear analysis, which generate the same response fields as those in nonlinear analysis. The algorithm is validated for convergence and optimality. The proposed algorithm is applied to a simple mathematical problem to verify the convergence and the optimality. The NROEL algorithm is applied to several structural problems with geometric and/or material nonlinearity. Conventional optimization with sensitivity analysis using the finite difference method is also applied to the same examples. The results of the optimizations are compared. The proposed NROEL method is found to be very efficient and good solutions are derived.",10.1016/j.cma.2006.09.001,https://dx.doi.org/10.1016/j.cma.2006.09.001
"Perko, Z | Gilli, L | Lathouwers, D | Kloosterman, JL",2014,Grid and basis adaptive polynomial chaos techniques for sensitivity and uncertainty analysis,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,13,3.25,"The demand for accurate and computationally affordable sensitivity and uncertainty techniques is constantly on the rise and has become especially pressing in the nuclear field with the shift to Best Estimate Plus Uncertainty methodologies in the licensing of nuclear installations. Besides traditional, already well developed methods - such as first order perturbation theory or Monte Carlo sampling - Polynomial Chaos Expansion (PCE) has been given a growing emphasis in recent years due to its simple application and good performance. This paper presents new developments of the research done at TU Delft on such Polynomial Chaos (PC) techniques. Our work is focused on the Non-Intrusive Spectral Projection (NISP) approach and adaptive methods for building the PCE of responses of interest. Recent efforts resulted in a new adaptive sparse grid algorithm designed for estimating the PC coefficients. The algorithm is based on Gerstner's procedure for calculating multidimensional integrals but proves to be computationally significantly cheaper, while at the same it retains a similar accuracy as the original method. More importantly the issue of basis adaptivity has been investigated and two techniques have been implemented for constructing the sparse PCE of quantities of interest. Not using the traditional full PC basis set leads to further reduction in computational time since the high order grids necessary for accurately estimating the near zero expansion coefficients of polynomial basis vectors not needed in the PCE can be excluded from the calculation. Moreover the sparse PC representation of the response is easier to handle when used for sensitivity analysis or uncertainty propagation due to the smaller number of basis vectors. The developed grid and basis adaptive methods have been implemented in Matlab as the Fully Adaptive Non-Intrusive Spectral Projection (FANISP) algorithm and were tested on four analytical problems. These show consistent good performance both in terms of the accuracy of the resulting PC representation of quantities and the computational costs associated with constructing the sparse PCE. Basis adaptivity also seems to make the employment of PC techniques possible for problems with a higher number of input parameters (-), alleviating a well known limitation of the traditional approach. The prospect of larger scale applicability and the simplicity of implementation makes such adaptive PC algorithms particularly appealing for the sensitivity and uncertainty analysis of complex systems and legacy codes.",10.1016/j.jcp.2013.12.025,https://dx.doi.org/10.1016/j.jcp.2013.12.025
"Demarty, J | Ottle, C | Braud, I | Olioso, A | Frangi, JP | Bastidas, LA | Gupta, HV",2004,Using a multiobjective approach to retrieve information on surface properties used in a SVAT model,Applications_JOURNAL OF HYDROLOGY,45,3.21,"The reliability of model predictions used in meteorology, agronomy or hydrology is partly linked to an adequate representation of the water and energy balances which are described in so-called SVAT (Soil Vegetation Atmosphere Transfer) models. These models require the specification of many surface properties which can generally be obtained from laboratory or field experiments, using time consuming techniques, or can be derived from textural information. The required accuracy of the surface properties depends on the model complexity and their misspecification can affect model performance. At various time and spatial resolutions, remote sensing provides information related to surface parameters in SVAT models or state variables simulated by SVAT models. In this context, the Simple Soil-Plant-Atmosphere Transfer-Remote Sensing (SiSPAT-RS) model was developed for remote sensing data assimilation objectives. This new version of the physically based SiSPAT model simulates the main surface processes (energy fluxes, soil water content profiles, temperatures) and remote sensing data in the visible, infrared and thermal infrared spectral domains. As a preliminary step before data assimilation in the model, the objectives of this study were () to apply a multiobjective approach for retrieving quantitative information about the surface properties from different surface measurements and () to determine the potential of the SiSPAT-RS model to be applied with 'little' a priori information about input parameters. To reach these goals, the ability of the Multiobjective Generalized Sensitivity Analysis (MOGSA) algorithm to determine and quantify the most influential input parameters of the SiSPAT-RS model on several simulated output variables, was investigated. The results revealed the main influential input parameters according to different contrasted environmental conditions, and contributed to the reduction of their a priori uncertainty range. A procedure for specifying surface properties from MOGSA results was tested on the thermal and hydraulic soil parameters, and evaluated through the SiSPAT-RS model performance. Although slightly lower than a reference simulation, the performance were satisfactory and suggested that complex-SVAT models can be driven with little a priori information",10.1016/j.jhydrol.2003.10.003,https://dx.doi.org/10.1016/j.jhydrol.2003.10.003
"Yang, RZ | Du, JB",2013,Microstructural topology optimization with respect to sound power radiation,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,16,3.2,The paper deals with the problem of topological design of microstructure with respect to minimization of the sound power radiation from a vibrating macrostructure. The macrostructure is excited at a single or a band of excitation frequencies by a time-harmonic mechanical loading with prescribed amplitude and spatial distribution. The structural damping is considered to be proportional damping. The sound power is calculated using a high frequency approximation formulation and thus the sensitivity analysis may be performed in a very efficient manner. The microstructure composed of two different solid isotropic materials is assumed to be identical from point to point at the macro-level which implies that the interface between the structure and the acoustic medium is unchanged during the design process. The equivalent material properties of the macrostructure are calculated using homogenization method and the bi-material SIMP model is employed to achieve zero-one design at the micro-scale. Numerical examples are given to validate the model developed. Some interesting features of acoustic microstructure topology optimization are revealed and discussed.,10.1007/s00158-012-0838-9,https://dx.doi.org/10.1007/s00158-012-0838-9
"Kolakowski, P | Wiklo, M | Holnicki-Szulc, J",2008,The virtual distortion method - a versatile reanalysis tool for structures and systems,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,32,3.2,"For  years of development, the virtual distortion method (VDM) has proved to be a versatile reanalysis tool in various applications, including structures and truss-like systems. This article presents a summary of principal achievements, demonstrating the capabilities of the VDM both in statics and dynamics, in linear and nonlinear analysis. The major advantage of VDM is its exactness and no need for matrix inversion in the reanalysis algorithm. The influence matrix-numerical core of the VDM-contains the whole mechanical knowledge about a structure, by looking at all global responses due to local disturbances. The strength of the method is demonstrated for truss structures.",10.1007/s00158-007-0158-7,https://dx.doi.org/10.1007/s00158-007-0158-7
"Wiebenga, JH | van den Boogaard, AH | Klaseboer, G",2012,Sequential robust optimization of a V-bending process using numerical simulations,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,19,3.17,"The coupling of finite element simulations to mathematical optimization techniques has contributed significantly to product improvements and cost reductions in the metal forming industries. The next challenge is to bridge the gap between deterministic optimization techniques and the industrial need for robustness. This paper introduces a generally applicable strategy for modeling and efficiently solving robust optimization problems based on time consuming simulations. Noise variables and their effect on the responses are taken into account explicitly. The robust optimization strategy consists of four main stages: modeling, sensitivity analysis, robust optimization and sequential robust optimization. Use is made of a metamodel-based optimization approach to couple the computationally expensive finite element simulations with the robust optimization procedure. The initial metamodel approximation will only serve to find a first estimate of the robust optimum. Sequential optimization steps are subsequently applied to efficiently increase the accuracy of the response prediction at regions of interest containing the optimal robust design. The applicability of the proposed robust optimization strategy is demonstrated by the sequential robust optimization of an analytical test function and an industrial V-bending process. For the industrial application, several production trial runs have been performed to investigate and validate the robustness of the production process. For both applications, it is shown that the robust optimization strategy accounts for the effect of different sources of uncertainty onto the process responses in a very efficient manner. Moreover, application of the methodology to the industrial V-bending process results in valuable process insights and an improved robust process design.",10.1007/s00158-012-0761-0,https://dx.doi.org/10.1007/s00158-012-0761-0
"Bang, Y | Abdel-Khalik, HS | Hite, JM",2012,Hybrid reduced order modeling applied to nonlinear models,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,19,3.17,"Reduced order modeling plays an indispensible role for most real-world complex models. The objective of this manuscript is to hybridize local and global sensitivity analysis methods to enable the application of reduced order modeling to complex nonlinear models, often encountered in real system design and analysis calculations, for example, nuclear reactors. This is achieved by first employing local variational methods to identify important nonlinear features of the original model that are required to reach a user-defined accuracy for the reduced model. This information is obtained by sampling local first-order derivatives of a pseudoresponse utilizing a modified representation of an infinite series expansion around some reference point. The resulting derivative information is aggregated in a subspace of dimension much less than the dimension of the input parameter space. The accuracy of the reduced model can be mathematically quantified using a bounding norm. Next, global sensitivity methods are employed to exhaustively search the reduced subspace for sensitivity information. The theory and implementation details of the proposed method are exposed in this manuscript. Numerical tests based on prototype nonlinear functions and radiation transport models with many input parameters and many responses are conducted as proof of principle.",10.1002/nme.4298,https://dx.doi.org/10.1002/nme.4298
"Ogee, J | Brunet, Y",2002,A forest floor model for heat and moisture including a litter layer,Applications_JOURNAL OF HYDROLOGY,50,3.12,"Forest soils are often covered with a litter that influences the rate of mass and energy transfer between the soil and the air above, thereby modifying the temperature and moisture fields in the soil. The presence of a litter should therefore be accounted for in forest SVAT models, especially when long-term simulations are to be performed. A heat and moisture litter model has been developed by adding two dynamical equations to a force-restore type soil model. The experimental data used for the model validation was collected in a pine forest canopy in the South-West of France, that was part of the Euroflux network. The model is tested and validated over a two-year period. It is shown to provide a fairly good simulation of soil and litter moisture, soil and litter temperature and turbulent fluxes measured above the forest floor. It is also shown that simulations without the litter layer are unable to reproduce all these variables simultaneously. We then perform a sensitivity analysis to the parameters whose values are either uncertain or likely to be variable in time and space, such as the litter thickness, the rainfall fraction intercepted by the litter or the maximum value of the surface resistance. A threshold value of the litter moisture used in the surface resistance parameterisation turns out to be the most critical parameter. Further work is needed to investigate the possible relationships between the various parameters describing the litter, but the present litter model can already be used in combination with other forest SVAT models.",10.1016/S0022-1694(01)00515-7,https://dx.doi.org/10.1016/S0022-1694(01)00515-7
"Allen, M | Maute, K",2004,Reliability-based design optimization of aeroelastic structures,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,43,3.07,"Aeroelastic phenomena are most often either ignored or roughly approximated when uncertainties are considered in the design optimization process of structures subject to aerodynamic loading, affecting the quality of the optimization results. Therefore, a design methodology is proposed that combines reliability-based design optimization and high-fidelity aeroelastic simulations for the analysis and design of aeroelastic structures. To account for uncertainties in design and operating conditions, a first-order reliability method (FORM) is employed to approximate the system reliability. To limit model uncertainties while accounting for the effects of given uncertainties, a high-fidelity nonlinear aeroelastic simulation method is used. The structure is modelled by a finite element method, and the aerodynamic loads are predicted by a finite volume discretization of a nonlinear Euler flow. The usefulness of the employed reliability analysis in both describing the effects of uncertainties on a particular design and as a design tool in the optimization process is illustrated. Though computationally more expensive than a deterministic optimum, due to the necessity of solving additional optimization problems for reliability analysis within each step of the broader design optimization procedure, a reliability-based optimum is shown to be an improved design. Conventional deterministic aeroelastic tailoring, which exploits the aeroelastic nature of the structure to enhance performance, is shown to often produce designs that are sensitive to variations in system or operational parameters.",10.1007/s00158-004-0384-1,https://dx.doi.org/10.1007/s00158-004-0384-1
"Fujihara, Y | Takase, K | Chono, S | Ichion, E | Ogura, A | Tanaka, K",2017,Influence of topography and forest characteristics on snow distributions in a forested catchment,Applications_JOURNAL OF HYDROLOGY,3,3.0,"Stored water within snowpack is important for the hydrological balance in many mountainous environments around the world. However, monitoring the spatial and temporal dynamics of snow in such mountainous environments remains rather challenging. We therefore developed a snow depth meter using small temperature loggers. Small temperature loggers were attached to poles at  cm intervals from the ground surface. Snow depths were estimated by assessing the daily variations in temperatures. Using this snow depth meter, we continuously observed snow depths at  stations in a forested catchment in Japan over three winter seasons. Using correlation analysis, we then analyzed the influence of topography (i.e., elevation and aspect) and forest (i.e., canopy openness) on snow depths. Moreover, we estimated daily snow distributions in the area using multi-regression analysis, thus describing seasonal characteristics of snow distributions. Finally we investigated the relation between number of stations and estimation accuracies of snow distributions using a Monte Carlo sensitivity analysis. We observed that the influence of topographical and forest characteristics changed considerably during the study period, with elevation having a major impact on snow depths. Further, aspect and forest cover had a great influence on the snow depths during the melting period. The regression of elevation slopes was .-. mm/m during rich snow years and .-. mm/m in little snow years. Also, the snow distribution during the melting period was found to be less uniform than during the snow accumulation period using histograms of snow depths. Monte Carlo sensitivity analysis shows that one station per .-. ha is enough to estimate accurate snow distributions. Given the above, we concluded that our proposed approach was quite useful for investigating the influence of topography and forest characteristics on snow accumulation and melting.",10.1016/j.jhydrol.2017.01.021,https://dx.doi.org/10.1016/j.jhydrol.2017.01.021
"Soto, MG | Adeli, H",2017,Many-objective control optimization of high-rise building structures using replicator dynamics and neural dynamics model,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,3,3.0,"Recently the authors presented a single-agent Centralized Replicator Controller (CRC) and a decentralized Multi-Agent Replicator Controller (MARC) for vibration control of high-rise building structures. It was shown that the use of agents and a decentralized approach enhances the vibration control system. Two key parameters in the proposed control methodologies using replicator dynamics are the total population (total available resources or the sum of actuators forces) and the growth rate. In the previous research, a sensitivity analysis was performed to determine the appropriate values for the population size and growth rate. In this paper, the aforementioned control methodologies are integrated with a multi-objective optimization algorithm in order to find Pareto optimal values for growth rates with the goal of achieving maximum structural performance with minimum energy consumption. A modified neural dynamic model of Adeli and Park is used in this research to solve the many-objective optimization problem where the Normal Boundary Intersection method is employed to find Pareto optimality. Sample results are presented using a -story steel benchmark structure subjected to historical and artificial accelerograms.",10.1007/s00158-017-1835-9,https://dx.doi.org/10.1007/s00158-017-1835-9
"Yoo, D | Lee, I",2014,Sampling-based approach for design optimization in the presence of interval variables,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,12,3.0,"This paper proposes a methodology for sampling-based design optimization in the presence of interval variables. Assuming that an accurate surrogate model is available, the proposed method first searches the worst combination of interval variables for constraints when only interval variables are present or for probabilistic constraints when both interval and random variables are present. Due to the fact that the worst combination of interval variables for probability of failure does not always coincide with that for a performance function, the proposed method directly uses the probability of failure to obtain the worst combination of interval variables when both interval and random variables are present. To calculate sensitivities of the constraints and probabilistic constraints with respect to interval variables by the sampling-based method, behavior of interval variables at the worst case is defined by the Dirac delta function. Then, Monte Carlo simulation is applied to calculate the constraints and probabilistic constraints with the worst combination of interval variables, and their sensitivities. A merit of using an MCS-based approach in the X-space is that it does not require gradients of performance functions and transformation from X-space to U-space for reliability analysis, thus there is no approximation or restriction in calculating sensitivities of constraints or probabilistic constraints. Numerical results indicate that the proposed method can search the worst case probability of failure with both efficiency and accuracy and that it can perform design optimization with mixture of random and interval variables by utilizing the worst case probability of failure search.",10.1007/s00158-013-0969-7,https://dx.doi.org/10.1007/s00158-013-0969-7
"Witteveen, JAS | Iaccarino, G",2013,Simplex stochastic collocation with ENO-type stencil selection for robust uncertainty quantification,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,15,3.0,"Multi-element uncertainty quantification approaches can robustly resolve the high sensitivities caused by discontinuities in parametric space by reducing the polynomial degree locally to a piecewise linear approximation. It is important to extend the higher degree interpolation in the smooth regions up to a thin layer of linear elements that contain the discontinuity to maintain a highly accurate solution. This is achieved here by introducing Essentially Non-Oscillatory (ENO) type stencil selection into the Simplex Stochastic Collocation (SSC) method. For each simplex in the discretization of the parametric space, the stencil with the highest polynomial degree is selected from the set of candidate stencils to construct the local response surface approximation. The application of the resulting SSC-ENO method to a discontinuous test function shows a sharper resolution of the jumps and a higher order approximation of the percentiles near the singularity. SSC-ENO is also applied to a chemical model problem and a shock tube problem to study the impact of uncertainty both on the formation of discontinuities in time and on the location of discontinuities in space.",10.1016/j.jcp.2012.12.030,https://dx.doi.org/10.1016/j.jcp.2012.12.030
"Duan, ZY | Yan, J | Zhao, GZ",2015,Integrated optimization of the material and structure of composites based on the Heaviside penalization of discrete material model,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,9,3.0,"Based on discrete material optimization and topology optimization technologies, this paper discusses the problem of integrated optimization design of the material and structure of fiber-reinforced composites by considering the characteristics of the discrete variable of fiber ply angle because of the manufacture requirements. An optimization model based on the minimum structural compliance with a specified composite volume constraint is established. The ply angle and the distribution of the composite material are introduced as independent variables in two geometric scales (material and structural scales). The void material is added into the optional discrete material set to realize the topology change of the structure. This paper proposes an improved HPDMO (Heaviside Penalization of Discrete Material Optimization) model to obtain a better convergent result, and an explicit sensitivity analysis is performed. The effects of the HPDMO model on the convergence rate of the optimization results, the objective function value and the iteration history are studied and compared with those from the classical Discrete Material Optimization model and the Continuous Discrete Material Optimization model in this paper. Numerical examples in this paper show that the HPDMO model can effectively achieve the integrated optimization of the fiber ply angle and its distribution in the structural domain, and can also considerably improve the convergence rate of the optimal results compared with other DMO models. This model will help to reduce the manufacture cost of the optimal design.",10.1007/s00158-014-1168-x,https://dx.doi.org/10.1007/s00158-014-1168-x
"Ganis, B | Klie, H | Wheeler, MF | Wildey, T | Yotov, I | Zhang, DX",2008,Stochastic collocation and mixed finite elements for flow in porous media,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,30,3.0,"The aim of this paper is to quantify uncertainty of flow in porous media through stochastic modeling and computation of statistical moments. The governing equations are based on Darcy's law with stochastic permeability. Starting from a specified covariance relationship, the log permeability is decomposed using a truncated Karhunen-Loeve expansion. Mixed finite element approximations are used in the spatial domain and collocation at the zeros of tensor product Hermite polynomials is used in the stochastic dimensions. Error analysis is performed and experimentally verified with numerical simulations. Computational results include incompressible and slightly compressible single and two-phase flow.",10.1016/j.cma.2008.03.025,https://dx.doi.org/10.1016/j.cma.2008.03.025
"Paris, J | Navarrina, F | Colominas, I | Casteleiro, M",2010,Stress constraints sensitivity analysis in structural topology optimization,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,24,3.0,"Sensitivity Analysis is an essential issue in the structural optimization field. The calculation of the derivatives of the most relevant quantities (displacements, stresses, strains) in optimum design of structures allows to estimate the structural response when changes in the design variables are introduced. This essential information is used by the most frequent conventional optimization algorithms (SLP, MMA, Feasible directions) in order to reach the optimal solution. According to this idea, the Sensitivity Analysis of the stress constraints in Topology Optimization problems is a crucial aspect to obtain the optimal solution when stress constraints are considered. Maximum stiffness approaches usually involve one linear constraint and one non-linear objective function. Thus, the computation of the required sensitivity analysis does not mean a crucial limitation. However, in the topology optimization problem with stress constraints, efficient and accurate computation of the derivatives is needed in order to reach appropriate optimal solutions. In this paper, a complete analytic and efficient procedure to obtain the Sensitivity Analysis of the stress constraints in topology optimization of continuum structures is analyzed. First order derivatives and second order directional derivatives of the stress constraints are analyzed and included in the optimization procedure. In addition, topology optimization problems usually involve thousands of design variables and constraints. Thus, an efficient implementation of the algorithms used in the computation of the Sensitivity Analysis is developed in order to reduce the computational cost required. Finally, the sensitivity analysis techniques presented in this paper are tested by solving some application examples.",10.1016/j.cma.2010.03.010,https://dx.doi.org/10.1016/j.cma.2010.03.010
"Conte, JP | Barbato, M | Spacone, E",2004,Finite element response sensitivity analysis using force-based frame models,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,42,3.0,"This paper presents a method to compute consistent response sensitivities of force-based finite element models of structural frame systems to both material constitutive and discrete loading parameters. It has been shown that force-based frame elements are superior to classical displacement-based elements in the sense that they enable, at no significant additional costs, a drastic reduction in the number of elements required for a given level of accuracy in the computed response of the finite element model. This advantage of force-based elements is of even more interest in structural reliability analysis, which requires accurate and efficient computation of structural response and structural response sensitivities. This paper focuses on material non-linearities in the context of both static and dynamic response analysis. The formulation presented herein assumes the use of a general-purpose non-linear finite element analysis program based on the direct stiffness method. It is based on the general so-called direct differentiation method (DDM) for computing response sensitivities. The complete analytical formulation is presented at the element level and details are provided about its implementation in a general-purpose finite element analysis program. The new formulation and its implementation are validated through some application examples, in which analytical response sensitivities are compared with their counterparts obtained using forward finite difference (FFD) analysis. The force-based finite element methodology augmented with the developed procedure for analytical response sensitivity computation offers a powerful general tool for structural response sensitivity analysis.",10.1002/nme.994,https://dx.doi.org/10.1002/nme.994
"Watts, S | Tortorelli, DA",2017,A geometric projection method for designing three-dimensional open lattices with inverse homogenization,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,3,3.0,"Topology optimization is a methodology for assigning material or void to each point in a design domain in a way that extremizes some objective function, such as the compliance of a structure under given loads, subject to various imposed constraints, such as an upper bound on the mass of the structure. Geometry projection is a means to parameterize the topology optimization problem, by describing the design in a way that is independent of the mesh used for analysis of the design's performance; it results in many fewer design parameters, necessarily resolves the ill-posed nature of the topology optimization problem, and provides sharp descriptions of the material interfaces. We extend previous geometric projection work to  dimensions and design unit cells for lattice materials using inverse homogenization. We perform a sensitivity analysis of the geometric projection and show it has smooth derivatives, making it suitable for use with gradient-based optimization algorithms. The technique is demonstrated by designing unit cells comprised of a single constituent material plus void space to obtain light, stiff materials with cubic and isotropic material symmetry. We also design a single-constituent isotropic material with negative Poisson's ratio and a light, stiff material comprised of  constituent solids plus void space.",10.1002/nme.5569,https://dx.doi.org/10.1002/nme.5569
"Koo, B | Yoon, M | Cho, S",2013,Isogeometric shape design sensitivity analysis using transformed basis functions for Kronecker delta property,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,15,3.0,"The isogeometric shape design sensitivity analysis (DSA) includes the desirable features; easy design parameterization and accurate shape sensitivity embedding the higher-order geometric information of curvature and normal vector. Due to the non-interpolatory property of NURBS basis, however, the imposition of essential boundary condition is not so straightforward in the isogeometric method. Taking advantages of geometrically exact property, an isogeometric DSA method is developed applying a mixed transformation to handle the boundary condition. A set of control point and NURBS basis function is added using the h-refinement and Newton iterations to precisely locate the control point to impose the boundary condition. In spite of additional transformation, its computation cost is comparable to the original one with penalty approach since the obtained Kronecker delta property enables to reduce the size of system matrix. Through demonstrative numerical examples, the effectiveness, accuracy, and computing cost of the developed DSA method are discussed.",10.1016/j.cma.2012.08.014,https://dx.doi.org/10.1016/j.cma.2012.08.014
"Wang, YQ | Ma, JZ | Guan, HD | Zhu, GF",2017,Determination of the saturated film conductivity to improve the EMFX model in describing the soil hydraulic properties over the entire moisture range,Applications_JOURNAL OF HYDROLOGY,3,3.0,"Difficulty in measuring hydraulic conductivity, particularly under dry conditions, calls for methods of predicting the conductivity from easily obtained soil properties. As a complement to the recently published EMFX model, a method based on two specific suction conditions is proposed to estimate saturated film conductivity from the soil water retention curve. This method reduces one fitting parameter in the previous EMFX model, making it possible to predict the hydraulic conductivity from the soil water retention curve over the complete moisture range. Model performance is evaluated with published data of soils in a broad texture range from sand to clay. The testing results indicate that ) the modified EMFX model (namely the EMFX-K model), incorporating both capillary and adsorption forces, provides good agreement with the conductivity data over the entire moisture range; ) a value of . for the tortuosity factor in the EMFX-K model as that in the Mualem's model gives comparable estimation of the relative conductivity associated with the capillary force; and ) a value of . x (-) J for the Hamaker constant, rather than the commonly used value of . x (-) J, appears to be more appropriate to represent solely the effect of the van der Waals forces and to predict the film conductivity. In comparison with the commonly used van Genuchten-Mualem model, the EMFX-K model significantly improves the prediction of hydraulic conductivity under dry conditions. The sensitivity analysis result suggests that the uncertainty in the film thickness estimation is important in explaining the model underestimation of hydraulic conductivity for the soils with fine texture, in addition to the uncertainties from the measurements and the model structure. High quality data that cover the complete moisture range for a variety of soil textures are required to further test the method.",10.1016/j.jhydrol.2017.03.063,https://dx.doi.org/10.1016/j.jhydrol.2017.03.063
"Liu, Y | Ren, LL | Hong, Y | Zhu, Y | Yang, XL | Yuan, F | Jiang, SH",2016,Sensitivity analysis of standardization procedures in drought indices to varied input data selections,Applications_JOURNAL OF HYDROLOGY,6,3.0,"Reasonable input data selection is of great significance for accurate computation of drought indices. In this study, a comprehensive comparison is conducted on the sensitivity of two commonly used standardization procedures (SP) in drought indices to datasets, namely the probability distribution based SP and the self-calibrating Palmer SP. The standardized Palmer drought index (SPDI) and the self-calibrating Palmer drought severity index (SC-PDSI) are selected as representatives of the two SPs, respectively. Using meteorological observations (-) in the Yellow River basin,  sub-datasets with a length of  years are firstly generated with the moving window method. Then we use the whole time series and  sub-datasets to compute two indices separately, and compare their spatiotemporal differences, as well as performances in capturing drought areas. Finally, a systematic investigation in term of changing climatic conditions and varied parameters in each SP is conducted. Results show that SPDI is less sensitive to data selection than SC-PDSI. SPDI series derived from different datasets are highly correlated, and consistent in drought area characterization. Sensitivity analysis shows that among the three parameters in the generalized extreme value (GEV) distribution, SPDI is most sensitive to changes in the scale parameter, followed by location and shape parameters. For SC-PDSI, its inconsistent behaviors among different datasets are primarily induced by the self-calibrated duration factors (p and q). In addition, it is found that the introduction of the self-calibrating procedure for duration factors further aggravates the dependence of drought index on input datasets compared with original empirical algorithm that Palmer uses, making SC-PDSI more sensitive to variations in data sample. This study clearly demonstrate the impacts of dataset selection on sensitivity of drought index computation, which has significant implications for proper usage of drought indices and related assessments, and potentially provide some valuable references for future researches on drought indices improvements.",10.1016/j.jhydrol.2016.04.073,https://dx.doi.org/10.1016/j.jhydrol.2016.04.073
"Sandhu, R | Poirel, D | Pettit, C | Khalil, M | Sarkar, A",2016,Bayesian inference of nonlinear unsteady aerodynamics from aeroelastic limit cycle oscillations,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,6,3.0,"A Bayesian model selection and parameter estimation algorithm is applied to investigate the influence of nonlinear and unsteady aerodynamic loads on the limit cycle oscillation (LCO) of a pitching airfoil in the transitional Reynolds number regime. At small angles of attack, laminar boundary layer trailing edge separation causes negative aerodynamic damping leading to the LCO. The fluid-structure interaction of the rigid, but elastically mounted, airfoil and nonlinear unsteady aerodynamics is represented by two coupled nonlinear stochastic ordinary differential equations containing uncertain parameters and model approximation errors. Several plausible aerodynamic models with increasing complexity are proposed to describe the aeroelastic system leading to LCO. The likelihood in the posterior parameter probability density function (pdf) is available semi-analytically using the extended Kalman filter for the state estimation of the coupled nonlinear structural and unsteady aerodynamic model. The posterior parameter pdf is sampled using a parallel and adaptive Markov Chain Monte Carlo (MCMC) algorithm. The posterior probability of each model is estimated using the Chib-Jeliazkov method that directly uses the posterior MCMC samples for evidence (marginal likelihood) computation. The Bayesian algorithm is validated through a numerical study and then applied to model the nonlinear unsteady aerodynamic loads using wind-tunnel test data at various Reynolds numbers.",10.1016/j.jcp.2016.03.006,https://dx.doi.org/10.1016/j.jcp.2016.03.006
"Miki, K | Panesi, M | Prudencio, EE | Prudhomme, S",2012,Probabilistic models and uncertainty quantification for the ionization reaction rate of atomic Nitrogen,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,18,3.0,"The objective in this paper is to analyze some stochastic models for estimating the ionization reaction rate constant of atomic Nitrogen (N + e(-) -> N+ + e(-)). Parameters of the models are identified by means of Bayesian inference using spatially resolved absolute radiance data obtained from the Electric Arc Shock Tube (EAST) wind-tunnel. The proposed methodology accounts for uncertainties in the model parameters as well as physical model inadequacies, providing estimates of the rate constant that reflect both types of uncertainties. We present four different probabilistic models by varying the error structure (either additive or multiplicative) and by choosing different descriptions of the statistical correlation among data points. In order to assess the validity of our methodology, we first present some calibration results obtained with manufactured data and then proceed by using experimental data collected at EAST experimental facility. In order to simulate the radiative signature emitted in the shock-heated air plasma, we use a one-dimensional flow solver with Park's two-temperature model that simulates non-equilibrium effects. We also discuss the implications of the choice of the stochastic model on the estimation of the reaction rate and its uncertainties. Our analysis shows that the stochastic models based on correlated multiplicative errors are the most plausible models among the four models proposed in this study. The rate of the atomic Nitrogen ionization is found to be (. +/- .) x () cm() mol(-) s(-) at , K.",10.1016/j.jcp.2012.01.005,https://dx.doi.org/10.1016/j.jcp.2012.01.005
"Tan, MHY | Geubelle, PH",2017,3D dimensionally reduced modeling and gradient-based optimization of microchannel cooling networks,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,3,3.0,"This paper presents a dimensionally reduced thermal model and gradient-based shape optimization scheme for the D computational design of actively cooled panels. A correction method previously used in wire-based electromagnetics is applied to address convergence issues associated with the singularity of the thermal solution along the microchannels. The numerical solution is obtained with the interface-enriched generalized finite element method (IGFEM), which greatly simplifies mesh generation by allowing for the use of a non-conforming mesh to capture the temperature gradient discontinuity across the microchannels. The temperature distribution calculated with the IGFEM on coarse meshes agrees with that obtained using significantly more complex and costly ANSYS FLUENT simulations. We then combine the IGFEM with a sensitivity analysis and the sequential quadratic programming algorithm in MATLAB to solve two sets of shape optimization problems related to actively cooled microvascular composite panels. These problems demonstrate a key advantage of the IGFEM in avoiding severe mesh distoition during shape optimization. Lastly, we present a semi-analytical model based on the concept of the zone of influence of a channel to estimate the maximum temperature of an actively cooled plate with straight embedded microchannels.",10.1016/j.cma.2017.05.024,https://dx.doi.org/10.1016/j.cma.2017.05.024
"McGill, JA | Ogunnaike, BA | Vlachos, DG",2012,Efficient gradient estimation using finite differencing and likelihood ratios for kinetic Monte Carlo simulations,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,18,3.0,"While many optimization and control methods for stochastic processes require gradient information from the process of interest, obtaining gradient information from experiments is prohibitively expensive and time-consuming. As a result, such information is often obtained from stochastic process simulations. Computing gradients efficiently and accurately from stochastic simulations is challenging, especially for simulations involving computationally expensive models with significant inherent noise. In this work, we analyze and characterize the applicability of two gradient estimation methods for kinetic Monte Carlo simulations: finite differencing and likelihood ratio. We developed a systematic method for choosing an optimal perturbation size for finite differencing and discuss, for both methods, important implementation issues such as scaling with respect to the number of elements in the gradient vector. Through a series of numerical experiments, the methods were compared across different time and size regimes to characterize the precision and accuracy associated with each method. We determined that the likelihood ratio method is appropriate for estimating gradients at short (transient) times or for systems with small population sizes, whereas finite differencing is better-suited for gradient estimation at long times (steady state) or for systems with large population sizes.",10.1016/j.jcp.2012.06.037,https://dx.doi.org/10.1016/j.jcp.2012.06.037
"Xiao, H | Wang, JX | Ghanem, RG",2017,A random matrix approach for quantifying model-form uncertainties in turbulence modeling,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,3,3.0,"With the ever-increasing use of Reynolds-Averaged Navier Stokes (RANS) simulations in mission-critical applications, the quantification of model-form uncertainty in RANS models has attracted attention in the turbulence modeling community. Recently, a physics-based nonparametric approach for quantifying model-form uncertainty in RANS simulations has been proposed, where Reynolds stresses are projected to physically meaningful dimensions and perturbations are introduced only in the physically realizable limits (Xiao et al., ). However, a challenge associated with this approach is to assess the amount of information introduced in the prior distribution and to avoid imposing unwarranted constraints. In this work we propose a random matrix approach for quantifying model-form uncertainties in RANS simulations with the realizability of the Reynolds stress guaranteed, which is achieved by construction from the Cholesky factorization of the normalized Reynolds stress tensor. Furthermore, the maximum entropy principle is used to identify the probability distribution that satisfies the constraints from available information but without introducing artificial constraints. We demonstrate that the proposed approach is able to ensure the realizability of the Reynolds stress, albeit in a different manner from the physics-based approach. Monte Carlo sampling of the obtained probability distribution is achieved by using polynomial chaos expansion to map independent Gaussian random fields to the Reynolds stress random field with the marginal distributions and correlation structures as specified. Numerical simulations on a typical flow with separation have shown physically reasonable results, which verify the proposed approach. Therefore, the proposed method is a promising alternative to the physics-based approach for model-form uncertainty quantification of RANS simulations. The method explored in this work is general and can be extended to other complex physical systems in applied mechanics and engineering.",10.1016/j.cma.2016.10.025,https://dx.doi.org/10.1016/j.cma.2016.10.025
"Tavakoli, R",2016,Optimal design of multiphase composites under elastodynamic loading,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,6,3.0,"An algorithm is proposed to optimize the performance of multiphase structures (composites) under elastodynamic loading conditions. The goal is to determine the distribution of material in the structure such that the time-averaged total stored energy of structure is minimized. A penalization strategy is suggested to avoid the checkerboard instability, simultaneously to generate near - topologies. As a result of this strategy, the solutions of presented algorithm are sufficiently smooth and possess the regularity of H- function space. A simple method for the continuum adjoint sensitivity analysis of the corresponding PDE-constrained optimization problem is presented. It is general and can be easily applied to a wide range of alternative problems. The success of the introduced algorithm is studied by numerical experiments on two-dimensional model problems for different numbers of phases. According to numerical results, the objective functional is reduced monotonically with iterations. Moreover, the final topologies at the optimal solutions are near -. The dynamic behavior of optimal designs is compared to that of initial ones to show the impact of optimization on the performance of structures.",10.1016/j.cma.2015.11.026,https://dx.doi.org/10.1016/j.cma.2015.11.026
"Munk, DJ | Vio, GA | Steven, GP",2017,A simple alternative formulation for structural optimisation with dynamic and buckling objectives,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,3,3.0,"Structural topology optimisation has mainly been applied to strength and stiffness objectives, due to the ease of calculating the sensitivities for such problems. In contrast, dynamic and buckling objectives require time consuming central difference schemes, or inefficient non-gradient algorithms, for calculation of the sensitivities. Further, soft-kill algorithms suffer from numerous numerical issues, such as localised artificial modes and mode switching. This has resulted in little focus on structural topology optimisation for dynamic and buckling objectives. In this work it is found that nominal stress contours can be derived from applying the vibration and buckling mode shapes as displacement fields, defined as the dynamic and buckling von Mises stress, respectively. This paper shows that there is an equivalence between the dynamic von Mises stress and the frequency sensitivity numbers for element removal and addition in bidirectional evolutionary structural optimisation. Likewise, it was found that the contours of buckling von Mises stress and buckling sensitivity numbers are analogous; therefore, an equivalence is shown for element removal and addition. The examples demonstrate consistent resulting topologies from the two different formulations for both dynamic and buckling criteria. This article aims to develop a simple alternative, based on visual correlation with a mathematical verification, for topology optimisation with dynamic and buckling criteria.",10.1007/s00158-016-1544-9,https://dx.doi.org/10.1007/s00158-016-1544-9
"Ostfeld, A | Salomons, S",2005,A hybrid genetic - instance based learning algorithm for CE-QUAL-W2 calibration,Applications_JOURNAL OF HYDROLOGY,39,3.0,"This paper presents a calibration model for CE-QUAL-W. CE-QUAL-W is a two-dimensional (D) longitudinal/vertical hydrodynamic and water quality model for surface water bodies, modeling eutrophication processes such as temperature-nutrient-algae-dissolved oxygen-organic matter and sediment relationships. The proposed methodology is a combination of a 'hurdle-race' and a hybrid Genetic-k-Nearest Neighbor algorithm (GA-kNN). The 'hurdle race' is formulated for accepting-rejecting a proposed set of parameters during a CE-QUAL-W simulation; the k-Nearest Neighbor algorithm (kNN)-for approximating the objective function response surface; and the Genetic Algorithm (GA)-for linking both. The proposed methodology overcomes the high, non-applicable, computational efforts required if a conventional calibration search technique was used, while retaining the quality of the final calibration results. Base runs and sensitivity analysis are demonstrated on two example applications: a synthetic hypothetical example calibrated for temperature, serving for tuning the GA-kNN parameters; and the Lower Columbia Slough case study in Oregon US calibrated for temperature and dissolved oxygen. The GA-kNN algorithm was found to be robust and reliable, producing similar results to those of a pure GA, while reducing running times and computational efforts significantly, and adding additional insights and flexibilities to the calibration process.",10.1016/j.jhydrol.2004.12.004,https://dx.doi.org/10.1016/j.jhydrol.2004.12.004
"Wang, H | Chen, LM | Ye, F | Chen, L",2017,Global sensitivity analysis for fiber reinforced composite fiber path based on D-MORPH-HDMR algorithm,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,3,3.0,"This study presents a quantitative sensitivity analysis for the assessment of fiber reinforced composites (FRCs). Global sensitivity analysis (GSA) approach is based on the variance based method incorporating Random Sampling-High Dimensional Model Representation (RS-HDMR) expansion in which component functions are determined by diffeomorphic modulation under observable response preserving homotopy (D-MORPH) regression. The advantage of the D-MORPH regression lies in its capability to solve linear algebraic equations with a limited number of sample points. The main purpose is to investigate the influence of fiber path, regarded as the design variable, on the formability and structural performance of FRCs. Wherein, spring-back and load-carrying capacity are two meaningful problems to be addressed. Two typical FRCs are included that an L-shaped part with straight fiber path using autoclave manufacturing process and a variable stiffness composite cylindrical shell under pure bending. The work not only focuses on the ranking of design variables but also hopes to find out their interactions represented by the second order global sensitivity indexes. After being tested by three typical numerical functions, the GSA algorithm highlights that spring-back of FRC using autoclave manufacturing process is most sensitive to fiber orientation angles on plies close to the tool. And buckling performance of the VS cylinder is dominated by fiber orientation angles at compression/tension regions.",10.1007/s00158-017-1681-9,https://dx.doi.org/10.1007/s00158-017-1681-9
"Elsheikh, AH | Wheeler, MF | Hoteit, I",2014,Hybrid nested sampling algorithm for Bayesian model selection applied to inverse subsurface flow problems,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,12,3.0,"A Hybrid Nested Sampling (HNS) algorithm is proposed for efficient Bayesian model calibration and prior model selection. The proposed algorithm combines, Nested Sampling (NS) algorithm, Hybrid Monte Carlo (HMC) sampling and gradient estimation using Stochastic Ensemble Method (SEM). NS is an efficient sampling algorithm that can be used for Bayesian calibration and estimating the Bayesian evidence for prior model selection. Nested sampling has the advantage of computational feasibility. Within the nested sampling algorithm, a constrained sampling step is performed. For this step, we utilize HMC to reduce the correlation between successive sampled states. HMC relies on the gradient of the logarithm of the posterior distribution, which we estimate using a stochastic ensemble method based on an ensemble of directional derivatives. SEM only requires forward model runs and the simulator is then used as a black box and no adjoint code is needed. The developed HNS algorithm is successfully applied for Bayesian calibration and prior model selection of several nonlinear subsurface flow problems.",10.1016/j.jcp.2013.10.001,https://dx.doi.org/10.1016/j.jcp.2013.10.001
"Ghosh, D | Avery, P | Farhat, C",2009,An FETI-preconditioned conjuerate gradient method for large-scale stochastic finite element problems,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,26,2.89,"In the spectral stochastic finite element method for analyzing an uncertain system. the uncertainty is represented by a set of random variables, and a quantity of Interest such as the system response is considered as a function of these random variables Consequently, the underlying Galerkin projection yields a block system of deterministic equations where the blocks are sparse but coupled. The solution of this algebraic system of equations becomes rapidly challenging when the size of the physical system and/or the level of uncertainty is increased This paper addresses this challenge by presenting a preconditioned conjugate gradient method for such block systems where the preconditioning step is based on the dual-primal finite element tearing and interconnecting method equipped with a Krylov subspace reusage technique for accelerating the iterative solution of systems with multiple and repeated right-hand sides. Preliminary performance results on a Linux Cluster suggest that the proposed Solution method is numerically scalable and demonstrate its potential for making the uncertainty quantification Of realistic systems tractable.",10.1002/nme.2595,https://dx.doi.org/10.1002/nme.2595
"Choi, JS | Yoo, J",2009,Simultaneous structural topology optimization of electromagnetic sources and ferromagnetic materials,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,26,2.89,"For the design of electric machines such as actuators, sensors and motors, the topology optimization for EM sources as well as ferromagnetic materials (FMs) is in great request. Generally, permanent magnets (PMs) and coils serve as electromagnetic (EM) sources in electric machines. However, most of researches in relation with topology optimization in electric machinery are focused mostly on the design of ferromagnetic materials such as yokes and stators. In this paper, the topology optimization methodology for the simultaneous design of PMs and FMs is developed and it is expanded to the design of FMs with coils. The proposed method makes it possible to obtain a clear topology with little gray density elements in the design domain and shows the stable convergence history. The proposed method is applied to the optimization problem of a C-core actuator and a magnetostrictive sensor. The sensitivity analysis and the update of design variables are accomplished by the adjoint variable method and the sequential linear programming, respectively.",10.1016/j.cma.2009.02.015,https://dx.doi.org/10.1016/j.cma.2009.02.015
"Xia, Q | Shi, TL | Wang, MY",2011,A level set based shape and topology optimization method for maximizing the simple or repeated first eigenvalue of structure vibration,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,20,2.86,"We present a level set based shape and topology optimization method for maximizing the simple or repeated first eigenvalue of structure vibration. Considering that a simple eigenvalue is Fr,chet differentiable with respect to the boundary of a structure but a repeated eigenvalue is only Gateaux or directionally differentiable, we take different approaches to derive the boundary variation that maximizes the first eigenvalue. In the case of simple eigenvalue, material derivative is obtained via adjoint method, and variation of boundary shape is specified according to the steepest descent method. In the case of N-fold repeated eigenvalue, variation of boundary shape is obtained as a result of a N-dimensional algebraic eigenvalue problem. Constraint of a structure's volume is dealt with via the augmented Lagrange multiplier method. Boundary variation is treated as an advection velocity in the Hamilton-Jacobi equation of the level set method for changing the shape and topology of a structure. The finite element analysis of eigenvalues of structure vibration is accomplished by using an Eulerian method that employs a fixed mesh and ersatz material. Application of the method is demonstrated by several numerical examples of optimizing D structures.",10.1007/s00158-010-0595-6,https://dx.doi.org/10.1007/s00158-010-0595-6
"Huang, XZ | Zhang, YM",2013,Reliability-sensitivity analysis using dimension reduction methods and saddlepoint approximations,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,14,2.8,"Reliabilitysensitivity, which is considered as an essential component in engineering design under uncertainty, is often of critical importance toward understanding the physical systems underlying failure and modifying the design to mitigate and manage risk. This paper presents a new computational tool for predicting reliability (failure probability) and reliabilitysensitivity of mechanical or structural systems subject to random uncertainties in loads, material properties, and geometry. The dimension reduction method is applied to compute response moments and their sensitivities with respect to the distribution parameters (e.g., shape and scale parameters, mean, and standard deviation) of basic random variables. Saddlepoint approximations with truncated cumulant generating functions are employed to estimate failure probability, probability density functions, and cumulative distribution functions. The rigorous analytic derivation of the parameter sensitivities of the failure probability with respect to the distribution parameters of basic random variables is derived. Results of six numerical examples involving hypothetical mathematical functions and solid mechanics problems indicate that the proposed approach provides accurate, convergent, and computationally efficient estimates of the failure probability and reliabilitysensitivity.",10.1002/nme.4412,https://dx.doi.org/10.1002/nme.4412
"Noori, R | Safavi, S | Shahrokni, SAN",2013,A reduced-order adaptive neuro-fuzzy inference system model as a software sensor for rapid estimation of five-day biochemical oxygen demand,Applications_JOURNAL OF HYDROLOGY,14,2.8,"The five-day biochemical oxygen demand (BOD) is one of the key parameters in water quality management. In this study, a novel approach, i.e., reduced-order adaptive neuro-fuzzy inference system (ROANFIS) model was developed for rapid estimation of BOD. In addition, an uncertainty analysis of adaptive neuro-fuzzy inference system (ANFIS) and ROANFIS models was carried out based on Monte-Carlo simulation. Accuracy analysis of ANFIS and ROANFIS models based on both developed discrepancy ratio and threshold statistics revealed that the selected ROANFIS model was superior. Pearson correlation coefficient (R) and root mean square error for the best fitted ROANFIS model were . and ., respectively. Furthermore, uncertainty analysis of the developed models indicated that the selected ROANFIS had less uncertainty than the ANFIS model and accurately forecasted BOD in the Sefidrood River Basin. Besides, the uncertainty analysis also showed that bracketed predictions by % confidence bound and d-factor in the testing steps for the selected ROANFIS model were % and ., respectively.",10.1016/j.jhydrol.2013.04.052,https://dx.doi.org/10.1016/j.jhydrol.2013.04.052
"Park, BU | Seo, YD | Sigmund, O | Youn, SK",2013,Shape optimization of the stokes flow problem based on isogeometric analysis,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,14,2.8,"Design-dependent loads related to boundary shape, such as pressure and convection loads, have been a challenging issue in optimization. Isogeometric analysis, where the analysis model has smooth boundaries described by spline functions can handle design-dependent loads with ease. In the present study, shape optimization based on isogeometric analysis is applied to the Stokes flow problems such as minimizing energy dissipation and drag force. The drag force objective is based on accurate integration of boundary pressures. Local control point insertion schemes are employed for accurate representation of geometry in an adaptive manner.",10.1007/s00158-013-0939-0,https://dx.doi.org/10.1007/s00158-013-0939-0
"Takezawa, A | Kitamura, M | Vatanabe, SL | Silva, ECN",2014,Design methodology of piezoelectric energy-harvesting skin using topology optimization,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,11,2.75,"This paper describes a design methodology for piezoelectric energy harvesters that thinly encapsulate the mechanical devices and exploit resonances from higher-order vibrational modes. The direction of polarization determines the sign of the piezoelectric tensor to avoid cancellations of electric fields from opposite polarizations in the same circuit. The resultant modified equations of state are solved by finite element method (FEM). Combining this method with the solid isotropic material with penalization (SIMP) method for piezoelectric material, we have developed an optimization methodology that optimizes the piezoelectric material layout and polarization direction. Updating the density function of the SIMP method is performed based on sensitivity analysis, the sequential linear programming on the early stage of the optimization, and the phase field method on the latter stage of the optimization to obtain clear optimal shapes without intermediate density. Numerical examples are provided that illustrate the validity and utility of the proposed method.",10.1007/s00158-013-0974-x,https://dx.doi.org/10.1007/s00158-013-0974-x
"Wei, P | Ma, HT | Wang, MY",2014,The stiffness spreading method for layout optimization of truss structures,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,11,2.75,"A novel optimization method, stiffness spreading method (SSM), is proposed for layout optimization of truss structures. In this method, stiffness matrices of the bar elements in a truss structure are represented by a set of equivalent stiffness matrices which are embedded in a weak background mesh. When the proposed method is used, it is unnecessary for the bar elements in a truss structure to be connected to each other during the optimization process, and each of the bar elements can move independently in the design domain to form an optimized design. Another feature of the method is that the sensitivity analysis can be done analytically, making gradient based optimization algorithms applicable in the solution. This method realizes the size, shape and topology design optimization of truss structures simultaneously and allows for more flexibility in topology change. Numerical examples illustrate the feasibility and effectiveness of the proposed method.",10.1007/s00158-013-1005-7,https://dx.doi.org/10.1007/s00158-013-1005-7
"Li, LS | Liu, JH | Liu, SH",2014,An efficient strategy for multidisciplinary reliability design and optimization based on CSSO and PMA in SORA framework,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,11,2.75,"The conventional reliability-based multidisciplinary design optimization (RBMDO) integrates the reliability-based design optimization and multidisciplinary design optimization (MDO) directly, which leads to a triple-level nested optimization loop. Especially, the multidisciplinary reliability analysis in the middle layer dominates the whole efficiency of RBMDO. To tackle this problem, first of all, a sequential multidisciplinary reliability analysis (SMRA) approach that integrates the concurrent subspace optimization (CSSO) strategy and the performance measure approach is proposed, in which the multidisciplinary analysis, system sensitivity analysis and reliability analysis are decoupled and arranged sequentially, making a recursive loop. The multidisciplinary analysis and system sensitivity analysis provide the value and gradient information of limit-state function for reliability analysis respectively. As a result, a great number of repeated iterations of the whole reliability analysis are eliminated. Secondly, the CSSO has been integrated with the sequential optimization and reliability assessment (SORA) to decouple the triple-level nested RBMDO procedures into a sequence of cycles of deterministic MDO and multidisciplinary reliability analysis. Therefore, the expensive computation of the whole reliability analysis model in each iteration of RBMDO is avoided. And also, the CSSO is adopted in the deterministic MDO to deal with medium-scale and coupled multidisciplinary systems. The procedures of the proposed approaches are presented in detail. The effectiveness of the proposed strategies is demonstrated and verified with two design examples.",10.1007/s00158-013-0966-x,https://dx.doi.org/10.1007/s00158-013-0966-x
"Subber, W | Sarkar, A",2014,A domain decomposition method of stochastic PDEs: An iterative solution techniques using a two-level scalable preconditioner,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,11,2.75,"Recent advances in high performance computing systems and sensing technologies motivate computational simulations with extremely high resolution models with capabilities to quantify uncertainties for credible numerical predictions. A two-level domain decomposition method is reported in this investigation to devise a linear solver for the large-scale system in the Galerkin spectral stochastic finite element method (SSFEM). In particular, a two-level scalable preconditioner is introduced in order to iteratively solve the large-scale linear system in the intrusive SSFEM using an iterative substructuring based domain decomposition solver. The implementation of the algorithm involves solving a local problem on each subdomain that constructs the local part of the preconditioner and a coarse problem that propagates information globally among the subdomains. The numerical and parallel scalabilities of the two-level preconditioner are contrasted with the previously developed one-level preconditioner for two-dimensional flow through porous media and elasticity problems with spatially varying non-Gaussian material properties. A distributed implementation of the parallel algorithm is carried out using MPI and PETSc parallel libraries. The scalabilities of the algorithm are investigated in a Linux cluster.",10.1016/j.jcp.2013.08.058,https://dx.doi.org/10.1016/j.jcp.2013.08.058
"Forsberg, J | Nilsson, L",2007,Topology optimization in crashworthiness design,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,30,2.73,"Topology optimization has developed rapidly, primarily with application on linear elastic structures subjected to static loadcases. In its basic form, an approximated optimization problem is formulated using analytical or semi-analytical methods to perform the sensitivity analysis. When an explicit finite element method is used to solve contact-impact problems, the sensitivities cannot easily be found. Hence, the engineer is forced to use numerical derivatives or other approaches. Since each finite element simulation of an impact problem may take days of computing time, the sensitivity-based methods are not a useful approach. Therefore, two alternative formulations for topology optimization are investigated in this work. The fundamental approach is to remove elements or, alternatively, change the element thicknesses based on the internal energy density distribution in the model. There is no automatic shift between the two methods within the existing algorithm. Within this formulation, it is possible to treat nonlinear effects, e.g., contact-impact and plasticity. Since no sensitivities are used, the updated design might be a step in the wrong direction for some finite elements. The load paths within the model will change if elements are removed or the element thicknesses are altered. Therefore, care should be taken with this procedure so that small steps are used, i.e., the change of the model should not be too large between two successive iterations and, therefore, the design parameters should not be altered too much. It is shown in this paper that the proposed method for topology optimization of a nonlinear problem gives similar result as a standard topology optimization procedures for the linear elastic case. Furthermore, the proposed procedures allow for topology optimization of nonlinear problems. The major restriction of the method is that responses in the optimization formulation must be coupled to the thickness updating procedure, e.g., constraint on a nodal displacement, acceleration level that is allowed.",10.1007/s00158-006-0040-z,https://dx.doi.org/10.1007/s00158-006-0040-z
"Geza, M | Poeter, EP | McCray, JE",2009,Quantifying predictive uncertainty for a mountain-watershed model,Applications_JOURNAL OF HYDROLOGY,24,2.67,"Watershed models require calibration before they are utilized as a decision-making tool. This paper describes a rigorous sensitivity analysis, automated parameter estimation and evaluation of prediction uncertainty for a Watershed Analysis Risk Management Framework (WARMF) model of the Turkey Creek Watershed. Sensitivity analysis was conducted using UCODE calibration and uncertainty-analysis software. Simulated stream flow is strongly sensitive to  of the  parameters evaluated: hydraulic conductivity, field capacity, total porosity, precipitation weighting factor, evaporation magnitude, evaporation skewness and snow melting rates; and parameter sensitivity is dependent on site-specific climate and soil conditions. Simulated stream flow matched observed stream flow fairly well with an R() value of ., Nash-Sutcliffe coefficient of efficiency (NSE) value of . and Root Mean Squared Error (RMSE) of . m()/s. The calibrated model was used to predict changes in stream flow that would result from changes in land use, including development of forested areas in parts of the watershed to commercial and residential areas. As expected, new development resulted in increased peak flows and reduced low flows. Uncertainty associated with all model parameters, including those not estimated by calibration by enhancing the parameter variance/covariance matrix, was considered when evaluating prediction uncertainties. Seventy percent of the time, predicted flows had uncertainties less than % with more of the uncertainty during low flow conditions.",10.1016/j.jhydrol.2009.07.025,https://dx.doi.org/10.1016/j.jhydrol.2009.07.025
"Lucas, LJ | Owhadi, H | Ortiz, M",2008,"Rigorous verification, validation, uncertainty quantification and certification through concentration-of-measure inequalities",Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,26,2.6,"We apply concentration-of-measure inequalities to the quantification of uncertainties in the performance of engineering systems. Specifically, we envision uncertainty quantification in the context of certification, i.e., as a tool for deciding whether a system is likely to perform safely and reliably within design specifications. We show that concentration-of-measure inequalities rigorously bound probabilities of failure and thus supply conservative certification criteria. In addition, they supply unambiguous quantitative definitions of terms such as margins, epistemic and aleatoric uncertainties, verification and validation measures, confidence factors, and others, as well as providing clear procedures for computing these quantities by means of concerted simulation and experimental campaigns. We also investigate numerically the tightness of concentration-of-measure inequalities with the aid of an imploding ring example. Our numerical tests establish the robustness and viability of concentration-of-measure inequalities as a basis for certification in that particular example of application.",10.1016/j.cma.2008.06.008,https://dx.doi.org/10.1016/j.cma.2008.06.008
"Zhao, CB | Hobbs, BE | Ord, A | Hornby, P | Peng, SL | Liu, LM",2007,Particle simulation of spontaneous crack generation problems in large-scale quasi-static systems,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,28,2.55,"To extend the application range of the distinct element method from a laboratory scale into a large scale such as a geological scale, we need to deal with an upscale issue associated with simulating spontaneous crack generation problems in large-scale quasi-static systems. Toward this direction, three important simulation issues, which may affect the quality of the particle simulation results of a quasi-static system, have been addressed in details in this paper. The first simulation issue is how to determine the particle-scale mechanical properties of a particle from the measured macroscopic mechanical properties of rocks. The second simulation issue is that the fictitious time, rather than the physical time, is used in the particle simulation of a quasi-static problem. The third simulation issue is that the conventional loading procedure used in the distinct element method is conceptually inaccurate, at least from the force propagation point of view. A new loading procedure is proposed to solve the conceptual problem resulting from the third simulation issue. The proposed loading procedure is comprised of two main types of periods, a loading period and a frozen period. Using the proposed loading procedure, the parameter selection problem stemming from the first issue can be somewhat solved. Since the second issue is an inherent one, it is strongly recommended that a particle-size sensitivity analysis of at least two different models, which have the same geometry but different smallest particle sizes, be carried out to confirm the particle simulation result of a large-scale quasi-static system. The related simulation results have demonstrated the usefulness and correctness of the proposed loading procedure for dealing with spontaneous crack generation problems in large-scale quasi-static geological systems.",10.1002/nme.1851,https://dx.doi.org/10.1002/nme.1851
"Pedersen, P | Pedersen, NL",2012,Interpolation/penalization applied for strength design of 3D thermoelastic structures,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,15,2.5,"With design independent loads and only a constrained volume (no local bounds), the same optimal design leads simultaneously to minimum compliance and maximum strength. However, for thermoelastic structures this is not the case and a maximum volume may not be an active constraint for minimum compliance. This is proved for thermoelastic structures by sensitivity analysis of compliance that facilitates localized determination of sensitivities, and the compliance is not identical to the total elastic energy (twice strain energy). An explicit formula for the difference is derived and numerically illustrated with examples. In compliance minimization for thermoelastic structures it may be advantageous to decrease the total volume, but for strength maximization it is argued to keep the total permissible volume. Linear interpolation (no penalization) may to a certain extent be argued for D thickness optimized designs, but for D design problems interpolation must be included and not only from the penalization point of view to obtain - designs. Three interpolation types are presented in a uniform manner, including the well known one parameter penalizations, named SIMP and RAMP. An alternative two parameter interpolation in explicit form is preferred, and the influence of interpolation on compliance sensitivity analysis is included. For direct strength maximization the sensitivity analysis of local von Mises stresses is demanding. An applied recursive procedure to obtain uniform energy density is presented in details, and it is shown by examples that the obtained designs are close to fulfilling also strength maximization. Explicit formulas for equivalent thermoelastic loads in D and D finite element analysis are derived and applied, including the sensitivity analysis.",10.1007/s00158-011-0755-3,https://dx.doi.org/10.1007/s00158-011-0755-3
"Liu, Y | Shi, Y | Zhou, Q | Xiu, RQ",2016,A sequential sampling strategy to improve the global fidelity of metamodels in multi-level system design,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,5,2.5,"In engineering design, complex systems that involve a multitude of decision variables and parameters are often decomposed into several submodels (also called subsystems and/or components) with a hierarchical (multi-level) manner to manage complexity. Metamodeling techniques are widely used to replace the original time-consuming computer simulation models to further reduce computational burden in multi-level system performance analysis and design optimization. However, due to the limited samples from simulation models, metamodels may contain metamodeling uncertainties at un-sampled sites. Such metamodeling uncertainties arising from metamodels across the entire hierarchy will propagate from the lower to upper levels and eventually impact the top-level response of interest. With the aim of improving the global fidelity of metamodels for multi-level system performance analysis and design optimization, a new sequential sampling strategy is proposed in this paper. The proposed method contains two basic elements: () quantifying metamodeling uncertainty propagated from the lower-level metamodels to the top-level response of interest and () seeking a new sample site at which the global fidelity of the multi-level system model can be maximally improved. As exemplified by the two numerical examples and a multi-scale bracket structure example, with the same amount of samples from computer simulation models, the new sequential sampling strategy is superior to existing sequential sampling strategies in terms of improving the global fidelity of the metamodels of multi-level systems.",10.1007/s00158-015-1379-9,https://dx.doi.org/10.1007/s00158-015-1379-9
"Beauheim, RL | Roberts, RM | Avis, JD",2014,"Hydraulic testing of low-permeability Silurian and Ordovician strata, Michigan Basin, southwestern Ontario",Applications_JOURNAL OF HYDROLOGY,10,2.5,"Straddle-packer hydraulic testing was performed in  Silurian intervals and  Ordovician intervals in six deep boreholes at the Bruce nuclear site, located near Tiverton, Ontario, as part of site-characterization activities for a proposed deep geologic repository (DGR) for low- and intermediate-level radioactive waste. The straddle-packer assembly incorporated a hydraulic piston to initiate in situ pulse tests within low hydraulic conductivity (<E- m/s) test intervals. Pressure transient data collected during the hydraulic tests were analyzed using the well-test simulator nSIGHTS to estimate the hydraulic properties specified as fitting parameters for the tested intervals, quantify parameter uncertainty, and define parameter correlations. Horizontal hydraulic conductivities of the Silurian test intervals range from approximately E- to E- m/s. The average horizontal hydraulic conductivities of the Ordovician intervals range from E- to E- m/s. The Lower Member of the Cobourg Formation, the proposed host formation of the DGR between  and  meters below ground surface, was found to have a horizontal hydraulic conductivity of E- to E- m/s. The formation pressures inferred from the hydraulic testing, confirmed by long-term monitoring, show that the Upper Ordovician and Middle Ordovician Trenton Group are significantly underpressured relative to a density-compensated hydrostatic condition and relative to the overlying Silurian strata and underlying Black River Group and Cambrian strata. These underpressures could not persist if hydraulic conductivities were not as low as those measured.",10.1016/j.jhydrol.2013.11.033,https://dx.doi.org/10.1016/j.jhydrol.2013.11.033
"MacDonald, RJ | Boon, S | Byrne, JM",2014,A process-based stream temperature modelling approach for mountain regions,Applications_JOURNAL OF HYDROLOGY,10,2.5,"Mountain streams have thermal regimes that provide critical habitat for native aquatic organisms. However, understanding stream temperature response to environmental change in mountain regions is difficult because there is typically a lack of observations. This work aims to address this issue by coupling two process-based models to simulate stream temperature in a groundwater-dominated mountain catchment, Alberta, Canada, and using a reach-scale field study for model development and verification. Results suggest that it is possible to produce spatial simulations of hydrometeorological variables needed for process-based stream temperature modelling. Simulated stream energy budget estimates compare well with results from field-based studies, and errors in stream temperature simulations (RMSE < .) are similar to other modelling studies, providing confidence in the methods developed. Model sensitivity analysis demonstrates the importance of incorporating meteorological, hydrological, and geomorphological controls on stream temperature in modelling studies. This study also demonstrates the current lack of process knowledge regarding in-stream ice cover and snowmelt effects on stream temperature, both of which can contribute substantially to stream thermal regimes. Future field-based and modelling studies should consider these processes in order to fully understand stream temperature response to environmental change.",10.1016/j.jhydrol.2014.02.009,https://dx.doi.org/10.1016/j.jhydrol.2014.02.009
"Haji-Ali, AL | Nobile, F | Tamellini, L | Tempone, R",2016,Multi-Index Stochastic Collocation for random PDEs,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,5,2.5,"In this work we introduce the Multi-Index Stochastic Collocation method (MISC) for computing statistics of the solution of a PDE with random data. MISC is a combination technique based on mixed differences of spatial approximations and quadratures over the space of random data. We propose an optimization procedure to select the most effective mixed differences to include in the MISC estimator: such optimization is a crucial step and allows us to build a method that, provided with sufficient solution regularity, is potentially more effective than other multi-level collocation methods already available in literature. We then provide a complexity analysis that assumes decay rates of product type for such mixed differences, showing that in the optimal case the convergence rate of MISC is only dictated by the convergence of the deterministic solver applied to a one dimensional problem. We show the effectiveness of MISC with some computational tests, comparing it with other related methods available in the literature, such as the Multi-Index and Multilevel Monte Carlo, Multilevel Stochastic Collocation, Quasi Optimal Stochastic Collocation and Sparse Composite Collocation methods.",10.1016/j.cma.2016.03.029,https://dx.doi.org/10.1016/j.cma.2016.03.029
"Yan, K | Cheng, GD | Wang, BP",2016,Adjoint methods of sensitivity analysis for Lyapunov equation,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,5,2.5,"The existing direct sensitivity analysis of optimal structural vibration control based on Lyapunov's second-method is computationally expensive when applied to finite element models with a large number of degree-of-freedom and design variables. A new adjoint sensitivity analysis method is proposed in this paper. Using the new method the sensitivity of the performance index, a time integral of a quadratic function of state variables, with respect to all design variables is calculated by solving two Lyapunov matrix equations. In consideration of computational cost reduction, the new adjoint method is further extended to the reduced order model by Guyan method. This makes the method applicable to large finite element models. Two numerical examples demonstrate the accuracy and efficiency of the proposed method.",10.1007/s00158-015-1323-z,https://dx.doi.org/10.1007/s00158-015-1323-z
"Le, TT | Guilleminot, J | Soize, C",2016,Stochastic continuum modeling of random interphases from atomistic simulations. Application to a polymer nanocomposite,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,5,2.5,"This paper is concerned with the probabilistic multiscale analysis of polymeric materials reinforced by nanoscopic fillers. More precisely, this work is devoted to the stochastic modeling and inverse identification of the random field associated with the elastic properties in the so-called interphase region. For illustration purposes, a prototypical polymer system reinforced by a Silica nanoscopic inclusion is considered. Molecular Dynamics (MD) simulations are first performed and used to characterize the conformational properties of the polymer chains in the neighborhood of the inclusion. It is shown that these chains are characterized by a specific tangential orientation which, together with the density profile and variations in chain mobility, allows for the geometric definition of the interphase region. Mechanical virtual testing is next completed on a set of initial configurations, hence providing a simulated database for model calibration. The results thus obtained are subsequently used to construct a random field model for the interphase stiffness. An inverse calibration procedure is finally proposed and relies on a stated equivalence between the apparent properties obtained from MD simulations and those computed by numerical homogenization in the continuum mechanics formulation. The interphase elasticity random field is seen to exhibit nonnegligible fluctuations, and the estimates of parameters related to spatial correlation are shown to be consistent with characteristic lengths of the atomistic model, such as the interphase thickness.",10.1016/j.cma.2015.10.006,https://dx.doi.org/10.1016/j.cma.2015.10.006
"Chowdhary, K | Najm, HN",2016,Bayesian estimation of Karhunen-Loeve expansions; A random subspace approach,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,5,2.5,"One of the most widely-used procedures for dimensionality reduction of high dimensional data is Principal Component Analysis (PCA). More broadly, low-dimensional stochastic representation of random fields with finite variance is provided via the well known Karhunen-Loeve expansion (KLE). The KLE is analogous to a Fourier series expansion for a random process, where the goal is to find an orthogonal transformation for the data such that the projection of the data onto this orthogonal subspace is optimal in the L- sense, i.e., which minimizes the mean square error. In practice, this orthogonal transformation is determined by performing an SVD (Singular Value Decomposition) on the sample covariance matrix or on the data matrix itself. Sampling error is typically ignored when quantifying the principal components, or, equivalently, basis functions of the KLE. Furthermore, it is exacerbated when the sample size is much smaller than the dimension of the random field. In this paper, we introduce a Bayesian KLE procedure, allowing one to obtain a probabilistic model on the principal components, which can account for inaccuracies due to limited sample size. The probabilistic model is built via Bayesian inference, from which the posterior becomes the matrix Bingham density over the space of orthonormal matrices. We use a modified Gibbs sampling procedure to sample on this space and then build probabilistic Karhunen-Loeve expansions over random subspaces to obtain a set of low-dimensional surrogates of the stochastic process. We illustrate this probabilistic procedure with a finite dimensional stochastic process inspired by Brownian motion.",10.1016/j.jcp.2016.02.056,https://dx.doi.org/10.1016/j.jcp.2016.02.056
"Tang, YT | Marshall, L | Sharma, A | Smith, T",2016,Tools for investigating the prior distribution in Bayesian hydrology,Applications_JOURNAL OF HYDROLOGY,5,2.5,"Bayesian inference is one of the most popular tools for uncertainty analysis in hydrological modeling. While much emphasis has been placed on the selection of appropriate likelihood functions within Bayesian hydrology, few researchers have evaluated the importance of the prior distribution in deriving appropriate posterior distributions. This paper describes tools for the evaluation of parameter sensitivity to the prior distribution to provide guidelines for defining meaningful priors. The tools described here consist of two measurements, the Kullback-Leibler Divergence (KLD) and the prior information elasticity. The Kullback-Leibler Divergence (KLD) is applied to calculate differences between the prior and posterior distributions for different cases. The prior information elasticity is then used to quantify the responsiveness of the KLD values to the change of prior distributions and length of available data. The tools are demonstrated via a Bayesian framework using an MCMC algorithm for a conceptual hydrologic model with both synthetic and real cases. The results of the application of this toolkit suggest the prior distribution can have a significant impact on the posterior distribution and should be more routinely assessed in hydrologic studies.",10.1016/j.jhydrol.2016.04.032,https://dx.doi.org/10.1016/j.jhydrol.2016.04.032
"Yoon, GH | Kim, YY",2007,Topology optimization of material-nonlinear continuum structures by the element connectivity parameterization,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,27,2.45,"The application of the element density-based topology optimization method to nonlinear continuum structures is limited to relatively simple problems such as bilinear elastoplastic material problems. Furthermore, it is very difficult to use analytic sensitivity when a commercial nonlinear finite element code is used. As an alternative to the element density formulation, the element connectivity parameterization (ECP) formulation is developed for the topology optimization of isotropic-hardening elastoplastic or hyperelastic continua by using commercial software. ECP varies the stiffness of zero-length linear elastic links that connect design domain-discretizing finite elements. Unloading was not considered. But the advantages of ECP in material-nonlinear problems were demonstrated: considerably simple analytic sensitivity calculation using a commercial code and simple link stiffness penalization regardless of nonlinear material behaviour.",10.1002/nme.1843,https://dx.doi.org/10.1002/nme.1843
"de Ruiter, MJ | van Keulen, F",2004,Topology optimization using a topology description function,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,34,2.43,"The topology description function (TDF) approach is a method for describing geometries in a discrete fashion, i.e. without intermediate densities. Hence, the TDF approach may be used to carry out topology optimization, i.e. to solve the material distribution problem. However, the material distribution problem may be ill-posed. This ill-posedness can be avoided by limiting the complexity of the design, which is accomplished automatically by limiting the number of design parameters used for the TDF. An important feature is that the TDF design description is entirely decoupled from a finite element (FE) model. The basic idea of the TDF approach is as follows. In the TDF approach, the design variables are parameters that determine a function on the so-called reference domain. Using a cut-off level, this function unambiguously determines a geometry. Then, the performance of this geometry is determined by a FE analysis. Several optimization techniques are applied to the TDF approach to carry out topology optimization. First, a genetic algorithm is applied, with (too) large computational costs. The TDF approach is shown to work using a heuristic iterative adaptation of the design parameters. For more effective and sound optimization methods, design sensitivities are required. The first results on design sensitivity analysis are presented, and their accuracy is studied. Numerical examples are provided for illustration.",10.1007/s00158-003-0375-7,https://dx.doi.org/10.1007/s00158-003-0375-7
"Bae, HR | Grandhi, RV | Canfield, RA",2006,Sensitivity analysis of structural response uncertainty propagation using evidence theory,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,29,2.42,"Sensitivity analysis for the quantified uncertainty in evidence theory is developed. In reliability quantification, classical probabilistic analysis has been a popular approach in many engineering disciplines. However, when we cannot obtain sufficient data to construct probability distributions in a large-complex system, the classical probability methodology may not be appropriate to quantify the uncertainty. Evidence theory, also called Dempster-Shafer Theory, has the potential to quantify aleatory (random) and epistemic (subjective) uncertainties because it can directly handle insufficient data and incomplete knowledge situations. In this paper, interval information is assumed for the best representation of imprecise information, and the sensitivity analysis of plausibility in evidence theory is analytically derived with respect to expert opinions and structural parameters. The results from the sensitivity analysis are expected to be very useful in finding the major contributors for quantified uncertainty and also in redesigning the structural system for risk minimization.",10.1007/s00158-006-0606-9,https://dx.doi.org/10.1007/s00158-006-0606-9
"Elsheikh, AH | Wheeler, MF | Hoteit, I",2013,An iterative stochastic ensemble method for parameter estimation of subsurface flow models,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,12,2.4,"Parameter estimation for subsurface flow models is an essential step for maximizing the value of numerical simulations for future prediction and the development of effective control strategies. We propose the iterative stochastic ensemble method (ISEM) as a general method for parameter estimation based on stochastic estimation of gradients using an ensemble of directional derivatives. ISEM eliminates the need for adjoint coding and deals with the numerical simulator as a blackbox. The proposed method employs directional derivatives within a Gauss-Newton iteration. The update equation in ISEM resembles the update step in ensemble Kalman filter, however the inverse of the output covariance matrix in ISEM is regularized using standard truncated singular value decomposition or Tikhonov regularization. We also investigate the performance of a set of shrinkage based covariance estimators within ISEM. The proposed method is successfully applied on several nonlinear parameter estimation problems for subsurface flow models. The efficiency of the proposed algorithm is demonstrated by the small size of utilized ensembles and in terms of error convergence rates.",10.1016/j.jcp.2013.01.047,https://dx.doi.org/10.1016/j.jcp.2013.01.047
"Wen, Z | Liu, K | Chen, XL",2013,Approximate analytical solution for non-Darcian flow toward a partially penetrating well in a confined aquifer,Applications_JOURNAL OF HYDROLOGY,12,2.4,"In this study, non-Darcian flow to a partially penetrating well in a confined aquifer was investigated. The flow in the horizontal direction was assumed to be non-Darcian, while the flow in the vertical direction was assumed to be Darcian. The Izbash equation was employed to describe the non-Darcian flow in the horizontal direction of the aquifer. We used a linearization procedure to approximate the non-linear term in the governing equation enabling the mathematical model to be solved using a combination of Laplace and Fourier cosine transforms. Approximate analytical solutions for the drawdown were obtained and the impacts of different parameters on the drawdown were analyzed. The results indicated that a larger power index n in the Izbash equation leads to a larger drawdown at early times, while a larger n results in a smaller drawdown at late times. The drawdowns along the vertical direction z are symmetric if the well screen is located in the center of the aquifer, and the drawdown at the center of the aquifer is the largest along the vertical direction for this case. The length of the well screen w has little impact on the drawdown at early times, while a larger length of the well screen results in a smaller drawdown at late times. The drawdown increases with K-r at early times, while it decreases as K-r increases at late times, in which K-r is the apparent radial hydraulic conductivity. A sensitivity analysis of the parameters, i.e., the specific storage S-s, w, n and K-r, indicated that the drawdown is not sensitive to them at early times, while it is very sensitive to these parameters at late times especially to the power index n.",10.1016/j.jhydrol.2013.06.027,https://dx.doi.org/10.1016/j.jhydrol.2013.06.027
"Sandu, A | Zhang, L",2008,Discrete second order adjoints in atmospheric chemical transport modeling,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,24,2.4,"Atmospheric chemical transport models (CTMs) are essential tools for the study of air pollution, for environmental policy decisions, for the interpretation of observational data, and for producing air quality forecasts. Many air quality studies require sensitivity analyses, i.e., the computation of derivatives of the model output with respect to model parameters. The derivatives of a cost functional (defined on the model output) with respect to a large number of model parameters can be calculated efficiently through adjoint sensitivity analysis. While the traditional (first order) adjoint models give the gradient of the cost functional with respect to parameters, second order adjoint models give second derivative information in the form of products between the Hessian of the cost functional and a vector (representing a perturbation in sensitivity analysis, a search direction in optimization, an eigenvector, etc.). In this paper we discuss the mathematical foundations of the discrete second order adjoint sensitivity method and present a complete set of computational tools for performing second order sensitivity studies in three-dimensional atmospheric CTMs. The tools include discrete second order adjoints of Runge-Kutta and of Rosenbrock time stepping methods for stiff equations together with efficient implementation strategies. Numerical examples illustrate the use of these computational tools in important applications like sensitivity analysis, optimization, uncertainty quantification and the calculation of directions of maximal error growth in three-dimensional atmospheric CTMs.",10.1016/j.jcp.2008.02.011,https://dx.doi.org/10.1016/j.jcp.2008.02.011
"Moreo, P | Perez, MA | Garcia-Aznar, JM | Doblare, M",2007,Modelling the mechanical behaviour of living bony interfaces,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,26,2.36,"The main purpose of this work is to develop a computational model for living interfaces with bone implants. The model is able to qualitatively capture the evolutive behaviour of bony interfaces: deterioration and bone ingrowth. We assume that the evolution of the variables that define the mechanical state of the interface can be formulated following the principles of continuum damage mechanics (CDM) with the additional feature that the variation of the internal variables may be negative to allow the interface to osseointegrate partially (repair) recovering its initial stiffness. Within the present study, the femoral component of total hip non-cemented arthroplasties has been analyzed by means of D finite element analysis (FEA). The dependence of the bone ingrowth pattern on the stem stiffness has been studied, concluding that stiffer stems improve primary fixation. Moreover, a sensitivity analysis has been performed studying the influence of patient activity, stem surface finishing and other model parameters. Overall, the model is able to reproduce the progressive deterioration and osseointegration of living bony interfaces, obtaining results that qualitatively agree with clinical observations.",10.1016/j.cma.2007.03.020,https://dx.doi.org/10.1016/j.cma.2007.03.020
"Medici, C | Wade, AJ | Frances, F",2012,Does increased hydrochemical model complexity decrease robustness?,Applications_JOURNAL OF HYDROLOGY,14,2.33,"The aim of this study was, within a sensitivity analysis framework, to determine if additional model complexity gives a better capability to model the hydrology and nitrogen dynamics of a small Mediterranean forested catchment or if the additional parameters cause over-fitting. Three nitrogen-models of varying hydrological complexity were considered. For each model, general sensitivity analysis (GSA) and Generalized Likelihood Uncertainty Estimation (GLUE) were applied, each based on , Monte Carlo simulations. The results highlighted the most complex structure as the most appropriate, providing the best representation of the non-linear patterns observed in the flow and streamwater nitrate concentrations between  and . Its % and % GLUE bounds, obtained considering a multi-objective approach, provide the narrowest band for streamwater nitrogen, which suggests increased model robustness, though all models exhibit periods of inconsistent good and poor fits between simulated outcomes and observed data. The results confirm the importance of the riparian zone in controlling the short-term (daily) streamwater nitrogen dynamics in this catchment but not the overall flux of nitrogen from the catchment. It was also shown that as the complexity of a hydrological model increases over-parameterisation occurs, but the converse is true for a water quality model where additional process representation leads to additional acceptable model simulations. Water quality data help constrain the hydrological representation in process-based models. Increased complexity was justifiable for modelling river-system hydrochemistry. Increased complexity was justifiable for modelling river-system hydrochemistry.",10.1016/j.jhydrol.2012.02.047,https://dx.doi.org/10.1016/j.jhydrol.2012.02.047
"Theodoropoulou, MA | Karoutsos, V | Kaspiris, C | Tsakiroglou, CD",2003,A new visualization technique for the study of solute dispersion in model porous media,Applications_JOURNAL OF HYDROLOGY,35,2.33,"A new technique of high resolution is developed to perform visualization experiments of the hydrodynamic dispersion of pollutants in transparent glass-etched pore networks, which are regarded as representative models of natural porous media and single fractures. The technique is based on the continuous detection of the sharp colour changes caused on an aqueous solution, as the solute concentration varies, because of the strong sensitivity of a system of indicators to pH. Image analysis is used for the transformation of the spatial distribution of colour intensity to solute concentration profiles. Unidirectional miscible displacement and single source-solute transport experiments are used to identify and quantify the transient and steady-state solute dispersion regimes in a pore network, and estimate the longitudinal and transverse dispersion coefficients as functions of Peclet number. The dispersion coefficients are estimated by fitting the spatial/temporal distribution of solute concentration over various regions of the network to analytic solutions of the convection-dispersion equation, obtained by using a flux-type boundary condition at solute sources. The experimental technique and the method of analysis of its results may be proved very useful for model validation, sensitivity analysis of dispersion coefficients with respect to pore space parameters, and identification of liquid pollutant dispersion regimes in underground aquifers.",10.1016/S0022-1694(02)00421-3,https://dx.doi.org/10.1016/S0022-1694(02)00421-3
"Mathias, SA | Wen, Z",2015,Numerical simulation of Forchheimer flow to a partially penetrating well with a mixed-type boundary condition,Applications_JOURNAL OF HYDROLOGY,7,2.33,"This article presents a numerical study to investigate the combined role of partial well penetration (PWP) and non-Darcy effects concerning the performance of groundwater production wells. A finite difference model is developed in MATLAB to solve the two-dimensional mixed-type boundary value problem associated with flow to a partially penetrating well within a cylindrical confined aquifer. Non-Darcy effects are incorporated using the Forchheimer equation. The model is verified by comparison to results from existing semi-analytical solutions concerning the same problem but assuming Darcy's law. A sensitivity analysis is presented to explore the problem of concern. For constant pressure production, Non-Darcy effects lead to a reduction in production rate, as compared to an equivalent problem solved using Darcy's law. For fully penetrating wells, this reduction in production rate becomes less significant with time. However, for partially penetrating wells, the reduction in production rate persists for much larger times. For constant production rate scenarios, the combined effect of PWP and non-Darcy flow takes the form of a constant additional drawdown term. An approximate solution for this loss term is obtained by performing linear regression on the modeling results.",10.1016/j.jhydrol.2015.02.015,https://dx.doi.org/10.1016/j.jhydrol.2015.02.015
"Shahkarami, P | Liu, LC | Moreno, L | Neretnieks, I",2015,Radionuclide migration through fractured rock for arbitrary-length decay chain: Analytical solution and global sensitivity analysis,Applications_JOURNAL OF HYDROLOGY,7,2.33,"This study presents an analytical approach to simulate nuclide migration through a channel in a fracture accounting for an arbitrary-length decay chain. The nuclides are retarded as they diffuse in the porous rock matrix and stagnant zones in the fracture. The Laplace transform and similarity transform techniques are applied to solve the model. The analytical solution to the nuclide concentrations at the fracture outlet is governed by nine parameters representing different mechanisms acting on nuclide transport through a fracture, including diffusion into the rock matrices, diffusion into the stagnant water zone, chain decay and hydrodynamic dispersion. Furthermore, to assess how sensitive the results are to parameter uncertainties, the Sobol method is applied in variance-based global sensitivity analyses of the model output. The Sobol indices show how uncertainty in the model output is apportioned to the uncertainty in the model input. This method takes into account both direct effects and interaction effects between input parameters. The simulation results suggest that in the case of pulse injections, ignoring the effect of a stagnant water zone can lead to significant errors in the time-of-first arrival and the peak value of the nuclides. Likewise, neglecting the parent and modeling its daughter as a single stable species can result in a significant overestimation of the peak value of the daughter nuclide. It is also found that as the dispersion increases, the early arrival time and the peak time of the daughter decrease while the peak value increases. More importantly, the global sensitivity analysis reveals that for time periods greater than a few thousand years, the uncertainty of the model output is more sensitive to the values of the individual parameters than to the interaction between them. Moreover, if one tries to evaluate the true values of the input parameters at the same cost and effort, the determination of priorities should follow a certain sequence.",10.1016/j.jhydrol.2014.10.060,https://dx.doi.org/10.1016/j.jhydrol.2014.10.060
"Van den Nieuwenhof, B | Coyette, JP",2003,Modal approaches for the stochastic finite element analysis of structures with material and geometric uncertainties,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,34,2.27,"This paper presents two efficient modal approaches as an alternative to direct formulations for the time-harmonic dynamic analysis of structures with random material and shape parameters. In both approaches, the structural eigenproblem is solved and complemented by a sensitivity analysis to the random parameters. The modal perturbation stochastic finite element method (SFEM) then condenses the response sensitivities to assess the response variability. The mixed perturbation/Monte-Carlo SFEM assesses the response statistics by sampling the structural eigenmodes according to the perturbation estimation of their probability distribution functions (PDFs). Geometric uncertainties are handled through an appropriate shape pararneterisation and a shape design sensitivity analysis. Two numerical applications examine both approaches in terms of accuracy, variability level and computational requirements. The applications involve a plate bending problem with random Young modulus or edge length and a plate with a random flatness default. Particular observations related to the influence of the parameter PDFs in simulation-based methods are also provided.",10.1016/S0045-7825(03)00371-2,https://dx.doi.org/10.1016/S0045-7825(03)00371-2
"Yeh, HD | Chen, YJ",2007,Determination of skin and aquifer parameters for a slug test with wellbore-skin effect,Applications_JOURNAL OF HYDROLOGY,25,2.27,"Slug test is considered to reflect the hydraulic parameters in the vicinity of the test well. The aquifer parameters are usually identified by fitting an appropriate mathematical solution or graphical type curves with slug test data. In this paper, we developed an approach by combining [Moench, A.F., Hsieh, P.A., . Analysis of slug test data in a well with finite-thickness skin. In: Memoirs of the th International Congress on the Hydrogeology of Rocks of Low Permeability, U.S.A. Members of the International Association of Hydrologists, Tucson, AZ, vol. , pp. -] and simulated annealing (SA) approach to estimate five parameters, i.e., three skin parameters and two aquifer parameters. The three skin parameters are hydraulic conductivity, specific storage, and thickness of the wellbore-skin zone, white the two aquifer parameters are hydraulic conductivity and specific storage of the formation zone. It is worthy to note although the thickness of the wellbore-skin zone is usually taken as an input data for the data-analyzed software, it is actually an unknown parameter that cannot be measured directly. This paper proposes a methodology for estimating the thickness of the wellbore-skin zone with other hydraulic parameters at the same time. Eight sets of well water-level. (WWL) data of aquifers with both positive and negative skins are generated by Moench and Hsieh [Moench and Hsieh, ] and four sets of standard normally distributed noise are then added to each set of WWL data. The results indicate that the negative-skin cases generally give a better estimated result than that of the positive-skin cases. Sensitivity analysis is also employed to demonstrate the physical behavior when slug test was performed under positive-skin effect. For the case of an aquifer with a positive-skin, the use of a longer series of WWL data for analysis is strongly recommended for better estimation of aquifer hydraulic conductivity. Analyzing the WWL data of the test and observation wells simultaneously could significantly improve the estimations on specific storages. Impetuously presuming an arbitrary value for the thickness of the wellbore-skin zone may lead to poor estimation for the other four parameters.",10.1016/j.jhydrol.2007.05.029,https://dx.doi.org/10.1016/j.jhydrol.2007.05.029
"Trigg, MA | Cook, PG | Brunner, P",2014,Groundwater fluxes in a shallow seasonal wetland pond: The effect of bathymetric uncertainty on predicted water and solute balances,Applications_JOURNAL OF HYDROLOGY,9,2.25,"The successful management of groundwater dependent shallow seasonal wetlands requires a sound understanding of groundwater fluxes. However, such fluxes are hard to quantify. Water volume and solute mass balance models can be used in order to derive an estimate of groundwater fluxes within such systems. This approach is particularly attractive, as it can be undertaken using measurable environmental variables, such as; rainfall, evaporation, pond level and salinity. Groundwater fluxes estimated from such an approach are subject to uncertainty in the measured variables as well as in the process representation and in parameters within the model. However, the shallow nature of seasonal wetland ponds means water volume and surface area can change rapidly and non-linearly with depth, requiring an accurate representation of the wetland pond bathymetry. Unfortunately, detailed bathymetry is rarely available and simplifying assumptions regarding the bathymetry have to be made. However, the implications of these assumptions are typically not quantified. We systematically quantify the uncertainty implications for eight different representations of wetland bathymetry for a shallow seasonal wetland pond in South Australia. The predictive uncertainty estimation methods provided in the Model-Independent Parameter Estimation and Uncertainty Analysis software (PEST) are used to quantify the effect of bathymetric uncertainty on the modelled fluxes. We demonstrate that bathymetry can be successfully represented within the model in a simple parametric form using a cubic Sexier curve, allowing an assessment of bathymetric uncertainty due to measurement error and survey detail on the derived groundwater fluxes compared with the fixed bathymetry models. Findings show that different bathymetry conceptualisations can result in very different mass balance components and hence process conceptualisations, despite equally good fits to observed data, potentially leading to poor management decisions for the wetlands. Model predictive uncertainty increases with the crudity of the bathymetry representation, however, approximations that capture the general shape of the wetland pond such as a power law or Bezier curve show only a small increase in prediction uncertainty compared to the full dGPS surveyed bathymetry, implying these may be sufficient for most modelling purposes.",10.1016/j.jhydrol.2014.06.020,https://dx.doi.org/10.1016/j.jhydrol.2014.06.020
"Fancello, EA",2006,Topology optimization for minimum mass design considering local failure constraints and contact boundary conditions,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,27,2.25,"The purpose of this work is to present a possible approach to the topology mass minimization of a body submitted to local material failure constraints, contact boundary conditions, and multiple load cases. The formulation combines the well-known SIMP approach ( Solid Isotropic Microstructure with intermediate mass Penalization) and the Augmented Lagrangian technique to deal with stress-based constraints. At every design step and load case, a contact solver is called to obtain the equilibrium deformed configuration. Assuming differentiability, the sensitivity analysis is performed analytically at the cost of a Newton iteration. Finally, some numerical examples are presented to explore the differences and similarities found in the final designs for this case and for the case of minimization of internal energy, also with contact boundary conditions.",10.1007/s00158-006-0019-9,https://dx.doi.org/10.1007/s00158-006-0019-9
"Esmaeili, S | Thomson, NR | Tolson, BA | Zebarth, BJ | Kuchta, SH | Neilsen, D",2014,Quantitative global sensitivity analysis of the RZWQM to warrant a robust and effective calibration,Applications_JOURNAL OF HYDROLOGY,9,2.25,"Sensitivity analysis is a useful tool to identify key model parameters as well as to quantify simulation errors resulting from parameter uncertainty. The Root Zone Water Quality Model (RZWQM) has been subjected to various sensitivity analyses; however, in most of these efforts a local sensitivity analysis method was implemented, the nonlinear response was neglected, and the dependency among parameters was not examined. In this study we employed a comprehensive global sensitivity analysis to quantify the contribution of  model input parameters (including  hydrological parameters and  nitrogen cycle parameters) on the uncertainty of key RZWQM outputs relevant to raspberry row crops in Abbotsford, BC, Canada. Specifically,  model outputs that capture various vertical-spatial and temporal domains were investigated. A rank transformation method was used to account for the nonlinear behavior of the model. The variance of the model outputs was decomposed into correlated and uncorrelated partial variances to provide insight into parameter dependency and interaction. The results showed that, in general, the field capacity (soil water content at -  kPa) in upper  cm of the soil horizon had the greatest contribution (>%) to the estimate of the water flux and evapotranspiration uncertainty. The most influential parameters affecting the simulation of soil nitrate content, mineralization, denitrification, nitrate leaching and plant nitrogen uptake were the transient coefficient of fast to intermediate humus pool, the carbon to nitrogen ratio of the fast humus pool, the organic matter decay rate in fast humus pool, and field capacity. The correlated contribution to the model output uncertainty was <% for the set of parameters investigated. The findings from this effort were utilized in two calibration case studies to demonstrate the utility of this global sensitivity analysis to reduce the risk of over-parameterization, and to identify the vertical location of observations that were the most effective to use as RZWQM calibration targets when water flux estimates are a key focus.",10.1016/j.jhydrol.2014.01.051,https://dx.doi.org/10.1016/j.jhydrol.2014.01.051
"Yang, XW | Li, YM",2014,Structural topology optimization on dynamic compliance at resonance frequency in thermal environments,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,9,2.25,"This paper [r] carries out topology optimization to minimize structural dynamic compliance at resonance frequencies in thermal environments. The resonance response is the main dynamic component, minimization of which could possibly change structural dynamic characteristics significantly. A bi-material square plate subjected to uniform temperature rise and driven by harmonic load is investigated in pre-buckling state. The compressive stress induced by thermal environment is considered as pre-stress in dynamic analysis, which could reduce stiffness of the structure and alter the optimal topology. Sensitivity analysis is carried out through adjoint method efficiently. As natural frequencies are constantly changing during the optimization, the associated sensitivity should be calculated in which multiple-frequency case is briefly discussed. Mode switching may occur during the optimization, and mode tracking technique is adopted. Numerical results show that the topology is mainly determined by the excited modes, and could be altered by the location of the applied load if different modes are excited. The natural frequencies become larger in optimal design and the dynamic compliance decreases in nearby frequency band. The critical buckling temperature increases as optimization proceeds, indicating the structure is always in pre-buckling state.",10.1007/s00158-013-0961-2,https://dx.doi.org/10.1007/s00158-013-0961-2
"Kim, NH | Chang, YM",2005,Eulerian shape design sensitivity analysis and optimization with a fixed grid,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,29,2.23,"Conventional shape optimization based on the finite element method uses Lagrangian representation in which the finite element mesh moves according to shape change, while modern topology optimization uses Eulerian representation. In this paper, an approach to shape optimization using Eulerian representation such that the mesh distortion problem in the conventional approach can be resolved is proposed. A continuum geometric model is defined on the fixed grid of finite elements. An active set of finite elements that defines the discrete domain is determined using a procedure similar to topology optimization, in which each element has a unique shape density. The shape design parameter that is defined on the geometric model is transformed into the corresponding shape density variation of the boundary elements. Using this transformation, it has been shown that the shape design problem can be treated as a parameter design problem, which is a much easier method than the former. A detailed derivation of how the shape design velocity field can be converted into the shape density variation is presented along with sensitivity calculation. Very efficient sensitivity coefficients are calculated by integrating only those elements that belong to the structural boundary. The accuracy of the sensitivity information is compared with that derived by the finite difference method with excellent agreement. Two design optimization problems are presented to show the feasibility of the proposed design approach.",10.1016/j.cma.2004.12.019,https://dx.doi.org/10.1016/j.cma.2004.12.019
"Han, JS | Kwak, BM",2004,Robust optimization using a gradient index: MEMS applications,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,31,2.21,"This paper discusses a simple and effective robust optimization formulation and illustrates its application to MicroElectroMechanical Systems (MEMS) devices. The proposed formulation improves robustness of the objective function by minimizing a gradient index (GI), defined as a function of gradients of performance functions with respect to uncertain variables. The level of constraint feasibility is also enhanced by adding a term determined by a constraint value and the gradient index. In the robust optimal design procedure, a deterministic optimization for performance improvement is followed by a sensitivity analysis with respect to uncertainties such as MEMS fabrication errors and changes of material properties. During the process of the deterministic optimization and sensitivity analysis, dominant performances and critical uncertain variables are identified to define the GI. Our approach for robust design requires no statistical information on the uncertainties and yet achieves robustness effectively. Two MEMS application examples including a micro accelerometer and a resonant-type micro probe are presented.",10.1007/s00158-004-0410-3,https://dx.doi.org/10.1007/s00158-004-0410-3
"Ayvaz, MT",2013,A linked simulation-optimization model for simultaneously estimating the Manning's surface roughness values and their parameter structures in shallow water flows,Applications_JOURNAL OF HYDROLOGY,11,2.2,"A linked simulation-optimization model is proposed for simultaneously estimating the Manning's surface roughness parameters and their associated parameter structures for one-dimensional shallow water flows. In the simulation part of the model, hydrodynamic flow process is simulated by modeling the given flow reach on HEC-RAS. The association of unknown parameter structure with the roughness values is accomplished by partitioning the given flow reach into sub-regions using the one dimensional Voronoi Diagram (VD). The developed simulation model is then linked to an optimization model where heuristic Particle Swarm Optimization algorithm is used. The main objective of the PSO based optimization model is to determine the roughness parameters and their associated parameter structures along the flow reach by minimizing the discrepancies between simulated and measured water elevations at several observation locations. The applicability of the model is evaluated on a flow reach of East Fork River, WY, USA under three scenarios by considering the known/unknown roughness parameter structures and unsteady flow conditions. Furthermore, a sensitivity analysis is conducted to determine the influence of different PSO parameters on solution accuracy. Identified results indicated that the model provides better results than those obtained by different optimization methods in literature and significantly improves the calibration statistics by considering the variability of roughness parameters along the flow reach.",10.1016/j.jhydrol.2013.07.019,https://dx.doi.org/10.1016/j.jhydrol.2013.07.019
"Degroote, J | Hojjat, M | Stavropoulou, E | Wuchner, R | Bletzinger, KU",2013,Partitioned solution of an unsteady adjoint for strongly coupled fluid-structure interactions and application to parameter identification of a one-dimensional problem,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,11,2.2,"Unsteady fluid-structure interaction (FSI) simulations are generally time-consuming. Gradient-based methods are preferred to minimise the computational cost of parameter identification studies (and more in general optimisation) with a high number of parameters. However, calculating the cost function's gradient using finite differences becomes prohibitively expensive for a high number of parameters. Therefore, the adjoint equations of the unsteady FSI problem are solved to obtain this gradient at a cost almost independent of the number of parameters. Here, both the forward and the adjoint problems are solved in a partitioned way, which means that the flow equations and the structural equations are solved separately. The application of interest is the identification of the arterial wall's stiffness by comparing the motion of the arterial wall with a reference, possibly obtained from non-invasive imaging. Due to the strong interaction between the fluid and the structure, quasi-Newton coupling iterations are applied to stabilise the partitioned solution of both the forward and the adjoint problem.",10.1007/s00158-012-0808-2,https://dx.doi.org/10.1007/s00158-012-0808-2
"Spank, U | Schwarzel, K | Renner, M | Moderow, U | Bernhofer, C",2013,Effects of measurement uncertainties of meteorological data on estimates of site water balance components,Applications_JOURNAL OF HYDROLOGY,11,2.2,"Numerical water balance models are widely used in ecological and hydro sciences. However, their application is related to specific problems and uncertainties. The reliability of model prediction depends on (i) model concept, (ii) parameters, (iii) uncertainty of input data, and (iv) uncertainty of reference data. How model concept (i) and parameters (ii) effect the model's performance is an often treated problem. However, the effects of (iii) and (iv) are typically ignored or only barely treated in context of regionalisation and generalisation. In this study, the actual measurement uncertainties of input and reference data are the main focus. Furthermore, the evaluation of model results is analysed with regard to uncertainties of reference data. A special feature is the use of evapotranspiration (measured via the eddy covariance) instead of runoff for evaluation of simulation results. It is shown that seemingly small uncertainties of measurements can create significant uncertainties in simulation results depending on the temporal scale of investigation. As an example, the uncertainty of measurements of daily global radiation sum up to an uncertainty of  MJ (equivalent to  mm) on an annual scale, which causes an uncertainty of  mm in simulated grass-reverence evapotranspiration. Summarised and generalised, the measurement uncertainties of all input data create an uncertainty on average of around % in the simulated annual evapotranspiration and of around % in the simulated annual seepage. However, the effects can be significantly higher in years with extreme events and can reach up to %. It is demonstrated that uncertainties of individual variables are not simply superposed but interact in a complex way. Thereby, it has become apparent that the effects of measurement uncertainties on model results are similar for complex and for simple models.",10.1016/j.jhydrol.2013.03.047,https://dx.doi.org/10.1016/j.jhydrol.2013.03.047
"Melnikova, NB | Krzhizhanovskaya, VV | Sloot, PMA",2013,Modeling earthen dikes using real-time sensor data,Applications_JOURNAL OF HYDROLOGY,11,2.2,"The paper describes the concept and implementation details of integrating a finite element module for dike stability analysis ""Virtual Dike"" into an early warning system for flood protection. The module operates in real-time mode and includes fluid and structural sub-models for simulation of porous flow through the dike and for dike stability analysis. Real-time measurements obtained from pore pressure sensors are fed into the simulation module, to be compared with simulated pore pressure dynamics. Implementation of the module has been performed for a real-world test case, an earthen levee protecting a sea-port in Groningen, The Netherlands. Sensitivity analysis and calibration of diffusivities have been performed based on pore pressure sensor data during tidal fluctuations. An algorithm for automatic diffusivities calibration for a heterogeneous dike is proposed and studied. Analytical solutions describing tidal propagation in a one-dimensional saturated aquifer are employed in the algorithm to generate initial estimates of diffusivities.",10.1016/j.jhydrol.2013.05.031,https://dx.doi.org/10.1016/j.jhydrol.2013.05.031
"Takezawa, A | Kitamura, M",2012,Geometrical design of thermoelectric generators based on topology optimization,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,13,2.17,"This paper discusses an application of the topology optimization method for the design of thermoelectric generators. The proposed methodology provides the optimized geometry in accordance with various arbitrary conditions such as the types of materials, the volume of materials, and the temperature and shape of the installation position. By considering the coupled equations of state for the thermoelectric problem, we introduce an analytical model subject to these equations, which mimics the closed circuit composed of thermoelectric materials, electrodes, and a resistor. The total electric power applied to the resistor and the conversion efficiency are formulated as objective functions to be optimized. The proposed optimization method for thermoelectric generators is implemented as a geometrical optimization method using the solid isotropic material with penalization method used in topology optimizations. Simple relationships are formulated between the density function of the solid isotropic material with penalization method and the physical properties of the thermoelectric material. A sensitivity analysis for the objective functions is formulated with respect to the density function and the adjoint equations required for calculating it. Depending on the sensitivity, the density function is updated using the method of moving asymptotes. Finally, numerical examples are provided to demonstrate the validity of the proposed method.",10.1002/nme.3375,https://dx.doi.org/10.1002/nme.3375
"Diez, M | Peri, D | Fasano, G | Campana, EF",2012,Hydroelastic optimization of a keel fin of a sailing boat: a multidisciplinary robust formulation for ship design,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,13,2.17,"The paper presents a formulation for multidisciplinary design optimization of vessels, subject to uncertain operating conditions. The formulation couples the multidisciplinary design analysis with the Bayesian approach to decision problems affected by uncertainty. In the present context, the design specifications are no longer given in terms of a single operating design point, but in terms of probability density function of the operating scenario. The optimal configuration is that which maximizes the performance expectation over the uncertain parameters variation. In this sense, the optimal solution is ""robust"" within the stochastic scenario assumed. Theoretical and numerical issues are addressed and numerical results in the hydroelastic optimization of a keel fin of a sailing yacht are presented.",10.1007/s00158-012-0783-7,https://dx.doi.org/10.1007/s00158-012-0783-7
"Lund, E | Moller, H | Jakobsen, LA",2003,Shape design optimization of stationary fluid-structure interaction problems with large displacements and turbulence,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,32,2.13,"This paper presents general and efficient methods for analysis and gradient based shape optimization of systems characterized as strongly coupled stationary fluid-structure interaction (FSI) problems. The incompressible fluid flow can be laminar or turbulent and is described using the Reynolds-averaged Navier-Stokes equations (RANS) together with the algebraic Baldwin-Lomax turbulence model. The structure may exhibit large displacements due to the interaction with the fluid domain, resulting in geometrically nonlinear structural behaviour and nonlinear interface coupling conditions. The problem is discretized using Galerkin and Streamline-Upwind/Petrov-Galerkin finite element methods, and the resulting nonlinear equations are solved using Newton's method. Due to the large displacements of the structure, an efficient update algorithm for the fluid mesh must be applied, leading to the use of an approximate Jacobian matrix in the solution routine. Expressions for Design Sensitivity Analysis (DSA) are derived using the direct differentiation approach, and the use of an inexact Jacobian matrix in the analysis leads to an iterative but very efficient scheme for DSA. The potential of gradient based shape optimization of fluid flow and FSI problems is illustrated by several examples.",10.1007/s00158-003-0288-5,https://dx.doi.org/10.1007/s00158-003-0288-5
"Slooten, LJ | Carrera, J | Castro, E | Fernandez-Garcia, D",2010,A sensitivity analysis of tide-induced head fluctuations in coastal aquifers,Applications_JOURNAL OF HYDROLOGY,17,2.12,"The response of coastal aquifers to sea-level fluctuations, notably tides, is known to contain much information about hydraulic parameters. We performed sensitivity analyses to assess how much, about what and where this information can be best obtained. It is well known that the response to harmonic fluctuations (and many harmonics can be superimposed to describe sea-level fluctuations) decreases exponentially with distance inland. The characteristic length of this decay is L(c) = root DP/pi, where D is hydraulic diffusivity and P is period. Maximum sensitivity is obtained for distances equal to L. which is where maximum information would be obtained if the aquifer is treated as homogeneous. However, sensitivity depends not only on the problem dynamics, but also on parameterization. In fact, if heterogeneity is acknowledged by finely discretizing hydraulic conductivity, we find that connection to the sea (i.e. K near the coast) is what can be characterized best, while the most informative measurements are located at around . L(c). Thin low conductivity zones near the coast lead to a stepwise decrease in the amplitude of groundwater head fluctuations. We find that the fluctuations are independent of buoyancy effects, so that they can be simulated by constant density codes. High information content and ease of use suggest that they should be helpful in characterizing the aquifer-sea connection, which is important for coastal aquifer protection against seawater intrusion.",10.1016/j.jhydrol.2010.08.032,https://dx.doi.org/10.1016/j.jhydrol.2010.08.032
"Doltsinis, I | Kang, Z",2006,Perturbation-based stochastic FE analysis and robust design of inelastic deformation processes,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,25,2.08,"The study addresses the perturbation-based stochastic finite element analysis and the robust design optimization of deformation processes of inelastic solids. The perturbation equations for the stochastic moment analysis of both steady-state and non-stationary processes are presented. An iteration scheme based on the secant system operator is given for the solution of the perturbation equations in case that a tangent matrix is not available, The robust design of deformation processes is stated as a two-criteria optimal design problem that attempts best mean properties of the outcome at minimum variability. The task is solved using optimization techniques based on sequential quadratic programming in conjunction with the stochastic finite element analysis. The proposed method is applied to the design of an extrusion die for robustness with respect to friction variability, and to a workpiece preform design problem. The numerical results show the potential of the method for applications regarding the design of robust deformation processes.",10.1016/j.cma.2005.05.004,https://dx.doi.org/10.1016/j.cma.2005.05.004
"Rais-Rohani, M | Singh, MN",2004,Comparison of global and local response surface techniques in reliability-based optimization of composite structures,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,29,2.07,"This paper discusses the development and application of two alternative strategies, in the form of global and sequential local response surface (RS) techniques, for the solution of reliability-based optimization (RBO) problems. The problem of a thin-walled composite circular cylinder under axial buckling instability is used as a demonstrative example. In this case, the global technique uses a single second-order RS model to estimate the axial buckling load over the entire feasible design space (FDS), whereas the local technique uses multiple first-order RS models, with each applied to a small subregion of the FDS. Alternative methods for the calculation of unknown coefficients in each RS model are explored prior to the solution of the optimization problem. The example RBO problem is formulated as a function of  uncorrelated random variables that include material properties, the thickness and orientation angle of each ply, the diameter and length of the cylinder, as well as the applied load. The mean values of the  ply thicknesses are treated as independent design variables. While the coefficients of variation of all random variables are held fixed, the standard deviations of the ply thicknesses can vary during the optimization process as a result of changes in the design variables. The structural reliability analysis is based on the first-order reliability method with the reliability index treated as the design constraint. In addition to the probabilistic sensitivity analysis of the reliability index, the results of the RBO problem are presented for different combinations of cylinder length and diameter and laminate ply patterns. The two strategies are found to produce similar results in terms of accuracy, with the sequential local RS technique having a considerably better computational efficiency.",10.1007/s00158-003-0353-0,https://dx.doi.org/10.1007/s00158-003-0353-0
"Tang, PS | Chang, KH",2001,Integration of topology and shape optimization for design of structural components,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,35,2.06,"This paper presents an integrated approach that supports the topology optimization and CAD-based shape optimization. The main contribution of the paper is using the geometric reconstruction technique that is mathematically sound and error bounded for creating solid models of the topologically optimized structures with smooth geometric boundary. This geometric reconstruction method extends the integration to -D applications. In addition, commercial Computer-Aided Design (CAD), finite element analysis (FEA), optimization, and application software tools are incorporated to support the integrated optimization process. The integration is carried out by first converting the geometry of the topologically optimized structure into smooth and parametric B-spline curves and surfaces. The B-spline curves and surfaces are then imported into a parametric CAD environment to build solid models of the structure. The control point movements of the B-spline curves or surfaces are defined as design variables for shape optimization, in which CAD-based design velocity field computations, design sensitivity analysis (DSA), and nonlinear programming are performed. Both -D plane stress and -D solid examples are presented to demonstrate the proposed approach.",10.1007/PL00013282,https://dx.doi.org/10.1007/PL00013282
"Long, XY | Jiang, C | Han, X | Gao, W | Bi, RG",2014,Sensitivity analysis of the scaled boundary finite element method for elastostatics,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,8,2.0,"As a semi-analytical structural analysis algorithm, the scaled boundary finite-element method (SBFEM) only discretizes the boundary of the analyzed domain without the need of fundamental solutions, which makes it powerful for problems with stress singularity or unbounded foundation media. In this paper, a sensitivity analysis method of SBFEM is proposed for elastostatics, through which the first order derivatives of the structural responses with respect to the design parameters can be obtained efficiently and accurately. An approach is suggested to compute the eigenvalue and eigenvector sensitivities of the Hamilton matrix, which are then used to calculate the analytical derivative of the stiffness matrix. Based on these calculations, the sensitivities of displacements and stresses are further obtained by a series of differential equations. The proposed sensitivity analysis method is also applied to the fracture mechanics problems. Three numerical examples are investigated to demonstrate the validity of the proposed method.",10.1016/j.cma.2014.03.005,https://dx.doi.org/10.1016/j.cma.2014.03.005
"Coelho, RF | Lebon, J | Bouillard, P",2011,Hierarchical stochastic metamodels based on moving least squares and polynomial chaos expansion,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,14,2.0,"While surrogate-based optimization has encountered a growing success in engineering design, the development of stochastic metamodels, i.e. capable of representing the complete random responses with respect to random inputs, is still an open issue, although they could be fruitfully used in optimization under uncertainty, both with single and multiple objectives. Therefore, the contribution of the paper is twofold. First, hierarchical stochastic metamodels based on moving least squares and spectral decomposition (by polynomial chaos expansion) are proposed in order to obtain a complete description of the random responses with respect to the deterministic and random input parameters. Then, these metamodels are incorporated into a novel multiobjective reliability-based formulation leaning on the concept of probabilistic nondominance. The whole procedure is applied to an analytical test case as well as to the design optimization of space truss structures, demonstrating the ability of the proposed method to provide accurate solutions at an affordable computational time.",10.1007/s00158-010-0608-5,https://dx.doi.org/10.1007/s00158-010-0608-5
"Tripp, DR | Niernann, JD",2008,Evaluating the parameter identifiability and structural validity of a probability-distributed model for soil moisture,Applications_JOURNAL OF HYDROLOGY,20,2.0,"The objective of this study is to evaluate the performance of a model that simulates local and spatial average soil moisture in a watershed. The model uses well-known expressions for infiltration, evapotranspiration, and groundwater recharge to describe soil moisture dynamics at the local scale. Then, the spatial mean soil moisture is simulated by integrating the local behavior over a probability density function that characterizes the spatial variability of soil saturation. Ultimately, the model requires input of precipitation and potential evapotranspiration (PET) time series and six parameters to simulate the dynamics of the spatial average soil moisture. The model is applied to the Fort Cobb watershed in Oklahoma using one year of data from September  through August . Model performance is evaluated in three main ways. First, the model's ability to reproduce observed local and spatial average soil moisture through calibration is examined. Second, the identifiability and stability of the parameter values are considered to assess uncertainty in the parameter values and errors in the model's mathematical structure. Third, a new method is developed that uses the sensitivities of soil moisture to precipitation and PET to assess the impacts of parameter uncertainty and structural errors on forecasts for unobserved conditions. At the local scale, the model reproduces the soil moisture with a similar degree of accuracy as a more physically-based model (HYDRUS  D), and both models exhibit some structural errors. The model can also be calibrated to approximately reproduce the spatial average soil moisture observations. However, the model produces a relatively wide range of plausible sensitivities and this range varies depending on the window of time from which the parameters are calibrated. This result implies that parameter uncertainty and model structural errors contribute substantially to model uncertainty for unobserved conditions.",10.1016/j.jhydrol.2008.01.028,https://dx.doi.org/10.1016/j.jhydrol.2008.01.028
"Nordstrom, J | Wahlsten, M",2015,Variance reduction through robust design of boundary conditions for stochastic hyperbolic systems of equations,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,6,2.0,"We consider a hyperbolic system with uncertainty in the boundary and initial data. Our aim is to show that different boundary conditions give different convergence rates of the variance of the solution. This means that we can with the same knowledge of data get a more or less accurate description of the uncertainty in the solution. A variety of boundary conditions are compared and both analytical and numerical estimates of the variance of the solution are presented. As an application, we study the effect of this technique on Maxwell's equations as well as on a subsonic outflow boundary for the Euler equations.",10.1016/j.jcp.2014.10.061,https://dx.doi.org/10.1016/j.jcp.2014.10.061
"Sonmez, FO",2007,Shape optimization of 2D structures using simulated annealing,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,22,2.0,"The goal of this study is to obtain globally optimum shapes for two-dimensional structures subject to quasi-static loads and restraints. For this purpose a technique is proposed, using which the volume (or weight) of a structure can be minimized. The emphasis is on how one can define the shape precisely, and find a shape that accurately reflects the globally optimum shape. As desian constraints, stresses developed in the structure should not exceed the maximum allowable stress, and connectivity of the structure should not be lost during shape changes. Optimization is achieved by a stochastic search algorithm called direct simulated annealing (DSA), which seeks the global minimum through randomly generated configurations. In order to obtain random configurations, a boundary variation technique is used. In this technique, a set of key points is chosen and connected by cubic splines to describe the boundary of the structure. Whenever the positions of the key points are changed in random directions, a new shape is obtained. Thus, coordinates of the key points serve as design variables. In order to apply the optimization procedure, a general computer code was developed using ANSYS Parametric Design Language. A number of cases were examined to test its effectiveness. The results show that this technique can be applied to two-dimensional shape optimization problems with high reliability even for cases where the entire free boundary is allowed to vary.",10.1016/j.cma.2007.01.019,https://dx.doi.org/10.1016/j.cma.2007.01.019
"Cho, SG | Jang, J | Kim, S | Park, S | Lee, TH | Lee, M | Choi, JS | Kim, HW | Hong, S",2016,Nonparametric approach for uncertainty-based multidisciplinary design optimization considering limited data,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,4,2.0,"Uncertainty-based multidisciplinary design optimization (UMDO) has been widely acknowledged as an advanced methodology to address competing objectives and reliable constraints of complex systems by coupling relationship of disciplines involved in the system. UMDO process consists of three parts. Two parts are to define the system with uncertainty and to formulate the design optimization problem. The third part is to quantitatively analyze the uncertainty of the system output considering the uncertainty propagation in the multidiscipline analysis. One of the major issues in the UMDO research is that the uncertainty propagation makes uncertainty analysis difficult in the complex system. The conventional methods are based on the parametric approach could possibly cause the error when the parametric approach has ill-estimated distribution because data is often insufficient or limited. Therefore, it is required to develop a nonparametric approach to directly use data. In this work, the nonparametric approach for uncertainty-based multidisciplinary design optimization considering limited data is proposed. To handle limited data, three processes are also adopted. To verify the performance of the proposed method, mathematical and engineering examples are illustrated.",10.1007/s00158-016-1540-0,https://dx.doi.org/10.1007/s00158-016-1540-0
"Schevenels, M | Sigmund, O",2016,On the implementation and effectiveness of morphological close-open and open-close filters for topology optimization,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,4,2.0,"This note reconsiders the morphological close-open and open-close filters for topology optimization introduced in an earlier paper (Sigmund Struct Multidiscip Optim (-):- ()). Close-open and open-close filters are defined as the sequential application of four dilation or erosion filters. In the original paper, these filters were proposed in order to provide length scale control in both the solid and the void phase. However, it was concluded that the filters were not useful in practice due to the computational cost of the sensitivity analysis. In this note, it is shown that the computational cost is much lower if the sensitivity analysis for each erosion or dilation step is performed sequentially. Unfortunately, it is also found that the close-open and open-close filters do not have the expected effect in terms of length scale control: each close or open operation ruins the effect of the preceding filters, resulting in a design with a minimum length scale in either the solid phase or the void phase, but not both.",10.1007/s00158-015-1393-y,https://dx.doi.org/10.1007/s00158-015-1393-y
"Alemazkoor, N | Meidani, H",2017,Divide and conquer: An incremental sparsity promoting compressive sampling approach for polynomial chaos expansions,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,2,2.0,"This paper introduces an efficient sparse recovery approach for Polynomial Chaos (PC) expansions, which promotes the sparsity by breaking the dimensionality of the problem. The proposed algorithm incrementally explores sub-dimensional expansions for a sparser recovery, and shows success when removal of uninfluential parameters that results in a lower coherence for measurement matrix, allows for a higher order and/or sparser expansion to be recovered. The incremental algorithm effectively searches for the sparsest PC approximation, and not only can it decrease the prediction error, but it can also reduce the dimensionality of PCE model. Four numerical examples are provided to demonstrate the validity of the proposed approach. The results from these examples show that the incremental algorithm substantially outperforms conventional compressive sampling approaches for PCE, in terms of both solution sparsity and prediction error.",10.1016/j.cma.2017.01.039,https://dx.doi.org/10.1016/j.cma.2017.01.039
"Shi, XN | Zhang, F | Lei, TW | Chuo, RY | Zhou, SM | Yan, Y",2012,Measuring shallow water flow velocity with virtual boundary condition signal in the electrolyte tracer method,Applications_JOURNAL OF HYDROLOGY,12,2.0,"Accurate measurement of hill-slope shallow water flow velocity is an important issue in soil erosion prediction. The pulse model of electrolyte tracer advanced by Lei et al. () indicated that the boundary condition is critical for accurate velocity measurement. To improve the measurement accuracy, Lei et al. () improved the method by employing practically measured boundary condition instead of the pulse assumption, by an additional sensor close to the electrolyte injection location. The measurement precision was improved significantly as compared with that by the pulse model. However, the measured boundary condition is not the actual boundary condition, which may still cause errors in the measured velocities. The sensor for boundary measurement can otherwise be used to measure flow velocity if it can be rationally determined without measurement. In this study, modifications are made to the previous methods. The boundary condition is not measured by an additional sensor but is determined by fitting the model to the experimental data so as to determine virtual/real boundary condition (Virtual B.C.) at its actual position and the flow velocity simultaneously. The measured velocities and the estimated boundary condition are verified using the experimentally measured data involved three flow rates (,  and  L min(-)), three slope gradients ( degrees,  degrees and  degrees) and six measurement positions (., ., ., ., ., and . m). The newly-proposed Virtual B.C. method improved the measurement precision of velocities as compared with those by the previous methods. An additional experimental data set is used for sensitivity analysis. This experiment involves three flow rates (,  and  L min(-)), three slope gradients ( degrees,  degrees and  degrees), four measurement positions (., ., ., and . m), and three electrolyte injection durations (., ., and . s). The sensitivity analysis on measured velocity indicates that the Virtual B.C. method shows significant advantage of stability, regardless of electrolyte injection duration. The improvement is relatively more significant for shorter distance measurement and shorter electrolyte injection duration. The results are very encouraging to claim that the improved method is sound for short distance measurement and for portable instrument development.",10.1016/j.jhydrol.2012.05.046,https://dx.doi.org/10.1016/j.jhydrol.2012.05.046
"Poette, G | Lucor, D",2012,Non intrusive iterative stochastic spectral representation with application to compressible gas dynamics,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,12,2.0,"In this paper, we propose a new iterative formulation improving the convergence of standard non intrusive stochastic spectral method for uncertainty quantification. We demonstrate that the method is more accurate than the classical approach with the same level of approximation and at no significant additional computational or memory cost, since it is deployed in a post-processing stage. Moreover, the accuracy of the representation improves no matter the regularity of the random quantity of interest. Therefore, the method is particularly well suited when nonlinear transformations of random variables are in play and can be viewed as a new way of tackling the Gibbs phenomenon. We apply the method to several test cases with different levels of regularity, dimensionality and complexity, including the case of compressible gas dynamics and long time-integration problems. The new and the classical approaches are compared for the resolution of a stochastic Riemann problem governed by an Euler system.",10.1016/j.jcp.2011.12.038,https://dx.doi.org/10.1016/j.jcp.2011.12.038
"Rahman, S | Ren, XC",2014,Novel computational methods for high-dimensional stochastic sensitivity analysis,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,8,2.0,"This paper presents three new computational methods for calculating design sensitivities of statistical moments and reliability of high-dimensional complex systems subject to random input. The first method represents a novel integration of the polynomial dimensional decomposition (PDD) of a multivariate stochastic response function and score functions. Applied to the statistical moments, the method provides mean-square convergent analytical expressions of design sensitivities of the first two moments of a stochastic response. The second and third methods, relevant to probability distribution or reliability analysis, exploit two distinct combinations built on PDD: the PDD-saddlepoint approximation (SPA) or PDD-SPA method, entailing SPA and score functions; and the PDD-Monte Carlo simulation (MCS) or PDD-MCS method, utilizing the embedded MCS of the PDD approximation and score functions. For all three methods developed, the statistical moments or failure probabilities and their design sensitivities are both determined concurrently from a single stochastic analysis or simulation. Numerical examples, including a -dimensional mathematical problem, indicate that the new methods developed provide not only theoretically convergent or accurate design sensitivities, but also computationally efficient solutions. A practical example involving robust design optimization of a three-hole bracket illustrates the usefulness of the proposed methods.",10.1002/nme.4659,https://dx.doi.org/10.1002/nme.4659
"Stromberg, N",2017,Reliability-based design optimization using SORM and SQP,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,2,2.0,"In this work a second order approach for reliability-based design optimization (RBDO) with mixtures of uncorrelated non-Gaussian variables is derived by applying second order reliability methods (SORM) and sequential quadratic programming (SQP). The derivation is performed by introducing intermediate variables defined by the incremental iso-probabilistic transformation at the most probable point (MPP). By using these variables in the Taylor expansions of the constraints, a corresponding general first order reliability method (FORM) based quadratic programming (QP) problem is formulated and solved in the standard normal space. The MPP is found in the physical space in the metric of Hasofer-Lind by using a Newton algorithm, where the efficiency of the Newton method is obtained by introducing an inexact Jacobian and a line-search of Armijo type. The FORM-based SQP approach is then corrected by applying four SORM approaches: Breitung, Hohenbichler, Tvedt and a recent suggested formula. The proposed SORM-based SQP approach for RBDO is accurate, efficient and robust. This is demonstrated by solving several established benchmarks, with values on the target of reliability that are considerable higher than what is commonly used, for mixtures of five different distributions (normal, lognormal, Gumbel, gamma and Weibull). Established benchmarks are also generalized in order to study problems with large number of variables and several constraints. For instance, it is shown that the proposed approach efficiently solves a problem with  variables and  constraints within less than  CPU minutes on a laptop. Finally, a most well-know deterministic benchmark of a welded beam is treated as a RBDO problem using the proposed SORM-based SQP approach.",10.1007/s00158-017-1679-3,https://dx.doi.org/10.1007/s00158-017-1679-3
"Zhu, XY | Linebarger, EM | Xiu, DB",2017,Multi-fidelity stochastic collocation method for computation of statistical moments,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,2,2.0,"We present an efficient numerical algorithm to approximate the statistical moments of stochastic problems, in the presence of models with different fidelities. The method extends the multi-fidelity approximation method developed in [,]. By combining the efficiency of low-fidelity models and the accuracy of high-fidelity models, our method exhibits fast convergence with a limited number of high-fidelity simulations. We establish an error bound of the method and present several numerical examples to demonstrate the efficiency and applicability of the multi-fidelity algorithm.",10.1016/j.jcp.2017.04.022,https://dx.doi.org/10.1016/j.jcp.2017.04.022
"Nagel, JB | Sudret, B",2016,Spectral likelihood expansions for Bayesian inference,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,4,2.0,A spectral approach to Bayesian inference is presented. It pursues the emulation of the posterior probability density. The starting point is a series expansion of the likelihood function in terms of orthogonal polynomials. From this spectral likelihood expansion all statistical quantities of interest can be calculated semi-analytically. The posterior is formally represented as the product of a reference density and a linear combination of polynomial basis functions. Both the model evidence and the posterior moments are related to the expansion coefficients. This formulation avoids Markov chain Monte Carlo simulation and allows one to make use of linear least squares instead. The pros and cons of spectral Bayesian inference are discussed and demonstrated on the basis of simple applications from classical statistics and inverse modeling.,10.1016/j.jcp.2015.12.047,https://dx.doi.org/10.1016/j.jcp.2015.12.047
"Sengupta, TK | Sengupta, A",2016,A new alternating bi-diagonal compact scheme for non-uniform grids,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,4,2.0,"A new compact scheme has been developed for any non-uniform grid. The compact scheme has been developed for spatial discretization and is analyzed here in conjunction with four-stage, fourth order Runge-Kutta (RK) scheme for time integration while solving the one-dimensional convection equation. The space-time discretization combination is calibrated by subjecting the system to global spectral analysis (GSA) which was developed by the authors' group. Here, the compact scheme has been obtained by using a combination of two bi-diagonal schemes. The novel aspect of this scheme is its application in the physical plane directly without the necessity of mapping or transformations. Some typical cases for problems in acoustics, as well as fluid mechanics, have been studied here and potential use in large eddy simulations (LES) has been demonstrated by solving Navier-Stokes equation for lid driven cavity.",10.1016/j.jcp.2016.01.014,https://dx.doi.org/10.1016/j.jcp.2016.01.014
"Rao, V | Sandu, A",2016,A time-parallel approach to strong-constraint four-dimensional variational data assimilation,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,4,2.0,A parallel-in-time algorithm based on an augmented Lagrangian approach is proposed to solve four-dimensional variational (D-Var) data assimilation problems. The assimilation window is divided into multiple sub-intervals that allows parallelization of cost function and gradient computations. The solutions to the continuity equations across interval boundaries are added as constraints. The augmented Lagrangian approach leads to a different formulation of the variational data assimilation problem than the weakly constrained D-Var. A combination of serial and parallel D-Vars to increase performance is also explored. The methodology is illustrated on data assimilation problems involving the Lorenz- and the shallow water models.,10.1016/j.jcp.2016.02.040,https://dx.doi.org/10.1016/j.jcp.2016.02.040
"Arnst, M | Alvarez, BA | Ponthot, JP | Boman, R",2017,Ito-SDE MCMC method for Bayesian characterization of errors associated with data limitations in stochastic expansion methods for uncertainty quantification,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,2,2.0,"This paper is concerned with the characterization and the propagation of errors associated with data limitations in polynomial-chaos-based stochastic methods for uncertainty quantification. Such an issue can arise in uncertainty quantification when only a limited amount of data is available. When the available information does not suffice to accurately determine the probability distributions that must be assigned to the uncertain variables, the Bayesian method for assigning these probability distributions becomes attractive because it allows the stochastic model to account explicitly for insufficiency of the available information. In previous work, such applications of the Bayesian method had already been implemented by using the Metropolis-Hastings and Gibbs Markov Chain Monte Carlo (MCMC) methods. In this paper, we present an alternative implementation, which uses an alternative MCMC method built around an Ito stochastic differential equation (SDE) that is ergodic for the Bayesian posterior. We draw together from the mathematics literature a number of formal properties of this Ito SDE that lend support to its use in the implementation of the Bayesian method, and we describe its discretization, including the choice of the free parameters, by using the implicit Euler method. We demonstrate the proposed methodology on a problem of uncertainty quantification in a complex nonlinear engineering application relevant to metal forming.",10.1016/j.jcp.2017.08.005,https://dx.doi.org/10.1016/j.jcp.2017.08.005
"Russo, S | Luchini, P",2017,A fast algorithm for the estimation of statistical error in DNS (or experimental) time averages,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,2,2.0,"Time- and space-averaging of the instantaneous results of DNS (or experimental measurements) represent a standard final step, necessary for the estimation of their means or correlations or other statistical properties. These averages are necessarily performed over a finite time and space window, and are therefore more correctly just estimates of the 'true' statistical averages. The choice of the appropriate window size is most often subjectively based on individual experience, but as subtler statistics enter the focus of investigation, an objective criterion becomes desirable. Here a modification of the classical estimator of averaging error of finite time series, i.e. 'batch means' algorithm, will be presented, which retains its speed while removing its biasing error. As a side benefit, an automatic determination of batch size is also included. Examples will be given involving both an artificial time series of known statistics and an actual DNS of turbulence.",10.1016/j.jcp.2017.07.005,https://dx.doi.org/10.1016/j.jcp.2017.07.005
"Mannina, G | Cosenza, A | Viviani, G",2017,Micropollutants throughout an integrated urban drainage model: Sensitivity and uncertainty analysis,Applications_JOURNAL OF HYDROLOGY,2,2.0,"The paper presents the sensitivity and uncertainty analysis of an integrated urban drainage model which includes micropollutants. Specifically, a bespoke integrated model developed in previous studies has been modified in order to include the micropollutant assessment (namely, sulfamethoxazole - SMX). The model takes into account also the interactions between the three components of the system: sewer system (SS), wastewater treatment plant (WWTP) and receiving water body (RWB). The analysis has been applied to an experimental catchment nearby Palermo (Italy): the Nocella catchment. Overall, five scenarios, each characterized by different uncertainty combinations of sub-systems (i.e., SS, WWTP and RWB), have been considered applying, for the sensitivity analysis, the Extended FAST method in order to select the key factors affecting the RWB quality and to design a reliable/useful experimental campaign. Results have demonstrated that sensitivity analysis is a powerful tool for increasing operator confidence in the modelling results. The approach adopted here can be used for blocking some non identifiable factors, thus wisely modifying the structure of the model and reducing the related uncertainty. The model factors related to the SS have been found to be the most relevant factors affecting the SMX modeling in the RWB when all model factors (scenario ) or model factors of SS (scenarios  and ) are varied. If the only factors related to the WWTP are changed (scenarios  and ), the SMX concentration in the RWB is mainly influenced (till to % influence of the total variance for S-SMX,S-max) by the aerobic sorption coefficient. A progressive uncertainty reduction from the upstream to downstream was found for the soluble fraction of SMX in the RWB.",10.1016/j.jhydrol.2017.09.026,https://dx.doi.org/10.1016/j.jhydrol.2017.09.026
"Gejadze, IY | Malaterre, PO",2016,Design of the control set in the framework of variational data assimilation,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,4,2.0,"Solving data assimilation problems under uncertainty in basic model parameters and in source terms may require a careful design of the control set. The task is to avoid such combinations of the control variables which may either lead to ill-posedness of the control problem formulation or compromise the robustness of the solution procedure. We suggest a method for quantifying the performance of a control set which is formed as a subset of the full set of uncertainty-bearing model inputs. Based on this quantity one can decide if the chosen 'safe' control set is sufficient in terms of the prediction accuracy. Technically, the method presents a certain generalization of the 'variational' uncertainty quantification method for observed systems. It is implemented as a matrix-free method, thus allowing high-dimensional applications. Moreover, if the Automatic Differentiation is utilized for computing the tangent linear and adjoint mappings, then it could be applied to any multi-input 'black-box' system. As application example we consider the full Saint-Venant hydraulic network model SIC, which describes the flow dynamics in river and canal networks. The developed methodology seem useful in the context of the future SWOT satellite mission, which will provide observations of river systems the properties of which are known with quite a limited precision.",10.1016/j.jcp.2016.08.029,https://dx.doi.org/10.1016/j.jcp.2016.08.029
"Bolster, CH | Vadas, PA | Boykin, D",2016,Model parameter uncertainty analysis for an annual field-scale P loss model,Applications_JOURNAL OF HYDROLOGY,4,2.0,"Phosphorous (P) fate and transport models are important tools for developing and evaluating conservation practices aimed at reducing P losses from agricultural fields. Because all models are simplifications of complex systems, there will exist an inherent amount of uncertainty associated with their predictions. It is therefore important that efforts be directed at identifying, quantifying, and communicating the different sources of model uncertainties. In this study, we conducted an uncertainty analysis with the Annual P Loss Estimator (APLE) model. Our analysis included calculating parameter uncertainties and confidence and prediction intervals for five internal regression equations in APLE. We also estimated uncertainties of the model input variables based on values reported in the literature. We then predicted P loss for a suite of fields under different management and climatic conditions while accounting for uncertainties in the model parameters and inputs and compared the relative contributions of these two sources of uncertainty to the overall uncertainty associated with predictions of P loss. Both the overall magnitude of the prediction uncertainties and the relative contributions of the two sources of uncertainty varied depending on management practices and field characteristics. This was due to differences in the number of model input variables and the uncertainties in the regression equations associated with each P loss pathway. Inspection of the uncertainties in the five regression equations brought attention to a previously unrecognized limitation with the equation used to partition surface-applied fertilizer P between leaching and runoff losses. As a result, an alternate equation was identified that provided similar predictions with much less uncertainty. Our results demonstrate how a thorough uncertainty and model residual analysis can be used to identify limitations with a model. Such insight can then be used to guide future data collection and model development and evaluation efforts.",10.1016/j.jhydrol.2016.05.009,https://dx.doi.org/10.1016/j.jhydrol.2016.05.009
"Zhu, JH | Zhang, WH",2006,Maximization of structural natural frequency with optimal support layout,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,23,1.92,"The optimal layout of supports is one of the key factors that dominates static and dynamic performances of the structure. In this work, supports are considered as elastic springs. The purpose is to carry out layout optimization of supports by means of topology optimization method. The technique of pseudo-density variables that transforms a discrete-variable problem into a continuous one is used in order that the problem is easily formulated and solved numerically. In this formulation, a power law of the so-called solid isotropic material with penalty model is employed to approximate the relation between the element stiffness matrix and density variable. Such a relation makes it easy to establish the computing scheme and sensitivity analysis of natural frequency. Support layout design that corresponds to optimization of boundary conditions is studied to maximize the natural frequency of structures. Numerical results show that reasonable distributions of supports can be generated effectively.",10.1007/s00158-005-0593-2,https://dx.doi.org/10.1007/s00158-005-0593-2
"Chen, BS | Tong, LY",2005,Thermomechanically coupled sensitivity analysis and design optimization of functionally graded materials,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,25,1.92,"This paper presents a systematic numerical technique for performing sensitivity analysis of coupled thermomechanical problem of functionally graded materials (FGMs). General formulations are presented based on finite element model by using the direct method and the adjoint method. In the modeling of spatial variances of material properties, the graded finite element method is employed to conduct the heat transfer analysis and structural analysis and their sensitivity analysis. The design variables are the volume fractions of FGMs constituents and structural shape parameters. The design optimization model is then constructed and solved by the sequential linear programming (SLP). Numerical examples are presented to demonstrate the accuracy and the applicability of the present method.",10.1016/j.cma.2004.07.005,https://dx.doi.org/10.1016/j.cma.2004.07.005
"Barbato, M | Zona, A | Conte, JP",2007,Finite element response sensitivity analysis using three-field mixed formulation: General theory and application to frame structures,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,21,1.91,"This paper presents a method to compute response sensitivities of finite element models of structures based on a three-field mixed formulation. The methodology is based on the direct differentiation method (DDM), and produces the response sensitivities consistent with the numerical finite element response. The general formulation is specialized to frame finite elements and details related to a newly developed steel-concrete composite frame element are provided. DDM sensitivity results are validated through the forward finite difference method (FDM) using a finite element model of a realistic steel-concrete composite frame subjected to quasi-static and dynamic loading. The finite element model of the structure considered is constructed using both monolithic frame elements and composite frame elements with deformable shear connection based on the three-field mixed formulation. The addition of the analytical sensitivity computation algorithm presented in this paper extends the use of finite elements based on a three-field mixed formulation to applications that require finite element response sensitivities. Such applications include structural reliability analysis, structural optimization, structural identification, and finite element model updating.",10.1002/nme.1759,https://dx.doi.org/10.1002/nme.1759
"Li, J | Xiu, DB",2008,On numerical properties of the ensemble Kalman filter for data assimilation,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,19,1.9,"Ensemble Kalman filter (EnKF) has been widely used as a sequential data assimilation method, primarily due to its ease of implementation resulting from replacing the covariance evolution in the traditional Kalman filter (KF) by an approximate Monte Carlo ensemble sampling. In this paper rigorous analysis on the numerical errors of the EnKF is conducted in a general setting. Error bounds are provided and convergence of the EnKF to the exact Kalman filter is established. The analysis reveals that the ensemble errors induced by the Monte Carlo sampling can be dominant, compared to other errors such as the numerical integration error of the underlying model equations. Methods to reduce sampling errors are discussed. In particular, we present a deterministic sampling strategy based on cubature rules (qEnKF) which offers much improved accuracy. The analysis also suggests a less obvious fact - more frequent data assimilation may lead to larger numerical errors of the EnKF. Numerical examples are provided to verify the theoretical findings and to demonstrate the improved performance of the qEnKF.",10.1016/j.cma.2008.03.022,https://dx.doi.org/10.1016/j.cma.2008.03.022
"Khosravi, P | Sedaghati, R",2008,"Design of laminated composite structures for optimum fiber direction and layer thickness, using optimality criteria",Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,19,1.9,"In this study, two optimality criteria are presented for optimum design of composite laminates using finite element method. Thickness of the layers and fiber orientation angles in each finite element are considered as the design variables. It will be shown that the optimum design of composite laminates with varying fiber orientations and layers thicknesses may be found by using these optimality criteria in an efficient way, without performing the sensitivity analysis.",10.1007/s00158-007-0207-2,https://dx.doi.org/10.1007/s00158-007-0207-2
"Bruls, O | Eberhard, P",2008,Sensitivity analysis for dynamic mechanical systems with finite rotations,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,19,1.9,"This paper presents a sensitivity analysis for dynamic systems with large rotations using a semi-analytical direct differentiation technique. The choice of a suitable time integration strategy for the rotation group appears to be critical for the development of an efficient sensitivity analysis. Three versions of the generalized-alpha time integration scheme are considered: a residual form, a linear form, and a geometric form. Their consistency is discussed, and we show that the residual form, which is the most direct extension of the generalized-alpha algorithm defined in structural dynamics, should not be used for problems with large rotations. The sensitivity analysis is performed and close connections are highlighted between the structure of the sensitivity equations and of the linearized dynamic equations. Hence, algorithms developed for the original problem can be directly reused for the sensitivities. Finally, a numerical example is analysed in detail.",10.1002/nme.2232,https://dx.doi.org/10.1002/nme.2232
"Jin, WY | Dennis, BH | Wang, BP",2010,Improved sensitivity analysis using a complex variable semi-analytical method,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,15,1.88,"The semi-analytical method (SAM) is a computationally efficient and easy to implement approach often used for the sensitivity analysis of finite element models. However, it is known to exhibit serious inaccuracy for shape sensitivity analysis for structures modeled by beam, frame, plate, or shell elements. In the present paper, we use a semi-analytical approach based on complex variables (SACVM) to compute the sensitivity of finite element models composed of beam and plate elements. The SACVM combines the complex variable method (CVM) with the semi-analytical method (SAM) to obtain the response sensitivity accurately and efficiently. The current approach maintains the computational efficiency of the semi-analytical method but with higher accuracy. In addition, the current approach is insensitive to the choice of step size, a feature that simplifies its use in practical problems. The method is applicable to any structural elements including beam, frame, plate, or shell elements and only requires minor modifications to existing finite element codes.",10.1007/s00158-009-0427-8,https://dx.doi.org/10.1007/s00158-009-0427-8
"Hill, MC",2010,"Comment on ""Two statistics for evaluating parameter identifiability and error reduction"" by John Doherty and Randall J. Hunt",Applications_JOURNAL OF HYDROLOGY,15,1.88,"Doherty and Hunt () present important ideas for first-order-second moment sensitivity analysis, but five issues are discussed in this comment. First, considering the composite-scaled sensitivity (CSS) jointly with parameter correlation coefficients (PCC) in a CSS/PCC analysis addresses the difficulties with CSS mentioned in the introduction. Second, their new parameter identifiability statistic actually is likely to do a poor job of parameter identifiability in common situations. The statistic instead performs the very useful role of showing how model parameters are included in the estimated singular value decomposition (SVD) parameters. Its close relation to CSS is shown. Third, the idea from p.  that a suitable truncation point for SVD parameters can be identified using the prediction variance is challenged using results from Moore and Doherty (). Fourth, the relative error reduction statistic of Doherty and Hunt is shown to belong to an emerging set of statistics here named perturbed calculated variance statistics. Finally, the perturbed calculated variance statistics OPR and PPR mentioned on p.  are shown to explicitly include the parameter null-space component of uncertainty. Indeed, OPR and PPR results that account for null-space uncertainty have appeared in the literature since .",10.1016/j.jhydrol.2009.10.011,https://dx.doi.org/10.1016/j.jhydrol.2009.10.011
"Becker, R | Vexler, B",2005,Mesh refinement and numerical sensitivity analysis for parameter calibration of partial differential equations,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,24,1.85,"We consider the calibration of parameters in physical models described by partial differential equations. This task is formulated as a constrained optimization problem with a cost functional of least squares type using information obtained from measurements. An important issue in the numerical solution of this type of problem is the control of the errors introduced, first, by discretization of the equations describing the physical model, and second, by measurement errors or other perturbations. Our strategy is as follows: we suppose that the user defines an interest functional , which might depend on both the state variable and the parameters and which represents the goal of the computation. First, we propose an a posteriori error estimator which measures the error with respect to this functional. This error estimator is used in an adaptive algorithm to construct economic meshes by local mesh refinement. The proposed estimator requires the solution of an auxiliary linear equation, Second, we address the question of sensitivity. Applying similar techniques as before, we derive quantities which describe the influence of small changes in the measurements on the value of the interest functional. These numbers, which we call relative condition numbers, give additional information on the problem under consideration. They can be computed by means of the solution of the auxiliary problem determined before. Finally, we demonstrate our approach at hand of a parameter calibration problem for a model flow problem.",10.1016/j.jcp.2004.12.018,https://dx.doi.org/10.1016/j.jcp.2004.12.018
"Grigoriu, M",2012,A method for solving stochastic equations by reduced order models and local approximations,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,11,1.83,"A method is proposed for solving equations with random entries, referred to as stochastic equations (SEs). The method is based on two recent developments. The first approximates the response surface giving the solution of a stochastic equation as a function of its random parameters by a finite set of hyperplanes tangent to it at expansion points selected by geometrical arguments. The second approximates the vector of random parameters in the definition of a stochastic equation by a simple random vector, referred to as stochastic reduced order model (SROM), and uses it to construct a SROM for the solution of this equation. The proposed method is a direct extension of these two methods. It uses SROMs to select expansion points, rather than selecting these points by geometrical considerations, and represents the solution by linear and/or higher order local approximations. The implementation and the performance of the method are illustrated by numerical examples involving random eigenvalue problems and stochastic algebraic/differential equations. The method is conceptually simple, non-intrusive, efficient relative to classical Monte Carlo simulation, accurate, and guaranteed to converge to the exact solution.",10.1016/j.jcp.2012.06.013,https://dx.doi.org/10.1016/j.jcp.2012.06.013
"Schleupen, A | Maute, K | Ramm, E",2000,Adaptive FE-procedures in shape optimization,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,33,1.83,"In structural optimization the quality of the optimization result strongly depends on the reliability of the underlying structural analysis. This comprises the quality and range of the mechanical model, e.g. linear elastic or geometrically and materially nonlinear, as well as the accuracy of the numerical model, e.g, the discretization error of the FE-model. The latter aspect is addressed in the present contribution. In order to guarantee the quality of the numerical results the discretization error of the finite element solution is controlled and the finite element discretization is adaptively refined during the optimization process. Conventionally, so-called global error estimates are applied in structural optimization which estimate the error of the total strain energy. In the present paper local error estimates are introduced in shape optimisation which allow to control directly the discretization error of local optimization criteria. In general! the adaptive refinement of the finite element discretization by remeshing affects the convergence of the optimization process if a gradient-based optimization algorithm is applied. In order to reduce this effect the sensitivity of the discretization error must also be restricted. Suitable refinement indicators are developed for globally and locally adaptive procedures. Finally, the potential of two techniques, which may improve the numerical efficiency of adaptive FE-procedures within the optimization process, is studied. The proposed methods and procedures are verified by -D shape optimization examples.",10.1007/s001580050125,https://dx.doi.org/10.1007/s001580050125
"Vikhansky, A | Kraft, M",2004,A Monte Carlo methods for identification and sensitivity analysis of coagulation processes,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,25,1.79,"A stochastic simulation algorithm is presented to calculate parametric derivatives of solutions of a population balance equation. The dispersed system is approximated by an N-particle stochastic weighted ensemble. The derivatives are accounted for through infinitesimal deviation of the statistical weights that are recalculated at each coagulation. Thus, all the parametric derivatives can be calculated along one trajectory of the process, given N sufficiently large. We use an operator-splitting technique to account for surface growth of the particles. The obtained solution is in good agreement with the available analytical solutions. As soon as the parametric derivatives are known the gradient-based methods can be applied to the control and identification of the coagulation process. The extension of the proposed technique to a multi-dimensional case is straightforward.",10.1016/j.jcp.2004.03.006,https://dx.doi.org/10.1016/j.jcp.2004.03.006
"Neto, MA | Ambrosio, JAC | Leal, RP",2009,Sensitivity analysis of flexible multibody systems using composite materials components,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,16,1.78,"In this paper, a general formulation for the computation of the first-order analytical sensitivities based on the direct method using automatic differentiation of flexible multibody systems is presented. The direct method for sensitivity calculation is obtained by differentiating the equations that define the response of the flexible multibody systems of composite materials with respect to the design variables, which are the ply orientations of the laminated. In order to appraise the benefits of the approach suggested and to highlight the risks of the procedure, the analytical sensitivities are compared with the numerical results obtained by using the finite difference method. For the beam composite material elements, the section properties and their sensitivities are found using an asymptotic procedure that involves a two-dimensional (-D) finite element analysis of their cross section. The equations of the sensitivities are obtained by automatic differentiation and integrated in time simultaneously with the equations of motion of the multibody systems. The equations of motion and the sensitivities of the flexible multibody system are solved and the accelerations, velocities and the sensitivities of accelerations and velocities are integrated in time using a multi-step multi-order integration algorithm. Through the application of the methodology to two simple flexible multibody systems the difficulties and benefits of the procedure, with respect to finite difference approaches or to the direct implementation of the analytic sensitivities, are discussed.",10.1002/nme.2417,https://dx.doi.org/10.1002/nme.2417
"Khadam, IM | Kaluarachchi, JJ",2006,Water quality modeling under hydrologic variability and parameter uncertainty using erosion-scaled export coefficients,Applications_JOURNAL OF HYDROLOGY,21,1.75,"Water quality modeling is important to assess the health of a watershed and to make necessary management decisions to control existing and future pollution of receiving water bodies. The existing export coefficient approach is attractive due to minimum data requirements; however, this method does not account for hydrologic variability. In this paper, an erosion-scaLed export coefficient approach is proposed that can model and explain the hydrologic variability in predicting the annual phosphorus (P) loading to the receiving stream. Here sediment discharge was introduced into the export coefficient model as a surrogate for hydrologic variability. Application of this approach to model P in the Fishtrap Creek of Washington State showed the superiority of this approach compared to the traditional export coefficient approach, white maintaining its simplicity and low data requirement characteristics. In addition, a Bayesian framework is proposed to assess the parameter uncertainty of the export coefficient method instead of subjective assignment of uncertainty. This work also showed through a joint variability-uncertainty analysis the importance of separate consideration of hydrologic variability and parameter uncertainty, as these represent two independent and important characteristics of the overall model uncertainty. The paper also recommends the use of a longitudinal data collection scheme to reduce the uncertainty in export coefficients.",10.1016/j.jhydrol.2006.03.033,https://dx.doi.org/10.1016/j.jhydrol.2006.03.033
"Babuska, I | Motamed, M | Tempone, R",2014,A stochastic multiscale method for the elastodynamic wave equation arising from fiber composites,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,7,1.75,"We present a stochastic multilevel global local algorithm for computing elastic waves propagating in fiber-reinforced composite materials. Here, the materials properties and the size and location of fibers may be random. The method aims at approximating statistical moments of some given quantities of interest, such as stresses, in regions of relatively small size, e.g. hot spots or zones that are deemed vulnerable to failure. For a fiber-reinforced cross-plied laminate, we introduce three problems (macro, meso, micro) corresponding to the three natural scales, namely the sizes of laminate, ply, and fiber. The algorithm uses the homogenized global solution to construct a good local approximation that captures the microscale features of the real solution. We perform numerical experiments to show the applicability and efficiency of the method.",10.1016/j.cma.2014.02.018,https://dx.doi.org/10.1016/j.cma.2014.02.018
"Stupkiewicz, S | Lengiewicz, J | Korelc, J",2010,Sensitivity analysis for frictional contact problems in the augmented Lagrangian formulation,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,14,1.75,"Direct differentiation method of sensitivity analysis is developed for frictional contact problems. As a result of the augmented Lagrangian treatment of contact constraints, the direct problem is solved simultaneously for the displacements and Lagrange multipliers using the Newton method. The main purpose of the paper is to show that this formulation of the augmented Lagrangian method is particularly suitable for sensitivity analysis because the direct differentiation method leads to a non-iterative exact sensitivity problem to be solved at each time increment. The approach is applied to a general class of three-dimensional frictional contact problems, and numerical examples are provided involving large deformations, multibody contact interactions, and contact smoothing techniques.",10.1016/j.cma.2010.03.021,https://dx.doi.org/10.1016/j.cma.2010.03.021
"de Gournay, F | Fehrenbach, J | Plouraboue, F",2014,Shape optimization for the generalized Graetz problem,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,7,1.75,"We apply shape optimization tools to the generalized Graetz problem which is a convection-diffusion equation. The problem boils down to the optimization of generalized eigenvalues on a two phases domain. Shape sensitivity analysis is performed with respect to the evolution of the interface between the fluid and solid phase. In particular physical settings, counterexamples where there is no optimal domains are exhibited. Numerical examples of optimal domains with different physical parameters and constraints are presented. Two different numerical methods (level-set and mesh-morphing) are show-cased and compared.",10.1007/s00158-013-1032-4,https://dx.doi.org/10.1007/s00158-013-1032-4
"Huang, CS | Lin, WS | Yeh, HD",2014,"Stream filtration induced by pumping in a confined, unconfined or leaky aquifer bounded by two parallel streams or by a stream and an impervious stratum",Applications_JOURNAL OF HYDROLOGY,7,1.75,"A mathematical model is developed for describing three-dimensional groundwater flow induced by a fully-penetrating vertical well in aquifers between two parallel streams. A general equation is adopted to represent the top boundary condition which is applicable to either a confined, unconfined or leaky aquifer. The Robin (third-type) boundary condition is employed to represent the low-permeability streambeds. The Laplace-domain head solution of the model is derived by the double-integral and Laplace transforms. The Laplace-domain solution for a stream depletion rate (SDR) describing filtration from the streams is developed based on Darcy's law and the head solution and inverted to the timedomain result by the Crump method. In addition, the time-domain solution of SDR for the confined aquifer is developed analytically after taking the inverse Laplace transform and the time-domain solutions of SDR for the leaky and unconfined aquifers are developed using the Fade approximation. Both approximate solutions of SDR are expressed in terms of simple series and give fairly good match with the Laplace-domain SDR solution and measured data from a field experiment in New Zealand. The uncertainties in SDR predictions for the aquifers are assessed by performing the sensitivity analysis and Monte Carlo simulation. With the aid of the time-domain solutions, we have found that the effect of the vertical groundwater flow on the temporal SDR for a leaky aquifer is dominated by two lumped parameters: it = K (v)x()()/(KhD) and k'=k'(B'K-v) where D is the aquifer thickness, x(o) is a distance between the well and nearer stream, K-h and K are the aquifer horizontal and vertical hydraulic conductivities, respectively, and K' and B' are the aquitard hydraulic conductivity and thickness, respectively. When it < , neglecting the vertical flow underestimates the SDR. When it >= , the effect of vertical flow is negligible. When K'<= (-), the aquitard can be regarded as impermeable, and the leaky aquifer behaves as a confined one.",10.1016/j.jhydrol.2014.03.039,https://dx.doi.org/10.1016/j.jhydrol.2014.03.039
"Jin, BT | Zheng, Y | Marin, L",2006,The method of fundamental solutions for inverse boundary value problems associated with the steady-state heat conduction in anisotropic media,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,21,1.75,"In this paper, the method of fundamental solutions is applied to solve some inverse boundary value problems associated with the steady-state heat conduction in an anisotropic medium. Since the resulting matrix equation is severely ill-conditioned, a regularized solution is obtained by employing the truncated singular value decomposition, while the optimal regularization parameter is chosen according to the L-curve criterion. Numerical results are presented for both two- and three-dimensional problems, as well as exact and noisy data. The convergence and stability of the proposed numerical scheme with respect to increasing the number of source points and the distance between the fictitious and physical boundaries, and decreasing the amount of noise added into the input data, respectively, are analysed. A sensitivity analysis with respect to the measure of the accessible part of the boundary and the distance between the internal measurement points and the boundary is also performed. The numerical results obtained show that the proposed numerical method is accurate, convergent, stable and computationally efficient, and hence it could be considered as a competitive alternative to existing methods for solving inverse problems in anisotropic steady-state heat conduction.",10.1002/nme.1526,https://dx.doi.org/10.1002/nme.1526
"Papoutsis-Kiachagias, EM | Kyriacou, SA | Giannakoglou, KC",2014,The continuous adjoint method for the design of hydraulic turbomachines,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,7,1.75,"This article presents the development and application of the continuous adjoint method for designing/optimizing the shape of hydraulic turbomachines. The Reynolds-averaged flow equations are solved in the rotating reference frame and the terms arising from the differentiation of the Coriolis and centripetal forces are taken into account in the formulation of the adjoint equations. The objective functions presented in this article can be used for achieving (a) the optimal collaboration of the runner impeller with the draft tube, by controlling the meridional and circumferential velocity profiles at the exit of the runner, (b) the operation at the desired hydraulic head and/or (c) the cavitation suppression. All of them are used to improve an existing Francis runner. It is important to note that the objective function related to cavitation is, by definition, non-differentiable and a way to effectively handle it, is proposed. The continuous adjoint method is presented in its most general form and could readily be adapted to other objective functions in the field of hydraulic turbomachines.",10.1016/j.cma.2014.05.018,https://dx.doi.org/10.1016/j.cma.2014.05.018
"Cheviron, B | Delmas, M | Cerdan, O | Mouchel, JM",2014,Calculation of river sediment fluxes from uncertain and infrequent measurements,Applications_JOURNAL OF HYDROLOGY,7,1.75,"This paper addresses feasibility issues in the calculation of fluxes of suspended particulate matter (SPM) from degraded-quality data for flow discharge (Q) and sediment concentration (C) under the additional constraints of infrequent and irregular sediment concentration samplings. A crucial setting of the scope involves establishing the number of data required to counterbalance limitations in the measurement accuracy and frequency of data collection. This study also compares the merits and drawbacks of the classical rating curve (C = aQ(b)) with those of an improved rating curve approach (IRCA: C=aQ(b) + a delta S) in which the correction term is an indicator of the variations in sediment storage, thus relating it to flow dynamics. This alternative formulation remedies the known systematic underestimations in the classical rating curve and correctly resists the degradation in data quality and availability, as shown in a series of problematic though realistic cases. For example, monthly concentration samplings (in average) with a random relative error in the [-%, +%] range combined with daily discharge records with a systematic relative error in the [-%, +%] range still yield SPM fluxes within factors of .-. of the real value, provided that  years of data are available. A shorter -day time interval (on average) between samplings lowers the relative error in the SPM fluxes to below %, a result directly related to the increased number of Q-C pairs available for fitting. For regional-scale applications, this study may be used to define the data quality level (uncertainty, frequency and/or number) compatible with reliable computation of river sediment fluxes. Provided that at least  concentration samplings are available, the use of a sediment rating curve model augmented to account for storage effects fulfils this purpose with satisfactory accuracy under real-life conditions.",10.1016/j.jhydrol.2013.10.057,https://dx.doi.org/10.1016/j.jhydrol.2013.10.057
"Mertens, J | Jacques, D | Vanderborght, J | Feyen, J",2002,Characterisation of the field-saturated hydraulic conductivity on a hillslope: in situ single ring pressure infiltrometer measurements,Applications_JOURNAL OF HYDROLOGY,28,1.75,"Spatial variability of surface hydraulic properties is an important factor for infiltration and runoff processes in agricultural fields. At  locations on a hillslope, steady-state infiltration rates were measured at two applied pressure heads with a single-ring infiltrometer. The solution of two steady-state infiltration equations for each location (the simultaneous-equation approach, SEA) yielded  negative alpha-values,  positive alpha values and  positive K(fs)-values. The sensitivity of K(fs) and alpha to small measurement errors was estimated using a Monte-Carlo simulation (MC). Results of this MC simulation showed that the uncertainty on alpha is extremely high while the uncertainty on K(fs) is fairly small. Hence, although the pressure infiltrometer technique as applied here is useful to estimate K(fs) at each measurement location, it is not suited for the estimation of an alpha-value at each measurement location. A new procedure is proposed for the simultaneous estimation of one overall 'field alpha' and the  K(fs) values of measurement locations having a positively calculated a using SEA. Using this field alpha, K(fs) values for the other locations with a negative a are hence determined. Finally, the spatial correlation of K(fs) on the hillslope is examined. Ranges of ln(K(fs)) between . and . m were observed, respectively, for the omnidirectional case and the y direction along the hillslope mu.",10.1016/S0022-1694(02)00052-5,https://dx.doi.org/10.1016/S0022-1694(02)00052-5
"Yamasaki, S | Nomura, T | Kawamoto, A | Sato, K | Izui, K | Nishiwaki, S",2010,A level set based topology optimization method using the discretized signed distance function as the design variables,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,14,1.75,"This paper deals with a new topology optimization method based on the level set method. In the proposed method, the discretized signed distance function, a kind of level set function, is used as the design variables, and these are then updated using their sensitivities. The signed distance characteristic of the design variables are maintained by performing a re-initialization at every update during the iterated optimization procedure. In this paper, a minimum mean compliance problem and a compliant mechanism design problem are formulated based on the level set method. In the formulations of these design problems, a perimeter constraint is imposed to overcome the ill-posedness of the structural optimization problem. The sensitivity analysis for the above structural optimization problems is conducted based on the adjoint variable method. The augmented Lagrangian method is incorporated to deal with multiple constraints. Finally, several numerical examples that include multiple constraints are provided to confirm the validity of the method, and it is shown that appropriate optimal structures are obtained.",10.1007/s00158-009-0453-6,https://dx.doi.org/10.1007/s00158-009-0453-6
"Song, J | Shanghvi, JY | Michaleris, P",2004,Sensitivity analysis and optimization of thermo-elasto-plastic processes with applications to welding side heater design,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,24,1.71,A computational scheme is presented to optimize quasi-static weakly coupled thermo-elasto-plastic processes in three dimensional Lagrangian reference frames. Sensitivity formulations are developed from the radial return algorithm based thermo-elasto-plastic finite element equations using the direct differentiation method. These formulations are exemplified in the optimization of the side heaters in the transient thermal tensioning welding process for minimum residual stress. The results of the direct differentiation sensitivity analysis are validated by comparing with finite difference sensitivity calculations. Optimization is performed using the BFGS line search method.,10.1016/j.cma.2004.03.007,https://dx.doi.org/10.1016/j.cma.2004.03.007
"Li, ST | Petzold, L",2004,Adjoint sensitivity analysis for time-dependent partial differential equations with adaptive mesh refinement,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,24,1.71,"A new adjoint sensitivity analysis approach is presented for time-dependent partial differential equations with adaptive mesh refinement. The new approach, called ADDA, combines the best features of both the adjoint of the discretization (AD) and discretization of the adjoint (DA) approaches. It removes the obstacles of applying AD to adaptive methods and, in contrast to DA, requires for its use only a minimal amount of knowledge about the formulation of adjoint PDEs and their boundary conditions. The effectiveness and efficiency of ADDA are demonstrated for several numerical examples.",10.1016/j.jcp.2003.01.001,https://dx.doi.org/10.1016/j.jcp.2003.01.001
"Milani, R | Quarteroni, A | Rozza, G",2008,Reduced basis method for linear elasticity problems with many parameters,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,17,1.7,"The reduced basis (RB) methods are proposed here for the solution of parametrized equations in linear elasticity problems. The fundamental idea underlying RB methods is to decouple the generation and projection stages (offline/online computational procedures) of the approximation process in order to solve parametrized equations in a rapid, inexpensive and reliable way. The method allows important computational savings with respect to the classical Galerkin-finite element method, ill suited to a repetitive environment like the parametrized contexts of optimization, many queries and sensitivity analysis. We consider different parametrization for the systems: either physical quantities - to model the materials and loads - and geometrical parameters - to model different geometrical configurations. Then we describe three different applications of the method in problems with isotropic and orthotropic materials working in plane stress and plane strain approximation and subject to harmonic loads.",10.1016/j.cma.2008.07.002,https://dx.doi.org/10.1016/j.cma.2008.07.002
"Park, KS | Youn, SK",2008,Topology optimization of shell structures using adaptive inner-front (AIF) level set method,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,17,1.7,"A new topology optimization using adaptive inner-front level set method is presented. In the conventional level set-based topology optimization, the optimum topology strongly depends on the initial level set due to the incapability of inner-front creation during the optimization process. In the present work, in this regard, an algorithm for inner-front creation is proposed in which the sizes, the positions, and the number of new inner-fronts during the optimization process can be globally and consistently identified. In the algorithm, the criterion of inner-front creation for compliance minimization problems of linear elastic structures is chosen as the strain energy density along with volumetric constraint. To facilitate the inner-front creation process, the inner-front creation map is constructed and used to define new level set function. In the implementation of inner-front creation algorithm, to suppress the numerical oscillation of solutions due to the sharp edges in the level set function, domain regularization is carried out by solving the edge smoothing partial differential equation (smoothing PDE). To update the level set function during the optimization process, the least-squares finite element method (LSFEM) is adopted. Through the LSFEM, a symmetric positive definite system matrix is constructed, and non-diffused and non-oscillatory solution for the hyperbolic PDE such as level set equation can be obtained. As applications, three-dimensional topology optimization of shell structures is treated. From the numerical examples, it is shown that the present method brings in much needed flexibility in topologies during the level set-based topology optimization process.",10.1007/s00158-007-0169-4,https://dx.doi.org/10.1007/s00158-007-0169-4
"Chen, XH | Chen, X",2003,Sensitivity analysis and determination of streambed leakance and aquifer hydraulic properties,Applications_JOURNAL OF HYDROLOGY,25,1.67,"A nonlinear regression method is used to calculate the hydraulic parameters of a stream-aquifer system using pumping test data. Five parameters, including the horizontal hydraulic conductivity (K(x)), aquifer anisotropy (K(a)), streambed leakance l, aquifer specific storage S(s), and specific yield S(y), can be calculated. MODFLOW, coupled to the regression method, simulates the groundwater flow that is affected by streams. Sensitivity analyses indicate that for a given stream-aquifer system, the quality of the stream-aquifer test data can be improved through a careful selection of observation and pumping wells, as well as an appropriate test duration. An optimal location of an observation well is where the magnitude of the sensitivities is enhanced and the correlation of the transient sensitivities of two parameters is reduced. Generally, a longer pumping period will increase the sensitivity for l and K(x) and reduce the correlation between S(y) and K(x) and between S(y) and l. Results from hypothetical examples and a field test suggest that a two-well analysis of pumping test data can significantly reduce the correlation of sensitivity coefficients; as a result, convergence occurs faster and the estimated standard errors are reduced.",10.1016/j.jhydrol.2003.08.004,https://dx.doi.org/10.1016/j.jhydrol.2003.08.004
"Tang, GP | Mayes, MA | Parker, JC | Yin, XPL | Watson, DB | Jardine, PM",2009,Improving parameter estimation for column experiments by multi-model evaluation and comparison,Applications_JOURNAL OF HYDROLOGY,15,1.67,"The equilibrium convection dispersion equation model is often unable to accurately simulate breakthrough curves from column experiments. While the non-equilibrium convection dispersion equation model may match the data well, uncertainty in parameter estimates is often large. In this work we investigate approaches to improve match for the equilibrium model and reduce parameter estimate uncertainty for the non-equilibrium model. Four column experiment data sets are selected from the literature for the illustration. For the equilibrium convection dispersion equation model, we show that measurement error, presence of immobile water, and other mechanisms can cause mismatch between model predictions and observations because the model is sensitive to water content. The mismatch may be overcome by calibrating the effective water content. For the non-equilibrium convection dispersion equation model, simultaneous fitting of multiple tracers with reduced number of calibration parameters (e.g., assuming the dispersivity and mobile water fraction to be identical for different tracers, the mass transfer coefficient to be proportional to tracer molecular diffusion coefficient) can reduce the uncertainty in parameter estimate and better identify/quantify the non-equilibrium processes. By evaluating and comparing the multiple estimates obtained with different choices of calibration parameters (e.g., fixing or estimating water content), parameterizations and models (e.g., equilibrium or non-equilibrium), the reliability of the data interpretation can be improved by quantifying uncertainty in the experiment, considering alternative transport processes, and following the principle of parsimony.",10.1016/j.jhydrol.2009.07.063,https://dx.doi.org/10.1016/j.jhydrol.2009.07.063
"Park, HM | Shin, HJ",2003,Shape identification for natural convection problems using the adjoint variable method,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,25,1.67,"An inverse geometry problem is investigated to identify the boundary shape of a domain from temperature measurements on the other boundary, where the temperature field is dominated by natural convection. The potential applications of the present investigation are the determination of a phase change isotherm in the Bridgman crystal growth or the thermal tomography which detects flaws in materials nondestructively. The inverse problem is posed as a minimization problem of the performance function, which is the sum of square residuals between calculated and observed temperature, by means of a conjugate gradient method employing the adjoint variable method. The present method is found to identify the domains reasonably accurately even with noisy temperature measurements.",10.1016/S0021-9991(03)00046-9,https://dx.doi.org/10.1016/S0021-9991(03)00046-9
"Lee, J | Seo, JH | Kikuchi, N",2010,Topology optimization of switched reluctance motors for the desired torque profile,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,13,1.62,"This paper presents the design optimization of switched reluctance motors (SRMs). The design goals are high average torque, low torque ripple and low mass under the constrained phase current of SRMs. To achieve these goals, we design not only the geometries of SRMs using topology optimization, but also voltage on-off angles. The performance analysis of SRMs is carried out to predict the torque profile. Fourier series expansion is used to obtain the explicit expression of inductance curves, which allows us to perform the analytical design sensitivity analysis of the torque profile. The optimization problem is solved using the sequential linear programming (SLP) method. In the optimized geometry, the arc lengths of poles are increased with the notched shape near airgap, and the holes inside the rotor are created. In addition, the mechanical characteristics of the optimized SRM are investigated using the modal and thermal analysis by MSC/NASTRAN.",10.1007/s00158-010-0547-1,https://dx.doi.org/10.1007/s00158-010-0547-1
"Heider, P | Berebichez, D | Kohn, RV | Weinstein, MI",2008,Optimization of scattering resonances,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,16,1.6,"The increasing use of micro- and nano-scale components in optical, electrical, and mechanical systems makes the understanding of loss mechanisms and their quantification issues of fundamental importance. In many situations, performance-limiting loss is due to scattering and radiation of waves into the surrounding structure. In this paper, we study the problem of systematically improving a structure by altering its design so as to decrease the loss. We use sensitivity analysis and local gradient optimization, applied to the scattering resonance problem, to reduce the loss within the class of piecewise constant structures. For a class of optimization problems where the material parameters are constrained by upper and lower bounds, it is observed that an optimal structure is piecewise constant with values achieving the bounds.",10.1007/s00158-007-0201-8,https://dx.doi.org/10.1007/s00158-007-0201-8
"Lee, I | Shin, J | Choi, KK",2013,Equivalent target probability of failure to convert high-reliability model to low-reliability model for efficiency of sampling-based RBDO,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,8,1.6,"This study presents a methodology to convert an RBDO problem requiring very high reliability to an RBDO problem requiring relatively low reliability by appropriately increasing the input standard deviations for efficient computation in sampling-based RBDO. First, for linear performance functions with independent normal random inputs, an exact probability of failure is derived in terms of the ratio of the input standard deviation, which is denoted by . Then, the probability of failure estimation is generalized for other types of random inputs and performance functions. For the generalization of the probability of failure estimation, two types of coefficients need to be determined by equating the probability of failure and its sensitivities with respect to the input standard deviation at the given design point. The sensitivities of the probability of failure with respect to the standard deviation are obtained using the first-order score function for the standard deviation. To apply the proposed method to an RBDO problem, a concept of an equivalent target probability of failure, which is an increased target probability of failure corresponding to the increased input standard deviations, is also introduced. Numerical results indicate that the proposed method can estimate the probability of failure accurately as a function of the input standard deviation compared to the Monte Carlo simulation results. As anticipated, the sampling-based RBDO using equivalent target probability of failure helps find the optimum design very efficiently while yielding reasonably accurate optimum design, which is close to the one obtained using the original target probability of failure.",10.1007/s00158-013-0905-x,https://dx.doi.org/10.1007/s00158-013-0905-x
"Lin, G | Su, CH | Karniadakis, GE",2008,Stochastic modeling of random roughness in shock scattering problems: Theory and simulations,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,16,1.6,"Random roughness is omnipresent in engineering applications and may often affect performance in unexpected way. Here, we employ synergistically stochastic simulations and second-order stochastic perturbation analysis to study supersonic flow past a wedge with random rough surface. The roughness (of length d) starting at the wedge apex is modeled as stochastic process (with zero mean and correlation length A) obtained from a new stochastic differential equation. A multi-element probabilistic collocation method (ME-PCM) on sparse grids is employed to solve the stochastic Euler equations while a WENO scheme is used to discretize the equations in two spatial dimensions. The perturbation analysis is used to verify the stochastic simulations and to provide insight for small values of A, where stochastic simulations become prohibitively expensive. We show that the random roughness enhances the lift and drag forces on the wedge beyond the rough region, and this enhancement is proportional to (d/A)(). The effects become more pronounced as the Mach number increases. These results can be used in designing smart rough skins for airfoils for maximum lift enhancement at a minimum drag penalty.",10.1016/j.cma.2008.02.025,https://dx.doi.org/10.1016/j.cma.2008.02.025
"Abenius, E | Strand, B",2006,Solving inverse electromagnetic problems using FDTD and gradient-based minimization,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,19,1.58,"We address time-domain inverse electromagnetic scattering for determining unknown characteristics of an object from observations of the scattered field. Applications include non-destructive characterization of media and optimization of material properties, for example, the design of radar absorbing materials. Another application is model reduction where a detailed model of a complex geometry is reduced to a simplified model. The inverse problem is formulated as an optimal control problem where the cost function to be minimized is the difference between the estimated and observed fields, and the control parameters are the unknown object characteristics. The problem is solved in a deterministic gradient-based optimization algorithm using a parallel D FDTD scheme. Highly accurate analytical gradients are computed from the adjoint formulation. The inverse method is applied to the characterization of layered dispersive media and the determination of parameters in subcell models for thin sheets and narrow slots.",10.1002/nme.1731,https://dx.doi.org/10.1002/nme.1731
"Wen, Z | Zhan, HB | Huang, GH | Jin, MG",2011,Constant-head test in a leaky aquifer with a finite-thickness skin,Applications_JOURNAL OF HYDROLOGY,11,1.57,"Constant-head test is a commonly used aquifer testing method in groundwater hydrology. A mathematical model for constant-head test in a leaky aquifer with a finite-thickness skin was developed in this study. Three different aquifer-aquitard systems were considered and the Laplace-domain solutions were obtained and then inverted numerically with the Stehfest method to yield the time-domain solutions. The well discharges for different cases were computed and a sensitivity analysis of the well discharge on different parameters was performed. The results indicated that the dimensionless transmissivity of the aquitard had little effect on the well discharge at early times while a larger transmissivity of the aquitard leaded to a larger well discharge at late times. The well discharge for the positive skin was smaller than that without the skin while the well discharge for the negative skin was larger than that without the skin, where positive and negative skins refer to the cases in which the permeability values of the skin zones are less and greater than that of the formation zone, respectively. A thicker skin resulted in a smaller well discharge for the positive skin case but leaded to a larger well discharge for the negative skin case at late times. We also found that the drawdown for the positive skin case was less than that for the negative skin case at the same time, and a positive skin might result in delayed response of the aquifer to pumping. The sensitivity analysis indicated that the well discharge was sensitive to the properties of the skin zone, but not sensitive to the properties of the aquitards for the aquifer-aquitard system presented in this study.",10.1016/j.jhydrol.2011.01.010,https://dx.doi.org/10.1016/j.jhydrol.2011.01.010
"Estep, D | Malqvist, A | Tavener, S",2009,Nonparametric density estimation. for randomly perturbed elliptic problems II: Applications and adaptive modeling,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,14,1.56,"In this paper, we develop and apply an efficient adaptive algorithm for computing the propagation of uncertainty into a quantity of interest computed from numerical solutions of an elliptic partial differential equation with it random, perturbed diffusion coefficient. The algorithm is well-suited for problems for which limited information about the random perturbations is available and when an approximation of the probability distribution of the Output is desired. We employ a nonparametric density estimation approach based on it very efficient method for computing random samples of elliptic problems described and analyzed in (SIAM J Sci. Comput. . DOI: XOMP-D--). We fully develop the adaptive algorithm suggested by the analysis in that paper. discuss details of its implementation, and illustrate its behavior using a realistic data set. Finally. we extend the analysis to include a 'modeling error' term that accounts for the effects of the resolution of the statistical description of the random variation We. modify the adaptive algorithm to adapt the resolution of the statistical description and Illustrate the behavior of the adaptive algorithm in several examples Copyright (C)  John Wiley & Sons. Ltd.",10.1002/nme.2547,https://dx.doi.org/10.1002/nme.2547
"Jiang, XM | Mahadevan, S",2009,Bayesian hierarchical uncertainty quantification by structural equation modeling,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,14,1.56,"Uncertainty quantification is playing an increasingly important role In assessing the performance, safety and reliability of complex physical systems in Bile absence of adequate amount of experimental data Simulation of a Complex System Involves multiple levels of modeling. such as material (lowest level) to component to subsystem to system (highest level). This paper presents a Bayesian modeling approach to quantify both epistemic and aleatoric uncertainties in hierarchical model development A generalized structural equation modeling with latent variables is presented to model three sets of relationships in hierarchical model development. namely, model predictions vs experimental observations at each individual level. model predictions at lower vs higher levels, and experimental data at lower vs higher levels. The three sets of relationships are represented by a hierarchical Bayes network, and the Influencing factors between them ire estimated by I Bayesian regression approach Both measurement and prediction errors at various levels are quantified through file Bayesian method The variability of input variables in the computational model is updated and quantified using various levels of measurement data via Bayesian inference and the structural equation modeling parameters The proposed methodology is illustrated with a transient heal conduction example problem Copyright (C)  John Wiley & Sons. Ltd",10.1002/nme.2550,https://dx.doi.org/10.1002/nme.2550
"Taroco, E",2000,Shape sensitivity analysis in linear elastic fracture mechanics,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,28,1.56,"Shape sensitivity analysis of an elastic solid in equilibrium with a known load system applied over its boundary is presented in this work. The domain and boundary integral expressions of the first- and second-order shape derivatives of the total potential energy are established, by using an arbitrary change of the domain characterized by a velocity held defined over the initial body configuration. In these expressions we recognize free divergence tensors that are denoted in this paper as energy shape change tensors. Next, shape sensitivity analysis is applied to cracked bodies. For that purpose, a suitable velocity distribution field is adopted to simulate the crack advance of a unit length in a two-dimensional body. Finally, the corresponding domain and the equivalent path-independent integral expressions of the first- and second-order potential energy release rate of fracture mechanics are also derived.",10.1016/S0045-7825(99)00356-4,https://dx.doi.org/10.1016/S0045-7825(99)00356-4
"Liao, HT",2016,Efficient sensitivity analysis method for chaotic dynamical systems,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,3,1.5,The direct differentiation and improved least squares shadowing methods are both developed for accurately and efficiently calculating the sensitivity coefficients of time averaged quantities for chaotic dynamical systems. The key idea is to recast the time averaged integration term in the form of differential equation before applying the sensitivity analysis method. An additional constraint-based equation which forms the augmented equations of motion is proposed to calculate the time averaged integration variable and the sensitivity coefficients are obtained as a result of solving the augmented differential equations. The application of the least squares shadowing formulation to the augmented equations results in an explicit expression for the sensitivity coefficient which is dependent on the final state of the Lagrange multipliers. The LU factorization technique to calculate the Lagrange multipliers leads to a better performance for the convergence problem and the computational expense. Numerical experiments on a set of problems selected from the literature are presented to illustrate the developed methods. The numerical results demonstrate the correctness and effectiveness of the present approaches and some short impulsive sensitivity coefficients are observed by using the direct differentiation sensitivity analysis method.,10.1016/j.jcp.2016.02.016,https://dx.doi.org/10.1016/j.jcp.2016.02.016
"Kim, SB | Corapcioglu, MY",2002,Contaminant transport in riverbank filtration in the presence of dissolved organic matter and bacteria: a kinetic approach,Applications_JOURNAL OF HYDROLOGY,24,1.5,"In riverbank filtration, the removal of organic contaminants is an important task for the production of good quality drinking water. The transport of an organic contaminant in riverbank filtration can be retarded by sorption on to the solid matrix and facilitated by the presence of mobile colloids. In the presence of dissolved organic matter (DOM) and bacteria, the subsurface environment can be modeled as a four-phase porous medium: two mobile colloidal phases, an aqueous phase, and a solid matrix. In this study, a kinetic model is developed to simulate the contaminant transport in riverbank filtration in the presence of DOM and bacteria. The bacterial deposition and the contaminant sorption on bacteria and DOM are expressed with kinetic expressions. The model equations are solved numerically with a fully implicit finite difference method. Simulation results show that the contaminant mobility increases greatly in riverbank filtration due to the presence of DOM. The mobility can be enhanced further when the bacteria and DOM are present together in the aquifer. In this system, the total aqueous phase contaminant concentration, C-ct(+) includes the contaminant dissolved in the aqueous phase, C-c(+) the contaminant sorbed to DOM, sigma(cd)(+), and the contaminant sorbed to mobile bacteria, C(b)(+)sigma(cbm)(+),(i.e. C-ct(+) = C-c(+) + sigma(cd)(+) + C-b(+) sigma(cbm)(+)) Sensitivity analysis illustrates that the distribution of the total aqueous phase contaminants among the dissolved phase, DOM and bacteria is changed significantly with various Damkohler numbers related to the contaminant sorption on mobile colloids.",10.1016/S0022-1694(02)00170-1,https://dx.doi.org/10.1016/S0022-1694(02)00170-1
"Najm, HN | Malorani, M",2014,Enforcing positivity in intrusive PC-UQ methods for reactive ODE systems,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,6,1.5,"We explore the relation between the development of a non-negligible probability of negative states and the instability of numerical integration of the intrusive Galerkin ordinary differential equation system describing uncertain chemical ignition. To prevent this instability without resorting to either multi-element local polynomial chaos (PC) methods or increasing the order of the PC representation in time, we propose a procedure aimed at modifying the amplitude of the PC modes to bring the probability of negative state values below a user-defined threshold. This modification can be effectively described as a filtering procedure of the spectral PC coefficients, which is applied on-the-fly during the numerical integration when the current value of the probability of negative states exceeds the prescribed threshold. We demonstrate the filtering procedure using a simple model of an ignition process in a batch reactor. This is carried out by comparing different observables and error measures as obtained by non-intrusive Monte Carlo and Gauss-quadrature integration and the filtered intrusive procedure. The filtering procedure has been shown to effectively stabilize divergent intrusive solutions, and also to improve the accuracy of stable intrusive solutions which are close to the stability limits.",10.1016/j.jcp.2014.03.061,https://dx.doi.org/10.1016/j.jcp.2014.03.061
"Akbari, J | Kim, NH | Ahmadi, MT",2010,Shape sensitivity analysis with design-dependent loadings-equivalence between continuum and discrete derivatives,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,12,1.5,"The purpose of this paper is twofold: () showing equivalence between continuum and discrete formulations in sensitivity analysis when a linear velocity field is used and () presenting shape sensitivity formulations for design-dependent loadings. The equations for structural analysis are often composed of the stiffness part and the applied loading part. The shape sensitivity formulations for the stiffness part were well-developed in the literature, but not for the loading part, especially for body forces and surface tractions. The applied loads are often assumed to be conservative or design-independent. In shape design problems, however, the applied loads are often functions of design variables. In this paper, shape sensitivity formulations are presented when the body forces and surface tractions depend on shape design variables. Especially, the continuum-discrete (C-D) and discrete-discrete (D-D) approaches are compared in detail. It is shown that the two methods are theoretically and numerically equivalent when the same discretization, numerical integration, and linear design velocity fields are used. The accuracy of sensitivity calculation is demonstrated using a cantilevered beam under uniform pressure and an arch dam crown cantilever under gravity and hydrostatic loading at the upstream face of the structure. It is shown that the sensitivity results are consistent with finite difference results, but different from the analytical sensitivity due to discretization and approximation errors of numerical analysis.",10.1007/s00158-009-0374-4,https://dx.doi.org/10.1007/s00158-009-0374-4
"Li, M | Hamel, J | Azarm, S",2010,Optimal uncertainty reduction for multi-disciplinary multi-output systems using sensitivity analysis,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,12,1.5,"We present a sensitivity analysis based uncertainty reduction approach, called Multi-dIsciplinary Multi-Output Sensitivity Analysis (MIMOSA), for the analysis model of a multi-disciplinary engineering system decomposed into multiple subsystems with each subsystem analysis having multiple inputs with reducible uncertainty and multiple outputs. MIMOSA can determine: () the sensitivity of system and subsystem outputs to input uncertainties at both system and subsystem levels, () the sensitivity of the system outputs to the variation from subsystem outputs, and () the optimal ""investment"" required to reduce uncertainty in inputs in order to obtain a maximum reduction in output variations at both the system and subsystem levels. A numerical and an engineering example with two and three subsystems, respectively, have been used to demonstrate the applicability of the MIMOSA approach.",10.1007/s00158-009-0372-6,https://dx.doi.org/10.1007/s00158-009-0372-6
"Thiagarajan, V | Shapiro, V",2016,Adaptively Weighted Numerical Integration in the Finite Cell Method,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,3,1.5,"With Adaptively Weighted (AW) numerical integration, for a given set of quadrature nodes, order and domain of integration, the quadrature weights are obtained by solving a system of suitable moment fitting equations in least square sense. The moments in the moment equations are approximated over a simplified domain that is homeomorphic to the original domain, and then are corrected for the deviation from the original domain using shape sensitivity analysis. In this paper, we demonstrate the application of AW integration scheme in the context of the Finite Cell Method which must perform numerical integration over arbitrary domains without meshing. The standard integration technique employed in FCM is the characteristic function method that converts the continuous integrand over a complex domain into a discontinuous integrand over a simple (box) domain. Then, well known integrand adaptivity techniques are employed to integrate the resulting discontinuous integrand over the box domain. Although this method is simple to implement, it becomes computationally very expensive for realistic complex D domains such as sculptures, bones and engines. In contrast, in AW scheme the quadrature weights directly adapt to the complex geometric domain without the need to making the integrand discontinuous leading to superior computational properties. In this paper, we demonstrate the computational efficiency of AW over the characteristic function method as it requires fewer subdivisions and less time to achieve a given accuracy in both two and three dimensions. In addition, AW offers a number of advantages including flexibility in the choice of quadrature points and basis functions.",10.1016/j.cma.2016.08.021,https://dx.doi.org/10.1016/j.cma.2016.08.021
"Pettersson, P | Tchelepi, HA",2016,Stochastic Galerkin framework with locally reduced bases for nonlinear two-phase transport in heterogeneous formations,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,3,1.5,"The generalized polynomial chaos method with multiwavelet basis functions is applied to the Buckley-Leverett equation. We consider a spatially homogeneous domain modeled as a random field. The problem is projected onto stochastic basis functions which yields an extended system of partial differential equations. Analysis and numerical methods leading to reduced computational cost are presented for the extended system of equations. The accurate representation of the evolution of a discontinuous stochastic solution over time requires a large number of stochastic basis functions. Adaptivity of the stochastic basis to reduce computational cost is challenging in the stochastic Galerkin setting since the change of basis affects the system matrix itself. To achieve adaptivity without adding overhead by rewriting the entire system of equations for every grid cell, we devise a basis reduction method that distinguishes between locally significant and insignificant modes without changing the actual system matrices. Results are presented for problems in one and two spatial dimensions, with varying number of stochastic dimensions. We show how to obtain stochastic velocity fields from realistic permeability fields and demonstrate the performance of the stochastic Galerkin method with local basis reduction. The system of conservation laws is discretized with a finite volume method and we demonstrate numerical convergence to the reference solution obtained through Monte Carlo sampling.",10.1016/j.cma.2016.07.013,https://dx.doi.org/10.1016/j.cma.2016.07.013
"Watts, S | Tortorelli, DA",2016,An n-material thresholding method for improving integerness of solutions in topology optimization,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,3,1.5,"It is common in solving topology optimization problems to replace an integer-valued characteristic function design field with the material volume fraction field, a real-valued approximation of the design field that permits 'fictitious' mixtures of materials during intermediate iterations in the optimization process. This is reasonable so long as one can interpolate properties for such materials and so long as the final design is integer valued. For this purpose, we present a method for smoothly thresholding the volume fractions of an arbitrary number of material phases which specify the design. This method is trivial for two-material design problems, for example, the canonical topology design problem of specifying the presence or absence of a single material within a domain, but it becomes more complex when three or more materials are used, as often occurs in material design problems. We take advantage of the similarity in properties between the volume fractions and the barycentric coordinates on a simplex to derive a thresholding, method which is applicable to an arbitrary number of materials. As we show in a sensitivity analysis, this method has smooth derivatives, allowing it to be used in gradient-based optimization algorithms. We present results, which show synergistic effects when used with Solid Isotropic Material with Penalty and Rational Approximation of Material Properties material interpolation functions, popular methods of ensuring integerness of solutions.",10.1002/nme.5265,https://dx.doi.org/10.1002/nme.5265
"Venturi, D | Karniadakis, GE",2012,New evolution equations for the joint response-excitation probability density function of stochastic solutions to first-order nonlinear PDEs,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,9,1.5,"By using functional integral methods we determine new evolution equations satisfied by the joint response-excitation probability density function (PDF) associated with the stochastic solution to first-order nonlinear partial differential equations (PDEs). The theory is presented for both fully nonlinear and for quasilinear scalar PDEs subject to random boundary conditions, random initial conditions or random forcing terms. Particular applications are discussed for the classical linear and nonlinear advection equations and for the advection-reaction equation. By using a Fourier-Galerkin spectral method we obtain numerical solutions of the proposed response-excitation PDF equations. These numerical solutions are compared against those obtained by using more conventional statistical approaches such as probabilistic collocation and multi-element probabilistic collocation methods. It is found that the response-excitation approach yields accurate predictions of the statistical properties of the system. In addition, it allows to directly ascertain the tails of probabilistic distributions, thus facilitating the assessment of rare events and associated risks. The computational cost of the response-excitation method is order magnitudes smaller than the one of more conventional statistical approaches if the PDE is subject to high-dimensional random boundary or initial conditions. The question of high-dimensionality for evolution equations involving multidimensional joint response-excitation PDFs is also addressed.",10.1016/j.jcp.2012.07.013,https://dx.doi.org/10.1016/j.jcp.2012.07.013
"Choi, M | Sapsis, TP | Karniadakis, GE",2014,On the equivalence of dynamically orthogonal and bi-orthogonal methods: Theory and numerical simulations,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,6,1.5,"The Karhunen-Loeve (KL) decomposition provides a low-dimensional representation for random fields as it is optimal in the mean square sense. Although for many stochastic systems of practical interest, described by stochastic partial differential equations (SPDEs), solutions possess this low-dimensional character, they also have a strongly time-dependent form and to this end a fixed-in-time basis may not describe the solution in an efficient way. Motivated by this limitation of standard KL expansion, Sapsis and Lermusiaux () [] developed the dynamically orthogonal (DO) field equations which allow for the simultaneous evolution of both the spatial basis where uncertainty 'lives' but also the stochastic characteristics of uncertainty. Recently, Cheng et al. () [] introduced an alternative approach, the bi-orthogonal (BO) method, which performs the exact same tasks, i.e. it evolves the spatial basis and the stochastic characteristics of uncertainty. In the current work we examine the relation of the two approaches and we prove theoretically and illustrate numerically their equivalence, in the sense that one method is an exact reformulation of the other. We show this by deriving a linear and invertible transformation matrix described by a matrix differential equation that connects the BO and the DO solutions. We also examine a pathology of the BO equations that occurs when two eigenvalues of the solution cross, resulting in an instantaneous, infinite-speed, internal rotation of the computed spatial basis. We demonstrate that despite the instantaneous duration of the singularity this has important implications on the numerical performance of the BO approach. On the other hand, it is observed that the BO is more stable in nonlinear problems involving a relatively large number of modes. Several examples, linear and nonlinear, are presented to illustrate the DO and BO methods as well as their equivalence.",10.1016/j.jcp.2014.03.050,https://dx.doi.org/10.1016/j.jcp.2014.03.050
"Hutton, C | Brazier, R",2012,"Quantifying riparian zone structure from airborne LiDAR: Vegetation filtering, anisotropic interpolation, and uncertainty propagation",Applications_JOURNAL OF HYDROLOGY,9,1.5,"Advances in remote sensing technology, notably in airborne Light Detection And Ranging (LiDAR), have facilitated the acquisition of high-resolution topographic and vegetation datasets over increasingly large areas. Whilst such datasets may provide quantitative information on surface morphology and vegetation structure in riparian zones, existing approaches for processing raw LiDAR data perform poorly in riparian channel environments. A new algorithm for separating vegetation from topography in raw LiDAR data, and the performance of the Elliptical Inverse Distance Weighting (EIDW) procedure for interpolating the remaining ground points, are evaluated using data derived from a semi-arid ephemeral river. The filtering procedure, which first applies a threshold (either slope or elevation) to classify vegetation highpoints, and second a regional growing algorithm from these high-points, avoids the classification of high channel banks as vegetation, preserving existing channel morphology for subsequent interpolation (.-.% calibration error; .-.% error in evaluation for slope threshold). EIDW, which accounts for surface anisotropy by converting the remaining elevation points to streamwise co-ordinates, can outperform isoptropic interpolation (IDW) on channel banks, however, performs less well in isotropic conditions, and when local anisotropy is different to that of the main channel. A key finding of this research is that filtering parameter uncertainty affects the performance of the interpolation procedure; resultant errors may propagate into the Digital Elevation Model (DEM) and subsequently derived products, such as Canopy Height Models (CHMs). Consequently, it is important that this uncertainty is assessed. Understanding and developing methods to deal with such errors is important to inform users of the true quality of laser scanning products, such that they can be used effectively in hydrological applications.",10.1016/j.jhydrol.2012.03.043,https://dx.doi.org/10.1016/j.jhydrol.2012.03.043
"Lee, K | Ho, HC | Marian, M | Wu, CH",2014,Uncertainty in open channel discharge measurements acquired with StreamPro ADCP,Applications_JOURNAL OF HYDROLOGY,6,1.5,"The StreamPro Acoustic Doppler Current Profiler (ADCP) is a favorite solution for making discharge measurements of medium and small streams in practical situations. This paper investigates the implementation of a rigorous uncertainty analysis protocol to measurements conducted with StreamPro ADCP operated with the stationary method (a.k.a. section-by-section). The assessment of individual uncertainty components is based on the best available information and the results of customized experiments. The main contributions of this paper are: (a) estimating StreamPro ADCP discharge measurement uncertainty by a rigorous, standardized approach; and (b) the assessment of some uncertainty sources in the functional relationship of the discharge measurement process. Uncertainties were estimated through a practical end-to-end approach, whereby the effect of the targeted uncertainty was isolated and subsequently estimated based on direct measurements, rather than analytically deriving them via uncertainty propagation equations. The implementation example illustrates that the standardized uncertainty analysis framework can be successfully applied for hydrometric measurements. Elements and framework of the uncertainty analysis presented in this paper can be applied to other instruments that estimate stream discharge using a section-by-section method.",10.1016/j.jhydrol.2013.11.031,https://dx.doi.org/10.1016/j.jhydrol.2013.11.031
"Allix, O | Vidal, P",2002,A new multi-solution approach suitable for structural identification problems,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,24,1.5,"In this paper. a multi-solution strategy aimed at greatly decreasing the cost of structural identification problems is proposed. It is based on the large time increment method and on the possibility offered by this approach to obtain a family of admissible displacement and stress fields in order to solve a nonlinear time-dependent problem. This technique relies on a time and space decomposition of the solution. It has been extended to solve at a low cost, the inverse problem of identification. for various techniques. In particular. it has been adapted to the minimization of the gradient of the gap between test and simulation. In this first attempt. it is applied to the simple case of bending of an elastic-viscoplastic beam. The issue of the numerical efficiency of this type of strategy is studied, (C)  Published by Elsevier Science B.V.",10.1016/S0045-7825(02)00211-6,https://dx.doi.org/10.1016/S0045-7825(02)00211-6
"Lee, C | Cao, J",2001,Shell element formulation of multi-step inverse analysis for axisymmetric deep drawing process,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,25,1.47,"Inverse analysis today is generally performed with membrane models in analysing sheet metal forming processes. Given the final desired configuration, it usually estimates the deformation in a one-step calculation. However, for some practical problems where the bending effect is significant and the strain history departs from a linear path, this calculation becomes not good enough to provide the optimal design values. In this paper, an axisymmetric shell element for the multi-step inverse analysis is developed for more accurate pre diction of design variables such as the initial blank shape, strain distributions, and intermediate shapes, etc. The algorithm has been applied to deep drawing processes for both thin and relatively thick sheet metal. Numerical examples demonstrate that the proposed combination of shell element and multi-step inverse analysis can provide more precise results than the previous algorithms used in inverse analysis.",10.1002/1097-0207(20010130)50:3<681::AID-NME45>3.0.CO;2-M,https://dx.doi.org/10.1002/1097-0207(20010130)50:3<681::AID-NME45>3.0.CO;2-M
"de Faria, JR | Novotny, AA",2009,On the second order topological asymptotic expansion,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,13,1.44,"The topological derivative provides the sensitivity of a given shape functional with respect to an infinitesimal (non smooth) domain perturbation at an arbitrary point of the domain. Classically, this derivative comes from the second term of the topological asymptotic expansion, dealing only with infinitesimal perturbations. However, for practical applications, we need to insert perturbations of finite size. Therefore, we consider one more term in the expansion which is defined as the second order topological derivative. In order to present these ideas, in this work we calculate first as well as second order topological derivatives for the total potential energy associated to the Laplace's equation, when the domain is perturbed with a hole. Furthermore, we also study the effects of different boundary conditions on the hole: Neumann and Dirichlet (both homogeneous). In the Neumann's case, the second order topological derivative depends explicitly on higher-order gradients of the state solution and also implicitly on the point where the hole is nucleated through the solution of an auxiliary problem. On the other hand, in the Dirichlet's case, the first order topological derivative depends explicitly on the state solution as well as implicitly through the solution of an auxiliary problem, and the second order topological derivative depends only explicitly on the solution associated to the original problem. Finally, we present two simple examples showing the influence of both terms in the second order topological asymptotic expansion for each case of boundary condition on the hole.",10.1007/s00158-009-0436-7,https://dx.doi.org/10.1007/s00158-009-0436-7
"Cho, S | Choi, KK",2000,Design sensitivity analysis and optimization of non-linear transient dynamics. Part I - sizing design,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,26,1.44,"A continuum-based sizing design sensitivity analysis (DSA) method is presented for the transient dynamic response of non-linear structural systems with elastic-plastic material and large deformation. The methodology is aimed for applications in non-linear dynamic problems, such as crashworthiness design. The first-order variations of the energy forms, load form, and kinematic and structural responses with respect to sizing design Variables are derived. To obtain design sensitivities, the direct differentiation method and updated Lagrangian formulation are used since they are more appropriate for the path-dependent problems than the adjoint variable method and the total Lagrangian formulation, respectively. The central difference method and the finite element method are used to discretize the temporal and spatial domains, respectively. The Hughes-Liu truss/beam element, Jaumann rate of Cauchy stress, rate of deformation tensor, and Jaumann rate-based incrementally objective stress integration scheme are used to handle the finite strain and rotation. An elastic-plastic material model with combined isotropic/kinematic hardening rule is employed. A key development is to use the radial return algorithm along with the secant iteration method to enforce the consistency condition that prevents the discontinuity of stress sensitivities at the yield point. Numerical results of sizing DSA using DYNAD yield very good agreement with the finite difference results. Design optimization is carried out using the design sensitivity information.",10.1002/(SICI)1097-0207(20000530)48:3<351::AID-NME878>3.0.CO;2-P,https://dx.doi.org/10.1002/(SICI)1097-0207(20000530)48:3<351::AID-NME878>3.0.CO;2-P
"Pedersen, NL",2004,Optimization of holes in plates for control of eigenfrequencies,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,20,1.43,"In this paper we study the optimization of a hole of given area which is placed in the interior of a plate with an arbitrary external boundary. To avoid stress concentrations the shape of the hole must be smooth (continuous curvature). The objectives of the optimization are the eigenfrequencies of the plate with the hole. The optimization is performed in relation to maximizing the first eigenfrequency or maximizing the gap between the first and second eigenfrequency. An inverse solution is also shown, i.e. finding the shape and position of a hole in the plate that result in a specified eigenfrequency. To obtain a smooth boundary of the hole we use an analytical description of the hole. A rather general parameterization with only seven design parameters is applied, including the possibility of going from an ellipse to a rectangle or even to a triangle. Optimal designs are obtained iteratively using mathematical programming, where each of the redesigns are based on finite element (FE) analysis and sensitivity analysis. Mindlin plate theory is the basis for the FE analysis and the semi-analytical sensitivity analysis includes only the elements on the boundary of the hole.",10.1007/s00158-004-0426-8,https://dx.doi.org/10.1007/s00158-004-0426-8
"Barcelos, M | Bavestrello, H | Maute, K",2006,A Schur-Newton-Krylov solver for steady-state aeroelastic analysis and design sensitivity analysis,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,17,1.42,"This paper presents a Newton-Krylov approach applied to a Schur complement formulation for the analysis and design sensitivity analysis of systems undergoing fluid-structure interaction. This solution strategy is studied for a three-field formulation of an aeroelastic problem under steady-state conditions and applied to the design optimization of three-dimensional wing structures. A Schur-Krylov solver is introduced for computing the design sensitivities. Comparing the Schur-Newton-Krylov solver with conventional Gauss-Seidel schemes shows that the proposed approach significantly improves robustness and convergence rates, in particular for problems with strong fluid-structure coupling. In addition, the numerical efficiency of the aeroelastic sensitivity analysis can be typically improved by more than a factor of ., especially if high accuracy is required.",10.1016/j.cma.2004.09.013,https://dx.doi.org/10.1016/j.cma.2004.09.013
"Larsbo, M | Jarvis, N",2006,Information content of measurements from tracer microlysimeter experiments designed for parameter identification in dual-permeability models,Applications_JOURNAL OF HYDROLOGY,17,1.42,"Parameters regulating the degree of preferential flow in the dual-permeability water flow and solute transport model MACRO are difficult or impossible to derive from direct measurements. The objectives were (i) to find an improved temporal measurement scheme for identification of these parameters using laboratory microlysimeter experiments and (ii) to evaluate the possibilities of parameter identification in the MACRO model. Artificial data from laboratory microlysimeter experiments consisting of high time-resolution 'measurements' of percolation rate, effluent concentration and resident concentrations at six depths were used with PIMLI (parameter identification method using the localisation of information). The data contained enough information to successfully reduce the uncertainty in the parameter governing mass exchange between pore domains, the saturated micropore hydraulic conductivity and the dispersivity for two hypothetical soils representing one typical clay and one loam. Parameters governing water flow in the macropores were shown to be sensitive in a screening analysis with the Morris method and the uncertainty in these parameters was also reduced by PIMLI. However, some of these parameters did not converge towards their true values probably because of parameter interdependence. In all cases, 'measurements' with large information content were found early in the experiments where less than . pore volumes of water had passed through the column. For successful identification of parameters determining the degree of preferential flow, efforts should be made to perform high time-resolution measurements during the first irrigations following solute application.",10.1016/j.jhydrol.2005.10.020,https://dx.doi.org/10.1016/j.jhydrol.2005.10.020
"Chung, SH | Fourment, L | Chenot, JL | Hwang, SM",2003,Adjoint state method for shape sensitivity analysis in non-steady forming applications,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,21,1.4,"This paper introduces the adjoint state method in non-steady state forming processes, such as forging, whereas this method is more frequently used for steady-state problems. Some practical details for an efficient implementation are given. The emphasis is put on the accuracy of the sensitivity analysis for problems with remeshings and transfers of variables. The precision of the calculated derivatives is verified by comparing its results with the direct differentiation method. The adjoint state method is then applied to shape optimization problems.",10.1002/nme.784,https://dx.doi.org/10.1002/nme.784
"Raulli, M | Maute, K",2005,Topology optimization of electrostatically actuated microsystems,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,18,1.38,"This study addresses the design of electrostatically actuated microelectromechanical systems by topology optimization. The layout of the structure and the electrode are simultaneously optimized. A novel, continuous, material-based description of the interface between the structural and electrostatic domains is presented that allows the optimization of the interface topology. The resulting topology optimization problem is solved by a gradient-based algorithm. The electromechanical system response is determined by a coupled high-fidelity finite element model and a staggered solution procedure. An adjoint formulation of the coupled electromechanical design sensitivity analysis is introduced, and the global sensitivity equations are solved by a staggered method. The proposed topology optimization method is applied to the design of mechanisms. The optimization results show the significant advantages of varying the interface topology and the layout of the electrode versus conventional approaches optimizing the structural layout only.",10.1007/s00158-005-0531-3,https://dx.doi.org/10.1007/s00158-005-0531-3
"Balakrishnan, S | Roy, A | Ierapetritou, MG | Flach, GP | Georgopoulos, PG",2005,A comparative assessment of efficient uncertainty analysis techniques for environmental fate and transport models: application to the FACT model,Applications_JOURNAL OF HYDROLOGY,18,1.38,"This work presents a comparative assessment of efficient uncertainty modeling techniques, including Stochastic Response Surface Method (SRSM) and High Dimensional Model Representation (HDMR). This assessment considers improvement achieved with respect to conventional techniques of modeling uncertainty (Monte Carlo). Given that traditional methods for characterizing uncertainty are very computationally demanding, when they are applied in conjunction with complex environmental fate and transport models, this study aims to assess how accurately these efficient (and hence viable) techniques for uncertainty propagation can capture complex model output uncertainty. As a part of this effort, the efficacy of HDMR, which has primarily been used in the past as a model reduction tool, is also demonstrated for uncertainty analysis. The application chosen to highlight the accuracy of these new techniques is the steady state analysis of the groundwater flow in the Savannah River Site General Separations Area (GSA) using the subsurface Flow And Contaminant Transport (FACT) code. Uncertain inputs included three-dimensional hydraulic conductivity fields, and a two-dimensional recharge rate field. The output variables under consideration were the simulated stream baseflows and hydraulic head values. Results show that the uncertainty analysis outcomes obtained using SRSM and HDMR are practically indistinguishable from those obtained using the conventional Monte Carlo method, while requiring orders of magnitude fewer model simulations.",10.1016/j.hydrol.2004.10.010,https://dx.doi.org/10.1016/j.hydrol.2004.10.010
"Thuburn, J | Haine, TWN",2001,Adjoints of nonoscillatory advection schemes,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,23,1.35,"Nonoscillatory advection schemes contain switches, so that the derivative of the numerical solution at any time step with respect to that at the previous time step may be discontinuous. In consequence, sensitivities calculated using the adjoint of the numerical scheme may be discontinuous or ambiguous. This discontinuity is not a property of the continuous advection equation; it is an artefact of the numerical schemes used to solve it. The problem is demonstrated in some simple one-dimensional test cases. We derive a result showing that there is no possibility of smoothing the switches in nonoscillatory advection schemes to remove the discontinuities while retaining an obvious and desirable scaling property. We discuss some alternative approaches to deriving the adjoint schemes needed for sensitivity calculations.",10.1006/jcph.2001.6799,https://dx.doi.org/10.1006/jcph.2001.6799
"Balagangadhar, D | Roy, S",2001,Design sensitivity analysis and optimization of steady fluid-thermal systems,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,23,1.35,"Design optimization of fluid-thermal systems has been an area of significant research interest for the aerospace and automotive industry. The subject studies the modification of internal and external flow passages under certain specified objective constraints while satisfying the governing flow equations. Amongst various available optimization procedures the analytical sensitivity analyses-based optimization is arguably the most efficient design tool for complex multi-dimensional practical problems. In this paper, we augmented the analysis capabilities of the computational fluid dynamics (CFD) code with design sensitivity analysis (DSA). The design sensitivities are computed efficiently via analytical differentiation methods. The CFD-DSA codes are then combined with numerical optimization schemes. Finally, CFD-DSA design optimization algorithm is applied to the optimization of heat exchanger fin and HVAC duct systems.",10.1016/S0045-7825(01)00224-9,https://dx.doi.org/10.1016/S0045-7825(01)00224-9
"Yang, ZJ | Chen, X | Kelly, R",2012,A topological optimization approach for structural design of a high-speed low-load mechanism using the equivalent static loads method,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,8,1.33,"In high-speed low-load mechanisms, the principal loads are the inertial forces caused by the high accelerations and velocities. Hence, mechanical design should consider lightweight structures to minimize such loads. In this paper, a topological optimization method is presented on the basis of the equivalent static loads method. Finite element (FE) models of the mechanism in different positions are constructed, and the equivalent loads are obtained using flexible multibody dynamics simulation. Kinetic DOFs are used to simulate the motion joints, and a quasi-static analysis is performed to obtain the structural responses. The element sensitivity is calculated according to the static-load-equivalent equilibrium, in such a way that the influence on the inertial force is considered. A dimensionless component sensitivity factor (strain energy caused by unit load divided by kinetic energy from unit velocity) is used, which quantifies the significance of each element. Finally, the topological optimization approach is presented on the basis of the evolutionary structural optimization method, where the objective is to find the maximum ratio of strain energy to kinetic energy. In order to show the efficiency of the presented method, we presented two numerical cases. The results of these analyses show that the presented method is more efficient and can be easily implemented in commercial FE analysis software.",10.1002/nme.3253,https://dx.doi.org/10.1002/nme.3253
"Zhang, B | Liu, XM",2015,Topology optimization study of arterial bypass configurations using the level set method,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,4,1.33,"We studied the arterial bypass design problem using a level set based topology optimization method. The blood flow in the artery was considered as the non-Newtonian flow governed by the Navier-Stokes equations coupled with the modified Cross model for the shear dependent viscosity. The fluid-solid interface is immersed in the design domain by the level set method and the fictitious porous material method. The sensitivity velocity derived by the level set based continuous adjoint method was utilized to control the evolution of the level set function. In order to accommodate the irregular analysis domains, the flow equations and the level set equations were computed on two different unstructured grids respectively. Three idealized arterial bypass configurations problems with the minimum flow shear stress objective were studied in the numerical examples. The results indicated that the optimal arterial bypass designs can effectively reduce integral of the squared shear rate in the artery and have a superior performance for the arterial grafting.",10.1007/s00158-014-1175-y,https://dx.doi.org/10.1007/s00158-014-1175-y
"Chen, G | Zhao, GZ | Chen, BAS",2009,Sensitivity analysis of coupled structural-acoustic systems subjected to stochastic excitation,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,12,1.33,"A development for computing the acoustic pressure spectral density and its sensitivity of coupled structural-acoustic systems subjected to stochastic excitation is presented. Previous work in the area of structural-acoustic response considered systems subject to deterministic excitations. The response computation depends on the excitation; therefore, new methods are developed to account for stochastic excitation. The structural-acoustic response is calculated using finite element methods and stochastic analysis techniques. An accurate and highly efficient algorithm series for structural stationary random response analysis, pseudo excitation method (PEM) is used. Numerical examples are given to demonstrate the effectiveness of the methods and the program.",10.1007/s00158-008-0320-x,https://dx.doi.org/10.1007/s00158-008-0320-x
"Wang, PX | Dai, H",2015,Calculation of eigenpair derivatives for asymmetric damped systems with distinct and repeated eigenvalues,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,4,1.33,"An algorithm is derived for the computation of eigenpair derivatives of asymmetric quadratic eigenvalue problem with distinct and repeated eigenvalues. In the proposed method, the eigenvector derivatives of the damped systems are divided into a particular solution and a homogeneous solution. By introducing an additional normalization condition, we construct two extended systems of linear equations with nonsingular coefficient matrices to calculate the particular solution. The method is numerically stable, and the homogeneous solutions are computed by the second-order derivatives of the eigenequations. Two numerical examples are used to illustrate the validity of the proposed method.",10.1002/nme.4901,https://dx.doi.org/10.1002/nme.4901
"Johansen, L | Lund, E",2009,Optimization of laminated composite structures using delamination criteria and hierarchical models,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,12,1.33,"In this paper, the problem of maximizing the safety against failure of a fully three dimensional laminated composite structure is studied. The geometrically linear formulation of an eight node equivalent single layer solid shell finite element which utilizes fully three dimensional linear elastic orthotropic material properties is presented. Sensitivity analysis with respect to localized failure criteria functions is performed through use of semi-analytically calculated design sensitivities derived by the direct differentiation approach. Models are hierarchically refined in a two stage procedure, where a coarse mesh model is refined through the laminate thickness to obtain fully three dimensional descriptions of the detailed stress-strain state in localized zones of interest, hereby making it possible to take into account in-plane and transverse delamination failure effects. Examples illustrate the methodology.",10.1007/s00158-008-0280-1,https://dx.doi.org/10.1007/s00158-008-0280-1
"Kang, HY | Kwak, BM",2009,Application of maximum entropy principle for reliability-based design optimization,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,12,1.33,"The maximum entropy principle (MEP) is used to generate a natural probability distribution among the many possible that have the same moment conditions. The MEP can accommodate higher order moment information and therefore facilitate a higher quality PDF model. The performance of the MEP for PDF estimation is studied by using more than four moments. For the case with four moments, the results are compared with those by the Pearson system. It is observed that as accommodating higher order moment, the estimated PDF converges to the original one. A sensitivity analysis formulation of the failure probability based on the MEP is derived for reliability-based design optimization (RBDO) and the accuracy is compared with that by finite difference method (FDM). Two RBDO examples including a realistic three-dimensional wing design are solved by using the derived sensitivity formula and the MEP-based moment method. The results are compared with other methods such as TR-SQP, FAMM + Pearson system, FFMM + Pearson system in terms of accuracy and efficiency. It is also shown that an improvement in the accuracy by including more moment terms can increase numerical efficiency of optimization for the three-dimensional wing design. The moment method equipped with the MEP is found flexible and well adoptable for reliability analysis and design.",10.1007/s00158-008-0299-3,https://dx.doi.org/10.1007/s00158-008-0299-3
"Zhang, ZQ | Zhou, JX | Zhou, N | Wang, XM | Zhang, L",2005,Shape optimization using reproducing kernel particle method and an enriched genetic algorithm,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,17,1.31,"Combining Reproducing Kernel Particle Method (RKPM) with the proposed Multi-Family Genetic Algorithm (MFGA), a novel approach to continuum-based shape optimization problems is brought forward in this paper. Taking full advantage of the features of meshfree method and the merits of MFGA, the new method solves shape optimization problems in such a unique way that remeshing is avoided and particularly the computation burden and errors caused by sensitivity analysis are eliminated completely. The effectiveness, versatility and performance of the proposed approach are demonstrated via three -D numerical examples.",10.1016/j.cma.2004.10.004,https://dx.doi.org/10.1016/j.cma.2004.10.004
"Lampoh, K | Charpentier, I | Daya, E",2011,A generic approach for the solution of nonlinear residual equations. Part III: Sensitivity computations,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,9,1.29,"Sensitivity analysis provides qualitative and quantitative information on the behaviour of the model under study, and offers an access to gradients that may be used for identification purposes. Such precious information may be obtained at a low development cost applying a generic automatic differentiation (AD) tool to the computer code implementing this model. Nonlinear residual problems solved through a path following method may be addressed too. In this paper, AD techniques are adapted to the Taylor-based asymptotic numerical method. A sensitivity study of a laminated glass beam to the perturbation of some material and geometric parameters, and the perturbation of elementary stiffness matrices illustrates the method.",10.1016/j.cma.2011.06.009,https://dx.doi.org/10.1016/j.cma.2011.06.009
"Takewaki, I",2000,Soil-structure random response reduction via TMD-VD simultaneous use,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,23,1.28,A new systematic method for optimal viscous damper (VD) placement in building structures with a tuned mass damper (TMD) is developed taking into account the response amplification due to the surface ground. Non-linear amplification of the surface ground is described by an equivalent linear model and local interaction with the surrounding soil is incorporated with a horizontal spring and a dashpot. Hysteretic damping of the surface ground and radiational damping into the semi-infinite visco-elastic ground are included in the model. An original steepest direction search algorithm is applied to the interaction model with a TMD. Closed-form expressions of the inverse of the coefficient matrix (tri-diagonal matrix) enable one to compute the transfer function and its derivative with respect to design variables very efficiently. It is shown that simultaneous use of a TMD and added viscous dampers is very effective in response reduction and the ratio of the fundamental natural period of the structure to that of the surface ground is a key parameter for characterizing the optimal damper placement. Several examples with and without a TMD for different soil conditions are presented to demonstrate the effectiveness and validity of the present method.,10.1016/S0045-7825(99)00434-X,https://dx.doi.org/10.1016/S0045-7825(99)00434-X
"Kim, YI | Kwak, BM",2002,Design space optimization using a numerical design continuation method,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,20,1.25,"A generalized optimization problem in which design space is also a design to be found is defined and a numerical implementation method is proposed. In conventional optimization, only a portion of structural parameters is designated as design variables while the remaining set of other parameters related to the design space are often taken for granted. A design space is specified by the number of design variables, and the layout or configuration. To solve this type of design space problems, a simple initial design space is selected and gradually improved while the usual design variables are being optimized. To make the design space evolve into a better one, one may increase the number of design variables, but, in this transition, there are discontinuities in the objective and constraint functions. Accordingly, the sensitivity analysis methods based on continuity will not apply to this discontinuous stage. To overcome the difficulties, a numerical continuation scheme is proposed based on a new concept of a pivot phase design space. Two new categories of structural optimization problems are formulated and concrete examples shown.",10.1002/nme.369,https://dx.doi.org/10.1002/nme.369
"Subber, W | Loisel, S",2014,Schwarz preconditioners for stochastic elliptic PDEs,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,5,1.25,"Increasingly the spectral stochastic finite element method (SSFEM) has become a popular computational tool for uncertainty quantification in numerous practical engineering problems. For large-scale problems however, the computational cost associated with solving the arising linear system in the SSFEM still poses a significant challenge. The development of efficient and robust preconditioned iterative solvers for the SSFEM linear system is thus of paramount importance for uncertainty quantification of large-scale industrially relevant problems. In the context of high performance computing, the preconditioner must scale to a large number of processors. Therefore in this paper, a two-level additive Schwarz pre-conditioner is described for the iterative solution of the SSFEM linear system. The proposed preconditioner can be viewed as a generalization of the mean based block-diagonal preconditioner commonly used in the literature. For the numerical illustrations, two-dimensional steady-state diffusion and elasticity problems with spatially varying random coefficients are considered. The performance of the algorithm is investigated with respect to the geometric parameters, strength of randomness, dimension and order of the stochastic expansion.",10.1016/j.cma.2013.12.016,https://dx.doi.org/10.1016/j.cma.2013.12.016
"Kegl, M | Brank, B",2006,Shape optimization of truss-stiffened shell structures with variable thickness,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,15,1.25,"This paper presents an effective approach to shape optimal design of statically loaded elastic shell-like structures. The shape parametrization is based on a design element technique. The chosen design element is a rational Bezier body, enhanced with a smoothly varying scalar field. A body-like design element makes possible to unify the shape optimization of both pure shells and truss-stiffened shell structures. The scalar field of the design element is obtained by attaching to each control point a scalar quantity, which is an add-on to the position and weight of the control point. This scalar field is linked to the shell thickness distribution, which can be optimized simultaneously with the shape of the shell. For linear and non-linear analysis of shell structures, a reliable -node shell finite element formulation is utilized. The presented optimization approach assumes the employment of a gradient-based optimization algorithm and the use of the discrete method of direct differentiation to perform the sensitivity analysis. Four numerical examples of shell and truss-stiffened shell optimization are presented in detail to illustrate the performance of the proposed approach.",10.1016/j.cma.2005.05.020,https://dx.doi.org/10.1016/j.cma.2005.05.020
"Liu, T | Li, B | Wang, ST | Gao, L",2014,Eigenvalue topology optimization of structures using a parameterized level set method,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,5,1.25,"Preventing a structure from resonance is important in many real-world applications. Because an external excitation frequency can be lower than the fundamental eigenfrequency or between the eigenfrequencies of a structure, there is a strong need for eigenfrequency optimization technology to optimize the fundamental eigenfrequency and, in addition, the k-th eigenfrequency and to maximize the gap between eigenfrequencies. However, previous optimization studies on vibrating elastic structures that used the level set method have been devoted to the optimization of the fundamental eigenfrequency, whereas the higher-order eigenfrequencies optimization problem has seldom been considered. This paper presents an eigenfrequency optimization technology that is based on the compactly supported radial basis functions (CS-RBFs) parameterized level-set method, using the fundamental eigenfrequency, the eigenfrequency of a given higher-order, and the gap between two consecutive eigenfrequencies as the optimization objectives. Furthermore, to address the oscillation problem of the objective function, we adopt an exponential weighted optimization model of a number of the lower eigenfrequencies for multiple eigenvalue optimizations, and we utilize mode-tracking technology for the single eigenvalue optimization.In addition, we further extend the CS-RBFs parameterized level-set method to an optimization that is performed with geometric constraints, which means that the size and position of the regular holes in the structure can be optimized with the shape and topology. This approach is useful in real-world applications. The effectiveness of this method is demonstrated by several widely investigated examples that have various objectives.",10.1007/s00158-014-1069-z,https://dx.doi.org/10.1007/s00158-014-1069-z
"Wallis, I | Moore, C | Post, V | Wolf, L | Martens, E | Promrner, H",2014,Using predictive uncertainty analysis to optimise tracer test design and data acquisition,Applications_JOURNAL OF HYDROLOGY,5,1.25,"Tracer injection tests are regularly-used tools to identify and characterise flow and transport mechanisms in aquifers. Examples of practical applications are manifold and include, among others, managed aquifer recharge schemes, aquifer thermal energy storage systems and, increasingly important, the disposal of produced water from oil and shale gas wells. The hydrogeological and geochemical data collected during the injection tests are often employed to assess the potential impacts of injection on receptors such as drinking water wells and regularly serve as a basis for the development of conceptual and numerical models that underpin the prediction of potential impacts. As all field tracer injection tests impose substantial logistical and financial efforts, it is crucial to develop a solid a-priori understanding of the value of the various monitoring data to select monitoring strategies which provide the greatest return on investment. In this study, we demonstrate the ability of linear predictive uncertainty analysis (i.e. ""data worth analysis"") to quantify the usefulness of different tracer types (bromide, temperature, methane and chloride as examples) and head measurements in the context of a field-scale aquifer injection trial of coal seam gas (CSG) co-produced water. Data worth was evaluated in terms of tracer type, in terms of tracer test design (e.g., injection rate, duration of test and the applied measurement frequency) and monitoring disposition to increase the reliability of injection impact assessments. This was followed by an uncertainty targeted Pareto analysis, which allowed the interdependencies of cost and predictive reliability for alternative monitoring campaigns to be compared directly. For the evaluated injection test, the data worth analysis assessed bromide as superior to head data and all other tracers during early sampling times. However, with time, chloride became a more suitable tracer to constrain simulations of physical transport processes, followed by methane. Temperature data was assessed as the least informative of the solute tracers. However, taking costs of data acquisition into account, it could be shown that temperature data when used in conjunction with other tracers was a valuable and cost-effective marker species due to temperatures low cost to worth ratio. In contrast, the high costs of acquisition of methane data compared to its muted worth, highlighted methanes unfavourable return on investment. Areas of optimal monitoring bore position as well as optimal numbers of bores for the investigated injection site were also established. The proposed tracer test optimisation is done through the application of common use groundwater flow and transport models in conjunction with publicly available tools for predictive uncertainty analysis to provide modelers and practitioners with a powerful yet efficient and cost effective tool which is generally applicable and easily transferrable from the present study to many applications beyond the case study of injection of treated CSG produced water.",10.1016/j.jhydrol.2014.04.061,https://dx.doi.org/10.1016/j.jhydrol.2014.04.061
"Lin, HT | Tan, YC | Chen, CH | Yu, HL | Wu, SC | Ke, KY",2010,Estimation of effective hydrogeological parameters in heterogeneous and anisotropic aquifers,Applications_JOURNAL OF HYDROLOGY,10,1.25,"Obtaining reasonable hydrological input parameters is a key challenge in groundwater modeling. Analysis of temporal evolution during pump-induced drawdown is one common approach used to estimate the effective transmissivity and storage coefficients in a heterogeneous aquifer. In this study, we propose a Modified Tabu search Method (MTM), an improvement drawn from an alliance between the Tabu Search (TS) and the Adjoint State Method (ASM) developed by Tan et al. (). The latter is employed to estimate effective parameters for anisotropic, heterogeneous aquifers. MTM is validated by several numerical pumping tests. Comparisons are made to other well-known techniques, such as the type-curve method (TCM) and the straight-line method (SLM), to provide insight into the challenge of determining the most effective parameter for an anisotropic, heterogeneous aquifer. The results reveal that MTM can efficiently obtain the best representative and effective aquifer parameters in terms of the least mean square errors of the drawdown estimations. The use of MTM may involve less artificial errors than occur with TCM and SLM, and lead to better solutions. Therefore, effective transmissivity is more likely to be comprised of the geometric mean of all transmissivities within the cone of depression based on a precise estimation of MTM. Further investigation into the applicability of MTM shows that a higher level of heterogeneity in an aquifer can induce an uncertainty in estimations, while the changes in correlation length will affect the accuracy of MTM only once the degree of heterogeneity has also risen.",10.1016/j.jhydrol.2010.05.021,https://dx.doi.org/10.1016/j.jhydrol.2010.05.021
"Kim, NH | Choi, KK | Chen, JS",2001,Die shape design optimization of sheet metal stamping process using meshfree method,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,21,1.24,"A die shape design sensitivity analysis (DSA) and optimization for a sheet metal stamping process is proposed based on a Lagrangian formulation. A hyperelasticity-based elastoplastic material model is used for the constitutive relation that includes a large deformation effect. The contact condition between a workpiece and a rigid die is imposed through the penalty method with a modified Coulomb friction model. The domain of the workpiece is discretized by a meshfree method. A continuum-based DSA with respect to the rigid die shape parameter is formulated using a design velocity concept. The die shape perturbation has an effect on structural performance through the contact variational form. The effect of the deformation-dependent pressure load to the design sensitivity is discussed. It is shown that the design sensitivity equation uses the same tangent stiffness matrix as the response analysis. The linear design sensitivity equation is solved at each converged load step without the need of iteration, which is quite efficient in computation. The accuracy of sensitivity information is compared to that of the finite difference method with an excellent agreement. A die shape design optimization problem is solved to obtain the desired shape of the workpiece to minimize spring-back effect and to show the feasibility of the proposed method.",10.1002/nme.181,https://dx.doi.org/10.1002/nme.181
"Boussaa, D",2009,Optimization of temperature-dependent functionally graded material bodies,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,11,1.22,"We consider thermoelastic bodies composed of two-constituent functionally graded materials under steady-state conditions and address the problem of the optimal choice of composition profile. First we formulate the problem as a partial-differiential-equation constrained optimization problem, where the control function is the composition profile. The formulation includes the temperature-dependence of the constituents' properties. Next, we derive the objective functional gradient using the continuous adjoint-field approach. Lastly, we use the gradient information into a gradient-based algorithm to optimize a thick-walled functionally graded sphere subjected to thermal gradients. For the numerical data we use, the optimal composition profile obtained is such that in the graded sphere the maximum von Mises stress, here used as a performance index, is about half that in the homogeneous sphere composed of either constituent.",10.1016/j.cma.2009.02.013,https://dx.doi.org/10.1016/j.cma.2009.02.013
"Lin, G | Karniadakis, GE",2009,Sensitivity analysis and stochastic simulations of non-equilibrium plasma flow,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,11,1.22,"We study the parametric uncertainties involved in plasma flows and apply stochastic sensitivity analysis to rank the importance of all inputs to guide large-scale stochastic simulations. Specifically, we employ different gradient-based sensitivity methods, namely Morris, multi-element probabilistic collocation method on sparse grids. Quasi-Monte Carlo and Monte Carlo methods. These approaches go beyond the standard 'One-At-a-Time' sensitivity analysis and provide it measure of the non-linear interaction effects for the uncertain inputs. The objective is to perform systematic stochastic simulations of plasma flows treating only as stochastic processes the inputs with the highest sensitivity index, hence reducing substantially the computational cost. Two plasma flow examples are presented to demonstrate the capability and efficiency of the stochastic sensitivity analysis. The first one is a two-fluid model in a shock tube whereas the second one is a one-fluid/two-temperature model in flow past a cylinder Copyright (C)  John Wiley & Sons, Ltd.",10.1002/nme.2582,https://dx.doi.org/10.1002/nme.2582
"Nieto, F | Hernandez, S | Jurado, JA",2009,Optimum design of long-span suspension bridges considering aeroelastic and kinematic constraints,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,11,1.22,"Cable supported bridges are wind prone structures. Therefore, their aerodynamic behaviour must be studied in depth in order to guarantee their safe performance. In the last decades important achievements have been reached in the study of bridges under wind-induced actions. On the other hand, non-conventional design techniques such as sensitivity analysis or optimum design have not been applied although they have proved their feasibility in the automobile or aeronautic industries. The aim of this research work is to demonstrate how non-conventional design techniques can help designers when dealing with long span bridges considering their aeroelastic behaviour. In that respect, the comprehensive analytical optimum design problem formulation is presented. In the application example the optimum design of the challenging Messina Strait Bridge is carried out. The chosen initial design has been the year  design proposal. Up to a % deck material saving has been obtained after finishing the optimization process.",10.1007/s00158-008-0314-8,https://dx.doi.org/10.1007/s00158-008-0314-8
"He, L | Kindermann, S | Sini, M",2009,Reconstruction of shapes and impedance functions using few far-field measurements,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,11,1.22,"We consider the reconstruction of complex obstacles from few far-field acoustic measurements. The complex obstacle is characterized by its shape and an impedance function distributed along its boundary through Robin type boundary conditions. This is done by minimizing an objective functional, which is the L- distance between the given far-field information g(infinity) and the far-field of the scattered wave u(infinity) corresponding to the computed shape and impedance function. We design all algorithm to update the shape and the impedance function alternatively along the descent direction of the objective functional. The derivative with respect to the shape or the impedance function involves solving the original Helmholtz problem and the corresponding adjoint problem, where boundary integral methods are used. Further we implement level set methods to update the shape of the obstacle. To combine level set methods and boundary integral methods we perform a parametrization step for a newly updated level set function. In addition since the computed shape derivative is defined only oil the boundary of the obstacle, we extend the shape derivative to the whole domain by a linear transport equation. Finally, we demonstrate by numerical experiments that Our algorithm reconstruct both shapes and impedance functions quite accurately for non-convex shape obstacles and constant or non-constant impedance functions. The algorithm is also shown to be robust with respect to the initial guess of the Shape, the initial guess of the impedance function and even large percentage of noise.",10.1016/j.jcp.2008.09.029,https://dx.doi.org/10.1016/j.jcp.2008.09.029
"Han, X | Jiang, C | Gong, S | Huang, YH",2008,Transient waves in composite-laminated plates with uncertain load and material property,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,12,1.2,"A method is suggested to deal with the wave propagation problems in composite-laminated plates subjected to uncertainty in load and material property based on the interval analysis method and the hybrid numerical method (HNM). The uncertain parameters are treated as intervals, in which only their bounds of the uncertainty are needed. Using the first-order Taylor expansion, the transient responses can be approximated as a linear function of the uncertain parameters. In this function, the transient responses at the midpoints of the uncertain parameters can be obtained directly through the HNM. A sensitivity analysis technique is suggested to calculate the first derivative of the transient responses with respect to each uncertain parameter based on two cases that the parameter exists in load or material property. Applying the interval extension in interval mathematics, the lower and upper bounds of the transient responses caused by the uncertainty can be finally obtained. The present method is applied to a numerical example, in which the uncertainty of the load, elastic constants of the layer material and ply orientations are all investigated, and the results demonstrate the effectiveness of the present method.",10.1002/nme.2248,https://dx.doi.org/10.1002/nme.2248
"Yi, K | Choi, KK | Kim, NH | Botkin, ME",2007,Design sensitivity analysis and optimization for minimizing springback of sheet-formed part,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,13,1.18,"The springback is a manufacturing defect in the stamping process and causes difficulty in product assembly. An impediment to the use of lighter-weight, higher-strength materials in manufacturing is relative lack of understanding about how these materials respond to complex forming processes. The springback can be reduced by using an optimized combination of die, punch, and blank holder shapes together with friction and blank-holding force. An optimized process can be determined using a gradient-based optimization to minimize the springback. For an effective optimization of the stamping process, development of an efficient design sensitivity analysis (DSA) for the springback with respect to these process parameters is crucial. A continuum-based shape and configuration DSA method for the stamping process has been developed using a non-linear shell model. The material derivative is used to develop the continuum-based design sensitivity. The design sensitivity equation is solved without iteration at each converged load step in the finite deformation elastoplastic non-linear analysis with frictional contact, which makes sensitivity calculation very efficient. Numerical implementation of the proposed shape and configuration DSA method is performed using the meshfree method. The accuracy and efficiency of the proposed method are illustrated by minimizing the springback in a benchmark S-rail problem.",10.1002/nme.1994,https://dx.doi.org/10.1002/nme.1994
"Vuik, C | Segal, A | Meijerink, JA | Wijma, GT",2001,The construction of projection vectors for a deflated ICCG method applied to problems with extreme contrasts in the coefficients,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,20,1.18,"To predict the presence of oil and natural gas in a reservoir, it is important to know the fluid pressure in the rock formations. A mathematical model for the prediction of the fluid pressure history is given by a time-dependent diffusion equation. Application of the finite-element method leads to systems of linear equations. A complication is that the underground consists of layers with very large contrasts in permeability. This implies that the symmetric and positive definite coefficient matrix has a very large condition number. Bad convergence behavior of the ICCG method has been observed, and a classical termination criterion is not valid in this problem. In [] we have shown that the number of small eigenvalues of the diagonally scaled matrix is equal to the number of high-permeability domains, which are not connected to a Dirichlet boundary. In this paper the proof is extended to an Incomplete Cholesky decomposition. To annihilate the bad effect of these small eigenvalues on the convergence. the Deflated ICCG method is used. In [] we have shown how to construct a deflation subspace for the case of a set of more or less parallel layers. That subspace proved to be a good approximation of the span of the ""small"" eigenvectors. As a result of this, the convergence Of DICCG is independent of the contrasts in the permeabilities. In this paper it is shown how to construct deflation vectors even in the case of very irregular shaped layers, and layers with so-called inclusions. A theoretical investigation and numerical experiments show that the DICCG method is not sensitive to small perturbations of the deflation vectors. The efficiency of the DICCG method is illustrated by numerical experiments.",10.1006/jcph.2001.6795,https://dx.doi.org/10.1006/jcph.2001.6795
"Congedo, PM | Colonna, P | Corre, C | Witteveen, JAS | Iaccarino, G",2012,Backward uncertainty propagation method in flow problems: Application to the prediction of rarefaction shock waves,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,7,1.17,"A computational method for taking into account backward uncertainty propagation in flow problems is presented and applied to the study of rarefaction shock waves (RSW) in a dense-gas shock tube. Previous theoretical and numerical studies have shown that a RSW is relatively weak and that the prediction of its occurrence and intensity are highly sensitive to uncertainties on the initial flow conditions and on the fluid thermodynamic model. The objective of this work is to introduce an innovative, flexible and efficient algorithm combining computational fluid dynamics (CFD), uncertainty quantification (UQ) tools and metamodel-based optimization in order to obtain a reliable estimate for the RSW probability of occurrence and to prescribe the experimental accuracy requirements ensuring the reproducibility of the measurements with sufficient confidence.",10.1016/j.cma.2011.12.009,https://dx.doi.org/10.1016/j.cma.2011.12.009
"Dong, J | Choi, KK | Wang, AM | Zhang, WG | Vlahopoulos, N",2005,Parametric design sensitivity analysis of high-frequency structural-acoustic problems using energy finite element method,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,15,1.15,"A design sensitivity analysis of high-frequency structural-acoustic problems is formulated and presented. The energy finite element method (EFEM) is used to predict structural-acoustic responses in the high frequency range. where the coupling between structural junctions and the structural-acoustic interface are modelled using power transfer coefficients. The continuum design sensitivity formulation is derived from the governing equation of EFEM and the discrete method is applied in the variation of the structural-structural and structural-acoustic coupling matrices. The direct differentiation and adjoint variable method are both developed for the sensitivity analysis, where the difficult), of the adjoint variable method is overcome by solving a transposed system equation. Parametric design variables such as panel thickness and material damping are considered for sensitivity analysis. and numerical sensitivity results show excellent agreement as compared to analytical finite difference results.",10.1002/nme.1190,https://dx.doi.org/10.1002/nme.1190
"Stegmann, J | Lund, E",2005,Nonlinear topology optimization of layered shell structures,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,15,1.15,"Topology stiffness (compliance) design of linear and geometrically nonlinear shell structures is solved using the SIMP approach together with a filtering scheme. A general anisotropic multi-layer shell model is employed to allow the formation of through-the-thickness holes or stiffening zones. The finite element analysis is performed using nine-node Mindlin-type shell elements based on the degenerated shell approach, which are capable of modeling both single and multi-layered structures exhibiting anisotropic or isotropic behavior. The optimization problem is solved using analytical compliance and constraint sensitivities together with the Method of Moving Asymptotes (MMA). Geometrically nonlinear problems are solved using iterative Newton-Raphson methods and an adjoint variable approach is used for the sensitivity analysis. Several benchmark tests are presented in order to illustrate the difference in optimal topologies between linear and geometrically nonlinear shell structures.",10.1007/s00158-004-0468-y,https://dx.doi.org/10.1007/s00158-004-0468-y
"Choi, JH | Penmetsa, RC | Grandhi, RV",2005,Shape optimization of the cavitator for a supercavitating torpedo,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,15,1.15,"The torpedo is a vital component of the naval arsenal, and efforts are continually directed toward improving the technology to make the torpedo more lethal and more stealthy. Recently, a new direction of research is considering the high-speed torpedo, which is capable of reaching up to  mph underwater. When a torpedo travels at this speed, the flow around the body separates and a cavity is formed. This cavity generation due to high speeds is called supercavitation. And the drag force acting on this supercavitating torpedo dictates the thrust requirements for the propulsion system, to maintain a required cavity at the operating speed. Therefore, any reduction in the drag force, obtained by modifying the shape of the cavitator or the nose of the torpedo, would result in lower propulsion requirements. In this work, shape optimization techniques were employed to determine the optimum ( minimum-drag) shape of the cavitator given certain operating conditions. Shape optimization was also used to determine the shape of the cavity for any given cavitator, using potential flow theory. Analytical sensitivities were derived for various parameters in order to implement a gradient-based optimization algorithm. The developed methodology is an optimization process where the cavity and cavitator shapes are determined simultaneously. The cavitator shape that induces minimum drag and the corresponding cavity shape can be used to model a supercavitating torpedo that fits in the generated cavity and satisfies the required performance characteristics.",10.1007/s00158-004-0466-0,https://dx.doi.org/10.1007/s00158-004-0466-0
"Park, KJ | Lee, JN | Park, GJ",2005,Structural shape optimization using equivalent static loads transformed from dynamic loads,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,15,1.15,"In structural optimization, static loads are generally utilized although real external forces are dynamic. Dynamic loads have been considered only in small-scale problems. Recently, an algorithm for dynamic response optimization using transformation of dynamic loads into equivalent static loads has been proposed. The transformation is conducted to match the displacement fields from dynamic and static analyses. This algorithm can be applied to large-scale problems. However, the application has been limited to size optimization. The present study applies the algorithm to shape optimization. Because the number of degrees of freedom of finite element models is usually very large in shape optimization, it is difficult to conduct dynamic response optimization with conventional methods that directly treat dynamic response in the time domain. The optimization process is carried out by interfacing an optimization system and an analysis system for structural dynamics. Various examples are solved to verify the algorithm. The results are compared to the results from static loads. It is found that the algorithm using static loads transformed from dynamic loads based on displacement is valid for very large-scale shape optimization problems.",10.1002/nme.1295,https://dx.doi.org/10.1002/nme.1295
"Keller, D",2011,Global laminate optimization on geometrically partitioned shell structures,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,8,1.14,"A method aimed at the optimization of locally varying laminates is investigated. The structure is partitioned into geometrical sections. These sections are covered by global plies. A variable-length representation scheme for an evolutionary algorithm is developed. This scheme encodes the number of global plies, their thickness, material, and orientation. A set of genetic variation operators tailored to this particular representation is introduced. Sensitivity information assists the genetic search in the placement of reinforcements and optimization of ply angles. The method is investigated on two benchmark applications. There it is able to find significant improvements. A case study of an airplane's side rudder illustrates the applicability of the method to typical engineering problems.",10.1007/s00158-010-0576-9,https://dx.doi.org/10.1007/s00158-010-0576-9
"Langelaar, M | Yoon, GH | Kim, YY | van Keulen, F",2011,Topology optimization of planar shape memory alloy thermal actuators using element connectivity parameterization,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,8,1.14,"This paper presents the direct application of topology optimization to the design of shape memory alloy (SMA) thermal actuators. Because SMAs exhibit strongly nonlinear, temperature-dependent material behavior, designing effective multidimensional SMA actuator structures is a challenging task. We pursue the use of topology optimization to address this problem. Conventional material scaling topology optimization approaches are hampered by the complexity of the SMA constitutive behavior combined with large actuator deflections. Therefore, for topology optimization we employ the element connectivity parameterization approach, which offers improved analysis convergence and robustness, as well as an unambiguous treatment of nonlinear materials. A path-independent SMA constitutive model, aimed particularly at the NiTi R-phase transformation, is employed, allowing efficient adjoint sensitivity analysis. The effectiveness of the proposed SMA topology optimization is demonstrated by numerical examples of constrained and unconstrained formulations of actuator stroke maximization, which provide insight into the characteristics of optimal SMA actuators.",10.1002/nme.3199,https://dx.doi.org/10.1002/nme.3199
"Kaminski, M",2011,On semi-analytical probabilistic finite element method for homogenization of the periodic fiber-reinforced composites,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,8,1.14,"The main aim of this paper is a development of the semi-analytical probabilistic version of the finite element method (FEM) related to the homogenization problem. This approach is based on the global version of the response function method and symbolic integral calculation of basic probabilistic moments of the homogenized tensor and is applied in conjunction with the effective modules method. It originates from the generalized stochastic perturbation-based FEM, where Taylor expansion with random parameters is not necessary now and is simply replaced with the integration of the response functions. The hybrid computational implementation of the system MAPLE with homogenization-oriented FEM code MCCEFF is invented to provide probabilistic analysis of the homogenized elasticity tensor for the periodic fiber-reinforced composites. Although numerical illustration deals with a homogenization of a composite with material properties defined as Gaussian random variables, other composite parameters as well as other probabilistic distributions may be taken into account. The methodology is independent of the boundary value problem considered and may be useful for general numerical solutions using finite or boundary elements, finite differences or volumes as well as for meshless numerical strategies.",10.1002/nme.3097,https://dx.doi.org/10.1002/nme.3097
"Clenet, S | Ida, N",2010,Error estimation in a stochastic finite element method in electrokinetics,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,9,1.12,"Input data to a numerical model are not necessarily well known. Uncertainties may exist both in material properties and in the geometry of the device. They can be due, for instance, to ageing or imperfections in the manufacturing process. Input data can be modelled as random variables leading to a stochastic model. In electromagnetism, this leads to solution of a stochastic partial differential equation system. The Solution can be approximated by a linear combination of basis functions rising from the tensorial product of the basis functions used to discretize the space (nodal shape function for example) and basis functions used to discretize the random dimension (a polynomial chaos expansion for example). Some methods (SSFEM, collocation) have been proposed in the literature to calculate such approximation. The issue is then how to compare the different approaches in an objective way. One solution is to use all appropriate a posteriori numerical error estimator. In this paper, we present an error estimator based on the constitutive relation error in electrokinetics, which allows the calculation of the distance between an average solution and the unknown exact solution. The method of calculation of the error is detailed in this paper from two Solutions that satisfy the two equilibrium equations. In an example, we compare two different approximations (Legendre and Hermite polynomial chaos expansions) for the random dimension using the proposed error estimator. In addition, we show how to choose the appropriate order for the polynomial chaos expansion for the proposed error estimator.",10.1002/nme.2735,https://dx.doi.org/10.1002/nme.2735
"Witteveen, JAS | Bijl, H",2008,An alternative unsteady adaptive stochastic finite elements formulation based on interpolation at constant phase,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,11,1.1,"The unsteady adaptive stochastic finite elements method based on time-independent parametrization (UASFE-ti) is an efficient approach for resolving the effect of random parameters in unsteady simulations. It achieves a constant accuracy in time with a constant number of samples, in contrast with the usually fast increasing number of samples required by other methods. In this paper, an alternative unsteady adaptive stochastic finite elements formulation based on interpolation at constant phase (UASFE-cp) is developed to further improve the accuracy and extend the applicability of UASFE-ti. In addition to achieving a constant number of samples in time, interpolation at constant phase: () eliminates the parametrization error of the time-independent parametrization; () resolves time-dependent functionals, which cannot be modeled by the parametrization; and () captures transient behavior of the samples, which is an important special case of time-dependent functionals. These three points are illustrated by the application of UASFE-cp to random parameters in a mass-spring-damper system, the damped nonlinear Duffing oscillator, and an elastically mounted airfoil with nonlinearity in the flow and the structure. Results for different types of probability distributions are compared to those of UASFE-ti and Monte Carlo simulations.",10.1016/j.cma.2008.09.005,https://dx.doi.org/10.1016/j.cma.2008.09.005
"Tsai, FTC | Sun, NZ | Yeh, WWG",2005,Geophysical parameterization and parameter structure identification using natural neighbors in groundwater inverse problems,Applications_JOURNAL OF HYDROLOGY,14,1.08,"A natural neighbor interpolation (NN) method based on Voronoi tessellation and Delaunay triangulation is developed for both parameter heterogeneity characterization and parameter structure identification in groundwater modeling. Honoring information from the natural neighboring basis points, NN generates a continuously distributed field. With NN, parameter structure identification seeks to identify the optimal parameter distribution in terms of the number, values, and locations of basis points. At each level of parameter structure complexity, the estimated distributed parameter is obtained by minimizing a regularized fitting residual. This is done sequentially, first by a genetic algorithm (GA) and then by a gradient-based algorithm (BFGS). In this study, sensitivity equations of state variables to both values and locations of basis points are developed for optimization as well as for parameter uncertainty analysis. The results obtained from numerical experiments show that, for a given set of head observations and point transmissivity measurements, the proposed inverse methodology successfully captures the variation of the true transmissivity field. The parameter structure complexity as represented by the parameter dimension is determined by considering the trade-off between the fitting residual and the parameter uncertainty error. An optimal parameter dimension is selected when the head fitting residual is close to the observation error and the uncertainty associated with identified transmissivity structure is acceptable.",10.1016/j.jhydrol.2004.11.004,https://dx.doi.org/10.1016/j.jhydrol.2004.11.004
"Elfeki, AMM",2006,Reducing concentration uncertainty using the coupled Markov chain approach,Applications_JOURNAL OF HYDROLOGY,13,1.08,"This paper presents a framework for coupling the stochastic technique that is called 'coupled Markov chain' (Elfeki and Dekking, ), which is used for stochastic site characterization, with numerical groundwater flow and transport models. The purpose is to study the reduction of uncertainty on concentration distribution by conditioning on a number of boreholes using the Monte-Carlo approach. This study addresses some issues that have not been given much attention in the literature, namely: () using the so-called CMC (Coupled Markov Chain) model for modeling heterogeneity as a non-Gaussian field characterized by multi-dimensional transition probabilities rather than variograms or autocorrelation functions, () considering a hydrodynamic flow field that is non-uniform in the mean flow due to boundary conditions where flow is driven from the left top corner moving to the right domain boundary, and () utilizing the concept of forward modeling in the framework of conditioning on geological borehole information (geometrical configuration), rather than conditioning on direct measurements of the hydrogeological parameters (e.g. hydraulic conductivity, porosity, etc.). The model is applied on an unconsolidated deposit located in the central Rhine-Meuse delta in the Netherlands. The data at the site is merely used to calculate the transition probabilities used in the CMC model to generate a reference geological model for the rest of the analysis. The results show the potential applicability of the CMC model in reducing the uncertainty in concentration fields when a sufficient number of boreholes are available. Reproduction of peak concentrations, breakthrough curves and plume spatial moments require many conditioning boreholes (in this case study - boreholes with - m spacing over a distance of  m). However, reproduction of plume shapes requires a lot less boreholes (in this case study five boreholes with  m spacing over a distance of  m).",10.1016/j.jhydrol.2005.04.029,https://dx.doi.org/10.1016/j.jhydrol.2005.04.029
"Ohsaki, M",2000,Optimization of geometrically non-linear symmetric systems with coincident critical points,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,19,1.06,"A new formulation is presented for optimum design of an elastic symmetric structure for specified non-linear buckling load factor. It is shown that the method of sensitivity analysis of bifurcation load factor developed in Ohsaki and Vetani (Int. J. Numer. Methods Engng. ;:-) can also be applied for the case where the structure reaches coincident critical points including a limit point. Based on the method of sensitivity analysis, an algorithm is presented for finding optimum designs for specified coincident critical points. The well-known danger of designing a structure that exhibits coincident buckling is discussed in detail. It is shown in the examples of trusses that the structural volume may be successfully reduced as a result of optimization even if the reduction of the maximum load factor due to possible asymmetric initial imperfection is considered.",10.1002/1097-0207(20000730)48:9<1345::AID-NME951>3.0.CO;2-O,https://dx.doi.org/10.1002/1097-0207(20000730)48:9<1345::AID-NME951>3.0.CO;2-O
"Elham, A | van Tooren, MJL",2017,Multi-fidelity wing aerostructural optimization using a trust region filter-SQP algorithm,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,1,1.0,"A trust region filter-SQP method is used for wing multi-fidelity aerostructural optimization. Filter method eliminates the need for a penalty function, and subsequently a penalty parameter. Besides, it can easily be modified to be used for multi-fidelity optimization. A low fidelity aerostructural analysis tool is presented, that computes the drag, weight and structural deformation of lifting surfaces as well as their sensitivities with respect to the design variables using analytical methods. That tool is used for a monofidelity wing aerostructral optimization using a trust region filter-SQP method. In addition to that, a multi-fidelity aerostructural optimization has been performed, using a higher fidelity CFD code to calibrate the results of the lower fidelity model. In that case, the lower fidelity tool is used to compute the objective function, constraints and their derivatives to construct the quadratic programming subproblem. The high fidelity model is used to compute the objective function and the constraints used to generate the filter. The results of the high fidelity analysis are also used to calibrate the results of the lower fidelity tool during the optimization. This method is applied to optimize the wing of an A like aircraft for minimum fuel burn. The results showed about  % reduction in the aircraft mission fuel burn.",10.1007/s00158-016-1613-0,https://dx.doi.org/10.1007/s00158-016-1613-0
"Xi, ML | Lu, DA | Gui, DW | Qi, ZM | Zhang, GN",2017,Calibration of an agricultural-hydrological model (RZWQM2) using surrogate global optimization,Applications_JOURNAL OF HYDROLOGY,1,1.0,"Robust calibration of an agricultural-hydrological model is critical for simulating crop yield and water quality and making reasonable agricultural management. However, calibration of the agricultural-hydrological system models is challenging because of model complexity, the existence of strong parameter correlation, and significant computational requirements. Therefore, only a limited number of simulations can be allowed in any attempt to find a near-optimal solution within an affordable time, which greatly restricts the successful application of the model. The goal of this study is to locate the optimal solution of the Root Zone Water Quality Model (RZWQM) given a limited simulation time, so as to improve the model simulation and help make rational and effective agricultural-hydrological decisions. To this end, we propose a computationally efficient global optimization procedure using sparse-grid based surrogates. We first used advanced sparse grid (SG) interpolation to construct a surrogate system of the actual RZWQM, and then we calibrate the surrogate model using the global optimization algorithm, Quantum-behaved Particle Swarm Optimization (QPSO). As the surrogate model is a polynomial with fast evaluation, it can be efficiently evaluated with a sufficiently large number of times during the optimization, which facilitates the global search. We calibrate seven model parameters against five years of yield, drain flow, and NO-N loss data from a subsurface-drained corn-soybean field in Iowa. Results indicate that an accurate surrogate model can be created for the RZWQM with a relatively small number of SG points (i.e., RZWQM runs). Compared to the conventional QPSO algorithm, our surrogate based optimization method can achieve a smaller objective function value and better calibration performance using a fewer number of expensive RZWQM executions, which greatly improves computational efficiency.",10.1016/j.jhydrol.2016.11.051,https://dx.doi.org/10.1016/j.jhydrol.2016.11.051
"Liggett, JE | Partington, D | Frei, S | Werner, AD | Simmons, CT | Fleckenstein, JH",2015,An exploration of coupled surface-subsurface solute transport in a fully integrated catchment model,Applications_JOURNAL OF HYDROLOGY,3,1.0,"Coupling surface and subsurface water flow in fully integrated hydrological codes is becoming common in hydrological research; however, the coupling of surface-subsurface solute transport has received much less attention. Previous studies on fully integrated solute transport focus on small scales, simple geometric domains, and have not utilised many different field data sources. The objective of this study is to demonstrate the inclusion of both flow and solute transport in a D, fully integrated catchment model, utilising high resolution observations of dissolved organic carbon (DOC) export from a wetland complex during a rainfall event. A sensitivity analysis is performed to span a range of transport conditions for the surface-subsurface boundary (e.g. advective exchange only, advection plus diffusion, advection plus full mechanical dispersion) and subsurface dispersivities. The catchment model captures some aspects of observed catchment behaviour (e.g. solute discharge at the catchment outlet, increasing discharge from wetlands with increased stream discharge, and counter-clockwise concentration-discharge relationships), although other known behaviours are not well represented in the model (e.g. slope of concentration-discharge plots). Including surface-subsurface solute transport aids in evaluating internal model processes, however there are challenges related to the influence of dispersion across the surface-subsurface interface, and non-uniqueness of the solute transport solution. This highlights that obtaining solute field data is especially important for constraining integrated models of solute transport.",10.1016/j.jhydrol.2015.09.006,https://dx.doi.org/10.1016/j.jhydrol.2015.09.006
"Werner, S | Stingl, M | Leugering, G",2017,Model-based control of dynamic frictional contact problems using the example of hot rolling,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,1,1.0,"We consider hot forming processes, in which a metal solid body is deformed by several rolls in order to obtain a desired final shape. To minimize cutting scrap and to ensure that this shape satisfies the required tolerances as precisely as possible, we formulate an optimal control problem where we use the trajectories of the rolls as control functions. The deformation of the solid body is described through the basic equations of nonlinear continuum mechanics, which are here coupled with an elasto-viscoplastic material model based on a multiplicative split of the deformation gradient. We assume that the deformations of the rolls can be neglected, thus we add unilateral frictional contact boundary conditions, resulting in an evolutionary quasi-variational inclusion. The associated control-to-observation map is non-differentiable due to changes of state between elastic and plastic material behavior, contact and separation and stick and slip motion, yet we still want to apply gradient-based methods to solve the optimal control problem and therefore have to make sure that derivatives of cost functional and constraints exist. To resolve this issue, we first regularize all non-differentiabilities and subsequently apply the direct differentiation method to obtain sensitivity information. Finally, we formulate a suitable algorithm and discuss numerical results for a real-world example to illustrate its capability.",10.1016/j.cma.2017.03.006,https://dx.doi.org/10.1016/j.cma.2017.03.006
"Cross, DM | Canfield, RA",2014,Local continuum shape sensitivity with spatial gradient reconstruction,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,4,1.0,"Utilizing gradient-based optimization for large scale, multidisciplinary design problems requires accurate and efficient sensitivity or design derivative analysis. In general, numerical sensitivity methods, such as the finite difference method, are easy to implement but can be computationally expensive and inaccurate. In contrast, analytic sensitivity methods, such as the discrete and continuum methods, are highly accurate but can be very difficult, if not infeasible, to implement. A popular compromise is the semi-analytic method, but it too can be highly inaccurate when computing shape design derivatives. Presented here is an alternative method, which is easy to implement and can be as accurate as conventional analytic sensitivity methods. In this paper a general local continuum shape sensitivity method with spatial gradient reconstruction (SGR) is formulated. It is demonstrated that SGR, a numerical technique, can be used to solve the continuous sensitivity equations (CSEs) in a non-intrusive manner. The method is used to compute design derivatives for a variety of applications, including linear static beam bending, linear transient gust analysis of a -D beam structure, linear static bending of rectangular plates, and linear static bending of a beam-stiffened plate. Analysis is conducted with Nastran, and both displacement and stress design derivative solutions are presented. For each example the design derivatives are validated with either analytic or finite difference solutions.",10.1007/s00158-014-1092-0,https://dx.doi.org/10.1007/s00158-014-1092-0
"Asl, RN | Shayegan, S | Geiser, A | Hojjat, M | Bletzinger, KU",2017,A consistent formulation for imposing packaging constraints in shape optimization using Vertex Morphing parametrization,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,1,1.0,"This paper aims at imposing no-penetration condition over arbitrary surfaces which act as bounding surfaces, also known as packaging constraints, on the design surface of shape optimization problem. We use Vertex Morphing technique for the shape parametrization. Vertex Morphing is a consistent surface control approach for node-based shape optimization. The suitability of this technique has been assessed and demonstrated for a wide range of engineering applications without geometric shape constraints. In this contribution, a consistent formulation is presented for the implementation of numerous point-wise geometric constraints in four main steps. First, a potential contact between optimization surface points and the bounding surface is identified via the so-called gap function. Second, the shape gradients of objective functions and active constraints are mapped onto the Vertex Morphing's control space, where the optimization problem is formulated. Third, the linear least squares method is used to project the steepest-descent search direction onto the subspace tangent to the mapped active constraints. Finally, the feasible design update is mapped onto the geometry space. To verify the perfect consistency between the geometry space (where the constraints are formulated) and the control space (where the optimization problem is solved) two applications of CFD shape optimization in the automotive industry are presented.",10.1007/s00158-017-1819-9,https://dx.doi.org/10.1007/s00158-017-1819-9
"Guignard, D | Nobile, F | Picasso, M",2017,A posteriori error estimation for the steady Navier-Stokes equations in random domains,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,1,1.0,"We consider finite element error approximations of the steady incompressible Navier-Stokes equations defined on a randomly perturbed domain, the perturbation being small. Introducing a random mapping, these equations are transformed into PDEs on a fixed reference domain with random coefficients. Under suitable assumptions on the random mapping and the input data, in particular the so-called small data assumption, we prove the well-posedness of the problem. We assume then that the mapping depends affinely on L independent random variables and adopt a perturbation approach expanding the solution with respect to a small parameter a that controls the amount of randomness in the problem. We perform an a posteriori error analysis for the first order approximation error, namely the error between the exact (random) solution and the finite element approximation of the first term in the expansion with respect to a. Numerical results are given to illustrate the theoretical results and the effectiveness of the error estimators.",10.1016/j.cma.2016.10.008,https://dx.doi.org/10.1016/j.cma.2016.10.008
"Lu, J | Luo, YM",2016,Solving membrane stress on deformed configuration using inverse elastostatic and forward penalty methods,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,2,1.0,"Wall stress in a curved membrane depends on the surface geometry and applied load, not the constitutive law of the wall material (static determinacy). This remarkable property suggests that the membrane stress can be determined without knowing of the material property. Being able to determine the wall stress without material knowledge is practically significant in numerous applications, particularly in in vivo biomedical stress analysis wherein patient-specific tissue properties are extremely difficult to obtain. In this article, we present two non-traditional methods that utilize the static determinacy to compute the membrane stress. A common theme is to start with a deformed configuration and use artificial material models. In one of these approaches, the equilibrium problem is formulated as an inverse elastostatic problem whereby the stress is determined by way of finding a (fictitious) reference configuration. The other approach utilizes the standard forward analysis, but in a penalty setting using artificially stiff material models. A continuum sensitivity analysis is implemented to quantitatively assess the influence of material parameters. These developments open new pathways of membrane analysis enabling stress prediction without using realistic material parameters. Numerical examples are provided to demonstrate and compare these two methods.",10.1016/j.cma.2016.05.017,https://dx.doi.org/10.1016/j.cma.2016.05.017
"Cross, DM | Canfield, RA",2015,Local continuum shape sensitivity with spatial gradient reconstruction for nonlinear analysis,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,3,1.0,"Gradient-based optimization for large-scale, multidisciplinary design problems requires accurate and efficient sensitivity analysis to compute design derivatives. Presented here is a nonintrusive analytic sensitivity method, that is relatively easy to implement. Furthermore, it can be as accurate as conventional analytic sensitivity methods, which are intrusive and tend to be difficult, if not infeasible, to implement. The nonintrusive local continuum shape sensitivity method with spatial gradient reconstruction (SGR) is formulated for nonlinear systems. This is an extension of the formulation previously published for linear systems. SGR, a numerical technique used to approximate spatial derivatives, can be leveraged to implement the sensitivity method in a nonintrusive manner. The method is used to compute design derivatives for a variety of applications, including nonlinear static beam bending, nonlinear transient gust response of a -D beam structure, and nonlinear static bending of rectangular plates. To demonstrate that the method is nonintrusive, all analyses are conducted using black box solvers. One limiting requirement of the method is that it requires the converged Jacobian or tangent stiffness matrix as output from the analysis tool. For each example the design derivatives of the structural displacement response are verified with finite difference calculations.",10.1007/s00158-014-1178-8,https://dx.doi.org/10.1007/s00158-014-1178-8
"Subber, W | Matous, K",2017,Asynchronous space-time domain decomposition method with localized uncertainty quantification,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,1,1.0,"The computational cost associated with uncertainty quantification of engineering problems featuring localized phenomenon can be reduced by confining the random variability of the model parameters within a region of interest. In this case, a localized treatment of mesh and time resolutions is required to capture the effect of the confined material uncertainty on the global response. We present a computational approach for localized uncertainty quantification with the capability of asynchronous treatment of mesh and time resolutions. In particular, we allow each subdomain to have its local uncertainty representation and the corresponding mesh and time resolutions. As a result, computing resources can be directed toward a small region of interest where a model with high spatial and temporal resolutions is required. To verify the numerical implementation, we consider elastic wave propagation in an axially loaded beam. Moreover, we perform convergence studies with respect to the spatial and temporal discretizations as well as the size of an uncertain subdomain. A projectile impacting a composite sandwich plate is considered as an engineering application for the proposed method.",10.1016/j.cma.2017.07.011,https://dx.doi.org/10.1016/j.cma.2017.07.011
"Guan, QG | Gunzburger, M | Webster, CG | Zhang, GN",2017,Reduced basis methods for nonlocal diffusion problems with random input data,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,1,1.0,"The construction, analysis, and application of reduced-basis methods for uncertainty quantification problems involving nonlocal diffusion problems with random input data is the subject of this work. Because of the lack of sparsity of discretized nonlocal models relative to analogous local partial differential equation models, the need for reduced-order modeling is much more acute in the nonlocal setting. In this effort, we develop reduced-basis approximations for nonlocal diffusion equations with affine random coefficients. Efficiency estimates of the proposed greedy reduced-basis methods are provided. Numerical examples are used to illustrate the effect varying various model parameters have on the efficiency and accuracy of the reduced-basis method relative to sparse-grid interpolation using the full finite element method. It is shown that the proposed reduced-basis approach can indeed provide substantial savings over standard sparse-grid methods.",10.1016/j.cma.2016.12.019,https://dx.doi.org/10.1016/j.cma.2016.12.019
"McElwee, CD",2002,Improving the analysis of slug tests,Applications_JOURNAL OF HYDROLOGY,16,1.0,"This paper examines several. techniques that have the potential to improve the quality of slug test analysis. These techniques are applicable in the range from low hydraulic conductivities with overdamped responses to high hydraulic conductivities with nonlinear oscillatory responses. Four techniques for improving slug test analysis will be discussed: use of an extended capability nonlinear model, sensitivity analysis, correction for acceleration and velocity effects, and use of multiple slug tests. The four-parameter nonlinear slug test model used in this work is shown to allow accurate analysis of slug tests with widely differing character. The parameter beta represents a correction to the water column length caused primarily by radius variations in the wellbore and is most useful in matching the oscillation frequency and amplitude. The water column velocity at slug initiation (V-) is an additional model parameter, which would ideally be zero but may not be due to the initiation mechanism. The remaining two model parameters are A (parameter for nonlinear effects) and K (hydraulic conductivity). Sensitivity analysis shows that in general beta and V- have the lowest sensitivity and K usually has the highest. However, for very high K values the sensitivity to A may surpass the sensitivity to K. Oscillatory slug tests involve higher accelerations and velocities of the water column; thus, the pressure transducer responses are affected by these factors and the model response must be corrected to allow maximum accuracy for the analysis. The performance of multiple slug tests will allow some statistical measure of the experimental accuracy and of the reliability of the resulting aquifer parameters.",10.1016/S0022-1694(02)00214-7,https://dx.doi.org/10.1016/S0022-1694(02)00214-7
"Lengiewicz, J | Stupkiewicz, S",2012,Continuum framework for finite element modelling of finite wear,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,6,1.0,"A finite deformation contact problem with friction and wear is studied in which the shape changes due to wear are finite. Accordingly, in addition to the initial configuration and the current configuration, an intermediate time-dependent configuration is introduced that corresponds to the undeformed body of the shape changed due to wear. Two time scales are also introduced in order to distinguish the fast time of the actual deformation (contact) problem from the slow time of the wear process (shape evolution problem). Separation of these time scales allows us to partially decouple the deformation problem and the shape evolution problem. Shape parameterization is introduced and the corresponding shape update scheme is formulated as a minimization problem. In particular, a second-order scheme is developed which exploits shape sensitivities of the deformation problem. Numerical examples are provided to illustrate the performance and accuracy of the proposed numerical schemes.",10.1016/j.cma.2010.12.020,https://dx.doi.org/10.1016/j.cma.2010.12.020
"Hardyanto, W | Merkel, B",2007,Introducing probability and uncertainty in groundwater modeling with FEMWATER-LHS,Applications_JOURNAL OF HYDROLOGY,11,1.0,"A three-dimensional finite element groundwater model for density-dependent flow and transport through saturated-unsaturated porous media was written, taking into account uncertainty modeling by means of Latin Hypercube Sampling (LHS) using a restricted pairing algorithm. The original FEMWATER code was rewritten and a LHS shell combined with rank order correlation was added. The new code was named FEMWATER-LHS and comes with an Argus Open Numerical Environments Graphical User Interfaces (Argus ONE (TM) GUI). The hybrid Lagrangian-Eulerian finite element method was incorporated in the transport module, thus the combined flow and transport can handle a wide range of real-world problems. To demonstrate the applicability for uncertainty and sensitivity analysis using Latin Hypercube Sampling, the FEMWATER-LHS code was applied to two benchmark test and good estimates of the cumulative distribution function in comparison with results from large random or LHS-samples were obtained. Latin Hypercube Samples estimates seem to improve slightly between m =  and . The statistical sensitivity was calculated by standardized rank regression coefficients and partial rank correlation coefficients to clarify the relationship between output variable and critical input parameter values. The saturated water content theta(s). or porosity in material one was the most sensitive parameter followed by the saturated hydraulic conductivity contributing to the uncertainty in concentration.",10.1016/j.jhydrol.2006.06.035,https://dx.doi.org/10.1016/j.jhydrol.2006.06.035
"Scott, MH | Azad, VJ",2017,Response sensitivity of material and geometric nonlinear force-based Timoshenko frame elements,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,1,1.0,"Response sensitivity is an essential component to understanding the complexity of material and geometric nonlinear finite element formulations of structural response. The direct differentiation method (DDM), a versatile approach to computing response sensitivity, requires differentiation of the equations that govern the state determination of an element and it produces accurate and efficient results. The DDM is applied to a force-based element formulation that utilizes curvature-shear-based displacement interpolation (CSBDI) in its state determination for material and geometric nonlinearity in the basic system of the element. The response sensitivity equations are verified against finite difference computations, and a detailed example shows the effect of parameters that control flexure-shear interaction for a stress resultant plasticity model. The developed equations make the CSBDI force-based element available for gradient-based applications such as reliability and optimization where efficient computation of response sensitivities is necessary for convergence of gradient-based search algorithms.",10.1002/nme.5479,https://dx.doi.org/10.1002/nme.5479
"Ozen, HC | Bal, G",2017,A dynamical polynomial chaos approach for long-time evolution of SPDEs,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,1,1.0,"We propose a Dynamical generalized Polynomial Chaos (DgPC) method to solve time-dependent stochastic partial differential equations (SPDEs) with white noise forcing. The long-time simulation of SPDE solutions by Polynomial Chaos (PC) methods is notoriously difficult as the dimension of the stochastic variables increases linearly with time. Exploiting the Markovian property of white noise, DgPC [] implements a restart procedure that allows us to expand solutions at future times in terms of orthogonal polynomials of the measure describing the solution at a given time and the future white noise. The dimension of the representation is kept minimal by application of a Karhunen-Loeve (KL) expansion. Using frequent restarts and low degree polynomials on sparse multi-index sets, the method allows us to perform long time simulations, including the calculation of invariant measures for systems which possess one. We apply the method to the numerical simulation of stochastic Burgers and Navier-Stokes equations with white noise forcing. Our method also allows us to incorporate time-independent random coefficients such as a random viscosity. We propose several numerical simulations and show that the algorithm compares favorably with standard Monte Carlo methods.",10.1016/j.jcp.2017.04.054,https://dx.doi.org/10.1016/j.jcp.2017.04.054
"Sullivan, TJ | Topcu, U | McKerns, M | Owhadi, H",2011,Uncertainty quantification via codimension-one partitioning,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,7,1.0,"We consider uncertainty quantification in the context of certification, i.e. showing that the probability of some 'failure' event is acceptably small. In this paper, we derive a new method for rigorous uncertainty quantification and conservative certification by combining McDiarmid's inequality with input domain partitioning and a new concentration-of-measure inequality. We show that arbitrarily sharp upper bounds on the probability of failure can be obtained by partitioning the input parameter space appropriately; in contrast, the bound provided by McDiarmid's inequality is usually not sharp. We prove an error estimate for the method (Proposition .); we define a codimension-one recursive partitioning scheme and prove its convergence properties (Theorem .); finally, we apply a new concentration-of-measure inequality to give confidence levels when empirical means are used in place of exact ones (Section ).",10.1002/nme.3030,https://dx.doi.org/10.1002/nme.3030
"Wan, J | Zabaras, N",2014,A probabilistic graphical model based stochastic input model construction,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,4,1.0,"Model reduction techniques have been widely used in modeling of high-dimensional stochastic input in uncertainty quantification tasks. However, the probabilistic modeling of random variables projected into reduced-order spaces presents a number of computational challenges. Due to the curse of dimensionality, the underlying dependence relationships between these random variables are difficult to capture. In this work, a probabilistic graphical model based approach is employed to learn the dependence by running a number of conditional independence tests using observation data. Thus a probabilistic model of the joint PDF is obtained and the PDF is factorized into a set of conditional distributions based on the dependence structure of the variables. The estimation of the joint PDF from data is then transformed to estimating conditional distributions under reduced dimensions. To improve the computational efficiency, a polynomial chaos expansion is further applied to represent the random field in terms of a set of standard random variables. This technique is combined with both linear and nonlinear model reduction methods. Numerical examples are presented to demonstrate the accuracy and efficiency of the probabilistic graphical model based stochastic input models.",10.1016/j.jcp.2014.05.002,https://dx.doi.org/10.1016/j.jcp.2014.05.002
"Miki, K | Panesi, M | Prudhomme, S",2015,Systematic validation of non-equilibrium thermochemical models using Bayesian inference,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,3,1.0,"The validation process proposed by Babuska etal. [] is applied to thermochemical models describing post-shock flow conditions. In this validation approach, experimental data is involved only in the calibration of the models, and the decision process is based on quantities of interest (QoIs) predicted on scenarios that are not necessarily amenable experimentally. Moreover, uncertainties present in the experimental data, as well as those resulting from an incomplete physical model description, are propagated to the QoIs. We investigate four commonly used thermochemical models: a one-temperature model (which assumes thermal equilibrium among all inner modes), and two-temperature models developed by Macheret etal. [], Marrone and Treanor [], and Park []. Up to  uncertain parameters are estimated using Bayesian updating based on the latest absolute volumetric radiance data collected at the Electric Arc Shock Tube (EAST) installed inside the NASA Ames Research Center. Following the solution of the inverse problems, the forward problems are solved in order to predict the radiative heat flux, QoI, and examine the validity of these models. Our results show that all four models are invalid, but for different reasons: the one-temperature model simply fails to reproduce the data while the two-temperature models exhibit unacceptably large uncertainties in the QoI predictions.",10.1016/j.jcp.2015.05.011,https://dx.doi.org/10.1016/j.jcp.2015.05.011
"Mishra, A | Mani, K | Mavriplis, D | Sitaraman, J",2015,Time dependent adjoint-based optimization for coupled fluid-structure problems,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,3,1.0,"A formulation for sensitivity analysis of fully coupled time-dependent aeroelastic problems is given in this paper. Both forward sensitivity and adjoint sensitivity formulations are derived that correspond to analogues of the fully coupled non-linear aeroelastic analysis problem. Both sensitivity analysis formulations make use of the same iterative disciplinary solution techniques used for analysis, and make use of an analogous coupling strategy. The information passed between fluid and structural solvers is dimensionally equivalent in all cases, enabling the use of the same data structures for analysis, forward and adjoint problems. The fully coupled adjoint formulation is then used to perform rotor blade design optimization for a four bladed HART rotor in hover conditions started impulsively from rest. The effect of time step size and mesh resolution on optimization results is investigated.",10.1016/j.jcp.2015.03.010,https://dx.doi.org/10.1016/j.jcp.2015.03.010
"Van Buren, KL | Hemez, FM",2016,Achieving robust design through statistical effect screening,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,2,1.0,"This work proposes a method for statistical effect screening to identify design parameters of a numerical simulation that are influential to performance while simultaneously being robust to epistemic uncertainty introduced by calibration variables. Design parameters are controlled by the analyst, but the optimal design is often uncertain, while calibration variables are introduced by modeling choices. We argue that uncertainty introduced by design parameters and calibration variables should be treated differently, despite potential interactions between the two sets. Herein, a robustness criterion is embedded in our effect screening to guarantee the influence of design parameters, irrespective of values used for calibration variables. The Morris screening method is utilized to explore the design space, while robustness to uncertainty is quantified in the context of info-gap decision theory. The proposed method is applied to the National Aeronautics and Space Administration Multidisciplinary Uncertainty Quantification Challenge Problem, which is a black-box code for aeronautic flight guidance that requires  input parameters. The application demonstrates that a large number of variables can be handled without formulating simplifying assumptions about the potential coupling between calibration variables and design parameters. Because of the computational efficiency of the Morris screening method, we conclude that the analysis can be applied to even larger-dimensional problems. (Approved for unlimited, public release on October , , LA-UR--, Unclassified.) Copyright (c)  John Wiley & Sons, Ltd.",10.1002/nme.4981,https://dx.doi.org/10.1002/nme.4981
"Phillips, EG | Elman, HC",2015,A stochastic approach to uncertainty in the equations of MHD kinematics,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,3,1.0,"The magnetohydrodynamic (MHD) kinematics model describes the electromagnetic behavior of an electrically conducting fluid when its hydrodynamic properties are assumed to be known. In particular, the MHD kinematics equations can be used to simulate the magnetic field induced by a given velocity field. While prescribing the velocity field leads to a simpler model than the fully coupled MHD system, this may introduce some epistemic uncertainty into the model. If the velocity of a physical system is not known with certainty, the magnetic field obtained from the model may not be reflective of the magnetic field seen in experiments. Additionally, uncertainty in physical parameters such as the magnetic resistivity may affect the reliability of predictions obtained from this model. By modeling the velocity and the resistivity as random variables in the MHD kinematics model, we seek to quantify the effects of uncertainty in these fields on the induced magnetic field. We develop stochastic expressions for these quantities and investigate their impact within a finite element discretization of the kinematics equations. We obtain mean and variance data through Monte Carlo simulation for several test problems. Toward this end, we develop and test an efficient block preconditioner for the linear systems arising from the discretized equations.",10.1016/j.jcp.2014.12.002,https://dx.doi.org/10.1016/j.jcp.2014.12.002
"Wang, HQ | Lin, G | Li, JL",2016,Gaussian process surrogates for failure detection: A Bayesian experimental design approach,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,2,1.0,"An important task of uncertainty quantification is to identify the probability of undesired events, in particular, system failures, caused by various sources of uncertainties. In this work we consider the construction of Gaussian process surrogates for failure detection and failure probability estimation. In particular, we consider the situation that the underlying computer models are extremely expensive, and in this setting, determining the sampling points in the state space is of essential importance. We formulate the problem as an optimal experimental design for Bayesian inferences of the limit state (i.e., the failure boundary) and propose an efficient numerical scheme to solve the resulting optimization problem. In particular, the proposed limit-state inference method is capable of determining multiple sampling points at a time, and thus it is well suited for problems where multiple computer simulations can be performed in parallel. The accuracy and performance of the proposed method is demonstrated by both academic and practical examples.",10.1016/j.jcp.2016.02.053,https://dx.doi.org/10.1016/j.jcp.2016.02.053
"Wu, KY | Li, JL",2016,A surrogate accelerated multicanonical Monte Carlo method for uncertainty quantification,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,2,1.0,"In this work we consider a class of uncertainty quantification problems where the system performance or reliability is characterized by a scalar parametery y. The performance parameter y is random due to the presence of various sources of uncertainty in the system, and our goal is to estimate the probability density function (PDF) of y. We propose to use the multicanonical Monte Carlo (MMC) method, a special type of adaptive importance sampling algorithms, to compute the PDF of interest. Moreover, we develop an adaptive algorithm to construct local Gaussian process surrogates to further accelerate the MMC iterations. With numerical examples we demonstrate that the proposed method can achieve several orders of magnitudes of speedup over the standard Monte Carlo methods.",10.1016/j.jcp.2016.06.020,https://dx.doi.org/10.1016/j.jcp.2016.06.020
"Wu, KL | Tang, HZ | Xiu, DB",2017,A stochastic Galerkin method for first-order quasilinear hyperbolic systems with uncertainty,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,1,1.0,"This paper is concerned with generalized polynomial chaos (gPC) approximation for first order quasilinear hyperbolic systems with uncertainty. The one-dimensional (D) hyperbolic system is first symmetrized with the aid of left eigenvector matrix of the Jacobian matrix. Then the gPC stochastic Galerkin method is applied to derive a provably symmetrically hyperbolic equations for the gPC expansion coefficients. The resulting deterministic gPC Galerkin system is discretized by a path-conservative finite volume WENO scheme in space and a third-order total variation diminishing Runge-Kutta method in time. The method is further extended to two-dimensional (D) quasilinear hyperbolic system with uncertainty, where the symmetric hyperbolicity of the one-dimensional gPC Galerkin system is carried over via an operator splitting technique. Several numerical experiments are conducted to demonstrate the accuracy and effectiveness of the proposed gPC stochastic Galerkin method.",10.1016/j.jcp.2017.05.027,https://dx.doi.org/10.1016/j.jcp.2017.05.027
"Latifi, M | van der Meer, FP | Sluys, LJ",2017,An interface thick level set model for simulating delamination in composites,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,1,1.0,"This paper presents a discontinuous thick level set model for modeling delamination initiation and propagation in composites. Interface elements are widely applied in delamination models to define a discontinuity between layers of a laminate. In this paper, the common damage definition in the constitutive relation of interface elements is replaced with a new definition developed using the thick level set approach. Following this approach, a band of damage with predefined length is considered where the damage is defined as a function of distance to the damage front. The specifications of suitable damage functions for the developed method are investigated, and an efficient damage function is introduced. A sensitivity analysis of numerical input parameters is performed, which proves that the model is not sensitive to the length of the damage band. Because the required element size is linked to the length of the damage band, the insensitivity to this length provides freedom to use coarser meshes. Furthermore, the model provides a direct link between fracture mechanics and damage mechanics, which enables further development of the model for fatigue analysis. Validation of this model is presented by conducting three-dimensional mode I, mode II, and mixed-mode simulations and comparing the results with analytical solutions.",10.1002/nme.5463,https://dx.doi.org/10.1002/nme.5463
"Yang, KR | Guha, N | Efendiev, Y | Mallick, BK",2017,Bayesian and variational Bayesian approaches for flows in heterogeneous random media,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,1,1.0,"In this paper, we study porous media flows in heterogeneous stochastic media. We propose an efficient forward simulation technique that is tailored for variational Bayesian inversion. As a starting point, the proposed forward simulation technique decomposes the solution into the sum of separable functions (with respect to randomness and the space), where each term is calculated based on a variational approach. This is similar to Proper Generalized Decomposition (PGD). Next, we apply a multiscale technique to solve for each term (as in []) and, further, decompose the random function into ID fields. As a result, our proposed method provides an approximation hierarchy for the solution as we increase the number of terms in the expansion and, also, increase the spatial resolution of each term. We use the hierarchical solution distributions in a variational Bayesian approximation to perform uncertainty quantification in the inverse problem. We conduct a detailed numerical study to explore the performance of the proposed uncertainty quantification technique and show the theoretical posterior concentration.",10.1016/j.jcp.2017.04.034,https://dx.doi.org/10.1016/j.jcp.2017.04.034
"Subramaniam, GM | Vedula, P",2017,A transformed path integral approach for solution of the Fokker-Planck equation,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,1,1.0,"A novel path integral (PI) based method for solution of the Fokker-Planck equation is presented. The proposed method, termed the transformed path integral (TPI) method, utilizes a new formulation for the underlying short-time propagator to perform the evolution of the probability density function (PDF) in a transformed computational domain where a more accurate representation of the PDF can be ensured. The new formulation, based on a dynamic transformation of the original state space with the statistics of the PDF as parameters, preserves the non-negativity of the PDF and incorporates short-time properties of the underlying stochastic process. New update equations for the state PDF in a transformed space and the parameters of the transformation (including mean and covariance) that better accommodate nonlinearities in drift and non-Gaussian behavior in distributions are proposed (based on properties of the SDE). Owing to the choice of transformation considered, the proposed method maps a fixed grid in transformed space to a dynamically adaptive grid in the original state space. The TPI method, in contrast to conventional methods such as Monte Carlo simulations and fixed grid approaches, is able to better represent the distributions (especially the tail information) and better address challenges in processes with large diffusion, large drift and large concentration of PDF. Additionally, in the proposed TPI method, error bounds on the probability in the computational domain can be obtained using the Chebyshev's inequality. The benefits of the TPI method over conventional methods are illustrated through simulations of linear and nonlinear drift processes in one-dimensional and multidimensional state spaces. The effects of spatial and temporal grid resolutions as well as that of the diffusion coefficient on the error in the PDF are also characterized.",10.1016/j.jcp.2017.06.002,https://dx.doi.org/10.1016/j.jcp.2017.06.002
"Chen, XJ | Li, JL",2017,A subset multicanonical Monte Carlo method for simulating rare failure events,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,1,1.0,"Estimating failure probabilities of engineering systems is an important problem in many engineering fields. In this work we consider such problems where the failure probability is extremely small (e.g. <= (-)). In this case, standard Monte Carlo methods are not feasible due to the extraordinarily large number of samples required. To address these problems, we propose an algorithm that combines the main ideas of two very powerful failure probability estimation approaches: the subset simulation (SS) and the multicanonical Monte Carlo (MMC) methods. Unlike the standard MMC which samples in the entire domain of the input parameter in each iteration, the proposed subset MMC algorithm adaptively performs MMC simulations in a subset of the state space, which improves the sampling efficiency. With numerical examples we demonstrate that the proposed method is significantly more efficient than both of the SS and the MMC methods. Moreover, like the standard MMC, the proposed algorithm can reconstruct the complete distribution function of the parameter of interest and thus can provide more information than just the failure probabilities of the systems.",10.1016/j.jcp.2017.04.051,https://dx.doi.org/10.1016/j.jcp.2017.04.051
"van den Bos, LMM | Koren, B | Dwight, RP",2017,Non-intrusive uncertainty quantification using reduced cubature rules,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,1,1.0,"For the purpose of uncertainty quantification with collocation, a method is proposed for generating families of, one-dimensional nested quadrature rules with positive weights and symmetric nodes. This is achieved through a reduction procedure: we start with a high-degree quadrature rule with positive weights and remove nodes while preserving symmetry and positivity. This is shown to be always possible, by a lemma depending primarily on Caratheodory's theorem. The resulting one-dimensional rules can be used within a Smolyak procedure to produce sparse multi-dimensional rules, but weight positivity is lost then. As a remedy, the reduction procedure is directly applied to multidimensional tensor-product cubature rules. This allows to produce a family of sparse cubature rules with positive weights, competitive with Smolyak rules. Finally the positivity constraint is relaxed to allow more flexibility in the removal of nodes. This gives a second family of sparse cubature rules, in which iteratively as many nodes as possible are removed. The new quadrature and cubature rules are applied to test problems from mathematics and fluid dynamics. Their performance is compared with that of the tensor-product and standard Clenshaw-Curtis Smolyak cubature rule.",10.1016/j.jcp.2016.12.011,https://dx.doi.org/10.1016/j.jcp.2016.12.011
"Vieilledent, D | Fourment, L",2001,Shape optimization of axisymmetric preform tools in forging using a direct differentiation method,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,16,0.94,"A method for computing shape sensitivity in the frame of non-linear and non-steady-state forging is presented. Derivatives of toot geometry, velocity and state variables with respect to the shape parameters are calculated by a direct differentiation of discrete equations. Because of the important part played by the accuracy of finite element calculations, an efficient transfer method is used between meshes during remeshings and the contact algorithms are carefully differentiated. The resulting inverse design procedure is successfully applied to two industrial examples of forging of automobile parts, with fold-over and piping defects occurring during the intermediate designs. It makes it possible to suggest reasonable preform shapes, with or without any available knowledge of the forging process.",10.1002/nme.256,https://dx.doi.org/10.1002/nme.256
"Parente, E | Vaz, LE",2001,Improvement of semi-analytical design sensitivities of non-linear structures using equilibrium relations,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,16,0.94,"The accuracy problem of the semi-analytical method for shape design sensitivity analysis has been reported for linear and non-linear structures. The source of error is the numerical differentiation of the element internal force vector, which is inherent to the semi-analytical approach. Such errors occur for structures whose displacement field is characterized by large rigid body rotations of individual elements. This paper presents a method for the improvement of semi-analytical sensitivities. The method is based on the element free body equilibrium conditions, and on the exact differentiation of the rigid body modes. The method is efficient, simple to code, and can be applied to linear and non-linear structures. The numerical examples show that this approach eliminates the abnormal errors that occur in the conventional semi-analytical method.",10.1002/nme.115,https://dx.doi.org/10.1002/nme.115
"Smith, DE",2003,Design sensitivity analysis and optimization for polymer sheet extrusion and mold filling processes,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,14,0.93,"A polymer processing design methodology is presented which can be used to improve the production of plastic components manufactured via extrusion and injection molding processes. The design method combines polymer process modelling, design sensitivity analysis and numerical optimization. It is applicable to systems with creeping flow of purely viscous non-Newtonian fluids through thin cavities where the lubrication approximation may be applied. An analysis and an adjoint design sensitivity analysis are presented for the steady-state coupled system which describes the pressure and residence time distributions in sheeting dies. The resulting methodology is then used to define the optimal die cavity shape and relevant process parameters in a sheeting die design example. A second example considers tooling and process design for the polymer injection molding process. A coupled transient mold filling analysis and design sensitivity analysis based on the direct differentiation method are developed where attention is given to describing the location of the gates in the mold cavity. The latter is illustrated through the process design of a plastic automotive component.",10.1002/nme.782,https://dx.doi.org/10.1002/nme.782
"Aryana, F | Bahai, H | Mirzaeifar, R | Yeilaghi, A",2007,Modification of dynamic characteristics of FGM plates with integrated piezoelectric layers using first- and second-order approximations,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,10,0.91,"This paper proposes a method for calculating the required changes in the physical properties of plates made of functionally graded materials (FGM) in order to achieve the desired eigenfrequency shifts in the structure. A finite element formulation based on the classical laminated plate theory (CLPT) is presented for an FGM plate with integrated piezoelectric layers. Using this formulation, an efficient method based on the first-order and second-order approximations in Taylor expansion is expressed to calculate the corresponding changes in the plate modal properties due to changes in parameters which characterize the structure's dynamic behaviour. An initial sensitivity analysis lis carried out to identify the regions within the structure where modifications are most effective on the structure's dynamic characteristics. The proposed algorithm is applied to a case study with two different boundary conditions and the results are validated against exact solutions. The influence of structural modifications on dynamic response of the plate is also studied using the Newmark-beta method.",10.1002/nme.1927,https://dx.doi.org/10.1002/nme.1927
"Hasselman, T | Lloyd, G",2008,"A top-down approach to calibration, validation, uncertainty quantification and predictive accuracy assessment",Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,9,0.9,"This paper describes a ""top-down"" uncertainty quantification (UQ) approach for calibration, validation and predictive accuracy assessment of the SNL Validation Workshop Structural Dynamics Challenge Problem. The top-down UQ approach differs from the more conventional (""bottom-up"") approach in that correlated statistical analysis is performed directly with the modal characteristics (frequencies, mode shapes and damping ratios) rather than using the modal characteristics to derive the statistics of physical model parameters (springs, masses and viscous damping elements in the present application). In this application, a stochastic subsystem model is coupled with a deterministic subsystem model to analyze stochastic system response to stochastic forcing functions. The weak nonlinearity of the stochastic subsystem was characterized by testing it at three different input levels, low, medium and high. The calibrated subsystem models were validated with additional test data using published NASA and Air Force validation criteria. The validated subsystem models were first installed in the accreditation test bed where system response simulations involving stochastic shock-type force inputs were conducted. The validated stochastic subsystem model was then installed in the target application and simulations involving limited duration segments of stationary random vibration excitation were conducted.",10.1016/j.cma.2007.07.031,https://dx.doi.org/10.1016/j.cma.2007.07.031
"Jog, CS",2004,"Higher-order shell elements based on a Cosserat model, and their use in the topology design of structures",Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,12,0.86,"For the purpose of carrying out topology optimization of shell structures, we show that it is advantageous to use shell elements based on a classical shell theory (such as the Cosserat theory) rather than elements based on the degenerated solid approach, since the shell thickness appears explicitly in the formulation, thereby greatly simplifying the sensitivity analysis. One of the well-known shell elements based on the Cosserat shell theory is the four-node element presented by Simo et al. [Comput. Methods Appl. Mech. Engrg.  ()   () ]. Although one could use this element, the use of lower-order elements often results in instabilities (Such as the ""checkerboard"" instability) in the resulting topologies; this provides the motivation for developing higher-order shell elements based on a classical shell theory. In this work, we present the formulation and implementation details for six-node and seven-node triangular, and nine-node quadrilateral shell elements, which are based on the variational formulation of Simo et al., and show that good accuracy is obtained even with coarse meshes in fairly demanding problems. We also present the topology optimization formulation, and some examples of optimal topologies that are obtained using this formulation.",10.1016/j.cma.2004.01.011,https://dx.doi.org/10.1016/j.cma.2004.01.011
"Rajadhyaksha, SM | Michaleris, P",2000,Optimization of thermal processes using an Eulerian formulation and application in laser surface hardening,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,15,0.83,"A systematic design approach has been developed for thermal processes combining the finite element method, design sensitivity analysis and optimization. Conductive heat transfer is solved in an Eulerian formulation, where-the heat flux is fixed in space and the material flows through a control volume. For constant Velocity and heat flux distribution, the Eulerian formulation reduces to a steady-state problem, whereas the Lagrangian formulation remains transient. The reduction to a steady-state problem drastically improves the computational efficiency. Streamline Upwinding Petrov-Galerkin stabilization is employed to suppress the spurious oscillations. Design sensitivities of the temperature field are computed using both the direct differentiation and the adjoint methods. The systematic approach is applied in optimizing the laser surfacing process, where a moving laser beam heats the surface of a prate, and hardening is achieved by rapid cooling due to the heat transfer below the surface. The optimization objective is to maximize the rate of surface hardening. Constraints are introduced on the computed temperature and temperature rate fields to ensure that phase transformations are activated and that melting does not occur.",10.1002/(SICI)1097-0207(20000420)47:11<1807::AID-NME819>3.0.CO;2-D,https://dx.doi.org/10.1002/(SICI)1097-0207(20000420)47:11<1807::AID-NME819>3.0.CO;2-D
"Nguyen, QA | Tong, LY | Gu, YX",2007,Evolutionary piezoelectric actuators design optimisation for static shape control of smart plates,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,9,0.82,"This paper presents a new evolutionary algorithm to solve various structural shape control problems of smart composite plate structures with active piezoelectric actuators. The linear least square (LLS) method and the features of evolutionary strategies are employed to find the applied voltages and shapes for the active piezoelectric actuators, respectively, in order to achieve the desired structural shapes by gradually removing the active piezoelectric material part of the element based on the error function sensitivity number. In the finite element (FE) analysis, an error function sensitivity number, including electro-mechanical effect, is one derived to compute the change in error functions that are defined in terms of least square difference between calculated and desired structural shapes. The evolutionary piezoelectric actuator design optimisation (EPADO) is proposed here to optimise the active piezoelectric actuator shape at a given applied voltage. Finally, several numerical examples are presented to verify that the proposed algorithm improves structural shape control by reducing the error function.",10.1016/j.cma.2007.07.018,https://dx.doi.org/10.1016/j.cma.2007.07.018
"Kleuter, B | Menzel, A | Steinmann, P",2007,Generalized parameter identification for finite viscoelasticity,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,9,0.82,"Elastomeric and other rubber-like materials are often simultaneously exposed to short- and long-time loads within engineering applications. When aiming at establishing a general simulation tool for viscoelastic media over these different time scales, a suitable material model and its corresponding material parameters can only be determined if an appropriate number of experimental data is taken into account. In this work we present an algorithm for the identification of material parameters for large strain viscoelasticity in which data of multiple experiments are considered. Based on this method the experimental loading intervals for long-time experiments can be shortened in time and the parameter identification procedure is now referred to experimental data of tests under short- and long-time loads without separating the parameters due to these different time scales. The employed viscoelastic material law is based on a nonlinear evolution law and valid far from thermodynamic equilibrium. The identification is carried out by minimizing a least squares functional comparing inhomogeneous displacement fields from experiments and FEM simulations at given (measured) force loads. Within this optimization procedure all material parameters are identified simultaneously by means of a gradient based method for which a semi-analytical sensitivity analysis is calculated. A representative numerical example is referred to measured data based on short-time and long-time tests of a non-cellular polyurethane. As an advantage, the developed identification scheme renders solely one single set of material parameters.",10.1016/j.cma.2007.03.010,https://dx.doi.org/10.1016/j.cma.2007.03.010
"van den Brink, C | Zaadnoordijk, WJ | Burgers, S | Griffioen, J",2008,Stochastic uncertainties and sensitivities of a regional-scale transport model of nitrate in groundwater,Applications_JOURNAL OF HYDROLOGY,8,0.8,"Groundwater quality management relies more and more on models in recent years. These models are used to predict the risk of groundwater contamination for various land uses. This paper presents an assessment of uncertainties and sensitivities to input parameters for a regional model. The model had been set up to improve and facilitate the decision-making process between stakeholders and in a groundwater quality conflict. The stochastic uncertainty and sensitivity analysis comprised a Monte Carlo simulation technique in combination with a Latin hypercube sampling procedure. The uncertainty of the calculated concentrations of nitrate leached into groundwater was assessed for the various combinations of land use, soil type, and depth of the groundwater table in a vulnerable, sandy region in The Netherlands. The uncertainties in the shallow groundwater were used to assess the uncertainty of the nitrate concentration in the abstracted groundwater. The confidence intervals of the calculated nitrate concentrations in shallow groundwater for agricultural land use functions did not overlap with those of non-agricultural land use such as nature, indicating significantly different nitrate leaching in these areas. The model. results were sensitive for almost all input parameters analyzed. However, the NSS is considered pretty robust because no shifts in uncertainty between factors occurred between factors towards systematic changes in fertilizer and manure inputs of the scenarios. In view of these results, there is no need to collect more data to allow science based decision-making in this planning process.",10.1016/j.jhydrol.2008.08.001,https://dx.doi.org/10.1016/j.jhydrol.2008.08.001
"Wang, YX | Chang, KH",2013,Continuum-based sensitivity analysis for coupled atomistic and continuum simulations for 2-D applications using bridging scale decomposition,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,4,0.8,"The concept of linking simulations at multiple length and time scales is found useful for studying local physical phenomena such as crack propagation. Many multi-scale methods, which couple molecular dynamics models with continuum models, have been proposed over the last decade. One of the most advanced methods developed recently is the bridging scale method, in which the total displacement is decomposed into orthogonal coarse and fine scales. This paper presents the continuum-based sensitivity analysis for two-dimensional coupled atomistic and continuum problems using the bridging scale method. A variational formulation for the bridging scale decomposition is developed based on the Hamilton's principle. The continuum-based variational formulation provides a uniform and generalized system of equations from which the differential equations can be obtained naturally. The sensitivity expressions for both direct differentiation method (DDM) and adjoint variable method (AVM) are derived in a continuum setting. Due to its efficiency for crack problems, the direct differentiation method is chosen to be implemented numerically and applied to two examples, including a crack propagation problem. Both material and sizing design variables are included to reveal the impact of design changes at the macroscopic level to the responses at the atomistic level. Also demonstrated is the feasibility of achieving the variation of the time history kernel computed using numerical procedures. The sensitivity coefficients calculated are shown to be accurate compared with overall finite difference method. The physical implications of the sensitivity results are also discussed, which accurately predict the behavior of the structural responses.",10.1007/s00158-012-0863-8,https://dx.doi.org/10.1007/s00158-012-0863-8
"Zhang, JP | Gong, SG | Huang, YQ | Qiu, AH | Chen, RK",2008,Structural dynamic shape optimization and sensitivity analysis based on RKPM,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,8,0.8,"A numerical method of structural dynamic shape optimization is presented by using reproducing kernel particle method (RKPM), by which the mesh distortion that exists in shape optimal method based on finite element can be eliminated completely and the optimal model for structural dynamic optimization design is built. The discreteness-based design sensitivity analysis in both natural frequency and dynamic response is proposed by using direct differentiation method and discrete derivatives on the basis of structural dynamic analysis, in which the penalty method is employed into imposing the essential boundary conditions, and the derivatives of shape functions with respect to design variables are derived. The algorithm of dynamic sensitivity analysis is testified by numerical example, and the numerical results obtained are in good agreement with those obtained using semi-analytical method and global finite differences method. Finally, by integrating the algorithm mentioned based on RKPM with parameterized descriptive method of boundary shape, two examples for structural dynamic shape optimization are performed.",10.1007/s00158-007-0166-7,https://dx.doi.org/10.1007/s00158-007-0166-7
"Meixner, T | Gutmann, C | Bales, R | Leydecker, A | Sickman, J | Melack, J | McConnell, J",2004,Multidecadal hydrochemical response of a Sierra Nevada watershed: sensitivity to weathering rate and changes in deposition,Applications_JOURNAL OF HYDROLOGY,11,0.79,"To address the responses of the very dilute waters in the Sierra Nevada, California, to acidic atmospheric deposition, the Alpine hydrochemical model (AHM) was used to simulate  years of runoff and solute concentrations in the Emerald Lake catchment. The ARM is a semi-distributed model of alpine watersheds that incorporates representations of the major hydrologic and biogeochemical processes that control stream chemical composition. Proxy data of discharge and snowfall were used to develop the necessary inputs for the -year runs. The long-term simulations were stable, but conflicts in the simulation of base cation and silica concentrations indicate that the model has a missing process or misrepresents mineral weathering. Sensitivity analysis of the weathering parameters indicates that a weathering rate of approximately % of the value fitted based on a one-year calibration would match the observed base saturation and the initial one year estimate had incorrect stoichiometry. Additionally, comparison of annual modeled mass flux to observed mass flux indicates that the model overestimates cation and silica export in dry years and underestimates export in wet years. Our results indicate that the Emerald Lake watershed; as represented by AHM, is not sensitive to chronic acidification with atmospheric deposition at current levels and that there would be little episodic acidification with a doubling in atmospheric deposition. However, in the simulations climate variability had an impact on stream water pH and this sensitivity should be taken into account in assessing alpine catchment sensitivity to changes in atmospheric deposition.",10.1016/j.jhydrol.2003.09.005,https://dx.doi.org/10.1016/j.jhydrol.2003.09.005
"Navarrina, F | Lopez-Fontan, S | Colominas, I | Bendito, E | Casteleiro, M",2000,High order shape design sensitivity: a unified approach,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,14,0.78,"Three basic analytical approaches have been proposed for the calculation of sensitivity derivatives in shape optimization problems. The first approach is based on differentiation of the discretised equations. The second approach is based on variation of the continuum equations and on the concept of material derivative. The third approach is based upon the existence of a transformation that links the material coordinate system with a Bred reference coordinate system. This is not restrictive, since such a transformation is inherent to FEM and BEM implementations. In this paper, we present a generalization of the latter approach on the basis of a generic unified procedure for integration in manifolds. Our aim is to obtain a single, unified, compact expression to compute arbitrarily high order directional derivatives, independent of the dimension of the material coordinates system and of the dimension of the elements. Special care has been taken on giving the final results in terms of easy-to-compute expressions, and special emphasis has been made in holding recurrence and simplicity of intermediate operations. The proposed scheme does not depend on any particular form of the state equations, and can be applied to both, direct and adjoint state formulations. Thus, its numerical implementation in standard engineering codes should be considered as a straightforward process. As an example, a second order sensitivity analysis is applied to the solution of a D shape design optimization problem.",10.1016/S0045-7825(99)00355-2,https://dx.doi.org/10.1016/S0045-7825(99)00355-2
"Loosvelt, L | De Baets, B | Pauwels, VRN | Verhoest, NEC",2014,Assessing hydrologic prediction uncertainty resulting from soft land cover classification,Applications_JOURNAL OF HYDROLOGY,3,0.75,"For predictions in ungauged basins (PUB), environmental data is generally not available and needs to be inferred by indirect means. Existing technologies such as remote sensing are valuable tools for estimating the lacking data, as these technologies become more widely available and have a high areal coverage. However, indirect estimates of the environmental characteristics are prone to uncertainty. Hence, an improved understanding of the quality of the estimates and the development of methods for dealing with their associated uncertainty are essential to evolve towards accurate PUB. In this study, the impact of the uncertainty associated with the classification of land cover based on multi-temporal SPOT imagery, resulting from the use of the Random Forests classifier, on the predictions of the hydrologic model TOPLATS is investigated through a Monte Carlo simulation. The results show that the predictions of evapotranspiration, runoff and baseflow are hardly affected by the classification uncertainty when area-averaged predictions are intended, implying that uncertainty propagation is only advisable in case a spatial distribution of the predictions is relevant for decision making or is coupled to other spatially distributed models. Based on the resulting uncertainty map, guidelines for additional data collection are formulated in order to reduce the uncertainty for future model applications. Because a Monte Carlo-based uncertainty analysis is computationally very demanding, especially when complex models are involved, we developed a fast indicative uncertainty assessment method that allows for generating proxies of the Monte Carlo-based result in terms of the mean prediction and its associated uncertainty based on a single model evaluation. These proxies are shown to perform well and provide a good indication of the impact of classification uncertainty on the prediction result.",10.1016/j.jhydrol.2014.05.049,https://dx.doi.org/10.1016/j.jhydrol.2014.05.049
"Wan, XL",2010,A note on stochastic elliptic models,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,6,0.75,"There exist two types of commonly studied stochastic elliptic models in literature: (I) - del center dot(a(x,omega)del u(x, omega)) = f(x) and (II) - del center dot(a(x,omega))lozenge del u(x,omega)) = f(x), where omega indicates randomness, lozenge the Wick product, and a(x,omega) is a positive random process. Model (I) is widely used in engineering and physical applications while model (II) is usually studied from the mathematical point of view. The difference between the above two stochastic elliptic models has not been fully clarified. In this work, we discuss the difference between models (I) and (II) when a(x,omega) is a log-normal random process. We show that the difference between models (I) and (II) is mainly characterized by a scaling factor, which is an exponential function of the degree of perturbation of a(x,w). We then construct a new stochastic elliptic model (III): -del center dot((a(-))(lozenge(-))lozenge del u(x,omega)) = f(x), which has the same scaling factor as model (I). After removing the divergence from the scaling factor, models (I) and (III) can be highly comparable for many cases. We demonstrate this by a numerical study for a one-dimensional problem.",10.1016/j.cma.2010.06.008,https://dx.doi.org/10.1016/j.cma.2010.06.008
"Mohammadi, B",2014,Uncertainty quantification by geometric characterization of sensitivity spaces,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,3,0.75,"We propose a systematic procedure for both aleatory and epistemic uncertainty quantification of numerical simulations through geometric characteristics of global sensitivity spaces. Two mathematical concepts are used to characterize the geometry of these spaces and to identify possible impacts of variability in data or changes in the models or solution procedures: the dimension of the maximal free generator subspace in vector spaces and the principal angles between subspaces. We show how these characters can be used as indications on the aleatory and epistemic uncertainties. In the case of large dimensional parameter spaces, these characterizations are established for quantile-based extreme scenarios and a multi-point moment-based sensitivity direction permits to propose a directional uncertainty quantification concept for directional extreme scenarios (DES). The approach is non-intrusive and exploits in parallel the elements of existing mono-point gradient-based design platforms. The ingredients of the paper are illustrated on a model problem with the Burgers equation with control and on a constrained aerodynamic performance analysis problem.",10.1016/j.cma.2014.07.021,https://dx.doi.org/10.1016/j.cma.2014.07.021
"Liu, SH | Li, Y | Liao, YL | Guo, ZZ",2014,Structural optimization of the cross-beam of a gantry machine tool based on grey relational analysis,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,3,0.75,"Crossbeam structural design of gantry machine tool is a multi-level, multi-index and multi-scheme decision-making problem. In order to solve the above problem, the optimum seeking model of crossbeam structure was built through using the grey relational analysis and Analytic Hierarchy Process. The finite element analysis of the static and dynamic performance parameters for four kinds of crossbeam structural schemes designed had been done, and the optimal design scheme was selected by using the optimum seeking model. After conducting sensitivity analysis for the optimal crossbeam selected, the reasonable design variables were obtained, and the dynamic optimization design model of crossbeam was established. Six groups of non-inferior solutions were obtained after solving the optimization design model. The optimal solution was selected from the non-inferior solution set through using the crossbeam structural optimization method based on grey relational analysis again, which makes the crossbeam's dynamic performance improving greatly. The dynamic experiments on the crossbeams before and after optimization design were conducted, then the experimental results show that the first four order natural frequencies of the crossbeam increase . %, . %, . % and . % respectively, which proves that the structural optimization design method based on grey relational analysis proposed in this paper is reasonable and practicable.",10.1007/s00158-013-1041-3,https://dx.doi.org/10.1007/s00158-013-1041-3
"Yang, DX | Liu, LB",2014,Reliability analysis of structures with complex limit state functions using probability density evolution method,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,3,0.75,"In reliability-based structural analysis and design optimization, there exist some limit state functions exhibiting disjoint failure domains, multiple design points and discontinuous responses. This study addresses this type of challenging problem of reliability assessment of structures with complex limit state functions based on the probability density evolution method (PDEM). Probability density function (PDF) of stochastic structures under static and dynamic loads can be acquired, which is independent of the specific form of limit state functions. Numerical results of several typical examples illustrate that, the time-invariant and instantaneous PDF curves and failure probabilities of stochastic structures with disjoint failure domains, multiple design points and discontinuous responses are calculated effectively and accurately. Moreover, the PDEM is validated to be more efficient than the Monte Carlo simulation and the subset simulation, and is a feasible and general approach to tackle the reliability analysis of complicated problems. In addition, the influence of random design parameters of structures on uncertainty propagation is also scrutinized.",10.1007/s00158-014-1048-4,https://dx.doi.org/10.1007/s00158-014-1048-4
"Lin, SP | Shi, L | Yang, RJ",2014,An alternative stochastic sensitivity analysis method for RBDO,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,3,0.75,"A new method, which is an alternative to the score function method, is developed. Unlike the score function from the literature, this proposed method uses the derivatives of response function incorporating with Kernel Density Estimation for stochastic design sensitivity analysis. Two analytical examples are used to demonstrate effectiveness and robustness of the proposed stochastic sensitivity analysis method. The sensitivity analysis method is then applied to solve two reliability-based design optimization examples.",10.1007/s00158-013-1008-4,https://dx.doi.org/10.1007/s00158-013-1008-4
"Kurdi, M | Beran, P | Stanford, B | Snyder, R",2010,Optimal actuation of nonlinear resonant systems,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,6,0.75,"In this study, we examine a new approach for actuation of dynamical systems with minimum work and maximum amplitude while maintaining constraints on the actuation force. Two methodology issues are addressed in the paper: sensitivity analysis about the nonlinear transient response and exploration of the strongly nonlinear relationship between the two objectives and the actuation design variables. The optimization analysis is carried out on lightly damped Duffing systems. The formulation of the optimization problem is found ideally suited to resolve the difficulty of dependence of response on initial conditions. The tradeoff curve of work and amplitude is computed. The optimal actuation strove to compensate for the limited force amplitude by an abrupt change in the force in time. Finally the optimization procedure is demonstrated on the kinematic design of hovering insect flight. The optimal design gives % reduction in power consumption, with a larger cutback for an actuation with high acceleration.",10.1007/s00158-009-0398-9,https://dx.doi.org/10.1007/s00158-009-0398-9
"Pedersen, P | Pedersen, NL",2014,A note on eigenfrequency sensitivities and structural eigenfrequency optimization based on local sub-domain frequencies,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,3,0.75,"Sensitivity (gradient) of a structural eigenfrequency with respect to a change in density (thickness) of a sub-domain is derived in a simple explicit form. The sub-domain is often an element of a finite element (FE) model, but may be a broader sub-domain, say with a group of elements. This simple result has many applications. It is therefore presented before specific use in optimization examples. The engineering approach of fully stressed design is a practical tool with a theoretical foundation. The analog approach to structural eigenfrequency optimization is presented here with its theoretical foundation. A numerical heuristic redesign procedure is proposed and illustrated with examples. For the ideal case, an optimality criterion is fulfilled if the design have the same sub-domain frequency (local Rayleigh quotient). Sensitivity analysis shows an important relation between squared system eigenfrequency and squared local sub-domain frequency for a given eigen-mode. Higher order eigenfrequencies may also be controlled in this manner. The presented examples are based on D finite element models with the use of subspace iteration for analysis and a heuristic recursive design procedure based on the derived optimality condition. The design that maximize a frequency depend on the total amount of available material and on a necessary interpolation as illustrated by different design cases. In this note we have assumed a linear and conservative eigenvalue problem without multiple eigenvalues. The presence of multiple, repeated eigenvalues would require extended sensitivity analysis.",10.1007/s00158-013-1014-6,https://dx.doi.org/10.1007/s00158-013-1014-6
"Herskovits, J | Dias, G | Santos, G | Soares, CMM",2000,Shape structural optimization with an interior point nonlinear programming algorithm,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,13,0.72,"The aim of this paper is to study the implementation of an efficient and reliable technique for shape optimization of solids, based on general nonlinear programming algorithms. We also study the practical behaviour for this kind of applications of a quasi-Newton algorithm, based on the Feasible Direction Interior Point Method for nonlinear constrained optimization. The optimal shape of the solid is obtained iteratively. At each iteration, a new shape is generated by B-spline curves and a new mesh is automatically generated. The control point coordinates are given by the design variables. Several illustrative two-dimensional examples are solved in a very efficient way. We conclude that the present approach is simple to formulate and to code and that our optimization algorithm is appropriate for this problem.",10.1007/s001580050142,https://dx.doi.org/10.1007/s001580050142
"Waber, HN | Gimmi, T | Smellie, JAT",2011,Effects of drilling and stress release on transport properties and porewater chemistry of crystalline rocks,Applications_JOURNAL OF HYDROLOGY,5,0.71,"The experimental verification of matrix diffusion in crystalline rocks largely relies on indirect methods performed in the laboratory. Such methods are prone to perturbations of the rock samples by collection and preparation and therefore the laboratory-derived transport properties and fluid composition might not represent in situ conditions. We investigated the effects induced by the drilling process and natural rock stress release by mass balance considerations and sensitivity analysis of analytical out-diffusion data obtained from originally saturated, large-sized drillcore material from two locations drilled using traced drilling fluid. For in situ stress-released drillcores of quartz-monzodiorite composition from the Aspo HRL, Sweden, tracer mass balance considerations and D and D diffusion modelling consistently indicated a contamination of <% of the original pore water. This chemically disturbed zone extends to a maximum of . mm into the drillcore (. mm x . mm) corresponding to about .% of the total pore volume (. vol.%). In contrast, the combined effects of stress release and the drilling process, which have influenced granodioritic drillcore material from  m below surface at Forsmark. Sweden, resulted in a maximum contamination of the derived porewater Cl(-) concentration of about %. The mechanically disturbed zone with modified diffusion properties covers the outermost similar to  mm of the drillcore ( mm x  mm), whereas the chemically disturbed zone extends to a maximum of . mm based on mass balance considerations, and to . mm to . mm into the drillcore based on fitting the observed tracer data. This corresponds to a maximum of .% of the total pore volume (. vol.%) being affected by the drilling-fluid contamination. The proportion of rock volume affected initially by drilling fluid or subsequently with experiment water during the laboratory diffusion and re-saturation experiments depends on the size of the drillcore material and will become larger the smaller the sample used for the experiment. The results are further in support of matrix diffusion taking place in the undisturbed matrix of crystalline rocks at least in the cm range.",10.1016/j.jhydrol.2011.05.029,https://dx.doi.org/10.1016/j.jhydrol.2011.05.029
"Poette, G | Despres, B | Lucor, D",2011,Treatment of uncertain material interfaces in compressible flows,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,5,0.71,"To treat uncertain interface position is an important issue for complex applications. In this paper, we address the characterization of randomly perturbed interfaces between fluids thanks to stochastic modeling and uncertainty quantification through the D Euler system. The perturbed interface is modeled as a random field and represented by a Karhunen-Loeve expansion. The stochastic D Euler system is solved applying Polynomial Chaos theory through the Intrusive Polynomial Moment Method (IPMM). This stochastic resolution method is fully explained and studied (theoretically and numerically). Stochastic Richtmyer-Meshkov unstable flows are solved and presented for several configurations of the uncertain interface (different rugosities) between the fluids. The probability density functions of the mass density of the fluid in the vicinity of the interface are computed built and compared for the different simulations: the system exhibits strong sensitivity with respect to the stochastic initially leading modes.",10.1016/j.cma.2010.08.011,https://dx.doi.org/10.1016/j.cma.2010.08.011
"Rao, BN | Rahman, S",2005,Continuum shape sensitivity analysis of a mixed-mode fracture in functionally graded materials,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,9,0.69,"This paper presents two new methods for conducting a continuum shape sensitivity analysis of a crack in an isotropic, linear-elastic functionally graded material. These methods involve the material derivative concept from continuum mechanics, domain integral representation of interaction integrals, known as the M-integral, and direct differentiation. Unlike virtual crack extension techniques, no mesh perturbation is needed to calculate the sensitivity of stress-intensity factors. Since the governing variational equation is differentiated prior to the process of discretization, the resulting sensitivity equations are independent of approximate numerical techniques, such as the meshless method, finite element method, boundary element method, or others. Three numerical examples are presented to calculate the first-order derivative of the stress-intensity factors. The results show that first-order sensitivities of stress intensity factors obtained using the proposed method are in excellent agreement with the reference solutions obtained using the finite-difference method for the structural and crack geometries considered in this study.",10.1016/j.cma.2004.06.027,https://dx.doi.org/10.1016/j.cma.2004.06.027
"Pedersen, P",2006,Analytical stiffness matrices for tetrahedral elements,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,8,0.67,"Stiffness matrices based on the non-linear Green-Lagrange strain definition may seem too complicated for analytical treatment. However, for the case of a linear displacement tetrahedron element no numerical integrations are needed. Closed form explicit analytical results are presented, making it possible to see the influence of each individual parameter and the results are directly suited for coding in a finite element program. The analytical secant and tangent element stiffness matrices are obtained by separating the dependence of the material constitutive parameters and of the stress/strain state from the dependence of the initial geometry and of the displacement assumption. The nodal positions of an element and the displacement assumption give six basic matrices of fourth order. These matrices do not depend on the material and the stress/strain state, and are thus unchanged during the necessary iterations for obtaining a solution based on the Green-Lagrange strain measure. This basic matrix approach on the directional level diminish the order of the involved matrices from  to . The presented resulting stiffness matrices are especially useful for design optimization, because analytical sensitivity analysis can then be performed. Another aspect of the paper is that linear strains imply erroneous displacement fields when rotations are involved, and we quantify these errors in relation to results based on Green-Lagrange strains.",10.1016/j.cma.2006.04.001,https://dx.doi.org/10.1016/j.cma.2006.04.001
"Wang, Q | Arora, JS",2009,Several simultaneous formulations for transient dynamic response optimization: An evaluation,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,6,0.67,"Several simultaneous formulations for transient dynamic response optimization are described, analyzed and compared. A key feature of the formulations is that the state variables, in addition to the real design variables, are treated as independent variables in the optimization process. The state variables include different combinations of generalized displacements, velocities and accelerations. Formulations based oil different discretization techniques for first-order and second-order differential equations are presented. Finite difference. Newmark's method and methods based on collocation are all discussed. Similar to the simultaneous analysis and design approach used for the optimization of structures subjected to static loads and the direct collocation/transcription method for optimal control, the equations of motion are treated as equality constraints. A major advantage of these formulations is that special design sensitivity analysis methods are no longer needed. However, the formulations have larger numbers of variables and constraints. Therefore, sparsity of the problem functions must be exploited in all the calculations. Advantages and disadvantages of the formulations are discussed. The numerical results for an example problem and performance features of the formulations are compared. All the formulations converge to the Optimum Solution for the example problem. They are quite efficient for a smaller number of time grid points: however, the computational times increase as the number of grid points is increased.",10.1002/nme.2655,https://dx.doi.org/10.1002/nme.2655
"Li, J | Stinis, P",2015,Mesh refinement for uncertainty quantification through model reduction,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,2,0.67,"We present a novel way of deciding when and where to refine a mesh in probability space in order to facilitate uncertainty quantification in the presence of discontinuities in random space. A discontinuity in random space makes the application of generalized polynomial chaos expansion techniques prohibitively expensive. The reason is that for discontinuous problems, the expansion converges very slowly. An alternative to using higher terms in the expansion is to divide the random space in smaller elements where a lower degree polynomial is adequate to describe the randomness. In general, the partition of the random space is a dynamic process since some areas of the random space, particularly around the discontinuity, need more refinement than others as time evolves. In the current work we propose a way to decide when and where to refine the random space mesh based on the use of a reduced model. The idea is that a good reduced model can monitor accurately, within a random space element, the cascade of activity to higher degree terms in the chaos expansion. In turn, this facilitates the efficient allocation of computational sources to the areas of random space where they are more needed. For the Kraichnan Orszag system, the prototypical system to study discontinuities in random space, we present theoretical results which show why the proposed method is sound and numerical results which corroborate the theory.",10.1016/j.jcp.2014.09.021,https://dx.doi.org/10.1016/j.jcp.2014.09.021
"Chen, SS | Hansen, JM | Tortorelli, DA",2000,Unconditionally energy stable implicit time integration: application to multibody system analysis and design,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,12,0.67,"This paper focuses on the development of an unconditionally stable time-integration algorithm for multibody dynamics that does not artificially dissipate energy. Unconditional stability is sought to alleviate any stability restrictions on the integration step size, while energy conservation is important for the accuracy of long-term simulations. In multibody system analysis, the time-integration scheme is complemented by a choice of coordinates that define the kinematics of the system. As such, the current approach uses a non-dissipative implicit Newmark method to integrate the equations of motion defined in terms of the independent joint co-ordinates of the system. In order to extend the unconditional stability of the implicit Newmark method to non-linear dynamic systems, a discrete energy balance is enforced. This constraint, however, yields spurious oscillations in the computed accelerations and therefore, a new acceleration corrector is developed to eliminate these instabilities and hence retain unconditional stability in an energy sense. An additional benefit of employing the non-linearly implicit time-integration method is that it allows for an efficient design sensitivity analysis. In this paper, design sensitivities computed via the direct differentiation method are used for mechanism performance optimization.",10.1002/(SICI)1097-0207(20000630)48:6<791::AID-NME859>3.0.CO;2-Z,https://dx.doi.org/10.1002/(SICI)1097-0207(20000630)48:6<791::AID-NME859>3.0.CO;2-Z
"Persson, M | Uvo, CB",2003,Estimating soil solution electrical conductivity from time domain reflectometry measurements using neural networks,Applications_JOURNAL OF HYDROLOGY,10,0.67,"Time domain reflectometry (TDR) is a widely used method for measuring the dielectric constant (K-a) and bulk electrical conductivity (sigma(a)) in soils. The TDR measured sigma(a) and K-a can be used to calculate the soil solution electrical conductivity, sigma(w.) The sigma(w), in turn, can be related to the concentration of an ionic tracer. Several models of the sigma(w)-sigma(a)-K-a relationship can be found in the literature. Most of these models require extensive calibration experiments in order to obtaining best-fit parameters. In this paper, we attempt to model the sigma(w)-sigma(a)-K-a relationship using neural networks (NN). We used TDR measured K-a and sigma(a) along with five different soil physical parameters (sand, silt, clay, and organic matter content and bulk density) measured in nine different soil types using three different sigma(w) levels in each soil type. In total,  K-a and sigma(a) measurements were obtained. The NN estimated sigma(w) was found to have a root mean square error (RMSE) of .-. dS m(-) for the nine different soil types whereas the RMSE of two traditional sigma(w)-sigma(a)-K-a models was .-. dS m(-). Furthermore, the traditional models exhibited larger errors for low sigma(a) and K-a, whereas the NN estimated sigma(w) did not show any trend in the errors. A sensitivity analysis showed that the NN model was more sensitive to small changes in sigma(a) compared to K-a. Of the five soil physical parameters, the silt and clay content affected the sigma(w)-sigma(a)-K-a relationship the most. The results presented shows that using NN, the sigma(w)-sigma(a)-K-a relationship can be predicted using soil physical parameters without need for elaborate soil specific calibration experiments.",10.1016/S0022-1694(02)00387-6,https://dx.doi.org/10.1016/S0022-1694(02)00387-6
"Shavit, U | Furman, A",2001,The location of deep salinity sources in the Israeli Coastal aquifer,Applications_JOURNAL OF HYDROLOGY,11,0.65,The salinization process of the Israeli Coastal aquifer has led to an average concentration of about  mgCl/l with a significant number of discrete salinity plumes in the middle and southern regions. The salinity of these plumes is high (- mgCl/l) and is increasing rapidly. Geochemical evidence has suggested that the salinity source in the Beer Tuvia plume (in the south part of the aquifer) is at the bottom of the aquifer. This paper describes a solution of the source inverse problem and its application in the Beer Tuvia plume. A transient two-dimensional finite element model was solved and the source terms were computed at each node in a  x  km() area. An error analysis has shown that when no errors are introduced in the input data the reconstruction is perfect. The results of a sensitivity analysis are presented and the actual reconstruction errors are estimated. Applying the model in the Beer Tuvia region indicates that a salinity source exists about  km to the west and . km to the north of the center of the salinity plume. This source is believed to be the plume source.,10.1016/S0022-1694(01)00406-1,https://dx.doi.org/10.1016/S0022-1694(01)00406-1
"Pedersen, NL",2007,On simultaneous shape and orientational design for eigenfrequency optimization,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,7,0.64,"Plates with an internal hole of fixed area are designed in order to maximize the performance with respect to eigenfrequencies. The optimization is performed by simultaneous shape, material, and orientational design. The shape of the hole is designed, and the material design is the design of an orthotropic material that can be considered as a fiber-net within each finite element. This fiber-net is optimally oriented in the individual elements of the finite element discretization. The optimizations are performed using the finite element method for analysis, and the optimization approach is a two-step method. In the first step, we find the best design on the basis of a recursive optimization procedure based on optimality criteria. In the second step, mathematical programming and sensitivity analysis are applied to find the final optimized design.",10.1007/s00158-006-0086-y,https://dx.doi.org/10.1007/s00158-006-0086-y
"Rohan, E | Whiteman, JR",2000,Shape optimization of elasto-plastic structures and continua,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,11,0.61,"This article is concerned with the sensitivity analysis and optimization of elasto-plastic bodies when isotropic strain hardening takes place. The elasto-plastic behaviour of the material is governed by a nonlinear complementarity problem. After discretization the evolutionary state problem, associated with the optimal shape problem, is formulated as a sequence of nonsmooth equations and a general form of the optimal design problem is treated by using a nonsmooth approach. The sensitivity analysis based on the adjoint-variable technique is derived for a history-dependent problem. As applications optimal design problems are solved for an elasto-plastic truss structure and for an elasto-plastic continuum.",10.1016/S0045-7825(99)00134-6,https://dx.doi.org/10.1016/S0045-7825(99)00134-6
"Hantush, MM | Govindaraju, RS",2003,Theoretical development and analytical solutions for transport of volatile organic compounds in dual-porosity soils,Applications_JOURNAL OF HYDROLOGY,9,0.6,"Predicting the behavior of volatile organic compounds in soils or sediments is necessary for managing their use and designing appropriate remedial systems to eliminate potential threats to the environment, particularly the air and groundwater resources. In this effort, based on continuity of mass flux, we derive a mass flux boundary condition of the. third type in terms of physically based mass transfer rate coefficients, describing the resistance to mass inflow of the soil-air interface, and, obtain one-dimensional analytical solutions for transport and degradation of volatile organic compounds in semi-infinite structured soils under steady, unsaturated flow conditions. The advective-dispersive mass balance formulation allows for mobile-immobile liquid phase and vapor diffusive mass transfer, with linear equilibrium adsorption and liquid-vapor phase partitioning in the dynamic and stagnant soil regions. The mass transfer rate coefficients of volatile organic chemicals across the soil-air interface are expressed in terms of solute properties and hydrodynamic characteristics of resistive soil and air-boundary layers. The solutions estimate solute vapor flux from soil surface and describe mobile-phase solute concentration as a function of depth in the soil and time. In particular, solutions were derived for: () zero-initial concentration in the soil profile subject to a continuous and pulsed source at the soil surface; and () depletion from the soil following an initially contaminated soil profile. Sensitivity analysis with respect to different dimensionless parameters is conducted and the effect on solute concentration and vapor flux of such parameters as volatilization mass transfer velocity relative to infiltration, soil Peclet number, biochemical decay, and diffusive mass transfer into the immobile phase, is plotted and the results are discussed. The mass transfer rate coefficients and the analytical solutions are applied to simulate transport of an example volatile organic compound in an aggregated soil. The simulated results indicate that macropore-aggregate vapor phase diffusion may profoundly impact transport of volatile compounds in aggregated soils.",10.1016/S0022-1694(03)00157-4,https://dx.doi.org/10.1016/S0022-1694(03)00157-4
"Putresza, JT | Kolakowski, P",2001,Sensitivity analysis of frame structures (virtual distortion method approach),Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,10,0.59,"A novel approach to sensitivity analysis of frame structures based on the virtual distortion method (VDM) has been presented. The sensitivity analysis has been performed for elasto-plastic material behaviour with respect to selected structural parameters e.g. cross-sectional area, hardening/softening coefficient and yield stress. Advantages of applying the virtual distortion method to sensitivity analysis have been emphasised i.e. making use of the so-called influence matrix, constant for the whole analysis and solving only local sets of equations corresponding to plastic locations. Theoretical background as well as the whole variety of examples has been presented.",10.1002/1097-0207(20010228)50:6<1307::AID-NME38>3.0.CO;2-Q,https://dx.doi.org/10.1002/1097-0207(20010228)50:6<1307::AID-NME38>3.0.CO;2-Q
"Kunstmann, H | Kastens, M",2006,Direct propagation of probability density functions in hydrological equations,Applications_JOURNAL OF HYDROLOGY,7,0.58,"Sustainable decisions in hydrological risk management require detailed information on the probability density function (pdf) of the model output. Only then probabilities for the failure of a specific management option or the exceedance of critical thresholds (e.g. of pollutants) can be derived. A new approach of uncertainty propagation in hydrological equations is developed that directly propagates the probability density functions of uncertain model input parameters into the corresponding probability density functions of model output. The basics of the methodology are presented and central applications to different disciplines in hydrology are shown. This work focuses on the following basic hydrological equations: () pumping test analysis (Theis-equation, propagation of uncertainties in recharge and transmissivity), () -dim groundwater contaminant transport equation (Gauss-equation, propagation of uncertainties in decay constant and dispersivity), () evapotranspiration estimation (Penman-Monteith-equation, propagation of uncertainty in roughness length). The direct propagation of probability densities is restricted to functions that are monotonically increasing or decreasing or that can be separated in corresponding monotonic branches so that inverse functions can be derived. In case no analytic solutions for inverse functions could be derived, semi-analytical approximations were used. It is shown that the results of direct probability density function propagation are in perfect agreement with results obtained from corresponding Monte Carlo derived frequency distributions. Direct pdf propagation, however, has the advantage that is yields exact solutions for the resulting hydrological pdfs rather than approximating discontinuous frequency distributions. It is additionally shown that the type of the resulting pdf depends on the specific values (order of magnitude, respectively) of the standard deviation of the input pdf. The dependency of skewness and kurtosis of the propagated pdf on the coefficient of variation of input parameter uncertainty is detected to be non-monotonic with distinctive maxima.",10.1016/j.jhydrol.2005.10.009,https://dx.doi.org/10.1016/j.jhydrol.2005.10.009
"Giner, E | Fuenmayor, FJ | Tarancon, JE",2004,An improvement of the EDI method in linear elastic fracture mechanics by means of an a posteriori error estimator in G,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,8,0.57,"In this paper, an error estimator that quantifies the effect of the finite element discretization error on the computation of the stress intensity factor in linear elastic fracture mechanics is presented. In order to obtain the proposed estimator, a shape design sensitivity analysis (SDSA) is applied to the fracture mechanics problem. Following this approach, one of the most efficient post-processing techniques for computing the strain energy release rate G, the well-known EDI method, may be interpreted as a continuum method of the SDSA. The proposed error estimator is based on the recovery of the gradient fields and its reliability has been checked by means of numerical problems, yielding very good estimations of the true error. The new estimator remarkably improves the results given by a previous error estimator, which is based on a discrete analytical approach of SDSA. As a consequence, the combination of the new error estimator and the result given by the EDI method provides a much more accurate estimation of G.",10.1002/nme.889,https://dx.doi.org/10.1002/nme.889
"Gu, YX | Zhao, GZ | Zhang, HW | Kang, Z | Grandhi, RV",2000,Buckling design optimization of complex built-up structures with shape and size variables,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,10,0.56,"The design optimization of buckling behaviour is studied for complex built-up structures composed of various kinds of elements and implemented within JIFEX, a general-purpose software for finite element analysis and design optimization. The direct and adjoint methods of sensitivity analysis for critical buckling loads are presented with detailed computational procedures. Particularly, the variations of prebuckling stresses and external loads have been accounted, for. The design model and solution methods presented in this paper are available for both shape and size optimization, and buckling optimization can also be combined with static, frequency and dynamic response optimization. The numerical examples show the applications of the buckling optimization method and the effectiveness of the methods and the program of this paper.",10.1007/s001580050101,https://dx.doi.org/10.1007/s001580050101
"Liu, XJ | Begg, DW",2000,On simultaneous optimisation of smart structures - Part I: Theory,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,10,0.56,"The emerging space missions not only support the idea of smart structures but also call for fast development and application of advanced structures having highly distributed sensors (analogous to a nervous system) and actuators (muscle-like materials) to yield structural functionality and distributed control function. A major concern of the development of the smart system is how to make the multidisciplinary system work efficiently and optimally. This problem is considered in the present paper. The optimal control, sen sitivity analysis and integrated optimisation of such a multidisciplinary system is presented. In particular the structural shape and topology are used as design variables, as augmenting the traditional optimal design concept for smart structures and leading to a relatively new field with a high potential for innovation. Discussion of the major problem solving methodologies is also presented to address the truly simultaneous optimisation of smart structures.",10.1016/S0045-7825(99)00010-9,https://dx.doi.org/10.1016/S0045-7825(99)00010-9
"Wagenknecht, T | Agarwal, J",2005,Structured pseudospectra in structural engineering,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,7,0.54,"This paper presents a new method for computing the pseudospectra of a matrix that respects a prescribed sparsity structure. The pseudospectrum is defined as the set of points in the complex plane to which an eigenvalue of the matrix can be shifted by a perturbation of a certain size. A canonical form for sparsity preserving perturbations is given and a computable formula for the corresponding structured pseudospectra is derived. This formula relates the computation of structured pseudospectra to the computation of the structured singular value (ssv) of an associated matrix. Although the computation of the ssv in general is an NP-hard problem, algorithms for its approximation are available and demonstrate good performance when applied to the computation of structured pseudospectra of medium-sized or highly sparse matrices. The method is applied to a wing vibration problem, where it is compared with the matrix polynomial approach, and to the stability analysis of truss structures. New measures for the vulnerability of a truss structure are proposed, which are related to the 'distance to singularity' of the associated stiffness matrix.",10.1002/nme.1414,https://dx.doi.org/10.1002/nme.1414
"Pedersen, P",2005,Analytical stiffness matrices with Green-Lagrange strain measure,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,7,0.54,"Separating the dependence on material and stress/strain state from the dependence on initial geometry, we obtain analytical secant and tangent stiffness matrices. For the case of a linear displacement triangle with uniform thickness and uniform constitutive behaviour closed-form results are listed, directly suited for coding in a finite element program. The nodal positions of an element and the displacement assumption give three basic matrices of order three. These matrices do not depend on material and stress/strain state, and thus are unchanged during the necessary iterations for obtaining a solution based on Green-Lagrange strain measure. The approach is especially useful in design optimization, because analytical sensitivity analysis then can be performed. The case of a three node triangular ring element for axisymmetric analysis involves small modifications and extension to four node tetrahedron elements should be straight forward.",10.1002/nme.1174,https://dx.doi.org/10.1002/nme.1174
"Kaminski, M",2003,Sensitivity analysis of homogenized characteristics for some elastic composites,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,8,0.53,"The main goal of the paper is to present theoretical aspects and the finite element method implementation of sensitivity analysis in homogenization of composite materials with linear elastic components using the effective modules approach. The sensitivity analysis of effective material properties is presented in a general form for n-component periodic composite and is illustrated using the examples of periodic ID as well as D heterogeneous structures. The sensitivity coefficients are determined for the effective Young's modulus and the effective elasticity tensor components. The structural response functional for the fiber-reinforced elastic composite is proposed in the form of total strain energy resulting from some uniform strain state of the composite representative volume element (RVE). The results of sensitivity analysis presented in the paper confirm the usefulness of the homogenization method in computational analysis of composite materials and its application in composite optimization, identification, shape sensitivity studies and, after some probabilistic extensions, in stochastic analysis of random composites.",10.1016/S0045-7825(03)00214-7,https://dx.doi.org/10.1016/S0045-7825(03)00214-7
"Baudoui, V | Klotz, P | Hiriart-Urruty, JB | Jan, S | Morel, F",2012,LOcal Uncertainty Processing (LOUP) method for multidisciplinary robust design optimization,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,3,0.5,"In this paper, we develop an easy-to-implement approximate method to take uncertainties into account during a multidisciplinary optimization. Multidisciplinary robust design usually involves setting up a full uncertainty propagation within the system, requiring major modifications in every discipline and on the shared variables. Uncertainty propagation is an expensive process, but robust solutions can be obtained more easily when the disciplines affected by uncertainties have a significant effect on the objectives of the problem. A heuristic method based on local uncertainty processing (LOUP) is presented here, allowing approximate solving of specific robust optimization problems with minor changes in the initial multidisciplinary system. Uncertainty is processed within the disciplines that it impacts directly, without propagation to the other disciplines. A criterion to verify a posteriori the applicability of the method to a given multidisciplinary system is provided. The LOUP method is applied to an aircraft preliminary design industrial test case, in which it allowed to obtain robust designs whose performance is more stable than the one of deterministic solutions, relatively to uncertain parameter variations.",10.1007/s00158-012-0798-0,https://dx.doi.org/10.1007/s00158-012-0798-0
"Gerzen, N | Barthold, FJ",2012,Enhanced analysis of design sensitivities in topology optimization,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,3,0.5,"This paper outlines an enhanced analysis of the design sensitivities beyond the standard computation of the gradient values. It is based on the analytical derivation and efficient computation of the Fr,chet derivatives of objectives and constraints with respect to the full space of all possible design variables. This overhead of sensitivity information is examined by a singular value decomposition (SVD) in order to detect major and minor influence and response modes of the considered structure. Thus, this methodology leads to valuable qualitative and quantitative insight which is so far unused in standard approaches to structural optimization. This knowledge enables the optimiser to understand and improve the models systematically which are usually set up entirely by engineering experience and intuition. Furthermore, a reduction of the complete design space to the most valuable subspace of design modifications demonstrates the information content of the decomposed sensitivities. The generic concept is applied to topology optimization which is a challenging model problem due to the large number of independent design variables. The details specific to topology optimization are outlined and the pros and cons are discussed. An illustrative example shows that reasonable optimal designs can be obtained with a small percentage of properly defined design variables. Nevertheless, further research is necessary to improve the overall computational efficiency.",10.1007/s00158-012-0778-4,https://dx.doi.org/10.1007/s00158-012-0778-4
"Kasolis, F | Wadbro, E | Berggren, M",2012,Fixed-mesh curvature-parameterized shape optimization of an acoustic horn,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,3,0.5,"We suggest a boundary shape optimization approach in which the optimization is carried out on the coefficients in a boundary parameterization based on a local, discrete curvature. A fixed mesh is used to numerically solve the governing equations, in which the geometry is represented through inhomogeneous coefficients, similarly as done in the material distribution approach to topology optimization. The method is applied to the optimization of an acoustic horn in two space dimensions. Numerical experiments show that this method can calculate the horn's transmission properties as accurately as a traditional, body-fitted approach. Moreover, the use of a fixed mesh allows the optimization to create shapes that would be difficult to handle with a traditional approach that uses deformations of a body-fitted mesh. The parameterization inherently promotes smooth designs without unduly restriction of the design flexibility. The optimized, smooth horns consistently show favorable transmission properties.",10.1007/s00158-012-0828-y,https://dx.doi.org/10.1007/s00158-012-0828-y
"Loizu, J | Alvarez-Mozos, J | Casali, J | Goni, M",2016,Evaluation of TOPLATS on three Mediterranean catchments,Applications_JOURNAL OF HYDROLOGY,1,0.5,"Physically based hydrological models are complex tools that provide a complete description of the different processes occurring on a catchment. The TOPMODEL-based Land-Atmosphere Transfer Scheme (TOPLATS) simulates water and energy balances at different time steps, in both lumped and distributed modes. In order to gain insight on the behavior of TOPLATS and its applicability in different conditions a detailed evaluation needs to be carried out. This study aimed to develop a complete evaluation of TOPLATS including: () a detailed review of previous research works using this model; () a sensitivity analysis (SA) of the model with two contrasted methods (Morris and Sobol) of different complexity; () a -step calibration strategy based on a multi-start Powell optimization algorithm; and () an analysis of the influence of simulation time step (hourly vs. daily). The model was applied on three catchments of varying size (La Tejeria, Cidacos and Arga), located in Navarre (Northern Spain), and characterized by different levels of Mediterranean climate influence. Both Morris and Sobol methods showed very similar results that identified Brooks-Corey Pore Size distribution Index (B), Bubbling pressure (psi(c)) and Hydraulic conductivity decay (f) as the three overall most influential parameters in TOPLATS. After calibration and validation, adequate streamflow simulations were obtained in the two wettest catchments, but the driest (Cidacos) gave poor results in validation, due to the large climatic variability between calibration and validation periods. To overcome this issue, an alternative random and discontinuous method of cal/val period selection was implemented, improving model results.",10.1016/j.jhydrol.2016.05.025,https://dx.doi.org/10.1016/j.jhydrol.2016.05.025
"Moshfegh, R | Nilsson, L | Larsson, M",2008,Estimation of process parameter variations in a pre-defined process window using a Latin hypercube method,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,5,0.5,"The aim of this paper is to present a methodology that provides an analytical tool for estimation of robustness and response variation within a pre-defined process window. To exemplify the developed methodology, the stochastic simulation technique is used for a sheet-metal forming application. A sampling plan based on the Latin hypercube sampling method for variation of design parameters is utilized, and the thickness reduction is specified as the response. Moreover, the response surface methodology is applied for understanding the quantitative relationship between design parameters and response value. The conclusions of this study are that the applied method gives a possibility to illustrate and interpret the variation of the response versus a design parameter variation. Consequently, it gives significant insights into the usefulness of individual design parameters. It has been shown that the method enables us to estimate the admissible design parameter variations and to predict the actual safe margin for given process parameters. Furthermore, the dominating design parameters can be predicated using sensitivity analysis, and this in its turn clarifies how the reliability criteria are met. Finally, the developed software can be used as an additional module for set-up of stochastic finite element simulations and to collect the numerical results from different solvers within different applications.",10.1007/s00158-007-0136-0,https://dx.doi.org/10.1007/s00158-007-0136-0
"Lelievre, N | Beaurepaire, P | Mattrand, C | Gayton, N | Otsmane, A",2016,On the consideration of uncertainty in design: optimization - reliability - robustness,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,1,0.5,"Structural designs proposed by engineers aim to ensure that specific defined requirements are satisfied. Structural optimization is also widely used to identify an admissible design with optimal performance. However, it is important to remember that real mechanical problems exhibit uncertainties in practice that might entail challenges when searching for admissible and/or optimal design solutions. One objective of this paper is to discuss the possible formulations of design problems in this type of uncertain context. By considering uncertainties in the constraint and/or objective functions underlying design problems, several design strategies might in fact be implemented. Especially, this paper should contribute to clarifying the use of the concepts of robustness and reliability in design. After an introduction, the featured design strategies are illustrated and discussed via three academic examples which involve multiple random and design variables. Results show that the structural solutions obtained can be quite sensitive to the formulation of the problem. Thus, the latter should be considered as an essential step during the design procedure.",10.1007/s00158-016-1556-5,https://dx.doi.org/10.1007/s00158-016-1556-5
"Mesgouez, A | Buis, S | Ruy, S | Lefeuve-Mesgouez, G",2014,Uncertainty analysis and validation of the estimation of effective hydraulic properties at the Darcy scale,Applications_JOURNAL OF HYDROLOGY,2,0.5,"The determination of the hydraulic properties of heterogeneous soils or porous media remains challenging. In the present study, we focus on determining the effective properties of heterogeneous porous media at the Darcy scale with an analysis of their uncertainties. Preliminary, experimental measurements of the hydraulic properties of each component of the heterogeneous medium are obtained. The properties of the effective medium, representing an equivalent homogeneous material, are determined numerically by simulating a water flow in a three-dimensional representation of the heterogeneous medium, under steady-state scenarios and using its component properties. One of the major aspects of this study is to take into account the uncertainties of these properties in the computation and evaluation of the effective properties. This is done using a bootstrap method. Numerical evaporation experiments are conducted both on the heterogeneous and on the effective homogeneous materials to evaluate the effectiveness of the proposed approach. First, the impact of the uncertainties of the component properties on the simulated water matric potential is found to be high for the heterogeneous material configuration. Second, it is shown that the strategy developed herein leads to a reduction of this impact. Finally, the adequacy between the mean of the simulations for the two configurations confirms the suitability of the homogenization approach, even in the case of dynamic scenarios. Although it is applied to green roof substrates, a two-component media composed of bark compost and pozzolan used in the construction of buildings, the methodology proposed in this study is generic.",10.1016/j.jhydrol.2014.02.065,https://dx.doi.org/10.1016/j.jhydrol.2014.02.065
"Malama, B | Kuhlman, KL | Brauchler, R | Bayer, P",2016,Modeling cross-hole slug tests in an unconfined aquifer,Applications_JOURNAL OF HYDROLOGY,1,0.5,"A modified version of a published slug test model for unconfined aquifers is applied to cross-hole slug test data collected in field tests conducted at the Widen site in Switzerland. The model accounts for water-table effects using the linearized kinematic condition. The model also accounts for inertial effects in source and observation wells. The primary objective of this work is to demonstrate applicability of this semi-analytical model to multi-well and multi-level pneumatic slug tests. The pneumatic perturbation was applied at discrete intervals in a source well and monitored at discrete vertical intervals in observation wells. The source and observation well pairs were separated by distances of up to  m. The analysis yielded vertical profiles of hydraulic conductivity, specific storage, and specific yield at observation well locations. The hydraulic parameter estimates are compared to results from prior pumping and single-well slug tests conducted at the site, as well as to estimates from particle size analyses of sediment collected from boreholes during well installation. The results are in general agreement with results from prior tests and are indicative of a sand and gravel aquifer. Sensitivity analysis show that model identification of specific yield is strongest at late-time. However, the usefulness of late-time data is limited due to the low signal-to-noise ratios.",10.1016/j.jhydrol.2016.06.060,https://dx.doi.org/10.1016/j.jhydrol.2016.06.060
"Bae, DS | Cho, HJ | Lee, S | Moon, W",2001,Recursive formulas for design sensitivity analysis of mechanical systems,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,8,0.47,"Design sensitivity analysis of a mechanical system is an essential tool for design optimization and trade-off studies. This paper presents a design sensitivity analysis method, using direct differentiation and generalized recursive formulas. The equations of motion are first generated in the Cartesian coordinate system and then transformed into the relative coordinate system by using a velocity transformation. The design-sensitivity equations are derived by directly differentiating the equations of motion. The equations of motion and of design sensitivity are discritized by using the backward difference formula (BDF) in time domain. The resulting equations constitute an overdetermined differential algebraic system (ODAS) and are treated as ordinary differential equations (ODEs) on manifolds. The computational structure of the resulting equations is examined to classify all necessary computations into several categories. The generalized recursive formula for each category is then developed and applied whenever such a category of computation is encountered in the equations of motion and of design sensitivity. Since the velocity transformation yields the equations in a compact form and computational efficiency is achieved by the generalized recursive formulas, the proposed method is not only easy to implement but also efficient. A practical example of a vehicle consisting of many joints, bushings, and tires is given to show the efficiency of the proposed method.",10.1016/S0045-7825(00)00303-0,https://dx.doi.org/10.1016/S0045-7825(00)00303-0
"Liu, JS | Parks, GT | Clarkson, PJ",2005,Topology/shape optimisation of axisymmetric continuum structures - a metamorphic development approach,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,6,0.46,"A novel topology/shape optimisation method for axisymmetric elastic solids, based on solid modeling and FE analysis, is presented. Optimal profiles of minimum-mass axisymmetric structures are sought by growing and degenerating simple initial structures subject to response constraints. The rates of the growth and degeneration are controlled based on the current objective and constraint functions of the optimisation problem under consideration. The optimal structures are developed metamorphically in specified infinite design domains using both quadrilateral and triangular axisymmetric finite elements that are ideally suited for modeling continua involving curved boundaries. The robustness of this fully automatic method is studied and validated with the first example of seeking the optimal shape of a centrally suspended axisymmetric object with minimum strain energy caused by self-weight. Then the method is applied to a practical industrial design problem: the design of a turbine disk. The variations of load and boundary conditions caused by shape change in these problems, including the gravitational and centrifugal loads, and temperature distribution are accommodated in the optimisation procedures. Thus, the design model closely resembles the real design problem. The results demonstrate the success of the method in generating optimal but realistic solutions to practical design problems.",10.1007/s00158-004-0445-5,https://dx.doi.org/10.1007/s00158-004-0445-5
"Li, DY | Peng, YH | Yin, JL",2007,Optimization of metal-forming process via a hybrid intelligent optimization technique,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,5,0.45,"In recent years, finite element simulation has been increasingly combined with optimization techniques and applied to optimization of various metal-forming processes. The robustness and efficiency of process optimization are critical factors to obtain ideal results, especially for those complicated metal-forming processes. Gradient-based optimization algorithms are subject to mathematical restrictions of discontinuous searching space, while nongradient optimization algorithms often lead to excessive computation time. This paper presents a novel intelligent optimization approach that integrates machine learning and optimization techniques. An intelligent gradient-based optimization scheme and an intelligent response surface methodology are proposed, respectively. By machine learning based on the rough set algorithm, initial total design space can be reduced to self-continuous hypercubes as effective searching spaces. Then optimization algorithms can be implemented more effectively to find optimal design results. An extrusion forging process and a U channel roll forming process are studied as application samples and the effectiveness of the proposed approach is verified.",10.1007/s00158-006-0075-1,https://dx.doi.org/10.1007/s00158-006-0075-1
"Falco, SA | Afonso, SMB | Vaz, LE",2004,Analysis and optimal design of plates and shells under dynamic loads - I: finite element and sensitivity analysis,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,6,0.43,"In this paper we consider the development, integration, and application of reliable and efficient computational tools for the geometry modeling, mesh generation, structural analysis, and sensitivity analysis of variable-thickness plates and free-form shells under dynamic loads. A flexible shape-definition tool for surface modeling using Coons patches is considered to represent the shape and the thickness distribution of the structure, followed by an automatic mesh generator for structured meshes on the shell surface. Nine-node quadrilateral Mindlin-Reissner shell elements degenerated from D elements and with an assumed strain field, the so-called Huang-Hinton elements, are used for the FE discretization of the structure. The Newmark direct integration algorithm is used for the time discretization of the dynamic equilibrium equations for both the structural analysis and the semi-analytical (SA) sensitivity analysis. Alternatively, the sensitivities are computed by using the global finite difference (FD) method. Several examples are considered. In a companion paper, the tools presented here are combined with mathematical programming algorithms to form a robust and reliable structural optimization process to achieve better dynamic performance on the shell designs.",10.1007/s00158-003-0359-7,https://dx.doi.org/10.1007/s00158-003-0359-7
"Padulo, M | Guenov, MD",2011,Worst-case robust design optimization under distributional assumptions,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,3,0.43,"Presented in this paper is a novel robust design optimization (RDO) methodology. The problem is reformulated in order to relax, when required, the assumption of normality of objectives and constraints, which often underlies RDO. In the second place, taking into account engineering considerations concerning the risk associated with constraint violation, suitable estimates of tail conditional expectations are introduced in the set of robustness metrics. A computationally affordable yet accurate implementation of the proposed formulation is guaranteed by the adoption of a reduced quadrature technique to perform the uncertainty propagation. The methodology is successfully demonstrated with the aid of an industrial test case performing the sizing of a mid-range passenger aircraft.",10.1002/nme.3203,https://dx.doi.org/10.1002/nme.3203
"Renac, F",2011,Improvement of the recursive projection method for linear iterative scheme stabilization based on an approximate eigenvalue problem,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,3,0.43,An algorithm for stabilizing linear iterative schemes is developed in this study. The recursive projection method is applied in order to stabilize divergent numerical algorithms. A criterion for selecting the divergent subspace of the iteration matrix with an approximate eigenvalue problem is introduced. The performance of the present algorithm is investigated in terms of storage requirements and CPU costs and is compared to the original Krylov criterion. Theoretical results on the divergent subspace selection accuracy are established. The method is then applied to the resolution of the linear advection-diffusion equation and to a sensitivity analysis for a turbulent transonic flow in the context of aerodynamic shape optimization. Numerical experiments demonstrate better robustness and faster convergence properties of the stabilization algorithm with the new criterion based on the approximate eigenvalue problem. This criterion requires only slight additional operations and memory which vanish in the limit of large linear systems.,10.1016/j.jcp.2011.03.057,https://dx.doi.org/10.1016/j.jcp.2011.03.057
"Huang, XZ | Zhang, YM | Jin, YJ | Lu, H",2011,An improved decomposition method in probabilistic analysis using Chebyshev approximations,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,3,0.43,"This paper presents an effective method for solving stochastic problems commonly encountered in structural engineering and applied mechanics. Design of experiments is conducted to obtain the response values of interest, where Chebyshev nodes are applied to collect discrete samples. Then Chebyshev polynomials are applied to approximate the univariate functional relationship between each variable and the structural response. The univariate dimension reduction method and saddlepoint approximation with truncated cumulant generating functions are employed to estimate the statistic cumulants, probability density function and cumulative distribution function of the response. The results of numerical examples indicate that the proposed approach provides accurate, convergent, and computationally efficient estimates of the probabilistic characteristics of random mathematical functions or the responses of structural systems.",10.1007/s00158-010-0606-7,https://dx.doi.org/10.1007/s00158-010-0606-7
"Kaminski, MM",2011,Structural sensitivity analysis in nonlinear and transient problems using the local response function technique,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,3,0.43,"The main aim of this work is the mathematical formulation, computational implementation and the application of the local version of the Response Function Method (RFM) to analyze structural design sensitivity in nonlinear structures and problems. This method is based on the Finite Element Method-based determination of the polynomial response function between design parameter and the structural state function like displacements or temperatures. One may use this numerical technique in its global version, where a single polynomial is determined for the entire computational domain or, in the case of nonlinear, transient analyses or the heterogeneous domains, in the local approach-where nodal response function are to be determined. The application of this methodology is illustrated with three examples--transient heat transfer in the homogeneous rod, the elastoplastic analysis of D truss as well as the eigenvibrations for a large scale D structure, where time, increment and eigenvalue dependent variations of the first and the second order sensitivities with respect to the physical and material parameters are computed. The first order gradients computed with the use of the RFM approach are contrasted with the finite difference computations.",10.1007/s00158-010-0565-z,https://dx.doi.org/10.1007/s00158-010-0565-z
"Ha, YD | Kim, MG | Kim, HS | Cho, S",2011,Shape design optimization of SPH fluid-structure interactions considering geometrically exact interfaces,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,3,0.43,"Fluid-structure interaction problems are solved by applying a smoothed particle hydrodynamics method to a weakly compressible Navier-Stokes equation as well as an equilibrium equation for geometrically nonlinear structures in updated Lagrangian formulation. The geometrically exact interface, consisting of B-spline basis functions and the corresponding control points, includes the high order geometric information such as tangent, normal, and curvature. The exactness of interface is kept by updating the control points according to the kinematics obtained from response analysis. Under the scheme of explicit time integration and updated Lagrangian formulation, the required shape design velocity should be updated at every single step. The update scheme of design velocity is developed using the sensitivity of physical velocity. The developed sensitivity analysis method is further utilized in gradient-based shape optimization problems and turns out to be very efficient since the interaction pairs of particles determined in the response analysis can be directly utilized.",10.1007/s00158-011-0645-8,https://dx.doi.org/10.1007/s00158-011-0645-8
"Podsechin, V | Tejakusuma, I | Schernewski, G | Pejrup, M",2006,On parameters estimation in dynamic model of suspended sediments,Applications_JOURNAL OF HYDROLOGY,5,0.42,"Dynamics of suspended sediments in the shallow Szczecin Lagoon, located on the border between Germany and Poland, were simulated with a simple box-type model. A direct search optimisation method was used to estimate the model's parameters. The sensitivity analysis of model parameters indicated that model output is most sensitive to settling velocity. The results point out the importance of prior assessment of parameters values before applying the formal optimisation procedure.",10.1016/j.jhydrol.2005.06.015,https://dx.doi.org/10.1016/j.jhydrol.2005.06.015
"Kim, HW | Bae, DS | Choi, KK",2001,Configuration design sensitivity analysis of dynamics for constrained mechanical systems,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,7,0.41,A continuum-based configuration design sensitivity analysis method is developed for dynamics of multibody systems. The configuration design variables of multibody systems define the shape and orientation changes. The equations of motion are directly differentiated to obtain the governing equations for the design sensitivity. The governing equation of the design sensitivity is formulated as an overdetermined differential algebraic equation and treated as ordinary differential equations on manifolds. The material derivative of a domain functional is performed to obtain the sensitivity due to shape and orientation changes. The configuration design sensitivities of a fly-ball governor system and a spatial four bar mechanism are obtained using the proposed method and are Validated against these obtained from the finite difference method.,10.1016/S0045-7825(00)00372-8,https://dx.doi.org/10.1016/S0045-7825(00)00372-8
"Chen, BS | Gu, YX | Zhang, HW | Zhao, GZ",2003,Structural design optimization on thermally induced vibration,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,6,0.4,"The numerical method of design optimization for structural thermally induced vibration is originally studied in this paper and implemented in, the software JIFEX The direct and adjoint methods of sensitivity analysis for thermal-induced vibration coupled with both linear and non-linear transient heat conduction is firstly proposed. Based on the finite element method, the linear structural dynamics is treated simultaneously with linear and non-linear transient heat conduction. In the heat conduction, the non-linear factors include the radiation and temperature-dependent materials. The sensitivity analysis of transient linear and non-linear heat conduction is performed with the precise time integration method; and then, the sensitivity analysis of structural transient responses is performed by the Newmark method. Both the direct method and the adjoint method are employed to derive the sensitivity equations of thermal vibration. In the adjoint method, two adjoint vectors of structure and of heat conduction are used to derive the adjoint equations. The coupling effect of heat conduction on thermal vibration in the sensitivity analysis is particularly investigated. With the coupling sensitivity analysis, the optimization model is constructed and solved by the sequential linear programming or sequential quadratic programming algorithm. Numerical examples are given to validate the proposed methods and to demonstrate the importance of the coupled design optimization.",10.1002/nme.814,https://dx.doi.org/10.1002/nme.814
"Gerzen, N | Barthold, FJ | Klinkel, S | Wagner, W | Materna, D",2013,Variational sensitivity analysis of a nonlinear solid shell element,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,2,0.4,"The paper is concerned with variational sensitivity analysis of a nonlinear solid shell element, which is based on the Hu-Washizu variational principle. The sensitivity information is derived on the continuous level and discretized to yield the analytical expressions on the computational level. Especially, the pseudo load matrix and the sensitivity matrix, which dominate design sensitivity analysis of shape optimization problems, are derived. Because of the mixed formulation, condensation of the pseudo load matrix on the element level is performed to compute the sensitivity matrix. An illustrative example from the field of geometry-based shape optimization demonstrates the possible application of the presented formulation.",10.1002/nme.4545,https://dx.doi.org/10.1002/nme.4545
"Bettebghor, D | Blondeau, C | Toal, D | Eres, H",2013,Bi-objective optimization of pylon-engine-nacelle assembly: weight vs. tip clearance criterion,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,2,0.4,"A realistic application of advanced structural and multi-objective optimization for the design of a fully assembled aircraft powerplant installation is presented. As opposed to the classical design process of powerplant installation that does not consider the influence of pylon sizing over engine efficiency, we develop in the present a fully integrated approach where both pylon and compressor intercase are designed at once. The main objective is to consider the impact of weight over tip clearance performance criterion and see how these two objectives are antagonistic. In this work, we perform in the same design session tasks traditionally devoted to the airframe manufacturer and aero-engine manufacturer. The overall weight of the assembly is minimized with respect to Specific Fuel Consumption (SFC) criterion. One interesting aspect of the process is that SFC criterion is based on highly proprietary models and its simulation and call within an optimization process is made available through the development of a webservice. One major phenomenon to consider in both pylon and engine design is Fan Blade Off (FBO) event, i.e. the sudden release of a blade. This event causes high impact loads and must be considered carefully in the design. Such a simulation is not an easy task and several nonlinear phenomena must be addressed (e.g. rotordynamics), not to mention the integration of this nonlinear dynamic response in a static structural optimization process. This article describes how the design of the full assembly is performed taking into account both objectives. Such a problem lies in multi-objective optimization field and then we describe the method we use to solve such a problem. The simulation of an FBO post-impact rotor dynamics is also described and we end up with the final results that show the influence of pylon-engine weight sizing over SFC.",10.1007/s00158-013-0908-7,https://dx.doi.org/10.1007/s00158-013-0908-7
"Sahai, T | Pasini, JM",2013,Uncertainty quantification in hybrid dynamical systems,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,2,0.4,"Uncertainty quantification (UQ) techniques are frequently used to ascertain output variability in systems with parametric uncertainty. Traditional algorithms for UQ are either system-agnostic and slow (such as Monte Carlo) or fast with stringent assumptions on smoothness (such as polynomial chaos and Quasi-Monte Carlo). In this work, we develop a fast UQ approach for hybrid dynamical systems by extending the polynomial chaos methodology to these systems. To capture discontinuities, we use a wavelet-based Wiener-Haar expansion. We develop a boundary layer approach to propagate uncertainty through separable reset conditions. We also introduce a transport theory based approach for propagating uncertainty through hybrid dynamical systems. Here the expansion yields a set of hyperbolic equations that are solved by integrating along characteristics. The solution of the partial differential equation along the characteristics allows one to quantify uncertainty in hybrid or switching dynamical systems. The above methods are demonstrated on example problems. (c)  United Technologies Corporation.",10.1016/j.jcp.2012.10.030,https://dx.doi.org/10.1016/j.jcp.2012.10.030
"Munoz-Rojas, PA | Fonseca, JSO | Creus, GJ",2004,A modified finite difference sensitivity analysis method allowing remeshing in large strain path-dependent problems,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,5,0.36,"Finite difference sensitivity analysis is simple and general yet usually inefficient and inaccurate compared to the analytical sensitivity approach. Although its high computational cost is not an issue in iteratively solved problems, its inaccuracies are critical in path-dependent problems when remeshing is required. In this case, the errors caused by parametric inversion and interpolation in variables transfer to the new mesh can be as large as the gradient components. This paper presents an efficient modified finite difference approach that allows remeshing either in path-independent or path-dependent problems, not being affected by the aforementioned errors. The strategy to cope with remeshing is extensive to the semi-analytical method which, for non-linear analyses, is shown to be a particular case of the proposed finite difference sensitivity approach. With this implementation, the finite difference, the semi-analytical and the analytical sensitivity methods all have comparable computational costs. The perturbation of unstructured meshes is performed with an inverse power Laplacian smoothing. The low cost and the accuracy of the sensitivity fields obtained after remeshing are shown in two examples, considering shape and constitutive design variables.",10.1002/nme.1102,https://dx.doi.org/10.1002/nme.1102
"Clark, LD | Davey, K | Hinduja, S",2001,Novel cooling channel shapes in pressure die casting,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,6,0.35,The pressure die casting involves die designs incorporating cooling channels positioned to facilitate the controlled extraction of energy from a solidifying casting. It is now known that subcooled nucleate boiling can occur in cooling channels and this paper is concerned with novel cooling channel shapes that are optimized to promote and enhance this boiling and thus reduce casting times. Shape sensitivity analysis is applied to a boundary element model using the material derivative adjoint variable technique. Mesh node positions on the cooling channels are used as the design parameters. The sensitivities are used in a conjugate gradient non-linear optimization routine. It is shown that with this approach cooling channels can be designed to maximize boiling heat transfer whilst at the same time allow some degree of control of spatial temperature variation over the die cavity surface. Simulation and experimental results are presented for a traditional die and an optimized die. A  per cent reduction in cycle time is achieved with the optimized die.,10.1002/nme.126,https://dx.doi.org/10.1002/nme.126
"Takezawa, A | Kitamura, M",2012,Topology optimization of compliant circular path mechanisms based on an aggregated linear system and singular value decomposition,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,2,0.33,"This paper proposes a topology optimization method for the design of compliant circular path mechanisms, or compliant mechanisms having a set of output displacement vectors with a constant norm, which is induced by a given set of input forces. To perform the optimization, a simple linear system composed of an input force vector, an output displacement vector and a matrix connecting them is constructed in the context of a discretized linear elasticity problem using FEM. By adding two constraints: , the dimensions of the input and the output vectors are equal; , the Euclidean norms of all local input force vectors are constant; from the singular value decomposition of the matrix connecting the input force vector and the output displacement vector, the optimization problem, which specifies and equalizes the norms of all output vectors, is formulated. It is a minimization problem of the weighted summation of the condition number of the matrix and the least square error of the second singular value and the specified value. This methodology is implemented as a topology optimization problem using the solid isotropic material with penalization method, sensitivity analysis and method of moving asymptotes. The numerical examples illustrate mechanically reasonable compliant circular path mechanisms and other mechanisms having multiple outputs with a constant norm.",10.1002/nme.3259,https://dx.doi.org/10.1002/nme.3259
"Hilding, D | Klarbring, A",2012,Optimization of structures in frictional contact,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,2,0.33,"This paper describes a new approach to optimization of linear elastic structures in frictional contact. It uses a novel method to determine an, in a specified sense, likely equilibrium state of the structure, using only the static equilibrium conditions. That is, no complex dynamic/quasi-static analyses have to be performed. The approach has the advantage that it is not necessary to know the complete load history, which is most often unknown for practical problems. To illustrate the theory, numerical results are given for the optimal design problem of sizing a truss to attain a more uniform normal contact force distribution.",10.1016/j.cma.2011.02.014,https://dx.doi.org/10.1016/j.cma.2011.02.014
"Zhang, BH | Konomi, BA | Sang, HY | Karagiannis, G | Lin, G",2015,Full scale multi-output Gaussian process emulator with nonseparable auto-covariance functions,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,1,0.33,"Gaussian process emulator with separable covariance function has been utilized extensively in modeling large computer model outputs. The assumption of separability imposes constraints on the emulator and may negatively affect its performance in some applications where separability may not hold. We propose a multi-output Gaussian process emulator with a nonseparable auto-covariance function to avoid limitations of using separable emulators. In addition, to facilitate the computation of nonseparable emulator, we introduce a new computational method, referred to as the Full-Scale approximation method with block modulating function (FSA-Block) approach. The FSA-Block is an effective and accurate covariance approximation method to reduce computations for Gaussian process models, which applies to both nonseparable and partially separable covariance models. We illustrate the effectiveness of our method through simulation studies and compare it with emulators with separable covariances. We also apply our method to a real computer code of the carbon capture system.",10.1016/j.jcp.2015.08.006,https://dx.doi.org/10.1016/j.jcp.2015.08.006
"Stephenson, KM | Novakowski, KS",2006,The analysis of pulse interference tests conducted in a fractured rock aquifer bounded by a constant free surface,Applications_JOURNAL OF HYDROLOGY,4,0.33,"An analytical model is presented for the analysis of pulse interference tests conducted in a double porosity medium. The special case of a horizontal fracture zone in a fractured rock environment with vertical connection to a high permeability zone at the water table is considered. The high permeability zone is modeled as a hydraulic boundary of constant head and the vertical fractures are modeled using a formulation based on equivalent porous media. Wellbore storage at the source and observation wells is accounted for using an approximate superposition technique. The solution is found using the Laplace transform method and numerical inversion into real space. The derivation is presented in dimensioned terms and a method for estimating the hydraulic conductivity of the vertical fractures is developed. Several alternate solutions describing differing system geometry and boundary conditions are presented for comparative purposes. A sensitivity analysis shows that the new model predicts unique values for horizontal transmissivity and vertical hydraulic conductivity over a range of realistic storage term values for a given distance to the constant head boundary. Storage values were not found to be uniquely determined with this method. Several field examples are presented in order to validate the applicability of the analysis to real data. The effects of the connection to the water table through double porosity were found to be significant, resulting in an observation well response that cannot be simulated using a single porosity model. Thus, the method is very useful for uniquely estimating the vertical hydraulic properties of fractured rock aquifers, parameters that are often difficult to measure in this setting (C)  Elsevier Ltd All rights reserved.",10.1016/j.jhydrol.2005.07.005,https://dx.doi.org/10.1016/j.jhydrol.2005.07.005
"Lee, HA | Park, GJ",2015,A software development framework for structural optimization considering non linear static responses,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,1,0.33,"In the real world, structural systems may not have linear static characteristics. However, structural optimization has been developed based on static responses because sensitivity analysis regarding static finite element analysis is developed quite well. Analyses other than static analysis are heavily required in the engineering community these days. Techniques for such analyses have been extensively developed and many software systems using the finite element method are easily available in the market. On the other hand, development of structural optimization using such analyses is fairly slow due to many obstacles. One obstacle is that it is very difficult and expensive to consider the nonlinearities or dynamic effects in the way of conventional optimization. Recently, the equivalent static loads method for non linear static response structural optimization (ESLSO) has been proposed for structural optimization with various responses: linear dynamic response, nonlinear static response, and nonlinear dynamic response. In ESLSO, finite element analysis other than static analysis is performed, equivalent static loads (ESLs) are generated, linear static response structural optimization is carried out with the ESLs and the process iterates. A software system for the automatic use of ESLSO is developed and described. One of the advantages of ESLSO is that it can use well developed commercial software systems for structural analysis and linear static response structural optimization. Various analysis and optimization systems are integrated in the developed system. The structure of the system is systematically defined and the software is developed by the C++ language on the Windows operating system.",10.1007/s00158-015-1228-x,https://dx.doi.org/10.1007/s00158-015-1228-x
"Sakata, Y | Imai, T | Ikeda, R | Nishigaki, M",2015,Analysis of partially penetrating slug tests in a stratified formation by alternating piezometer and tube methods,Applications_JOURNAL OF HYDROLOGY,1,0.33,"In partially penetrating slug tests, hydraulic conductivity (K) estimates might not necessarily be valid because of vertical flows in heterogeneous formations. We assess the error in hypothetical stratified formations by numerical sensitivity analysis, and propose an effective method for compensation by incorporating two types of casing configuration (piezometer and tube). The hypothetical stratified formation consists of completely horizontal layers, each I m thick; the permeability is different between, but not within, layers. In this study, conductivity estimates in the piezometer and tube methods are calculated by assigning various patterns of conductivity to the test, upper, and lower layers: K-T, K-U and K-L. The effect of vertical flow becomes significant when K-T is small relative to K-U or K-L, and K-L is more important than K-U because the base of the borehole is open to the lower formation. The conductivity ratios (estimate over actual value) are treated as approximately linearly dependent on logarithms of K-T/K-U and K-T/K-L, so that conductivity estimates can be straightforwardly derived from one piezometer measurement and two tube measurements at the top and bottom of the screen. The linear relations are evaluated and constant parameters are determined under specific conditions. This study also recommends alternating piezometer and tube methods in the drilling procedure because the actual variation of K with depth is larger than that found using isolated measurements, as shown in a field study of alluvial fan gravel deposits in Sapporo, Japan.",10.1016/j.jhydrol.2015.06.019,https://dx.doi.org/10.1016/j.jhydrol.2015.06.019
"Cardoso, JB | Sousa, LG | Castro, JA | Valido, AJ",2002,Optimal design of laminated composite beam structures,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,5,0.31,"This work deals with design sensitivity analysis and optimal design of composite structures modelled as thin-walled beams. The structures are treated as a torsion-bending resistant beams. The analysis problem is discretized by a finite element technique. A two-node Hermitean beam element is used. The beam sections are made from an assembly of elements that correspond to flat layered laminated composite panels. Optimal design is performed with respect to the lamina orientations and thickness of the laminates. The structural weight is considered as the objective function. Constraints are imposed on stresses, displacements, critical load and natural frequencies. Two failure criteria are used to limit the structural strength: Tsai-Hill and maximum stress. The Tsai-Hill criterion is also adopted to predict the first-ply-failure loads. The design sensitivity analysis is analytically formulated and implemented. An adjoint variable method is used to derive the response sensitivities with respect to the design. A mathematical programming approach is used for the optimization process. Numerical examples are performed on three-dimensional structures.",10.1007/s00158-002-0230-2,https://dx.doi.org/10.1007/s00158-002-0230-2
"Zenner, MA",2002,Analysis of slug tests in bypassed wells,Applications_JOURNAL OF HYDROLOGY,5,0.31,"In Berlin many weils are constructed with an additional small diameter bypass tube flanged on to the large diameter casing and running parallel to the later upward to the surface. The response to a slug test conducted in such a well is usually characterised by significant water level oscillations developing inside the branched pipe system. In the present paper, a general non-linear model is developed for the analysis of slug tests performed in bypassed wells, The model includes skin effects, non-linear head losses due to internal well bore fluid friction minor losses originating at radius change, along the flow path inside the well and inertial effects of the water columns contained within the primary casing and the bypass Pipe flow inside the branched tubing system is described by the one-dimensional conservation law of the mechanical energy of the well bore fluid. Aquifer flow is quartified by a convolution-type approach. The coupled well-aquifer system is governed by all integro-differential equation representing the mean mechanical energy of the total kell bore fluid and, coupled to it, by a non-linear ordinary differential equation which accounts for mechanical energy fluctuations between the water columns inside the main casing and the bypass. Both equations are solved by finite difference approximations coupled to point iterative numerical techniques. An application of the model to field data suggests that non-linear hydraulic head losses originating inside the well bore may cause the measured head responses to depend on the initial displacement. A packer used to initiate a slug test can induce these non-linear distortions through the combined effect of minor losses originating at the inlet and the outlet of a small diameter packer flow-through tube and internal fluid friction potentially developing inside that packer passage. As classical linear theories do not allow for the analysis of such phenomena a model accounting, for these non-linear processes should be used if field data show significant dependencies on the initial slug height.",10.1016/S0022-1694(02)00048-3,https://dx.doi.org/10.1016/S0022-1694(02)00048-3
"Coyette, JP | Meerbergen, K | Robbe, ML",2005,Time integration for spherical acoustic finite-infinite element models,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,4,0.31,"In this paper, we analyse the numerical time integration of models of exterior acoustics. The major challenge lies in the instabilities that may arise from the infinite elements. In this paper we consider the special case of spherical infinite elements formulations, which have shown their relevance for industrial applications. We propose a method that combines Crank-Nicholson's method with a filtering step by the backward Euler method. The paper is illustrated with an example relevant to industry.",10.1002/nme.1419,https://dx.doi.org/10.1002/nme.1419
"Yu, Y | Kwak, BM",2011,Design sensitivity analysis of acoustical damping and its application to design of musical bells,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,2,0.29,"Damping characteristics of a musical bell plays an important role in characterizing the musical sound. The total damping consists of acoustical damping and internal damping. Acoustical damping depends upon resonating frequencies and vibration patterns while internal damping is a material property. The acoustical damping of a vibrating structure is formulated via boundary element method and finite element method using eigenmode decomposition. The design sensitivity of acoustical damping is derived using an adjoint variable method of the eigenvalue problem. Design optimization of a musical bell is then performed in terms of acoustical parameters. The goal of the optimization problem is to design a harmonically tuned bell with given acoustical damping values. The proposed automated design process integrates finite element analysis, boundary element analysis, design sensitivity analysis, mode-tracking algorithm and optimization module, seamlessly. It is demonstrated by numerical examples to show practical applications.",10.1007/s00158-011-0651-x,https://dx.doi.org/10.1007/s00158-011-0651-x
"Okubo, S | Tortorelli, DA",2004,"Control of nonlinear, continuous, dynamic systems via finite elements, sensitivity analysis, and optimization",Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,4,0.29,"A general methodology to design open loop controllers for nonlinear, dynamic, continuous systems is presented and applied to control a single flexible link (SFL). In this application, the partial differential equations that describe the beam system are first analyzed via the finite element method (FEM) and Newmark integration method. Two open loop control inputs to achieve specified system performance criteria are then computed by posing and solving inverse dynamics problems. These analyses use nonlinear programming (NLP) algorithms and analytical gradients that are computed by the direct sensitivity method. The open loop control is verified experimentally. Closed loop controller synthesis for linear time invariant (LTI) and linear time varying systems (LTV) is relatively well understood. To apply this knowledge base to the control of the SFL, the nonlinear finite element plant model is linearized and recast in standard state space form.",10.1007/s00158-003-0338-z,https://dx.doi.org/10.1007/s00158-003-0338-z
"Falco, SA | Afonso, SMB | Vaz, LE",2004,Analysis and optimal design of plates and shells under dynamic loads - II: optimization,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,4,0.29,"In this paper a structural sizing and shape optimization procedure is used to obtain optimum designs for plates and shells under dynamic loads. The optimum designs are obtained by using an integrated system for structural optimization of shells that is formed by a set of independent 'subsystems'. They are surface and mesh generation, finite element analysis, sensitivity analysis, and optimization algorithms. The description of the geometry and mesh generation tools, as well as the implementation and test of the structural dynamic and sensitivity analysis, were given in Part I of this paper. In this part the tools presented in Part I are used along with an optimization algorithm to perform structural shape and sizing optimization (SSO) of shell structures. The sequential quadratic programming algorithm is used to solve the optimization problem. In the present paper special attention is devoted to the problem of elimination of clusters of multiple eigenvalues in the optimal design of symmetrical shells. Several examples are considered that illustrate the potential of the developed tool.",10.1007/s00158-003-0360-1,https://dx.doi.org/10.1007/s00158-003-0360-1
"Abi-Zeid, I | Parent, E | Bobee, B",2004,The stochastic modeling of low flows by the alternating point processes approach: methodology and application,Applications_JOURNAL OF HYDROLOGY,4,0.29,"The increasing demand on water resources requires better management of the water deficit situation, may it be unusual droughts or yearly recurring low flows. We propose to use alternating non-homogeneous point processes to model the occurrences and durations of water deficits and surplus. The advantage of this approach is that it is a relatively simple way to model complex, dependent and non-stationary phenomena. It can be used as an alternative to developing more intricate physically based models. Furthermore, alternating point processes can be used to generate synthetic data that may reveal useful for analysis purposes. Most importantly, they allow to subsequently quantify risks related to deficit durations. Our methodology, developed for two types of events under non-homogeneous conditions, consists of six steps: identification of event type, choice of point process, choice of intensity function shape, estimation of intensity function parameters, testing model fitness, and sensitivity analysis. Also included are two new models based on the non-homogeneous Poisson conditional process that take into account external variables such as precipitation and temperatures. We begin with a classification of some point processes and their relation to each other. We describe the procedure for using the pertinent statistical tests. We present our modeling results for  Canadian rivers where we observed that in most cases, the simpler point processes were quite adequate.",10.1016/j.jhydrol.2003.08.009,https://dx.doi.org/10.1016/j.jhydrol.2003.08.009
"Weiss, AY | Huisinga, W",2011,Error-controlled global sensitivity analysis of ordinary differential equations,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,2,0.29,"We propose a novel strategy for global sensitivity analysis of ordinary differential equations. It is based on an error-controlled solution of the partial differential equation (PDE) that describes the evolution of the probability density function associated with the input uncertainty/variability. The density yields a more accurate estimate of the output uncertainty/variability, where not only some observables (such as mean and variance) but also structural properties (e.g., skewness, heavy tails, bi-modality) can be resolved up to a selected accuracy. For the adaptive solution of the PDE Cauchy problem we use the Rothe method with multiplicative error correction, which was originally developed for the solution of parabolic PDEs. We show that, unlike in parabolic problems, conservation properties necessitate a coupling of temporal and spatial accuracy to avoid accumulation of spatial approximation errors over time. We provide convergence conditions for the numerical scheme and suggest an implementation using approximate approximations for spatial discretization to efficiently resolve the coupling of temporal and spatial accuracy. The performance of the method is studied by means of low-dimensional case studies. The favorable properties of the spatial discretization technique suggest that this may be the starting point for an error-controlled sensitivity analysis in higher dimensions.",10.1016/j.jcp.2011.05.011,https://dx.doi.org/10.1016/j.jcp.2011.05.011
"Baldomir, A | Hernandez, S | Diaz, J | Fontan, A",2011,Sensitivity analysis of optimum solutions by updating active constraints: application in aircraft structural design,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,2,0.29,"Often the parameters considered as constants in an optimization problem have some uncertainty and it is interesting to know how the optimum solution is modified when these values are changed. The only way to continue having the optimal solution is to perform a new optimization loop, but this may require a high computational effort if the optimization problem is large. However, there are several procedures to obtain the new optimal design, based on getting the sensitivities of design variables and objective function with respect to a fixed parameter. Most of these methods require obtaining second derivatives which has a significant computational cost. This paper uses the feasible direction-based technique updating the active constraints to obtain the approximate optimum design. This procedure only requires the first derivatives and it is noted that the updating set of active constraints improves the result, making possible a greater fixed parameter variation. This methodology is applied to an example of very common structural optimization problems in technical literature and to a real aircraft structure.",10.1007/s00158-011-0665-4,https://dx.doi.org/10.1007/s00158-011-0665-4
"Silva, CAC | Bittencourt, ML",2000,An object-oriented structural optimization program,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,5,0.28,"In this paper, implementation concepts of a structural optimization software using object-oriented programming (OOP) in CS ++ is presented. A brief mathematical formulation of structural optimization and continuum-based sensitivity analysis is presented. The requirements of a computational optimization environment are derived from this formulation. The OOP characteristics are analysed and this paradigm is employed in the implementation of design variables, structural performance functionals, velocity fields, design model and mathematical programming algorithms using CI-Jr. Finally, the program obtained is applied to D linear elastic examples of sizing and shape optimization.",10.1007/s001580050146,https://dx.doi.org/10.1007/s001580050146
"Lee, SL | Woodward, CS | Graziani, F",2003,Analyzing radiation diffusion using time-dependent sensitivity-based techniques,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,4,0.27,"In this paper, we discuss the computation and use of solution sensitivities for analyzing radiation diffusion problems and the dependence of solutions on input parameters. The derivation of the sensitivity equations is given, along with a description of the method for solving them in tandem with the simulation. The parameter values express material opacity as a power-law of material temperature and density. The computed sensitivities reveal important qualitative details about the temperature coupling and diffusion processes. It is also shown that these sensitivities are valuable for ranking the parameters from most to least influential, designing improved experiments, and quantifying uncertainty in the simulation results. Lastly, the numerical examples show that these various types of sensitivity analysis are only moderately expensive to perform relative to solving the simulation by itself.",10.1016/j.jcp.2003.07.031,https://dx.doi.org/10.1016/j.jcp.2003.07.031
"Paul, S | Michaleris, P | Shanghvi, JY",2003,Optimization of thermo-elasto-plastic processes using Eulerian sensitivity analysis,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,4,0.27,"A computational scheme for the analysis and optimization of quasi-static then-no-mechanical processes is presented in this paper. In order to obtain desirable mechanical transformations in a workpiece using a thermal treatment process, the optimal control parameters need to be determined. The problem is addressed by posing the process as a decoupled thermo-mechanical finite element problem and performing an optimization using gradient methods. The forward problem is solved using the Eulerian formulation since it is computationally more efficient compared to an equivalent Lagrangian formulation. The design sensitivities required for the optimization are developed analytically using direct differentiation. This systematic design approach is applied to optimize a laser forming process. The objective is to maximize the angular distortion of a specimen subject to the constraint that the phase transition temperature is not exceeded at any point in the model.",10.1002/nme.605,https://dx.doi.org/10.1002/nme.605
"Kim, JY | Aluru, NR | Tortorelli, DA",2003,Improved multi-level Newton solvers for fully coupled multi-physics problems,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,4,0.27,An algorithm is suggested to improve the efficiency of the rnulti-level Newton method that is used to solve multi-physics problems. It accounts for full coupling between the subsystems by using the direct differentiation method rather than error prone finite difference calculations and retains the advantage of greater flexibility over the tightly coupled approaches. Performance of the algorithm is demonstrated by solving a fluid-structure interaction problem.,10.1002/nme.751,https://dx.doi.org/10.1002/nme.751
"Callejo, A | de Jalon, JG",2014,A hybrid direct-automatic differentiation method for the computation of independent sensitivities in multibody systems,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,1,0.25,"The usefulness of sensitivity analyses in mechanical engineering is very well-known. Interesting examples of sensitivity analysis applications include the computation of gradients in gradient-based optimization methods and the determination of the parameter relevance on a specific response or objective. In the field of multibody dynamics, analytical sensitivity methods tend to be very complex, and thus, numerical differentiation is often used instead, which degrades numerical accuracy. In this work, a simple and original method based on state-space motion differential equations is presented. The number of second-order motion differential equations equals the number of DOFs, that is, there is one differential equation per independent acceleration. The dynamic equations are then differentiated with respect to the parameters by using automatic differentiation and without manual intervention from the user. By adding the sensitivity equations to the dynamic equations, the forward dynamics and the independent sensitivities can be robustly computed using standard integrators. Efficiency and accuracy are assessed by analyzing three numerical examples (a double pendulum, a four-bar linkage, and an -DOF coach) and by comparing the results with those of the numerical differentiation approach. The results show that the integration of independent sensitivities using automatic differentiation is stable and accurate to machine precision.",10.1002/nme.4804,https://dx.doi.org/10.1002/nme.4804
"Aghabeigi, M | Movahhedy, MR",2014,An algorithm for geometrical uncertainty analysis in planar truss structures,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,1,0.25,"An algorithm is proposed for description of error accumulation during assembly process of planar truss structures. Specifically, given the tolerances of pre-fabricated members, the uncertainty in the position of nodes is calculated in terms of variances and covariances. Twoconstituent elements of planar trusses (i.e. two-bar unit cell and redundant member) are formulated based on linear models of kinematics and deformation mechanics. Subsequently, these models are combined together in a single algorithm to construct a suitable procedure for estimation of uncertainty zone of each node. Accuracy of the developed method is verified by comparing its results with outcomes of Monte Carlo simulations. Also, appropriate numerical examples are presented to investigate important characteristics of error propagation in planar truss structures and obtain configurations with minimum shape deviation.",10.1007/s00158-013-0968-8,https://dx.doi.org/10.1007/s00158-013-0968-8
"Giusti, SM | Mello, LAM | Silva, ECN",2014,Piezoresistive device optimization using topological derivative concepts,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,1,0.25,"A piezoresistive sensor is composed of a piezoresistive membrane attached to a flexible plate. The piezoresistive material is anisotropic, and its electrical properties change when subjected to mechanical stresses. In this work, the topology design of a piezoresistive pressure sensor is addressed. More specifically, an optimization technique based on topological sensitivity analysis is proposed in order to obtain the optimized distribution of piezoresistive material over the plate. In most of the works regarding topological sensitivity analysis, isotropic materials are considered. However, the problem of conductivity in an anisotropic non-homogeneous domain has been recently addressed, and a closed form for the topological derivative associated to the energy shape functional has been presented. In this work, on the other hand, a closed form for the topological derivative associated with a multi-objective shape functional related to the steady-state anisotropic current density diffusion problem is presented. To illustrate the applicability of the closed formula and the proposed optimization procedure, numerical examples regarding the conceptual design of piezoresistive sensors, considering distinct optimization parameters and boundary conditions in the conductivity problem, are presented.",10.1007/s00158-014-1064-4,https://dx.doi.org/10.1007/s00158-014-1064-4
"Mahnken, R",2000,An inverse finite-element algorithm for parameter identification of thermoelastic damage models,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,4,0.22,"In this contribution an algorithm for parameter identification of thermoelastic damage models is proposed, in which non-uniform distributions of the state variables such as stresses, strains, damage variables and temperature are taken into account. To this end a least-squares functional consisting of experimental data and simulated data is minimized, whereby the latter are obtained with the finite-element method. In order to improve the efficiency of the minimization process, a gradient-based optimization algorithm is applied, and therefore the corresponding sensitivity analysis for the coupled variational problem is described in a systematic manner. For illustrative purpose, the performance of the algorithm is demonstrated for a non-homogeneous shear problem with thermal loading.",10.1002/(SICI)1097-0207(20000710)48:7<1015::AID-NME912>3.0.CO;2-4,https://dx.doi.org/10.1002/(SICI)1097-0207(20000710)48:7<1015::AID-NME912>3.0.CO;2-4
"Strand, JS | Goldstein, DB",2013,Global sensitivity analysis for DSMC simulations of hypersonic shocks,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,1,0.2,"Two global, Monte Carlo based sensitivity analyses were performed to determine which reaction rates most affect the results of Direct Simulation Monte Carlo (DSMC) simulations for a hypersonic shock in five-species air. The DSMC code was written and optimized with shock tube simulations in mind, and includes modifications to allow for the efficient simulation of a D hypersonic shock. The TCE model is used to convert Arrhenius-form reaction rate constants into reaction cross-sections, after modification to allow accurate modeling of reactions with arbitrarily large rates relative to the VHS collision rate. The square of the Pearson correlation coefficient was used as the measure for sensitivity in the first of the analyses, and the mutual information was used as the measure in the second. The quantity of interest (QoI) for these analyses was the NO density profile across a D shock at similar to  m/s (M-infinity approximate to ). This vector QoI was broken into a set of scalar QoIs, each representing the density of NO at a specific point downstream of the shock, and sensitivities were calculated for each scalar QoI based on both measures of sensitivity. Profiles of sensitivity vs. location downstream of the shock were then integrated to determine an overall sensitivity for each reaction. A weighting function was used in the integration in order to emphasize sensitivities in the region of greatest thermal and chemical non-equilibrium. Both sensitivity analysis methods agree on the six reactions which most strongly affect the density of NO. These six reactions are the N- dissociation reaction N- + N reversible arrow N, the O- dissociation reaction O- + O reversible arrow O, the NO dissociation reactions NO + N reversible arrow N + O and NO + O reversible arrow N + O, and the exchange reactions N- + O reversible arrow NO + N and NO + O reversible arrow O- + N. This analysis lays the groundwork for the application of Bayesian statistical methods for the calibration of parameters relevant to modeling a hypersonic shock layer with the DSMC method.",10.1016/j.jcp.2013.03.035,https://dx.doi.org/10.1016/j.jcp.2013.03.035
"Pereira, CEL | Bittencourt, ML",2008,Topological sensitivity analysis in large deformation problems,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,2,0.2,"The aim of the present work is to apply the topological sensitivity analysis (TSA) to large-deformation elasticity based on the total Lagrangian formulation. The TSA results in a scalar function, denominated topological derivative, that gives for each point of the domain the sensitivity of a given cost function when a small hole is created. An approximated expression for the topological derivative is obtained by numerical asymptotic analysis. Numerical results of the presented approach are considered for elastic plane problems.",10.1007/s00158-007-0223-2,https://dx.doi.org/10.1007/s00158-007-0223-2
"de Boer, H | van Keulen, F | Vervenne, K",2002,Refined second order semi-analytical design sensitivities,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,3,0.19,Accurate and efficient calculation of second order design sensitivities in a finite element context is often difficult. The semi-analytical (SA) method is efficient and easy to implement but has accuracy problems even for first order shape design sensitivities. To overcome accuracy problems a refined semi-analytical (RSA) method has been developed for first order sensitivities. The present paper investigates the application of the RSA method to second order design sensitivities. It is found that second order RSA sensitivities are significantly more accurate than their SA counterparts.,10.1002/nme.533,https://dx.doi.org/10.1002/nme.533
"Stupkiewicz, S",2001,Approximate response sensitivities for nonlinear problems in explicit dynamic formulation,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,3,0.18,Explicit and implicit time integration schemes are discussed in the context of sensitivity analysis of dynamic problems. The application of the fully explicit central difference method (CDM) proves to be efficient for many nonlinear problems. In the case of the corresponding dynamic sensitivity problem the CDM is less advantageous both from efficiency and accuracy points of view. Approximate sensitivity expressions are derived in the paper for nonlinear path-dependent problems allowing the application of an unconditionally stable implicit time integration scheme with the time step much larger than the time step of the explicit CDM scheme of the direct problem. The method seems to be particularly suitable for problems of quasi-static nature in which the dynamic terms are artificially introduced to allow explicit CDM solution of highly nonlinear equations.,10.1007/s001580100105,https://dx.doi.org/10.1007/s001580100105
"Michler, AK | Heinrich, R",2012,Surrogate-enhanced simulation of aircraft in trimmed state,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,1,0.17,"This paper presents a surrogate-enhanced methodology for high-fidelity simulation of aircraft in trimmed state. Surrogates of the aerodynamic loads are used to increase robustness and reduce the number of costly high-fidelity simulations necessary during trim simulations. This is accomplished in two steps: () by finding a suitable estimate for the solution by global and fast local searches performed on surrogates, and () by applying a hybrid local search to check the potential solution and to further increase its quality if necessary. For the local searches in both steps, a Newton method is employed, whereas a genetic algorithm is used only in the first step. For the Newton method, the entries of the Jacobian are provided by means of finite difference approximations and the complex-step derivative method (CSD) based on the surrogates, and it is shown how to make CSD applicable to surrogates based on radial basis functions (RBFs). In the hybrid search, the derivatives are calculated by means of surrogates, while high-fidelity simulations are used for the main calculations of the Newton method. Updating the surrogates with the results of the latter is found to significantly accelerate the convergence of the hybrid local search. The methodology is applied to challenging trim test cases, in which several of the control surfaces of a generic fighter aircraft are deflected during trim simulation. Control surface deflection is performed via RBF-based mesh deformation, and RBFs are also used for surrogate modeling, thereby fostering code modularity and re-use.",10.1016/j.cma.2012.01.010,https://dx.doi.org/10.1016/j.cma.2012.01.010
"Rahman, S | Rao, BN",2006,A mode decoupling continuum shape sensitivity method for fracture analysis of functionally graded materials,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,2,0.17,"In this paper new mode decoupling continuum shape sensitivity method for calculating mixed-mode stress-intensity factors for a stationary crack in two-dimensional, linear-elastic, functionally graded materials with arbitrary geometry is presented. The proposed method involves the mode decoupling of deformations, the material derivative concept taken from continuum mechanics, and direct differentiation. The discrete form of the energy release rate is simple and easy to calculate, as it only requires multiplication of displacement vectors and stiffness sensitivity matrices. By judiciously selecting the velocity field, the method only requires displacement response in a sub domain close to the crack tip, thus making the method computationally efficient. Excellent agreement is obtained between stress-intensity factors predicted by the proposed method and available reference solutions in the literature. Therefore, mode decoupling shape sensitivity analysis provides an attractive alternative to fracture analysis of cracks in homogeneous and non-homogeneous materials.",10.1016/j.cma.2005.09.015,https://dx.doi.org/10.1016/j.cma.2005.09.015
"Yi, KY | Choi, KK | Kim, NH | Botkin, ME",2006,Continuum-based design sensitivity analysis and optimization of nonlinear shell structures using meshfree method,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,2,0.17,"A continuum-based shape and configuration design sensitivity analysis (DSA) method for a finite deformation elastoplastic shell structure has been developed. Shell elastoplasticity is treated using the projection method that performs the return mapping on the subspace defined by the zero-normal stress condition. An incrementally objective integration scheme is used in the context of finite deformation shell analysis, wherein the stress objectivity is preserved for finite rotation increments. The material derivative concept is used to develop a continuum-based shape and configuration DSA method. Significant computational efficiency is obtained by solving the design sensitivity equation without iteration at each converged load step using the same consistent tangent stiffness matrix. Numerical implementation of the proposed shape and configuration DSA is carried out using the meshfree method. The accuracy and efficiency of the proposed method is illustrated using numerical examples.",10.1002/nme.1710,https://dx.doi.org/10.1002/nme.1710
"Castillo, E | Minguez, R | Ruiz-Teran, A | Fernandez-Canteli, A",2005,Design of a composite beam using the failure probability-safety factor method,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,2,0.15,"The paper shows the practical importance of the failure probability-safety factor method for designing engineering works. The method provides an automatic design tool by optimizing an objective function subject to the standard geometric and code constraints, and two more sets of constraints, that guarantee some given safety factors and failure probability bounds, associated with a given set of failure modes. Since a direct solution of the optimization problem is not possible, the method proceeds as a sequence of three steps: (a) an optimal classical design, based on given safety factors, is done, (b) failure probabilities or bounds of all failure modes are calculated, and (c) safety factors bounds are adjusted. This implies a double safety check that leads to safer structures and designs less prone to wrong or unrealistic probability assumptions, and to excessively small (unsafe) or large (costly) safety factors. Finally, the actual global or combined probabilities of the different failure modes and their correlation are calculated using a Monte Carlo simulation. In addition, a sensitivity analysis is performed. To this end, the optimization problems are transformed into another equivalent ones, in which the data parameters are converted into artificial variables. In this way, some variables of the dual associated problems become the desired sensitivities. The method is illustrated by its application to the design of a composite beam.",10.1002/nme.1215,https://dx.doi.org/10.1002/nme.1215
"Lange, KJ | Anderson, WK",2010,Using sensitivity derivatives for design and parameter estimation in an atmospheric plasma discharge simulation,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,1,0.12,"The problem of applying sensitivity analysis to a one-dimensional atmospheric radio frequency plasma discharge simulation is considered. A fluid simulation is used to model an atmospheric pressure radio frequency helium discharge with a small nitrogen impurity Sensitivity derivatives are computed for the peak electron density with respect to physical inputs to the simulation. These derivatives are verified using several different methods to compute sensitivity derivatives It is then demonstrated how sensitivity derivatives can be used within a design cycle to change these physical Inputs so as to Increase the peak electron density It is also shown how sensitivity analysis can be used in conjunction with experimental data to obtain better estimates for rate and transport parameters. Finally, it is described how sensitivity analysis could be used to compute an upper bound on the uncertainty for results from a simulation (C)  Elsevier Inc All rights reserved",10.1016/j.jcp.2010.04.038,https://dx.doi.org/10.1016/j.jcp.2010.04.038
"Kammer, DC | Alvin, KF | Malkus, DS",2002,Combining metamodels with rational function representations of discretization error for uncertainty quantification,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,2,0.12,"Techniques for producing metamodels for the efficient Monte Carlo simulation of high consequence systems are presented. The bias of f.e.m mesh discretization errors is eliminated or minimized by extrapolation, using rational functions, rather than the power series representation of Richardson extrapolation. Examples, including estimation of the vibrational frequency of a one-dimensional bar, show that the rational function model gives more accurate estimates using fewer terms than Richardson extrapolation, an important consideration for computational reliability assessment of high-consequence systems, where small biases in solutions can significantly affect the accuracy of small-magnitude probability estimates. Rational function representation of discretization error enable the user to accurately extrapolate to the continuum from numerical experiments performed outside the asymptotic region of the usual power series, allowing use of coarser meshes in the numerical experiments, resulting in significant savings.",10.1016/S0045-7825(01)00328-0,https://dx.doi.org/10.1016/S0045-7825(01)00328-0
"Christensen, S",2005,"Comment on ""Sensitivity analysis and determination of streambed leakance and aquifer hydraulic properties"" by X. Chen and X. Chen",Applications_JOURNAL OF HYDROLOGY,1,0.08,"Chen and Chen [Chen, X., Chen, X., . Sensitivity analysis and determination of streambed leakance and aquifer hydraulic properties. Journal of Hydrology , -] carried out a study that is very similar to that of Christensen [Christensen, S., . On the estimation of stream flow depletion parameters by drawdown analysis. Ground Water  (), -]. Chen and Chen [Chen, X., Chen, X., . Sensitivity analysis and determination of streambed leakance and aquifer hydraulic properties. Journal of Hydrology , -] made the analysis for a few specific cases with very similar hydraulic properties, while that of Christensen [Christensen, S., . On the estimation of stream flow depletion parameters by drawdown analysis. Ground Water  (), -] presents results in dimensionless form that can be used for a wide range of hydraulic parameter values, pumping rates, pumping durations, and error magnitudes. In one case Chen and Chen [Chen, X., Chen, X., . Sensitivity analysis and determination of streambed leakance and aquifer hydraulic properties. Journal of Hydrology , -] made a mistake in their analysis. In other cases inaccuracies in the approximations used by Chen and Chen [Chen, X., Chen, X., . Sensitivity analysis and determination of streambed leakance and aquifer hydraulic properties. Journal of Hydrology , -] seem to have lead to results that they misinterpret. Chen and Chen [Chen. X., Chen, X., . Sensitivity analysis and determination of streambed leakance and aquifer hydraulic properties. Journal of Hydrology , -] would have been concerned about this if they had studied and compared with the results of Christensen [Christensen, S., . On the estimation of stream flow depletion parameters by drawdown analysis. Ground Water  (), -] that were published more than  years before their submittal. This would also have shown that the major conclusions of Chen and Chen [Chen, X., Chen, X., . Sensitivity analysis and determination of streambed leakance and aquifer hydraulic properties. Journal of Hydrology , -] were already made by Christensen [Christensen, S., . On the estimation of stream flow depletion parameters by drawdown analysis. Ground Water  (), -]. that the results and conclusions of Christensen [Christensen, S., . On the estimation of stream flow depletion parameters by drawdown analysis. Ground Water  (), -] have more general applicability than those of Chen and Chen [Chen, X., Chen, X., . Sensitivity analysis and determination of streambed leakance and aquifer hydraulic properties. Journal of Hydrology , -], and that Christensen [Christensen, S., . On the estimation of stream flow depletion parameters by drawdown analysis. Ground Water  (), -] carried the study further and shows how parameter uncertainties propagate into uncertainty of stream flow predictions. Chen and Chen [Chen, X., Chen, X., . Sensitivity analysis and determination of streambed leakance and aquifer hydraulic properties. Journal of Hydrology , -] applied the technique to a field case study. Similar applications were published by Nyholm et al. [Nyholm, T., Christensen, S., Rasmussen, K.R., . Flow depletion in a small stream caused by ground water abstraction from wells. Ground Water  (), -] and by Kollet and Zlotnik [Kollet, S.J., Zlotnik, V.A., . Stream depletion predictions using pumping test data from a heterogeneous stream-aquifer system (a case study from the Great Plains, USA). Journal o Hydrology , -].",10.1016/j.jhydrol.2004.09.025,https://dx.doi.org/10.1016/j.jhydrol.2004.09.025
"Zaicenco, AG",2017,Sparse collocation method for global sensitivity analysis and calculation of statistics of solutions in SPDEs,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,0,0.0,"This paper explores advantages offered by the stochastic collocation method based on the Smolyak grids for the solution of differential equations with random inputs in the parameter space. We use sparse Smolyak grids and the Chebyshev polynomials to construct multidimensional basis and approximate decoupled stochastic differential equations via interpolation. Disjoint set of grid points and basis functions allow us to gain significant improvement to conventional Smolyak algorithm. Density function and statistical moments of the solution are obtained by means of quadrature rules if inputs are uncorrelated and uniformly distributed. Otherwise, the Monte Carlo analysis can run inexpensively using obtained sparse approximation. An adaptive technique to sample from a multivariate density function using sparse grid is proposed to reduce the number of required sampling points. Global sensitivity analysis is viewed as an extension of the sparse interpolant construction and is performed by means of the Sobol' variance-based or the Kullback-Leibler entropy methods identifying the degree of contribution from the individual inputs as well as the cross terms.",10.1002/nme.5454,https://dx.doi.org/10.1002/nme.5454
"Najafi, AR | Safdari, M | Tortorelli, DA | Geubelle, PH",2017,Shape optimization using a NURBS-based interface-enriched generalized FEM,Applications_INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,0,0.0,"This study presents a gradient-based shape optimization over a fixed mesh using a non-uniform rational B-splines-based interface-enriched generalized finite element method, applicable to multi-material structures. In the proposed method, non-uniform rational B-splines are used to parameterize the design geometry precisely and compactly by a small number of design variables. An analytical shape sensitivity analysis is developed to compute derivatives of the objective and constraint functions with respect to the design variables. Subtle but important new terms involve the sensitivity of shape functions and their spatial derivatives. Verification and illustrative problems are solved to demonstrate the precision and capability of the method.",10.1002/nme.5482,https://dx.doi.org/10.1002/nme.5482
"Shang, LY | Zhao, GZ | Zhai, JJ",2017,Topology optimization for coupled acoustic-structural systems under random excitation,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,0,0.0,"Topology optimization for coupled acoustic-structural systems subjected to stationary random excitations is investigated. Bi-material elastic continuum structures without damping are considered. The finite element method is utilized to deal with coupled acoustic-structural problems. An accurate and highly efficient algorithm series for stationary random analysis techniques, pseudo excitation method, is extended to calculate the acoustic random response generated by the vibrating structures. Minimization of the auto power spectral density of sound pressure is taken as design objective and its sensitivities with respect to topological variables are derived by adjoint method. A penalty term is proposed to suppress the intermediate volumetric densities. Numerical examples are given to demonstrate the validity of the presented methods.",10.1007/s00158-017-1687-3,https://dx.doi.org/10.1007/s00158-017-1687-3
"Funke, SW | Farrell, PE | Piggott, MD",2017,Reconstructing wave profiles from inundation data,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,0,0.0,"This paper applies variational data assimilation to inundation problems governed by the shallow water equations with wetting and drying. The objective of the assimilation is to recover an unknown time-varying wave profile at an open ocean boundary from inundation observations. This problem is solved with derivative-based optimisation and an adjoint wetting and drying scheme to efficiently compute sensitivity information. The capabilities of this approach are demonstrated on an idealised sloping beach setup in which the profile of an incoming wave is reconstructed from wet/dry interface observations. The method is robust to noise in the observations if a regularisation term is added to the optimisation objective. Finally, the method is applied to a laboratory experiment of the Hokkaido-Nansei-Oki tsunami, where the wave profile is reconstructed with a relative L-infinity error of less than %.",10.1016/j.cma.2017.04.019,https://dx.doi.org/10.1016/j.cma.2017.04.019
"Lee, W",2014,Adaptive approximation of higher order posterior statistics,Applications_JOURNAL OF COMPUTATIONAL PHYSICS,0,0.0,"Filtering is an approach for incorporating observed data into time-evolving systems. Instead of a family of Dirac delta masses that is widely used in Monte Carlo methods, we here use the Wiener chaos expansion for the parametrization of the conditioned probability distribution to solve the nonlinear filtering problem. The Wiener chaos expansion is not the best method for uncertainty propagation without observations. Nevertheless, the projection of the system variables in a fixed polynomial basis spanning the probability space might be a competitive representation in the presence of relatively frequent observations because the Wiener chaos approach not only leads to an accurate and efficient prediction for short time uncertainty quantification, but it also allows to apply several data assimilation methods that can be used to yield a better approximate filtering solution. The aim of the present paper is to investigate this hypothesis. We answer in the affirmative for the (stochastic) Lorenz- system based on numerical simulations in which the uncertainty quantification method and the data assimilation method are adaptively selected by whether the dynamics is driven by Brownian motion and the near-Gaussianity of the measure to be updated, respectively.",10.1016/j.jcp.2013.11.015,https://dx.doi.org/10.1016/j.jcp.2013.11.015
"Diez, P | Zlotnik, S | Huerta, A",2017,Generalized parametric solutions in Stokes flow,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,0,0.0,"Design optimization and uncertainty quantification, among other applications of industrial interest, require fast or multiple queries of some parametric model. The Proper Generalized Decomposition (PGD) provides a separable solution, a computational vademecum explicitly dependent on the parameters, efficiently computed with a greedy algorithm combined with an alternated directions scheme and compactly stored. This strategy has been successfully employed in many problems in computational mechanics. The application to problems with saddle point structure raises some difficulties requiring further attention. This article proposes a PGD formulation of the Stokes problem. Various possibilities of the separated forms of the PGD solutions are discussed and analyzed, selecting the more viable option. The efficacy of the proposed methodology is demonstrated in numerical examples for both Stokes and Brinkman models.",10.1016/j.cma.2017.07.016,https://dx.doi.org/10.1016/j.cma.2017.07.016
"Pisaroni, M | Nobile, F | Leyland, P",2017,A Continuation Multi Level Monte Carlo (C-MLMC) method for uncertainty quantification in compressible inviscid aerodynamics,Applications_COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,0,0.0,"In this work we apply the Continuation Multi-Level Monte Carlo (C-MLMC) algorithm proposed in Collier et al. () to efficiently propagate operating and geometric uncertainties in inviscid compressible aerodynamics numerical simulations. The key idea of MLMC is that one can draw MC samples simultaneously and independently on several approximations of the problem under investigations on a hierarchy of nested computational grids (levels). The expectation of an output quantity is computed as a sample average using the coarsest solutions and corrected by averages of the differences of the solutions of two consecutive grids in the hierarchy. By this way, most of the computational effort is transported from the finest level (as in a standard Monte Carlo approach) to the coarsest one. The continuation algorithm (C-MLMC) is a robust and self-tuning version that estimates on the fly the optimal number of level and realizations per level. In this work we describe in detail how C-MLMC can be adapted to perform uncertainty quantification analysis in compressible aerodynamics and we apply it to the relevant test cases of a quasi D convergent-divergent Laval nozzle and the D transonic RAE- airfoil.",10.1016/j.cma.2017.07.030,https://dx.doi.org/10.1016/j.cma.2017.07.030
"Sun, GD | Peng, F | Mu, M",2017,Uncertainty assessment and sensitivity analysis of soil moisture based on model parameter errors - Results from four regions in China,Applications_JOURNAL OF HYDROLOGY,0,0.0,"Model parameter errors are an important cause of uncertainty in soil moisture simulation. In this study, a conditional nonlinear optimal perturbation related to parameter (CNOP-P) approach and a sophisticated land surface model (the Common Land Model, CoLM) are employed in four regions in China to explore extent of uncertainty in soil moisture simulations due to model parameter errors. The CNOP-P approach facilitates calculation of the upper bounds of uncertainty due to parameter errors and investigation of the nonlinear effects of parameter combination on uncertainties in simulation and prediction. The range of uncertainty for simulated soil moisture was found to be from . to . m() m(-). Based on the CNOP-P approach, a new approach is applied to explore a relatively sensitive and important parameter combination for soil moisture simulations and predictions. It is found that the relatively sensitive parameter combination is region- and season-dependent. Furthermore, the results show that simulation of soil moisture could be improved if the errors in these important parameter combinations are reduced. In four study regions, the average extent of improvement (.%) in simulating soil moisture using the new approach based on the CNOP-P is larger than that (.%) using the one-at-a-time (OAT) approach. These results indicate that simulation and prediction of soil moisture is improved by considering the nonlinear effects of important physical parameter combinations. In addition, the new approach based on the CNOP-P is found to be an effective method to discern the nonlinear effects of important physical parameter combinations on numerical simulation and prediction.",10.1016/j.jhydrol.2017.09.059,https://dx.doi.org/10.1016/j.jhydrol.2017.09.059
"Qiu, HC | Qiu, ZP",2017,A modified stochastic perturbation algorithm for closely-spaced eigenvalues problems based on surrogate model,Applications_STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,0,0.0,"Aiming at uncertainty propagation and dynamic reanalysis of closely-spaced eigenvalues, with consideration of uncertainties in design variables, a modified stochastic perturbation method is proposed. Concerning quasi-symmetric or partial-symmetric structures that frequently appear, one of their primary features is closely-distributed natural frequencies. For structure with closely-spaced eigenvalues, due to its instability and sensitivity to the changes of design variables and its excessively concentrated adjacent eigenvalues, conventional uncertainty analysis or dynamic reanalysis methods for distinct eigenvalue are no longer available. Initially, the spectral decompositions of stiffness and mass matrices are provided; by transfer technique, the eigen-problem of closely-spaced eigenvalues is converted to that of repeated eigenvalues with two perturbation parts appended; then the perturbed closely-spaced eigenvalue is rewritten as the sum of original closely-spaced eigenvalues' mean value and surrogate model which approximates the first-order perturbation term by polynomial chaos expansions. According to this method, statistical quantities of perturbed closely-spaced eigenvalues are calculated directly and accurately, which contributes to its uncertainty analysis and dynamic reanalysis. Furthermore, the capability of proposed method in dealing with relatively large uncertainties and complex engineering structure is demonstrated. The accuracy and efficiency of proposed method have been verified sufficiently by numerical examples.",10.1007/s00158-017-1660-1,https://dx.doi.org/10.1007/s00158-017-1660-1
