,id,year,title,keywords,abstract,copyright,AU,SO,DE,DOI
0,WOS:000375818700005,2016,River-to-sea pressure retarded osmosis: Resource utilization in a full-scale facility,POWER-GENERATION SALINITY GRADIENTS REVERSE ELECTRODIALYSIS OSMOTIC POWER SEAWATER DESALINATION ENERGY EFFICIENCY RO-PRO WATER SYSTEM PERFORMANCE,"Pressure retarded osmosis (PRO) is a technology that could be utilized to recover energy from the mixing of freshwater with seawater. This source of renewable energy is sizeable and in the past decade several investigations analyzed its potential. The vast majority of studies focused on mass transfer problems across the membrane in order to improve membrane productivity and just recently studies started to look at membrane module efficiencies and parasitic loads within the PRO facility. In this article, the net specific energy production from a facility-scale PRO system was determined and optimized by using a novel simulation method that integrates parasitic loads and efficiencies of the PRO facility components and combines the model with an optimization software in a linked system optimization scheme. It was found that the overall net specific energy that may be recovered by a river-to-sea PRO facility is approximately . kWh per m() of permeate. Furthermore, a sensitivity analysis was performed to elucidate the relationship between net specific energy and power density as functions of membrane area, flow rates, and operating pressures. In general, in order to maximize resource recovery, a low power density, thus a low membrane productivity, must be accepted.", (C) 2016 Elsevier B.V. All rights reserved.,"O'Toole, G|Jones, L|Coutinho, C|Hayes, C|Napoles, M|Achilli, A",DESALINATION,renewable energy salinity gradient power pressure retarded osmosis net specific energy power density facility analysis,10.1016/j.desal.2016.01.012
1,WOS:000088879600005,2000,Scales and similarities in runoff processes with respect to geomorphometry,SPATIAL VARIABILITY CATCHMENT SOIL,"Numerous investigations using various techniques have been carried out towards a more detailed understanding of relationships and interactions between catchment morphometry and rainfall-runoff processes. Recently, this research question has become more relevant through the need for accurate, yet simple, computer models simulating the water balance of large areas. Moreover, advances in the analysis of landform morphometry through the availability of high-resolution digital elevation models (DEMs) and powerful geographical information systems (GIS) have enhanced research efforts with this aim. In this study several computer techniques and models were applied to investigate the effects of geomorphometry on rainfall-runoff processes at different scales. The sensitivity of dynamic hydrological processes to comparatively static boundary conditions requires different methods for modelling, analysis and visualization of different kinds of data appropriate to different scales. Therefore an approach integrating several geocomputational concepts, including spatial analysis of different types of geodata, static modelling of spatial structures, dynamic four-dimensional modelling of hydrological processes and statistical techniques was chosen. Geomorphometric analysis of the study sites was carried out with GIS packages (including ARC/INFO and GRASS), special purpose software and self-developed tools. Soil-morphometry relationships were modelled within a GIS environment. Hydrological models (SAKE and TOPMODEL) were then used to simulate rainfall-runoff processes, and finally statistical tools and sensitivity analysis were applied to gain an insight into the hydrological significance of the various geomorphometric properties. The results demonstrate the importance of small subregions of the catchment, particularly those having low slope angles, low flow lengths and concavities. The spatial distribution of soil types significantly influences modelled runoff. Spatial distributions of soil types are partly related to morphometry and can be captured using soil-morphometry models. Further results show that catchments which differ significantly in morphometry show different runoff responses and different hydrological sensitivity to changes in boundary conditions. A crude derivation of geomorphometric-hydrological landform types could be reached. Therefore, geomorphometric classifications of catchment types could form a basis for representative hydrological modelling at the large scale. Models describing soil distribution in relation to geomorphometry could assist regionalization of spatial heterogeneity and structure of soil parameters relevant in hydrological modelling. Moreover, quantification of geomorphometric catchment structure, e.g. in terms of contributing areas, is needed to describe significant geomorphometric catchment characteristics."," Copyright (C) 2000 John Wiley & Sons, Ltd.","Schmidt, J|Hennrich, K|Dikau, R",HYDROLOGICAL PROCESSES,hydrological modelling geomorphometry gis dem soil-morphometry relationship,10.1002/1099-1085(20000815/30)14:11/12<1963::AID-HYP48>3.0.CO;2-M
2,WOS:000338551800010,2014,Risk and uncertainty analysis of gas pipeline failure and gas combustion consequence,MODELS DETONATION METHANE,"Taking into account a general concept of risk parameters and knowing that natural gas provides very significant portion of energy, firstly, it is important to insure that the infrastructure remains as robust and reliable as possible. For this purpose, authors present available statistical information and probabilistic analysis related to failures of natural gas pipelines. Presented historical failure data is used to model age-dependent reliability of pipelines in terms of Bayesian methods, which have advantages of being capable to manage scarcity and rareness of data and of being easily interpretable for engineers. The performed probabilistic analysis enables to investigate uncertainty and failure rates of pipelines when age-dependence is significant and when it is not relevant. The results of age-dependent modeling and analysis of gas pipeline reliability and uncertainty are applied to estimate frequency of combustions due to natural gas release when pipeline failure occurs. Estimated age-dependent combustion frequency is compared and proposed to be used instead of conservative and age-independent estimate. The rupture of a high-pressure natural gas pipeline can lead to consequences that can pose a significant threat to people and property in the close vicinity to the pipeline fault location. The dominant hazard is combustion and thermal radiation from a sustained fire. The second purpose of the paper is to present the combustion consequence assessment and application of probabilistic uncertainty analysis for modeling of gas pipeline combustion effects. The related work includes performance of the following tasks: to study gas pipeline combustion model, to identify uncertainty of model inputs noting their variation range, and to apply uncertainty and sensitivity analysis for results of this model. The performed uncertainty analysis is the part of safety assessment that focuses on the combustion consequence analysis. Important components of such uncertainty analysis are qualitative and quantitative analysis that identifies the most uncertain parameters of combustion model, assessment of uncertainty, analysis of the impact of uncertain parameters on the modeling results, and communication of the results' uncertainty. As outcome of uncertainty analysis the tolerance limits and distribution function of thermal radiation intensity are given. The measures of uncertainty and sensitivity analysis were estimated and outcomes presented applying software system for uncertainty and sensitivity analysis. Conclusions on the importance of the parameters and sensitivity of the results are obtained using a linear approximation of the model under analysis. The outcome of sensitivity analysis confirms that distance from the fire center has the greatest influence on the heat flux caused by gas pipeline combustion.",,"Alzbutas, R|Iesmantas, T|Povilaitis, M|Vitkute, J",STOCHASTIC ENVIRONMENTAL RESEARCH AND RISK ASSESSMENT,risk parameters natural gas pipelines age-dependent reliability bayesian inference gas combustion uncertainty and sensitivity analysis,10.1007/s00477-013-0845-4
3,WOS:000329561100024,2014,A comprehensive evaluation of various sensitivity analysis methods: A case study with a hydrological model,RAINFALL-RUNOFF MODELS IMPROVED CALIBRATION COMPUTER EXPERIMENTS GLOBAL OPTIMIZATION REGRESSION INDEXES SYSTEMS OUTPUT,"Sensitivity analysis (SA) is a commonly used approach for identifying important parameters that dominate model behaviors. We use a newly developed software package, a Problem Solving environment for Uncertainty Analysis and Design Exploration (PSUADE), to evaluate the effectiveness and efficiency of ten widely used SA methods, including seven qualitative and three quantitative ones. All SA methods are tested using a variety of sampling techniques to screen out the most sensitive (i.e., important) parameters from the insensitive ones. The Sacramento Soil Moisture Accounting (SAC-SMA) model, which has thirteen tunable parameters, is used for illustration. The South Branch Potomac River basin near Springfield, West Virginia in the U.S. is chosen as the study area. The key findings from this study are: () For qualitative SA methods, Correlation Analysis (CA), Regression Analysis (RA), and Gaussian Process (GP) screening methods are shown to be not effective in this example. Morris One-At-a-Time (MOAT) screening is the most efficient, needing only  samples to identify the most important parameters, but it is the least robust method. Multivariate Adaptive Regression Splines (MARS), Delta Test (DT) and Sum-Of-Trees (SOT) screening methods need about - samples for the same purpose. Monte Carlo (MC), Orthogonal Array (OA) and Orthogonal Array based Latin Hypercube (OALH) are appropriate sampling techniques for them; () For quantitative SA methods, at least  samples are needed for Fourier Amplitude Sensitivity Test (FAST) to identity parameter main effect. McKay method needs about  samples to evaluate the main effect, more than  samples to assess the two-way interaction effect. OALH and LP tau (LPTAU) sampling techniques are more appropriate for McKay method. For the Sobol' method, the minimum samples needed are  to compute the first-order and total sensitivity indices correctly. These comparisons show that qualitative SA methods are more efficient but less accurate and robust than quantitative ones. (C)  The Authors. Published by", Elsevier Ltd. All rights reserved.,"Gan, YJ|Duan, QY|Gong, W|Tong, C|Sun, YW|Chu, W|Ye, AZ|Miao, CY|Di, ZH",ENVIRONMENTAL MODELLING & SOFTWARE,uncertainty quantification sensitivity analysis parameter screening space-filling sampling psuade,10.1016/j.envsoft.2013.09.031
4,WOS:000388889800001,2016,A New Software Reliability Growth Model: Multigeneration Faults and a Power-Law Testing-Effort Function,ERROR-DETECTION OPTIMIZATION DEPENDENCY COST,"Software reliability growth models (SRGMs) based on a nonhomogeneous Poisson process (NHPP) are widely used to describe the stochastic failure behavior and assess the reliability of software systems. For these models, the testing-effort effect and the fault interdependency play significant roles. Considering a power-law function of testing effort and the interdependency of multigeneration faults, we propose a modified SRGM to reconsider the reliability of open source software (OSS) systems and then to validate the model's performance using several real-world data. Our empirical experiments show that the model well fits the failure data and presents a high-level prediction capability. We also formally examine the optimal policy of software release, considering both the testing cost and the reliability requirement. By conducting sensitivity analysis, we find that if the testing-effort effect or the fault interdependency was ignored, the best time to release software would be seriously delayed and more resources would be misplaced in testing the software.",,"Li, F|Yi, ZL",MATHEMATICAL PROBLEMS IN ENGINEERING,,10.1155/2016/9276093
5,WOS:000333743900027,2014,Comparative Life Cycle Assessment (LCA) of Accelerated Carbonation Processes Using Steelmaking Slag for CO2 Fixation,ROTATING PACKED-BED OXYGEN FURNACE SLAG METALWORKING WASTE-WATER CALCIUM-CARBONATE MINERAL CARBONATION STEEL SLAG CAPTURE STORAGE DIOXIDE SEQUESTRATION,"Carbon capture, utilization, and storage (CCUS) is one of the most prominent emerging technologies for mitigating global climate change. In this study, a comparative evaluation for CO fixation by carbonation of steelmaking slag was performed by life cycle assessment (LCA) using Umberto .. software, with the Swiss Eco-invent . database. Six scenarios of carbonation for basic oxygen furnace slag (BOFS), steel converted slag (SCS), and blended hydraulic slag cement (BHC) if different types of reactors and/or method were established. The environmental impacts for each scenario are quantified using the valuation system of ReCiPe, where global warming potential (GWP), ecosystem quality potential (EQP), an human health potential (HHP) were evaluated. In addition, sensitivity analysis was carried out to evaluate the relevant uncertainties of heating efficiency on the GHG emissions in direct carbonation processes. According to the results of LCA and sensitivity analysis, the direct carbonation of steelmaking slag in a slurry reactor was found to be the most attractive method, since the GWP was the lowest among the selected scenarios. Furthermore, the best available technology (BAT) for CO capture by carbonation processes of alkaline wastes was proposed according to the key performance indicators (KPIs) with respect to engineering considerations and environmental impacts. It was concluded that the accelerated carbonation of steelmaking slag should be performed by combining the slurry reactor with a rotating packed bed (RPB) to maximize carbonation conversion and minimize environmental impacts and additional CO emissions.",,"Xiao, LS|Wang, R|Chiang, PC|Pan, SY|Guo, QH|Chang, EE",AEROSOL AND AIR QUALITY RESEARCH,technology assessment umberto environmental impacts recipe sensitivity analysis rotating packed bed,10.4209/aaqr.2013.04.0121
6,WOS:000323644600031,2013,Nitrous Oxide Emissions from Cropland: a Procedure for Calibrating the DayCent Biogeochemical Model Using Inverse Modelling,CARBON-DIOXIDE SOIL N2O DENITRIFICATION SIMULATIONS COLORADO SYSTEMS DNDC,"DayCent is a biogeochemical model of intermediate complexity widely used to simulate greenhouse gases (GHG), soil organic carbon and nutrients in crop, grassland, forest and savannah ecosystems. Although this model has been applied to a wide range of ecosystems, it is still typically parameterized through a traditional ""trial and error"" approach and has not been calibrated using statistical inverse modelling (i.e. algorithmic parameter estimation). The aim of this study is to establish and demonstrate a procedure for calibration of DayCent to improve estimation of GHG emissions. We coupled DayCent with the parameter estimation (PEST) software for inverse modelling. The PEST software can be used for calibration through regularized inversion as well as model sensitivity and uncertainty analysis. The DayCent model was analysed and calibrated using NO flux data collected over  years at the Iowa State University Agronomy and Agricultural Engineering Research Farms, Boone, IA. Crop year  data were used for model calibration and  data were used for validation. The optimization of DayCent model parameters using PEST significantly reduced model residuals relative to the default DayCent parameter values. Parameter estimation improved the model performance by reducing the sum of weighted squared residual difference between measured and modelled outputs by up to  %. For the calibration period, simulation with the default model parameter values underestimated mean daily NO flux by  %. After parameter estimation, the model underestimated the mean daily fluxes by  %. During the validation period, the calibrated model reduced sum of weighted squared residuals by  % relative to the default simulation. Sensitivity analysis performed provides important insights into the model structure providing guidance for model improvement.",,"Rafique, R|Fienen, MN|Parkin, TB|Anex, RP",WATER AIR AND SOIL POLLUTION,daycent model inverse modelling parameter estimation (pest) nitrous oxide sensitivity analysis automatic calibration validation,10.1007/s11270-013-1677-z
7,WOS:000273504300008,2010,Predicting the Long-Term Performance of a Structural Best Management Practice with the BMP ToolBox Model,NONPOINT-SOURCE POLLUTION CONSTRUCTED WETLAND WASTE-WATER TAIWAN SYSTEM REMOVAL DESIGN,"It is costly to constantly sample to monitor the performance of a structural best management practice (BMP). Alternatively, occasional sampling might not be adequate. The BMP ToolBox model, developed by Tetra Tech and Prince George's County in Maryland, USA, assesses the performance of a structural BMP treatment site. The study applied the BMP ToolBox model to a BMP site in Taiwan to test its validity. The case study site was designed to remove pollution from nonpoint sources (tea gardens) in order to maintain water quality in the Feitsui Reservoir. The BMP ToolBox model was calibrated and verified using two years of sample data. Results were satisfactory with the coefficient of determination (R-) for calibration and verification being . and ., respectively. Furthermore, the one-factor-at-a-time method (OFAT) was applied in a sensitivity analysis to identify sensitive model parameters. Several hydrographs were created to predict BMP performance. The positive relationship between the total phosphorus (TP) removal rate and the recurrence interval was observed: rainfall with longer durations showed increased removal rates compared to shorter periods of rainfall. The BMP Toolbox model was successfully applied, and a process for evaluating the long-term operation of structural BMP sites was established.",,"Chen, CF|Lin, JY|Kang, SF|Lee, YJ|Yang, CH",ENVIRONMENTAL ENGINEERING SCIENCE,structural bmp pollution removal evaluation bmp toolbox,10.1089/ees.2009.0105
8,WOS:000302794000003,2012,Techno-Economics for Conversion of Lignocellulosic Biomass to Ethanol by Indirect Gasification and Mixed Alcohol Synthesis,,"This techno-economic study investigates the production of ethanol and a higher alcohols coproduct by conversion of lignocelluosic biomass to syngas via indirect gasification followed by gas-to-liquids synthesis over a precommercial heterogeneous catalyst. The design specifies a processing capacity of , dry U.S. tons (, dry metric tonnes) of woody biomass per day and incorporates  research targets from NREL and other sources for technologies that will facilitate the future commercial production of cost-competitive ethanol. Major processes include indirect steam gasification, syngas cleanup, and catalytic synthesis of mixed alcohols, and ancillary processes include feed handling and drying, alcohol separation, steam and power generation, cooling water, and other operations support utilities. The design and analysis is based on research at NREL, other national laboratories, and The Dow Chemical Company, and it incorporates commercial technologies, process modeling using Aspen Plus software, equipment cost estimation, and discounted cash flow analysis. The design considers the economics of ethanol production assuming successful achievement of internal research targets and n(th)-plant costs and financing. The design yields . gallons of ethanol and . gallons of higher-molecular-weight alcohols per U.S. ton of biomass feedstock. A rigorous sensitivity analysis captures uncertainties in costs and plant performance."," (C) 2012 American Institute of Chemical Engineers Environ Prog, 31: 182-190, 2012","Dutta, A|Talmadge, M|Hensley, J|Worley, M|Dudgeon, D|Barton, D|Groenendijk, P|Ferrari, D|Stears, B|Searcy, E|Wright, C|Hess, JR",ENVIRONMENTAL PROGRESS & SUSTAINABLE ENERGY,biomass thermochemical conversion indirect gasification tar reforming mixed alcohols process design,10.1002/ep.10625
9,WOS:000412253100005,2017,NHPP software reliability model considering the uncertainty of operating environments with imperfect debugging and testing coverage,RANDOM-FIELD ENVIRONMENTS FAULT-DETECTION RATE GROWTH-MODELS COST MODELS DEPENDENCY,"In this paper, we propose a testing-coverage software reliability model that considers not only the imperfect debugging (ID) but also the uncertainty of operating environments based on a non-homogeneous Poisson process (NHPP). Software is usually tested in a given control environment, but it may be used in different operating environments by different users, which are unknown to the developers. Many NHPP software reliability growth models (SRGMs) have been developed to estimate the software reliability measures, but most of the underlying common assumptions of these models are that the operating environment is the same as the developing environment. But in fact, due to the unpredictability of the uncertainty in the operating environments for the software, environments may considerably influence the reliability and software's performance in an unpredictable way. So when a software system works in a field environment, its reliability is usually different from the theory reliability, and also from all its similar applications in other fields. In this paper, a new model is proposed with the consideration of the fault detection rate based on the testing coverage and examined to cover ID subject to the uncertainty of operating environments. We compare the performance of the proposed model with several existing NHPP SRGMs using three sets of real software failure data based on seven criteria. Improved normalized criteria distance (NCD) method is also used to rank and select the best model in the context of a set of goodness-of-fit criteria taken all together. All results demonstrate that the new model can give a significant improved goodness-of-fit and predictive performance. Finally, the optimal software release time based on cost and reliability requirement and its sensitivity analysis are discussed.", (C) 2017 Elsevier Inc. All rights reserved.,"Li, QY|Pham, H",APPLIED MATHEMATICAL MODELLING,testing coverage uncertainty operating environment imperfect debugging (id) software reliability growth models (srgms) non-homogeneous poisson process (nhpp),10.1016/j.apm.2017.06.034
10,WOS:000288865100006,2011,Estimating Water Budgets and Vertical Leakages for Karst Lakes in North-Central Florida (United States) Via Hydrological Modeling,RAINFALL-RUNOFF MODELS AUTOMATIC CALIBRATION CONDUCTANCE,"Newnans, Lochloosa, and Orange Lakes are closely hydrologically connected karst lakes located in north-central Florida, United States. The complex karst hydrology in this region poses a great challenge to the hydrological modeling that is essential to the development of Total Maximum Daily Loads for these lakes. We used a Hydrological Simulation Program - Fortran model coupled with the parallel Parameter ESTimation model calibration and uncertainty analysis software to estimate effectively the hydrological interactions between the lakes and the underlying upper Floridan aquifer and the water budgets for these three lakes. The net results of the lake-groundwater interactions in Newnans and Orange Lakes are that both lakes recharge the underlying upper Floridan aquifer, with the recharge rate of the latter one magnitude greater than that of the former. However, for Lochloosa Lake, the net lake-groundwater interaction is that the lake gains water from groundwater in a significant amount, approximately % of its total terrestrial water input. The annual average vertical leakages estimated for Newnans, Lochloosa, and Orange Lakes are . x , -. x , and . x  m, respectively. The average vertical hydraulic conductance (K(v)/b) of the units between a lake bottom and the underlying upper Floridan aquifer in this region are also estimated to be from . x - to . x - day-.",,"Lin, ZL",JOURNAL OF THE AMERICAN WATER RESOURCES ASSOCIATION,hspf karst hydrology lake water budget optimization surface water groundwater interaction,10.1111/j.1752-1688.2010.00513.x
11,WOS:000302910100010,2012,Robust design in aerodynamics using third-order sensitivity analysis based on discrete adjoint. Application to quasi-1D flows,SHAPE OPTIMIZATION DATA ASSIMILATION 1ST,"In this paper, the second-order second moment approach, coupled with an adjoint-based steepest descent algorithm, for the solution of the so-called robust design problem in aerodynamics is proposed. Because the objective function for the robust design problem comprises first-order and second-order sensitivity derivatives with respect to the environmental parameters, the application of a gradient-based method , which requires the sensitivities of this function with respect to the design variables, calls for the computation of third-order mixed derivatives. To compute these derivatives with the minimum CPU cost, a combination of the direct differentiation and the discrete adjoint variable method is proposed. This is presented for the first time in the relevant literature and is the most efficient among other possible schemes on condition that the design variables are much more than the environmental ones; this is definitely true in most engineering design problems. The proposed approach was used for the robust design of a duct, assuming a quasi-D flow model; the coordinates of the Bezier control points parameterizing the duct shape are used as design variables, whereas the outlet Mach number and the DarcyWeisbach friction coefficient are used as environmental ones. The extension to D and D flow problems, after developing the corresponding direct differentiation and adjoint variable methods and software, is straightforward."," Copyright (C) 2011 John Wiley & Sons, Ltd.","Papoutsis-Kiachagias, EM|Papadimitriou, DI|Giannakoglou, KC",INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN FLUIDS,robust aerodynamic shape optimization discreteadjoint method third-order sensitivity derivatives method of moments,10.1002/fld.2604
12,WOS:000168569900007,2001,COOPT - a software package for optimal control of large-scale differential-algebraic equation systems,,"This paper describes the functionality and implementation of COOPT. This software package implements a direct method with modified multiple shooting type techniques for solving optimal control problems of large-scale differential-algebraic equation (DAE) systems. The basic approach in COOPT is to divide the original time interval into multiple shooting intervals, with the DAEs solved numerically on the subintervals at each optimization iteration. Continuity constraints are imposed across the subintervals, The resulting optimization problem is solved by sparse sequential quadratic programming (SQP) methods. Partial derivative matrices needed for the optimization are generated by DAE sensitivity software. The sensitivity equations to be solved are generated via automatic differentiation. COOPT has been successfully used in solving optimal control problems arising from a wide variety of applications, such as chemical vapor deposition of superconducting thin films, spacecraft trajectory design and contingency/recovery problems, and computation of cell traction forces in tissue engineering. (C)  IMACS.", Published by Elsevier Science B.V. All rights reserved.,"Serban, R|Petzold, LR",MATHEMATICS AND COMPUTERS IN SIMULATION,differential-algebraic equations sensitivity analysis optimal control sequential quadratic programming methods,10.1016/S0378-4754(01)00289-0
13,WOS:000254595200018,2008,Waste management modeling with PC-based model - EASEWASTE,SYSTEMS,"As lite-cycle-thinking becomes more integrated into waste management, quantitative tools are needed for assessing waste management systems and technologies. This article presents a decision support model to deal with integrated solid waste management planning problems at a regional or national level. The model is called EASEWASTE (environmental assessment of solid waste systems and technologies). The model consists of a number of modules (submodels), each describing a process in a real waste management system, and these modules may combine to represent a complete waste management system in a scenario. EASEWASTE generates data on emissions (inventory), which are translated and aggregated into different environmental impact categories, e.g. the global warming, acidification, and toxicity. To facilitate a ""first level"" screening evaluation, default values for process parameters have been provided, wherever possible. The EASEWASTE model for life-cycle-assessment of waste management is described and applied to a case study for illustrative purposes. The case study involving hypothetical but realistic data demonstrates the functionality, usability, and flexibilities of the model. The design and implementation of the software successfully address the substantial challenges in integrating process modeling, life-cycle inventory (M), and impact assessment (LCIA) modeling, and optimization into an interactive decision support platform."," (c) 2008 American Institute of Chemical Engineers Environ Prog, 27: 133-142, 2008.","Bhander, GS|Hauschild, MZ|Christensen, TH",ENVIRONMENTAL PROGRESS,life cycle assessment waste management modeling easewaste model systems analysis waste planning environmental assessment waste technologies sensitivity analysis,10.1002/ep.10250
14,WOS:000231058700001,2005,A user's guide to the brave new world of designing simulation experiments,COMPUTER EXPERIMENTS SENSITIVITY-ANALYSIS SAMPLING CRITERIA ROBUST DESIGN OUTPUT MODEL OPTIMIZATION METHODOLOGY METAMODELS MANAGEMENT,"Many simulation practitioners can get more from their analyses by using the statistical theory on design of experiments (DOE) developed specifically for exploring computer models. We discuss a toolkit of designs for simulators with limited DOE expertise who want to select a design and an appropriate analysis for their experiments. Furthermore, we provide a research agenda listing problems in the design of simulation experiments-as opposed to real-world experiments-that require more investigation. We consider three types of practical problems: () developing a basic understanding of a particular simulation model or system, () finding robust decisions or policies as opposed to so-called optimal solutions, and () comparing the merits of various decisions or policies. Our discussion emphasizes aspects that are typical for simulation, such as having many more factors than in real-world experiments, and the sequential nature of the data collection. Because the same problem type may be addressed through different design types, we discuss quality attributes of designs, such as the ease of design construction, the flexibility for analysis, and efficiency considerations. Moreover, the selection of the design type depends on the metamodel (response surface) that the analysts tentatively assume; for example, complicated metamodels require more simulation runs. We present several procedures to validate the metamodel estimated from a specific design, and we summarize a case study illustrating several of our major themes. We conclude with a discussion of areas that merit more work to achieve the potential benefits-either via new research or incorporation into standard simulation or statistical packages.",,"Kleijnen, JPC|Sanchez, SM|Lucas, TW|Cioppa, TM",INFORMS JOURNAL ON COMPUTING,simulation design of experiments metamodels latin hypercube sequential bifurcation robust design,10.1287/ijoc.1050.0136
15,WOS:000173726400003,2002,Emerging issues in population viability analysis,SPOTTED OWL METAPOPULATION INBREEDING DEPRESSION ELASTICITY ANALYSIS CONSERVATION BIOLOGY GENTIANA-PNEUMONANTHE MAMMALIAN POPULATIONS SPECIES CONSERVATION PLANT-POPULATIONS GENETIC-VARIATION ET-AL,"Population viability analysis (PVA) has become a commonly used tool in endangered species management. There is no single process that constitutes PVA, but all approaches have in common all assessment of a population's risk of extinction (or quasi extinction) or its projected population growth either under current conditions or expected from proposed management. As model sophistication increases, and software programs that facilitate PVA without the need for modeling expertise become more available, there is greater potential for the misuse of models and increased confusion over interpreting their results. Consequently, we discuss the practical use and limitations of PVA in conservation planning, and we discuss some emerging issues of PVA. We review extant issues that have become prominent in PTA, including spatially explicit modeling, sensitivity analysis, incorporating genetics into PVA, PVA in plants, and PVA software packages, but our coverage of emerging issues is not comprehensive. We conclude that PVA is a powerful tool in conservation biology for comparing alternative research plans and relative extinction risks among species, but the suggest caution in its use: () because PVA is a model, its validity depends on the appropriateness of the model's structure and data quality; () results should be presented with appropriate assessment of confidence; () model construction and results should be subject to external review, and () model structure, input, and results should be treated as hypotheses to be tested. We also suggest () restricting the definition of PVA to development of a formal quantitative model, () focusing more research on determining how pervasive density-dependence feedback is across species, and () not using PVA to determine minimum population size or () the specific probability of reaching extinction. The most appropriate use of PVA may be for comparing the relative effects of potential management actions on population growth or persistence.",,"Reed, JM|Mills, LS|Dunning, JB|Menges, ES|McKelvey, KS|Frye, R|Beissinger, SR|Anstett, MC|Miller, P",CONSERVATION BIOLOGY,,10.1046/j.1523-1739.2002.99419.x
16,WOS:000233938100033,2005,Utility of dynamic-landscape metapopulation models for sustainable forest management,BIRDS INDICATORS VIABILITY FRAGMENTATION BIODIVERSITY FIRE,"We evaluated the utility of combining metapopulation models with landscape-level forest-dynamics models to assess the sustainability of forest management practices. We used the Brown Creeper (Certhia americana) in the boreal forests of northern Ontario as a case study. We selected the Brown Creeper as a potential indicator of sustainability because it is relatively common in the region but is dependent on snags and old trees for nesting and foraging; hence, it may be sensitive to timber harvesting. For the modeling we used RAMAS Landscape, a software package that integrates RAMAS GIS, population-modeling software, and LANDIS, forest-dynamics modeling software. Predictions about the future floristic composition and structure of the landscape tinder a variety of management and natural disturbance scenarios were derived using LANDIS. We modeled eight alternative forest management scenarios, ranging in intensity from no timber harvesting and a natural fire regime to intensive timber harvesting with salvage logging after fire. We predicted the response of The Brown Creeper metapopulation over a -year period and used future population size and expected minimum population size to compare the sustainability of the various management scenarios. The modeling methods were easy to apply and model predictions were sensitive to the differences among management scenarios, indicating that these methods may be useful for assessing and ranking the sustainability of forest management options. Primary concerns about the method are the practical difficulties associated with incorporating fire stochasticity in prediction uncertainty and the number of model assumptions that must be made and tested with sensitivity analysis. We wrote new software to bell) quantify the contribution of landscape stochasticity to model prediction uncertainty.",,"Wintle, BA|Bekessy, SA|Venier, LA|Pearce, JL|Chisholm, RA",CONSERVATION BIOLOGY,brown creeper landscape ecology population model population viability analysis succession model,10.1111/j.1523-1739.2005.00276.x
17,WOS:000269569700009,2009,Using a Monte Carlo approach to evaluate seawater intrusion in the Oristano coastal aquifer: A case study from the AQUAGRID collaborative computing platform,,"Uncertainties in the physical parameters of a groundwater system, due to the lack of direct access to the subsurface, strongly affect the design of water management policies, so that the risk of mismanagement becomes a critical factor in complex ecological and economic analyses. Stochastic modeling may help provide uncertainty quantification and also add robustness to the analysis by means of probabilistic forecasts. in this study a stochastic approach has been employed to model hydraulic conductivity of a confining formation in a multi-layered coastal aquifer system, under conditions of uncertainty. A Monte Carlo simulation, based on a coupled flow and transport groundwater D model, has been carried out to propagate the hydraulic conductivity parameter uncertainty to groundwater model outputs, namely pressure head and salt concentration. The aim of the study is to assess the risk of seawater intrusion into the aquifer by means of probabilistic threshold analysis on the simulated groundwater concentrations for different aquifer exploitation schemes. Maximum difference for nodal concentrations, with reference to homogeneous aquitard configuration, was found equal to %, proving how important can be the impact of the spatial variability of the hydraulic conductivity of the confining layer on the simulated salt concentrations. Such analysis enables to take better decisions about the management of the groundwater resource and to make additional field investigations consistent with environmental protection. The application workflow, based on the integration of both in-house developed and public domain software tools with hydrogeological data, has been deployed on a problem solving Grid platform (http://grida.crs.it). Further developments will include the planning of cost-effective additional field data acquisition based on the outcome of the stochastic model. (C) ", Elsevier Ltd. All rights reserved.,"Lecca, G|Cau, P",PHYSICS AND CHEMISTRY OF THE EARTH,3d groundwater model monte carlo simulation grid computing collaborative web platform optimal management of coastal aquifers,10.1016/j.pce.2009.03.002
18,WOS:000265341800006,2009,Estimating storm discharge and water quality data uncertainty: A software tool for monitoring and modeling applications,AGRICULTURAL WATERSHEDS SAMPLE PRESERVATION REACTIVE PHOSPHORUS INORGANIC-PHOSPHATE RISK-ASSESSMENT RIVER WATER NITROGEN STRATEGIES SEDIMENT SORPTION,"Uncertainty estimates corresponding to measured hydrologic and water quality data can contribute to improved monitoring design, decision-making, model application, and regulatory formulation. With these benefits in mind, the Data Uncertainty Estimation Tool for Hydrology and Water Quality (DUET-H/WQ) was developed from an existing uncertainty estimation framework for small watershed discharge, sediment, and N and P data. Both the software and its framework-basis utilize the root mean square error propagation methodology to provide uncertainty estimates instead of more rigorous approaches requiring detailed statistical information, which is rarely available. DUET-H/WQ lists published uncertainty information for data collection procedures to assist the user in assigning appropriate data-specific uncertainty estimates and then calculates the uncertainty for individual discharge, concentration, and load values. Results of DUET-H/WQ application in several studies indicated that substantial uncertainty can be contributed by each procedural category (discharge measurement, sample collection, sample preservation/storage, laboratory analysis, and data processing and management). For storm loads, the uncertainty was typically least for discharge (+/- -%), greater for sediment (+/- -%) and dissolved N and P (+/- -%) loads, and greater yet for total N and P (+/- -%). When these uncertainty estimates for individual values were aggregated within study periods (i.e. total discharge, average concentration, and total load), uncertainties followed the same pattern (Q < TSS < dissolved N and P < total N and P). This rigorous demonstration of uncertainty in discharge and water quality data illustrates the importance of uncertainty analysis and the need for appropriate tools. It is our hope that DUET-H/WQ contributes to making uncertainty estimation a routine data collection and reporting procedure and thus enhances environmental monitoring, modeling, and decision-making. Hydrologic and water quality data are too important for scientists to continue to ignore the inherent uncertainty.", Published by Elsevier Ltd.,"Harmel, RD|Smith, DR|King, KW|Slade, RM",ENVIRONMENTAL MODELLING & SOFTWARE,error propagation data collection hydrology nutrients watershed models,10.1016/j.envsoft.2008.12.006
19,WOS:000321425100048,2013,Life cycle assessment of the production of hydrogen and transportation fuels from corn stover via fast pyrolysis,LAND-USE CHANGE BIOMASS FAST PYROLYSIS BIO-OIL TECHNOECONOMIC ANALYSIS ETHANOL-PRODUCTION AQUEOUS FRACTION COMBUSTION CATALYSTS BIOFUELS SWITCHGRASS,"This life cycle assessment evaluates and quantifies the environmental impacts of the production of hydrogen and transportation fuels from the fast pyrolysis and upgrading of corn stover. Input data for this analysis come from Aspen Plus modeling, a GREET (Greenhouse Gases, Regulated Emissions, and Energy Use in Transportation) model database and a US Life Cycle Inventory Database. SimaPro . software is employed to estimate the environmental impacts. The results indicate that the net fossil energy input is . MJ and . MJ per km traveled for a light-duty vehicle fueled by gasoline and diesel fuel, respectively. Bio-oil production requires the largest fossil energy input. The net global warming potential (GWP) is . kg CO()eq and . kg CO()eq per km traveled for a vehicle fueled by gasoline and diesel fuel, respectively. Vehicle operations contribute up to % of the total positive GWP, which is the largest greenhouse gas footprint of all the unit processes. The net GWPs in this study are % and % lower than for petroleum-based gasoline and diesel fuel ( baseline), respectively. Biomass transportation has the largest impact on ozone depletion among all of the unit processes. Sensitivity analysis shows that fuel economy, transportation fuel yield, bio-oil yield, and electricity consumption are the key factors that influence greenhouse gas emissions.",,"Zhang, YN|Hu, GP|Brown, RC",ENVIRONMENTAL RESEARCH LETTERS,life cycle assessment fast pyrolysis bio-oil upgrading greenhouse gas emission energy demand,10.1088/1748-9326/8/2/025001
20,WOS:000276271600005,2010,Application of generic data assimilation tools (DATools) for flood forecasting purposes,,"This paper describes the generic data assimilation software tool DATools. DATools can be used as standalone or within Delft-FEWS. DATools is completely configurable via XML configuration. DATools is built up of three components: a Filter, a Stochastic Modeler, and a Stochastic Observer. Configuration of all these three parts is explained in detail. At the moment two data assimilation filters are available within DATools: () ensemble Kalman Filter and () the residual resampling filter. Results of a twin experiment with both filters with DATtools show similar results as a previous study performed with custom implementations. It is also shown that DATools can function inside Delft-FEWS software used for operational flood forecasting. Applying EnKF to a D hydrodynamic SOBEK-RE model of the river Rhine within the operational system FEWS-NL Rhine and Meuse improves the forecasts at the Lobith gaugin station and downstream of Lobith. DATools has been coupled with the HBV-, SOBEK, and REW models and will be coupled to MODFLOW, Delft-D, and the geotechnical model MSetlle in the near future. Uncertainty analysis with this tool is also possible and calibration will be added later this year. (C) ", Elsevier Ltd. All rights reserved.,"Weerts, AH|El Serafy, GY|Hummel, S|Dhondia, J|Gerritsen, H",COMPUTERS & GEOSCIENCES,data assimilation hydrology ensemble kalman filter residual resampling filter operational system delft-fews,10.1016/j.cageo.2009.07.009
21,WOS:000413389300005,2017,Detailed analysis of reverse osmosis systems in hot climate conditions,WATER DESALINATION MULTIOBJECTIVE OPTIMIZATION MEDITERRANEAN SEA PLANT-OPERATION ENERGY TEMPERATURE EFFICIENCY DESIGN IMPACT,"Hot climate countries require large amounts of desalinated water. The Reverse osmosis (RO) technique is currently considered the most reliable technique for brackish water and seawater desalination. However, its power consumption is considerably higher than all other techniques. Therefore, the present study investigates the performance of reverse osmosis plants in hot climate conditions. A typical Reverse osmosis system was designed, constructed and investigated. The ROSA software was also used for the analysis of seven different membrane elements. The experimental data were utilized in order to validate the simulation results of the ROSA software. A variance-based sensitivity analysis was performed in order to define the most effective design and operating parameters. The present investigation shows that the tap and brackish water membrane elements are more sensitive to the feed water temperature rather than the feed water pressure and concentration. Meanwhile, seawater membrane elements are more affected by the feed concentration. The detailed investigation of the different membrane elements shows that wastewater reclamation using reverse osmosis technology could be a significant source of low-cost fresh water for hot climate countries.",,"Shaaban, S|Yahya, H",DESALINATION,desalination reverse osmosis membrane sensitivity analysis optimization,10.1016/j.desal.2017.09.002
22,WOS:000227978000007,2005,"Investigating uncertainty and sensitivity in integrated, multimedia environmental models: tools for FRAMES-3MRA",FRAMEWORK SYSTEM,"Elucidating uncertainty and sensitivity structures in environmental models can be a difficult task, even for low-order, single-medium constructs driven by a unique set of site-specific data. Quantitative assessment of integrated, multimedia models that simulate hundreds of sites, spanning multiple geographical and ecological regions, will ultimately require a comparative approach using several techniques, coupled with sufficient computational power. The Framework for Risk Analysis in Multimedia Environmental Systems - Multimedia, Multipathway, and Multireceptor Risk Assessment (FRAMES-MRA) is an important software model being developed by the United States Environmental Protection Agency for use in risk assessment of hazardous waste management facilities. The MRA modeling system includes a set of  science modules that collectively simulate release, fate and transport, exposure, and risk associated with hazardous contaminants disposed of in land-based waste management units (WMU). The MRA model encompasses  multi-dimensional input variables, over  of which are explicitly stochastic. Design of SuperMUSE, a  GHz PC-based, Windows-based Supercomputer for Model Uncertainty and Sensitivity Evaluation is described. Developed for MRA and extendable to other computer models, an accompanying platform-independent, Java-based parallel processing software toolset is also discussed. For MRA, comparison of stand-alone PC versus SuperMUSE simulation executions showed a parallel computing overhead of only . seconds/simulation, a relative cost increase of .% over average model runtime. Parallel computing software tools represent a critical aspect of exploiting the capabilities of such modeling systems. The Java toolset developed here readily handled machine and job management tasks over the Windows cluster, and is currently capable of completing over  million MRA model simulations per month on SuperMUSE. Preliminary work is reported for an example uncertainty analysis of Benzene disposal that describes the relative importance of various exposure pathways in driving risk levels for ecological receptors and human health. Incorporating landfills, waste piles, aerated tanks, surface impoundments, and land application units, the site-based data used in the analysis included  facilities across the United States representing  site-WMU combinations.", Published by Elsevier Ltd.,"Babendreier, JE|Castleton, KJ",ENVIRONMENTAL MODELLING & SOFTWARE,multimedia model parallel computing pc-based supercomputing uncertainty analysis sensitivity analysis benzene disposal java,10.1016/j.envsoft.2004.09.013
23,WOS:000417388800006,2017,Direct effect of atmospheric turbulence on plume rise in a neutral atmosphere,LARGE-EDDY SIMULATION DIRECT NUMERICAL-SIMULATION POLLUTANT DISPERSION CROSS-FLOW BOUNDARY-LAYERS FINITE-ELEMENT CFD SIMULATION ENVIRONMENT TERRAINS MODELS,"The direct effect of atmospheric turbulence on plume rise in the current research work is studied through examining the turbulence intensity parameter. A hybrid unsteady Reynolds averaged Navier Stokes (RANS) and large eddy simulation (LES) numerical approach is applied with a new mixed scale sub-grid parameterization technique in the commercial ANSYS Fluent software in order to simulate the buoyant plume behavior in a turbulent crossflow. The accuracy of the simulation method is crosschecked against the wind tunnel data available in the literature. The numerical simulation results in various operating conditions are used to derive a new plume rise formula in which the direct effect of atmospheric turbulence intensity at stack height (I-Air) is explicitly introduced in the plume rise formula. Furthermore, the buoyancy parameter of the flue gas is determined at some distances upstream of the stack top surface to include the whole effects of source buoyancy on the plume rise. The value of I-Air at stack height is obtained by measuring the standard deviation of wind velocity at stack height. The sensitivity analysis showed that by increasing the atmospheric turbulence intensity, the final plume rise decreases because of the updraft and downdraft motions of turbulence and it has been found that there is a linear dependency between the plume rise and ( I-Air)(-.). The quantile-quantile plots show that the new model can predict the simulated plume rise with a deviation factor of . whereas the conventional models overestimate the final plume rise at least by a factor of ..", (C) 2017 Turkish National Committee for Air Pollution Research and Control. Production and hosting by Elsevier B.V. All rights reserved.,"Ashrafi, K|Orkomi, AA|Motlagh, MS",ATMOSPHERIC POLLUTION RESEARCH,neutral numerical model plume rise rans-les method turbulence,10.1016/j.apr.2017.01.001
24,WOS:000331688500003,2014,Parallel flow routing in SWMM 5,URBAN DRAINAGE SYSTEMS SENSITIVITY-ANALYSIS MODEL PERFORMANCE IMPACT,"The hydrodynamic rainfall-runoff and urban drainage simulation model SWMM (Storm Water Management Model) is a state of the art software tool applied likewise in research and practice. In order to reduce the computational burden of long simulation runs and to use the extra power of modern multicore computers, a parallel version of SWMM is presented herein. The challenge has been to modify the software in such minimal way that the resulting code enhancement may find its way into the commercial and non-commercial software tools that depend on SWMM for its calculation engine. A pragmatic approach to identify and enhance only the critical parts of the software in terms of run-time was chosen in order to keep the code changes as low as possible. The enhanced software was first tested for coherence against the original code and then benchmarked on four different input scenarios ranging from a very small village to a medium sized urban area. For the investigated sewer systems a speedup of six to ten times on a twelve core system was realized, thus decreasing the execution time to an acceptable level even for tedious system analysis. (C) ", Elsevier Ltd. All rights reserved.,"Burger, G|Sitzenfrei, R|Kleidorfer, M|Rauch, W",ENVIRONMENTAL MODELLING & SOFTWARE,multi-core openmp parallel computing storm water management model urban drainage modeling,10.1016/j.envsoft.2013.11.002
25,WOS:000253118500008,2008,"Borehole Optimisation System (BOS) - A case study assessing options for abstraction of urban groundwater in Nottingham, UK",TRIASSIC SANDSTONE AQUIFER BIODEGRADATION CONTAMINATION BIOTRANSFORMATION TRANSFORMATIONS TRICHLOROETHENE HYDROCARBONS ATTENUATION RECHARGE MTBE,"The recognition that urban groundwater is a potentially valuable resource for potable and industrial uses due to growing pressures on perceived less polluted rural groundwater has led to a requirement to assess the groundwater contamination risk in urban areas from industrial contaminants such as chlorinated solvents. The development of a probabilistic risk based management tool that predicts groundwater quality at potential new urban boreholes is beneficial in determining the best sites for future resource development. The Borehole Optimisation System (BOS) is a custom Geographic Information System (GIs) application that has been developed with the objective of identifying the optimum locations for new abstraction boreholes. BOS can be applied to any aquifer subject to variable contamination risk. The system is described in more detail by Tait et al. [Tait, N.G., Davison, J.J., Whittaker, J.J., Lehame, S.A. Lerner, D.N., a. Borehole Optimisation System (BOS) - a GIs based risk analysis tool for optimising the use of urban groundwater. Environmental Modelling and Software , -]. This paper applies the BOS model to an urban Permo-Triassic Sandstone aquifer in the city centre of Nottingham, UK. The risk of pollution in potential new boreholes from the industrial chlorinated solvent tetrachloroethene (PCE) was assessed for this region. The risk model was validated against contaminant concentrations from  actual field boreholes within the study area. In these studies the model generally underestimated contaminant concentrations. A sensitivity analysis showed that the most responsive model parameters were recharge, effective porosity and contaminant degradation rate. Multiple simulations were undertaken across the study area in order to create surface maps indicating areas of low PCE concentrations, thus indicating the best locations to place new boreholes. Results indicate that northeastern, eastern and central regions have the lowest potential PCE concentrations in abstraction groundwater and therefore are the best sites for locating new boreholes. These locations coincide with aquifer areas that are confined by low permeability Mercia Mudstone deposits. Conversely southern and northwestern areas are unconfined and have shallower depth to groundwater. These areas have the highest potential PCE concentrations. These studies demonstrate the applicability of BOS as a tool for informing decision makers on the development of urban groundwater resources. (c) ", Elsevier Ltd. All rights reserved.,"Tait, NG|Davison, RM|Leharne, SA|Lerner, DN",ENVIRONMENTAL MODELLING & SOFTWARE,borehole optimisation system gis pce probabilistic risk modelling urban groundwater,10.1016/j.envsoft.2007.09.001
26,WOS:000418736700067,2017,"A Practical, Robust Methodology for Acquiring New Observation Data Using Computationally Expensive Groundwater Models",EMPIRICAL ORTHOGONAL FUNCTIONS BAYESIAN EXPERIMENTAL-DESIGN REDUCED-ORDER MODEL PREDICTIVE UNCERTAINTY PARAMETER-IDENTIFICATION GENETIC ALGORITHM NETWORK DESIGN DATA-WORTH REDUCTION FLOW,"Regional groundwater flow models play an important role in decision making regarding water resources; however, the uncertainty embedded in model parameters and model assumptions can significantly hinder the reliability of model predictions. One way to reduce this uncertainty is to collect new observation data from the field. However, determining where and when to obtain such data is not straightforward. There exist a number of data-worth and experimental design strategies developed for this purpose. However, these studies often ignore issues related to real-world groundwater models such as computational expense, existing observation data, high-parameter dimension, etc. In this study, we propose a methodology, based on existing methods and software, to efficiently conduct such analyses for large-scale, complex regional groundwater flow systems for which there is a wealth of available observation data. The method utilizes the well-established d-optimality criterion, and the minimax criterion for robust sampling strategies. The so-called Null-Space Monte Carlo method is used to reduce the computational burden associated with uncertainty quantification. And, a heuristic methodology, based on the concept of the greedy algorithm, is proposed for developing robust designs with subsets of the posterior parameter samples. The proposed methodology is tested on a synthetic regional groundwater model, and subsequently applied to an existing, complex, regional groundwater system in the Perth region of Western Australia. The results indicate that robust designs can be obtained efficiently, within reasonable computational resources, for making regional decisions regarding groundwater level sampling. Plain Language Summary Water supply for the public, industry, and the environment can be heavily reliant on groundwater resources. Therefore, decision makers must be able to make predictions about how a groundwater system will respond to management options. These predictions often contain a significant degree of uncertainty. This uncertainty must be reduced in order for decision makers to make optimal use of groundwater resources with minimal risk to the environment. One way to reduce this uncertainty is to obtain more information about the nature of the groundwater system by collecting new measurement data from the study site. However, it is often not clear where and when to collect this data. This study proposes a new methodology for collecting data in an optimal fashion so that the information acquired is maximized. The method incorporates any existing information, examines the characteristics of uncertainty, and alleviates the high computing costs associated with conducting the necessary calculations. The procedure is applied to a regional groundwater system in the Perth area of Western Australia. The results are consistent with the hydrogeologic conceptualization of the Perth system, and provide important insight into where new observation wells could be constructed to obtain information about the hydraulic nature of faults.",,"Siade, AJ|Hall, J|Karelse, RN",WATER RESOURCES RESEARCH,groundwater modeling uncertainty assessment experimental design calibration monitoring network,10.1002/2017WR020814
27,WOS:000400267300014,2017,Hierarchical approach to hydrological model calibration,WATER-QUALITY MODELS UNCERTAINTY ANALYSIS RIVER-BASIN SWAT OPTIMIZATION FLOW EQUIFINALITY PREDICTIONS SENSITIVITY VALIDATION,"Hydrological models have been widely used for water resources management. Successful application of hydrological models depends on careful calibration and uncertainty analysis. Spatial unit of water balance calculations may differ widely in different models from grids to hydrological response units (HRU). The Soil and Water Assessment Tool (SWAT) software uses HRU as the spatial unit. SWAT simulates hydrological processes at sub-basin level by deriving HRUs by thresholding areas of soil type, land use, and slope combinations. This may ignore some important areas, which may have great impact on hydrological processes in the watershed. In this study, a hierarchical HRU approach was developed in order to increase model performance and reduce computational complexity simultaneously. For hierarchical optimization, HRUs are first divided into two-HRU types and are optimized with respect to some relevant influence parameters. Then, each HRU is further divided into two. Each child HRU inherits the optimum parameter values of the parent HRU as its initial value. This approach decreases the total calibration time while obtaining a better result. The performance of the hierarchical methodology is demonstrated on two basins, namely Sarisu-Eylikler and Namazgah Dam Lake Basins in Turkey. In Sarisu-Eylikler, we obtained good results by a combination of curve number (CN), soil hydraulic conductivity, and slope for generating HRUs, while in Namazgah use of only CN gave better results.",,"Ozdemir, A|Leloglu, UM|Abbaspour, KC",ENVIRONMENTAL EARTH SCIENCES,swat calibration hydrological response unit sufi2 optimization,10.1007/s12665-017-6560-6
28,WOS:000362848700009,2015,A Multi-Attribute Decision Analysis for Decommissioning Offshore Oil and Gas Platforms,UTILITY MEASUREMENT,"The  oil and gas platforms off the coast of southern California are reaching the end of their economic lives. Because their decommissioning involves large costs and potential environmental impacts, this became an issue of public controversy. As part of a larger policy analysis conducted for the State of California, we implemented a decision analysis as a software tool (PLATFORM) to clarify and evaluate decision strategies against a comprehensive set of objectives. Key options selected for in-depth analysis are complete platform removal and partial removal to  feet below the water line, with the remaining structure converted in place to an artificial reef to preserve the rich ecosystems supported by the platform's support structure. PLATFORM was instrumental in structuring and performing key analyses of the impacts of each option (e.g., on costs, fishery production, air emissions) and dramatically improved the team's productivity. Sensitivity analysis found that disagreement about preferences, especially about the relative importance of strict compliance with lease agreements, has much greater effects on the preferred option than does uncertainty about specific outcomes, such as decommissioning costs. It found a near-consensus of stakeholders in support of partial removal and ""rigs-to-reefs"" program. The project's results played a role in the decision to pass legislation enabling an expanded California ""rigs-to-reefs"" program that includes a mechanism for sharing cost savings between operators and the state.", (C) 2015 SETAC,"Henrion, M|Bernstein, B|Swamy, S",INTEGRATED ENVIRONMENTAL ASSESSMENT AND MANAGEMENT,decision analysis decommissioning multi-attribute utility oil and gas platforms rigs-to-reefs,10.1002/ieam.1693
29,WOS:000247276500009,2007,Application of non-linear automatic optimization techniques for calibration of HSPF,GLOBAL OPTIMIZATION MODELS,"Development of TMDLs (total maximum daily loads) is often facilitated by using the software system BASINS (Better Assessment Science Integrating point and Nonpoint Sources). One of the key elements of BASINS is the watershed model HSPF (Hydrological Simulation Program Fortran) developed by USEPA. Calibration of HSPF is a very tedious and time consuming task, more than  parameters are involved in the calibration process. In the current research, three non-linear automatic optimization techniques are applied and compared, as well an efficient way to calibrate HSPF is suggested. Parameter optimization using local and global optimization techniques for the watershed model is discussed. Approaches to automatic calibration of HSPF using the nonlinear parameter estimator PEST (Parameter Estimation Tool) with its Gauss-Marquardt-L evenberg (GMI-) method, Random multiple Search Method (RSM), and Shuffled Complex Evolution method developed at the University of Arizona (SCE-UA) are presented. Sensitivity analysis was conducted and the most and the least sensitive parameters were identified. It was noted that sensitivity depends on number of adjustable parameters. As more parameters were optimized simultaneously - a wider range of parameter values can maintain the model in the calibrated state. Impact of GML, RSM, and SCE-UA variables on ability to find the global minimum of the objective function (OF) was studied and the best variables are suggested. All three methods proved to be more efficient than manual HSPF calibration. Optimization results obtained by these methods are very similar, although in most cases RSM out performs GML and SCE-UA outperforms RSM. GML is a very fast method, it can perform as well as SCE-UA when the variables are properly adjusted, initial guess is good and insensitive parameters are eliminated from the optimization process. SCE-UA is very robust and convenient to use. Logical definition of key variables in most cases leads to the global minimum.",,"Iskra, I|Droste, R",WATER ENVIRONMENT RESEARCH,calibration hspf pest gauss-marquardt-levenberg method sce-ua,10.2175/106143007X156862
30,WOS:000346751800031,2014,Plant Modelling Framework: Software for building and running crop models on the APSIM platform,MEDICAGO-SATIVA L. WATER EXTRACTION PHENOLOGICAL DEVELOPMENT SYSTEMS SIMULATION TEMPERATE CLIMATE SOIL-WATER LUCERNE GROWTH WHEAT ENVIRONMENT,"The Plant Modelling Framework (PMF) is a software framework for creating models that represent the plant components of farm system models in the agricultural production system simulator (APSIM). It is the next step in the evolution of generic crop templates for APSIM, building on software and science lessons from past versions and capitalising on new software approaches. The PMF contains a top-level Plant class that provides an interface with the APSIM model environment and controls the other classes in the plant model. Other classes include mid-level Organ, Phenology, Structure and Arbitrator classes that represent specific elements or processes of the crop and sub-classes that the mid-level classes use to represent repeated data structures. It also contains low-level Function classes which represent generic mathematical, logical, procedural or reference code and provide values to the processes carried out by mid-level classes. A plant configuration file specifies which mid-level and Function classes are to be included and how they are to be arranged and parameterised to represent a particular crop model. The PMF has an integrated design environment to allow plant models to be created visually. The aims of the PMF are to maximise code reuse and allow flexibility in the structure of models. Four examples are included to demonstrate the flexibility of application of the PMF; . Slurp, a simple model of the water use of a static crop, . Oat, an annual grain crop model with detailed growth, development and resource use processes, . Lucerne. perennial forage model with detailed growth, development and resource use processes, . Wheat, another detailed annual crop model constructed using an alternative set of organ and process classes. These examples show the PMF can be used to develop models of different complexities and allows flexibility in the approach for implementing crop physiology concepts into model set up. (C)  The Authors.", Published by Elsevier Ltd.,"Brown, HE|Huth, NI|Holzworth, DP|Teixeira, EI|Zyskowski, RF|Hargreaves, JNG|Moot, DJ",ENVIRONMENTAL MODELLING & SOFTWARE,canopy dynamics biomass and nitrogen partitioning integrated design environment phenological and morphological development reusable organ and function classes,10.1016/j.envsoft.2014.09.005
31,WOS:000409151600011,2017,"Simulation, identification and statistical variation in cardiovascular analysis (SISCA) - A software framework for multi-compartment lumped modeling",SENSITIVITY-ANALYSIS BLOOD-FLOW ARTERIAL WINDKESSEL PRESSURE SYSTEM HEART,"It has not yet been possible to obtain modeling approaches suitable for covering a wide range of real world scenarios in cardiovascular physiology because many of the system parameters are uncertain or even unknown. Natural variability and statistical variation of cardiovascular system parameters in healthy and diseased conditions are characteristic features for understanding cardiovascular diseases in more detail. This paper presents SISCA, a novel software framework for cardiovascular system modeling and its MATLAB implementation. The framework defines a multi-model statistical ensemble approach for dimension reduced, multi-compartment models and focuses on statistical variation, system identification and patient-specific simulation based on clinical data. We also discuss a data-driven modeling scenario as a use case example. The regarded dataset originated from routine clinical examinations and comprised typical pre and post surgery clinical data from a patient diagnosed with coarctation of aorta. We conducted patient and disease specific pre/post surgery modeling by adapting a validated nominal multi-compartment model with respect to structure and parametrization using metadata and MRI geometry. In both models, the simulation reproduced measured pressures and flows fairly well with respect to stenosis and stent treatment and by pre-treatment cross stenosis phase shift of the pulse wave. However, with post-treatment data showing unrealistic phase shifts and other more obvious inconsistencies within the dataset, the methods and results we present suggest that conditioning and uncertainty management of routine clinical data sets needs significantly more attention to obtain reasonable results in patient-specific cardiovascular modeling.",,"Huttary, R|Goubergrits, L|Schutte, C|Bernhard, S",COMPUTERS IN BIOLOGY AND MEDICINE,windkessel elements lumped models od modeling multi-compartment modeling cardiovascular simulation distributed parameter modeling clinical data set coarctation of aorta patient-specific models disease-specific models multiscale modeling,10.1016/j.compbiomed.2017.05.021
32,WOS:000414958700005,2017,Spatial analysis and health risk assessment of heavy metals concentration in drinking water resources,MONTE-CARLO-SIMULATION SURFACE-WATER SENSITIVITY-ANALYSIS GROUNDWATER CONTAMINATION POLLUTION CHINA RIVER GIS EXPOSURE,"The heavy metals available in drinking water can be considered as a threat to human health. Oncogenic risk of such metals is proven in several studies. Present study aimed to investigate concentration of the heavy metals including As, Cd, Cr, Cu, Fe, Hg, Mn, Ni, Pb, and Zn in  water supply wells and  water reservoirs within the cities Ardakan, Meibod, Abarkouh, Bafgh, and Bahabad. The spatial distribution of the concentration was carried out by the software ArcGIS. Such simulations as non-carcinogenic hazard and lifetime cancer risk were conducted for lead and nickel using Monte Carlo technique. The sensitivity analysis was carried out to find the most important and effective parameters on risk assessment. The results indicated that concentration of all metals in  wells (except iron in  cases) reached the levels mentioned in EPA, World Health Organization, and Pollution Control Department standards. Based on the spatial distribution results at all studied regions, the highest concentrations of metals were derived, respectively, for iron and zinc. Calculated HQ values for non-carcinogenic hazard indicated a reasonable risk. Average lifetime cancer risks for the lead in Ardakan and nickel in Meibod and Bahabad were shown to be . x (-), . x (-), and  x (-), respectively, demonstrating high carcinogenic risk compared to similar standards and studies. The sensitivity analysis suggests high impact of concentration and BW in carcinogenic risk.",,"Fallahzadeh, RA|Ghaneian, MT|Miri, M|Dashti, MM",ENVIRONMENTAL SCIENCE AND POLLUTION RESEARCH,groundwater metals health risk assessment monte carlo simulation sensitivity analysis geographic information systems,10.1007/s11356-017-0102-3
33,WOS:000389389400004,2016,Simulation of environmental impact of an existing natural gas dehydration plant using a combination of thermodynamic models,SENSITIVITY-ANALYSIS MIXING RULES EQUATIONS STATE,"A new approach was presented to improve the simulation results of an existing TEG based natural gas dehydration plant, using Aspen Plus software. Furthermore, the environmental impact of the plant was investigated. The plant consists of four main unit operations including an absorber, a flash tank, a stripper and a regenerator. Twelve thermodynamic models were assigned to these units. In the first step of the study, only one thermodynamic model was assigned to all of the units while in other steps, combinations of thermodynamic models were employed. The most accurate model combination was found to be RKSMHV for the absorber and stripper and PSRK for the flash tank and regenerator. It was found that a proper combination of thermodynamic models may improve the simulation results. As solvent circulation rate increased, BTEX, VOC and greenhouse gas emissions enhanced. (C)  Institution of Chemical Engineers.", Published by Elsevier B.V. All rights reserved.,"Torkmahalleh, MA|Magazova, G|Magazova, A|Rad, SJH",PROCESS SAFETY AND ENVIRONMENTAL PROTECTION,natural gas dehydration btex voc greenhouse gas aspen plus thermodynamic models,10.1016/j.psep.2016.08.008
34,WOS:000419231500134,2017,Passive Optimization Design Based on Particle Swarm Optimization in Rural Buildings of the Hot Summer and Warm Winter Zone of China,ENERGY PERFORMANCE RESIDENTIAL BUILDINGS SENSITIVITY-ANALYSIS MULTIOBJECTIVE OPTIMIZATION HIGHRISE BUILDINGS THERMAL COMFORT SIMULATION CONSUMPTION ENVELOPE SYSTEM,"The development of green building is an important way to solve the environmental problems of China's construction industry. Energy conservation and energy utilization are important for the green building evaluation criteria (GBEC). The objective of this study is to evaluate the quantitative relationship between building shape parameter, envelope parameters, shading system, courtyard and the energy consumption (EC) as well as the impact on indoor thermal comfort of rural residential buildings in the hot summer and warm winter zone (HWWZ). Taking Quanzhou (Fujian Province of China) as an example, based on the field investigation, EnergyPlus is used to build the building performance model. In addition, the classical particle swarm optimization algorithm in GenOpt software is used to optimize the various factors affecting the EC. Single-objective optimization has provided guidance to the multi-dimensional optimization and regression analysis is used to find the effects of a single input variable on an output variable. Results shows that the energy saving rate of an optimized rural residence is about -% corresponding to the existing rural residence. Moreover, the payback period is about  years. A simple case study is used to demonstrate the accuracy of the proposed optimization analysis. The optimization can be used to guide the design of new rural construction in the area and the energy saving transformation of the existing rural houses, which can help to achieve the purpose of energy saving and comfort.",,"Lu, SL|Wang, R|Zheng, SQ",SUSTAINABILITY,rural residence green building energy consumption multidimensional optimization particle swarm optimization regression analysis,10.3390/su9122288
35,WOS:000320010900007,2013,UNCERTAINTY QUANTIFICATION IN DAMAGE MODELING OF HETEROGENEOUS MATERIALS,POLYMER COMPOSITES FAILURE ANALYSIS MATRIX CRACKING CARBON-FIBER FATIGUE CALIBRATION BEHAVIOR,"This manuscript investigates the use of Bayesian statistical methods for calibration and uncertainty quantification in rate-dependent damage modeling of composite materials. The epistemic and aleatory uncertainties inherent in the model prediction due to model parameter uncertainty, model form error, solution approximations, and measurement errors are investigated. Gaussian process surrogate models are developed to replace expensive finite element models in the analysis. A viscous damage model is employed with a solution algorithm designed for implementation within a commercial finite element software package (Abaqus). Experimental results from a suite of monotonic load tests conducted on unidirectional glass fiber reinforced epoxy composite samples at multiple strain rates and strain orientations are used to quantify the uncertainty in the prediction of the composite response within a Bayesian framework.",,"Bogdanor, MJ|Mahadevan, S|Oskay, C",INTERNATIONAL JOURNAL FOR MULTISCALE COMPUTATIONAL ENGINEERING,composite materials bayesian calibration rate-dependent damage gaussian process surrogate model,10.1615/IntJMultCompEng.2013005821
36,WOS:000355760000014,2015,Optimization of the motion control mechanism of the hatch door of airliner,FLEXIBLE MULTIBODY SYSTEMS DESIGN SENSITIVITY-ANALYSIS FINITE SEGMENT APPROACH DYNAMIC-SYSTEMS RECURSIVE FORMULATION PARALLEL MANIPULATORS LOOP SYSTEMS EQUATIONS METHODOLOGY ALGORITHM,"This paper deals with the problem of parameter optimization of the motion control mechanism of the hatch door of ARJ-, a regional airliner of China. Motion improvement of the hatch door is implemented by two kinds of passive designs. Firstly, a single-layer optimization model for trajectory modification is developed to find the optimum size of the key parts of the control mechanism. Secondly, a novel nested bi-level optimization model is presented for the design of the size tolerance limits of the selected parts. The design objective is minimization of the total extremum deviation of the motion trajectory of the objective point of the hatch door, where the extremum deviation is obtained by solution of the inner-level size optimization problem for the fixed size tolerance limits. The optimization models for motion control of the hatch door mechanism are solved using the response surface method. Numerical examples show that the precision of the real running trajectory of the objective point of the hatch door mechanism may be improved effectively by using the methods presented. A home-made multi-body dynamics solver (THUSOLVER) and the corresponding optimization software have been developed to implement the above tasks.",,"Du, JB|Huang, ZT|Yang, RZ",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,bi-level optimization model motion control mechanism tolerance design response surface method size optimization,10.1007/s00158-014-1191-y
37,WOS:000388119400013,2016,ASSESSMENT OF INDUSTRIAL SOLID WASTE USING THE INTELLIGENT DECISION SYSTEM (IDS) METHOD,EVIDENTIAL REASONING APPROACH MANAGEMENT UNCERTAINTY SAFETY,"About  tons of daily solid waste disposal is one of the consequences of the speedy industrial expansion in the province of Khuzestan in the south west of Iran. There are more often than not diverse criteria for assessing the resulted pollution loads from solid waste disposal. In this paper, a new application for the Intelligent Decision System (IDS) is demonstrated for industrial solid waste assessment. IDS software is a Windows-based package for handling Multiple Criteria Decision Making (MCDM) problems considering both qualitative and quantitative criteria under uncertainties. The basis of IDS is a recently developed theory named the Evidential Reasoning ( ER) approach. The major features, superiority and excellence of IDS will be clarified through its application to the ranking of the industrial units located in the Khuzestan Province. Moreover, as a complimentary assessment, a sensitivity analysis is carried out in which the effect of decision maker's attitude toward risk on the total utility which each industry would gain is investigated. The results show that Ahwaz, Abadan, and Khoramshahr are respectively the three most polluting cities in the Khuzestan province. It is concluded that IDS can be utilized not only to handle problems which traditional methods can work out, but also to arrange and evaluate more difficult decision problems that traditional methods are not sufficiently expert of handling.",,"Abed-Elmdoust, A|Kerachian, R",ENVIRONMENTAL ENGINEERING AND MANAGEMENT JOURNAL,evidential reasoning approach industrial solid waste assessment intelligent decision system khuzestan province multiple-criteria-decision-making (mcdm),10.30638/eemj.2016.191
38,WOS:000262275400013,2008,Evaluating the effect of scale in flood inundation modelling in urban environments,SCANNING LASER ALTIMETRY DIFFUSION-WAVE TREATMENT RASTER-BASED MODEL RESOLUTION SIMULATION VEGETATION 1D,"Cellular-based approaches for flood inundation modelling have been extensively calibrated and evaluated for this prediction of flood flows on rural river reaches. However, there has only been limited application of these approaches to urban environments, where the need for flood management is greatest. Practical application of two-dimensional (D) flood inundation models is often limited by computation time and processing power on standard desktop PCs when attempting to resolve flows on the high-resolution grids necessary to replicate urban features. Consequently, it is necessary to evaluate the effectiveness of coarse grids to represent flood flows through urban environments. To examine these effects, LOSFLOOD-FP, a D storage cell model, is applied to hypothetical flooding scenarios in Greenfields, Glasgow, Grid resampling techniques in GIS software packages are evaluated and a bilinear griddling technique appears to provide the most accurate and physically intuitive results. A gridding method maintaining sharp elevation changes at building interfaces and neighbouring land is presented and estimates of the discretization noise associated with the coarse resolution grids suggest little improvement over current gridding methods. The variation in model results from the friction sensitivity analysis suggests a non-stationary response to Manning's n with changing model resolution. Model results suggests that a coarse resolution model for urban applications is limited by the representation or urban media in coarse model grids. Furthermore, critical length scales related to building dimensions and building separation distances exist in urban areas that determine maximum possible grid resolutions for hydraulic models of urban flooding."," Copyright (C) 2008 John Wiley & Sons, Ltd.","Fewtrell, TJ|Bates, PD|Horritt, M|Hunter, NM",HYDROLOGICAL PROCESSES,friction sensitivity hydraulic modelling scale urban flooding,10.1002/hyp.7148
39,WOS:000306721000030,2012,Assessing health impacts of CO2 leakage from a geological storage site into buildings: Role of attenuation in the unsaturated zone and building foundation,CARBON SEQUESTRATION SITES SUBCRITICAL CONDITIONS INCLUDING TRANSITIONS SALINE AQUIFERS PHASE-CHANGE GASEOUS CO2 GAS HAZARD TRANSPORT MODEL DIOXIDE,"Geological storage of the greenhouse gas CO has the potential to be a widespread and effective option to mitigate climate change. As any industrial activity, CO storage may lead to adverse impact on human health and the environment in the case of unexpected leakage from the reservoir. These potential impacts should be considered in a risk assessment process. We present an approach to assess the impacts on human health in case of CO leakage emerging in the unsaturated zone under a building. We first focus on the migration of the CO in the unsaturated zone and the foundation through numerical simulation with sensitivity analysis. Our results show that the intrusion of CO into a building is substantially attenuated by the unsaturated zone and the foundation and may lead only under very specific conditions (very low ventilated parts of buildings, high flow rate and/or building situated very close to a leaking pathway) to hazardous CO indoor concentrations. We have then integrated the former results in a global toolbox that provides an efficient and easy-to-use tool for decision support, which enables to assess the impacts on human health of CO leakage from the reservoir to a building. (C) ", Elsevier Ltd. All rights reserved.,"de Lary, L|Loschetter, A|Bouc, O|Rohmer, J|Oldenburg, CM",INTERNATIONAL JOURNAL OF GREENHOUSE GAS CONTROL,geological storage leakage co2 health unsaturated zone indoor exposure,10.1016/j.ijggc.2012.04.011
40,WOS:000282320800018,2010,Effect of fracture zone on DNAPL transport and dispersion: a numerical approach,MULTIPHASE FLOW GROUNDWATER DISSOLUTION MIGRATION AQUIFER POOLS MEDIA,"Two numerical simulation techniques have been used to identify a suitable method to assist in the characterization of DNAPL movement within fractured porous rock aquifers. Both MODFLOW and UTCHEM software modeling suites were used to simulate different scenarios in fracture dip and hydraulic conductivities. The complexity and the physical structure of fracture characterization were shown to have a significant effect on modeling results, to the extent that fracture zone should be characterized fully before simulation models are used for DNAPL simulations. Sensitivity analysis was conducted on both the hydraulic conductivity and fracture dip values. DNAPL movement in the subsurface showed a high sensitivity to fracture dip variation.",,"Dennis, I|Pretorius, J|Steyl, G",ENVIRONMENTAL EARTH SCIENCES,dnapl fracture dip numerical simulation hydraulic conductivity,10.1007/s12665-010-0468-8
41,WOS:000404558100014,2017,"RIVICE-A Non-Proprietary, Open-Source, One-Dimensional River-Ice Model",SAINT JOHN RIVER ATHABASCA RIVER COVER FORMATION DAUPHIN RIVER CANADA SASKATCHEWAN MANITOBA FLOODS FLOWS RISK,"Currently, no river ice models are available that are free and open source software (FOSS), which can be a hindrance to advancement in the field of modelling river ice processes. This paper introduces a non-proprietary (conditional), open-source option to the scientific and engineering community, the River Ice Model (RIVICE). RIVICE is a one-dimensional, fully-dynamic wave model that mimics key river ice processes such as ice generation, ice transport, ice cover progression (shoving, submergence and juxtapositioning) and ice jam formation, details of which are highlighted in the text. Three ice jam events at Fort McMurray, Alberta, along the Athabasca River, are used as case studies to illustrate the steps of model setup, model calibration and results interpretation. A local sensitivity analysis reveals the varying effects of parameter and boundary conditions on backwater flood levels as a function of the location of ice jam lodgment along the river reach and the location along the ice jam cover. Some limitations of the model and suggestions for future research and model development conclude the paper.",,"Lindenschmidt, KE",WATER,athabasca river fort mcmurray ice jam flooding river ice modelling rivice,10.3390/w9050314
42,WOS:000253663600007,2008,A computational algorithm for the multiple generation of nonlinear mathematical models and stability study,,"This paper presents an algorithm that generates families of mathematical models with nonlinear parameters, and includes the study of linear models, based on the experimental data of the intervening variables. The implementation of this algorithm has been named poly-model and is based on the application of the Gauss-Newton algorithm for obtaining the parameters of nonlinear models [Verdu F. Un Algoritmo para la construccion multiple de modelos matematicos no lineales y el estudio de su estabilidad. Doctoral Tesis. Universidad de Alicante, ]. One of its characteristics is a search among different nonlinear models within the parameters; unlike the methods found in the scientific literature [Camacho Rosales J. Estadistica con SPSS para windows. Ed. Ra-Ma, ; Mathsoft Inc. Splus-. Guide to Statistics. Seattle, ], the user does not intervene in their generation. A pruning criteria has also been introduced that is based on the stability analysis of models generated from perturbations, applying studies carried out by the authors and published in [Verdu F, Villacampa Y. A computer program for a Monte Carlo analysis of sensitivity in equations of environmental modelling obtained from experimental data. Advances in Engineering Software, ]. Object-oriented Pascal has been used in Delphi .", (c) 2007 Published by Elsevier Ltd.,"Verdu, F|Villacampa, Y",ADVANCES IN ENGINEERING SOFTWARE,modelling nonlinear regression sensitivity analysis,10.1016/j.advengsoft.2007.03.004
43,WOS:000249177200007,2007,Determination of skin and aquifer parameters for a slug test with wellbore-skin effect,OPTIMAL GROUNDWATER-MANAGEMENT PARTIALLY PENETRATING WELLS FINITE-THICKNESS SKIN HYDRAULIC CONDUCTIVITY,"Slug test is considered to reflect the hydraulic parameters in the vicinity of the test well. The aquifer parameters are usually identified by fitting an appropriate mathematical solution or graphical type curves with slug test data. In this paper, we developed an approach by combining [Moench, A.F., Hsieh, P.A., . Analysis of slug test data in a well with finite-thickness skin. In: Memoirs of the th International Congress on the Hydrogeology of Rocks of Low Permeability, U.S.A. Members of the International Association of Hydrologists, Tucson, AZ, vol. , pp. -] and simulated annealing (SA) approach to estimate five parameters, i.e., three skin parameters and two aquifer parameters. The three skin parameters are hydraulic conductivity, specific storage, and thickness of the wellbore-skin zone, white the two aquifer parameters are hydraulic conductivity and specific storage of the formation zone. It is worthy to note although the thickness of the wellbore-skin zone is usually taken as an input data for the data-analyzed software, it is actually an unknown parameter that cannot be measured directly. This paper proposes a methodology for estimating the thickness of the wellbore-skin zone with other hydraulic parameters at the same time. Eight sets of well water-level. (WWL) data of aquifers with both positive and negative skins are generated by Moench and Hsieh [Moench and Hsieh, ] and four sets of standard normally distributed noise are then added to each set of WWL data. The results indicate that the negative-skin cases generally give a better estimated result than that of the positive-skin cases. Sensitivity analysis is also employed to demonstrate the physical behavior when slug test was performed under positive-skin effect. For the case of an aquifer with a positive-skin, the use of a longer series of WWL data for analysis is strongly recommended for better estimation of aquifer hydraulic conductivity. Analyzing the WWL data of the test and observation wells simultaneously could significantly improve the estimations on specific storages. Impetuously presuming an arbitrary value for the thickness of the wellbore-skin zone may lead to poor estimation for the other four parameters.", (C) 2007 Elsevier B.V. All rights reserved.,"Yeh, HD|Chen, YJ",JOURNAL OF HYDROLOGY,groundwater parameter estimation sensitivity analysis simulated annealing skin thickness slug test,10.1016/j.jhydrol.2007.05.029
44,WOS:000227978000005,2005,Sensitivity testing of a model for exploring water resources utilisation and management options,NORTHERN THAILAND HYDROLOGIC RESPONSE UNGAUGED CATCHMENTS YIELD LAND,"This paper investigates the model sensitivities to input parameter values in a Biophysical Toolbox for integrated catchment assessment and management of land and water resources. The toolbox was developed for application in the highland regions of northern Thailand. It incorporates the IHACRES rainfall-runoff model, a crop model (CATCHCROP), and an erosion model (a Universal Soil Loss Equation [USLE], modified for application to northern Thailand). In developing the individual models in the Biophysical Toolbox, emphasis was placed on limiting model complexity. The toolbox was developed and tested using data from the Mae Chaem catchment in northern Thailand. Due to the short duration and sparse distribution of the available data, the complexity of the model structure is constrained to consider only the key processes of interest. Despite the relative simplicity of the individual models, linkages between the models encompassed in the Biophysical Toolbox increase the complexity of the modelling system. This paper explores sensitivities in the Biophysical Toolbox to the parameters of CATCHCROP as this component has the greatest potential for propagating errors though the Toolbox. A simple sensitivity analysis was undertaken, whereby parameter values were perturbed to ascertain the effect of these perturbations on output indicators. The hydrologic component showed strong non-linearity to the infiltration parameters of the CATCHCROP model, although this did not greatly impact estimates of total annual discharge. Additionally, a number of CATCHCROP parameters did not greatly impact some output indicators of the toolbox. While the CATCHCROP model is relatively parsimonious when compared with many crop models, there are still  model parameters that must be determined from the field or prescribed from the literature. There is potential for the model to be further simplified although more extensive model testing is required to ensure that such simplifications would not adversely impact on the utility of the toolbox. (c) ", Elsevier Ltd. All rights reserved.,"Merritt, WS|Croke, BFW|Jakeman, AJ",ENVIRONMENTAL MODELLING & SOFTWARE,water resources catchcrop ihacres northern thailand sensitivity analysis,10.1016/j.envsoft.2004.09.011
45,WOS:000224365000004,2004,MVC1: an integrated MatLab toolbox for first-order multivariate calibration,PARTIAL LEAST-SQUARES ORTHOGONAL SIGNAL CORRECTION WAVELENGTH SELECTION PROPAGATION ALGORITHM ERROR PLS,"Multivariate calibration  (MVC), a MatLab(R) toolbox for implementing up to  different first-order calibration methodologies through easily managed graphical user interfaces, is presented. The toolbox accepts different input data formats (either arranged as matrices or vectors contained in raw data files or in already existing MatLab variables) and incorporates many preprocessing algorithms in order to improve prediction capabilities. The development and validation of each model and its subsequent application to unknown samples are straightforward. Prediction results are produced along analytical figures of merit and standard errors calculated by uncertainty propagation. Moreover, the toolbox allows one to manually select working sensor regions, or to automatically find which region provides the minimum error. It also generates many different plots regarding model performance, including outliers detection, facilitating both model evaluation and interpretation.", (C) 2004 Elsevier B.V. All rights reserved.,"Oliveri, AC|Goicoechea, HC|Inon, FA",CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS,,10.1016/j.chemolab.2004.03.004
46,WOS:000259895700004,2008,Design of sustainable chemical processes: Systematic retrofit analysis generation and evaluation of alternatives,EFFICIENCY NETWORKS,"The objective of this paper is to present a generic and systematic methodology for identifying the feasible retrofit design alternatives of any chemical process. The methodology determines a set of mass and energy indicators from steady-state process data, establishes the operational and design targets, and through a sensitivity-based analysis, identifies the design alternatives that can match a set of design targets. The significance of this indicator-based method is that it is able to identify alternatives, where one or more performance criteria (factors) move in the same direction thereby eliminating the need to identify tradeoff-based solutions. These indicators are also able to reduce (where feasible) a set of safety indicators. An indicator sensitivity analysis algorithm has been added to the methodology to define design targets and to generate sustainable process alternatives. A computer-aided tool has been developed to facilitate the calculations needed for the application of the methodology. The application of the indicator-based methodology and the developed software are highlighted through a process flowsheet for the production of vinyl chlorine monomer (VCM). (C)  The Institution of Chemical Engineers.", Published by Elsevier B.V. All rights reserved.,"Carvalho, A|Gani, R|Matos, H",PROCESS SAFETY AND ENVIRONMENTAL PROTECTION,process design sustainability metrics mass and energy indicators safety index indicator sensitivity algorithm,10.1016/j.psep.2007.11.003
47,WOS:000397072800012,2017,On the variational data assimilation problem solving and sensitivity analysis,LAPLACE TRANSFORM INVERSION COVARIANCE MATRICES CONDITION NUMBER REGULARIZATION IMPLEMENTATION,"We consider the Variational Data Assimilation (VarDA) problem in an operational framework, namely, as it results when it is employed for the analysis of temperature and salinity variations of data collected in closed and semi closed seas. We present a computing approach to solve the main computational kernel at the heart of the VarDA problem, which outperforms the technique nowadays employed by the oceanographic operative software. The new approach is obtained by means of Tikhonov regularization. We provide the sensitivity analysis of this approach and we also study its performance in terms of the accuracy gain on the computed solution. We provide validations on two realistic oceanographic data sets.", (C) 2017 Elsevier Inc. All rights reserved.,"Arcucci, R|D'Amore, L|Pistoia, J|Toumi, R|Murli, A",JOURNAL OF COMPUTATIONAL PHYSICS,data assimilation sensitivity analysis inverse problem,10.1016/j.jcp.2017.01.034
48,WOS:000325218500007,2013,Uncertainty in floodplain delineation: expression of flood hazard and risk in a Gulf Coast watershed,,"This paper investigates the development of flood hazard and flood risk delineations that account for uncertainty as improvements to standard floodplain maps for coastal watersheds. Current regulatory floodplain maps for the Gulf Coastal United States present % flood hazards as polygon features developed using deterministic, steady-state models that do not consider data uncertainty or natural variability of input parameters. Using the techniques presented here, a standard binary deterministic floodplain delineation is replaced with a flood inundation map showing the underlying flood hazard structure. Additionally, the hazard uncertainty is further transformed to show flood risk as a spatially distributed probable flood depth using concepts familiar to practicing engineers and software tools accepted and understood by regulators. A case study of the proposed hazard and risk assessment methodology is presented for a Gulf Coast watershed, which suggests that storm duration and stage boundary conditions are important variable parameters, whereas rainfall distribution, storm movement, and roughness coefficients contribute less variability. The floodplain with uncertainty for this coastal watershed showed the highest variability in the tidally influenced reaches and showed little variability in the inland riverine reaches. Additionally, comparison of flood hazard maps to flood risk maps shows that they are not directly correlated, as areas of high hazard do not always represent high risk."," Copyright (c) 2012 John Wiley & Sons, Ltd.","Christian, J|Duenas-Osorio, L|Teague, A|Fang, Z|Bedient, P",HYDROLOGICAL PROCESSES,flood hazard flood risk floodplain mapping uncertainty analysis coastal watersheds storm surge,10.1002/hyp.9360
49,WOS:000263990600027,2009,Life cycle assessment study of a Chinese desktop personal computer,POLYBROMINATED DIPHENYL ETHERS MOBILE PHONE NETWORKS E-WASTE IMPACT ASSESSMENT FLOW,"Associated with the tremendous prosperity in world electronic information and telecommunication industry, there continues to be an increasing awareness of the environmental impacts related to the accelerating mass production, electricity use, and waste management of electronic and electric products (e-products). China's importance as both a consumer and supplier of e-products has grown at an unprecedented pace in recent decade. Hence, this paper aims to describe the application of life cycle assessment (LCA) to investigate the environmental performance of Chinese e-products from a global level. A desktop personal computer system has been selected to carry out a detailed and modular LCA which follows the ISO  series. The LCA is constructed by SimaPro, software version . and expressed with the Eco-indicator' life cycle impact assessment method. For a sensitivity analysis of the overall LCA results, the so-called CML method is used in order to estimate the influence of the choice of the assessment method on the result Life cycle inventory information is complied by ecoinvent . databases, combined with literature and field investigations on the present Chinese situation. The established LCA study shows that that the manufacturing and the use of such devices are of the highest environmental importance. In the manufacturing of such devices, the integrated circuits (Is) and the Liquid Crystal Display (LCD) are those parts contributing most to the impact. As no other aspects are taken into account during the use phase, the impact is due to the way how the electricity is produced. The final process steps - i.e. the end of life phase - lead to a clear environmental benefit if a formal and modem, up-to-date technical system is assumed, like here in this study.", Crown Copyright (C) 2008 Published by Elsevier B.V. All rights reserved.,"Duan, HB|Eugster, M|Hischier, R|Streicher-Porte, M|Li, JH",SCIENCE OF THE TOTAL ENVIRONMENT,lca personal computer environmental impact electronics china,10.1016/j.scitotenv.2008.10.063
50,WOS:000363966900091,2015,Energy analyses and greenhouse gas emissions assessment for saffron production cycle,CROCUS-SATIVUS L. ARTIFICIAL NEURAL-NETWORKS CROP PRODUCTION SYSTEMS INPUT-OUTPUT-ANALYSIS SENSITIVITY-ANALYSIS ECONOMICAL ANALYSIS CARBON SEQUESTRATION ACTIVE CONSTITUENTS POTATO PRODUCTION WHEAT PRODUCTION,"Population growth and world climate changes are putting high pressure on agri-food production systems. Exacerbating use of energy sources and expanding the environmental damaging symptoms are the results of these difficult situations. This study was conducted to determine the energy balance for saffron production cycle and investigate the corresponding greenhouse gas (GHG) emissions in Iran. Saffron (Crocus sativus L.) is one of the main spice that historically cultivated in Iran. Data were obtained from  randomly selected saffron growers using a face to face questionnaire technique. The results revealed that in  years of saffron production cycle, the overall input and output energy use were to be ,. and ,. MJ ha(-), respectively. The highest-level of energy consumption belongs to seeds (. %) followed by chemical fertilizers (. %). Energy use efficiency, specific energy, net energy, and energy productivity of saffron production were ., . MJ kg(-), ,.MJ ha(-), and . kgMJ(-), respectively. The result shows that the cultivation of saffron emits . kg CO()eq. ha(-) greenhouse gas, in which around . % belonged to electricity followed by chemical fertilizers. In addition the Cobb-Douglas production function was applied into EViews  software to define the functional relationship. The results of econometric model estimation showed that the impact of human labor, electricity, and water for irrigation on stigma, human labor, electricity, and seed on corm and also human labor and farmyard manure (FYM) on flower and leaf yield were found to be statistically significant. Sensitivity analysis results of the energy inputs demonstrated that the marginal physical productivity (MPP) worth of electricity energy was the highest for saffron stigma and corm, although saffron flower and leaf had more sensitivity on chemicals energy inputs. Moreover, MPP values of renewable and indirect energies were higher than non-renewable and direct energies, respectively.",,"Bakhtiari, AA|Hematian, A|Sharifi, A",ENVIRONMENTAL SCIENCE AND POLLUTION RESEARCH,energy input efficiency environment econometric model ghgemissions cobb-douglas function,10.1007/s11356-015-4843-6
51,WOS:000255415000009,2008,Estimation of process parameter variations in a pre-defined process window using a Latin hypercube method,,"The aim of this paper is to present a methodology that provides an analytical tool for estimation of robustness and response variation within a pre-defined process window. To exemplify the developed methodology, the stochastic simulation technique is used for a sheet-metal forming application. A sampling plan based on the Latin hypercube sampling method for variation of design parameters is utilized, and the thickness reduction is specified as the response. Moreover, the response surface methodology is applied for understanding the quantitative relationship between design parameters and response value. The conclusions of this study are that the applied method gives a possibility to illustrate and interpret the variation of the response versus a design parameter variation. Consequently, it gives significant insights into the usefulness of individual design parameters. It has been shown that the method enables us to estimate the admissible design parameter variations and to predict the actual safe margin for given process parameters. Furthermore, the dominating design parameters can be predicated using sensitivity analysis, and this in its turn clarifies how the reliability criteria are met. Finally, the developed software can be used as an additional module for set-up of stochastic finite element simulations and to collect the numerical results from different solvers within different applications.",,"Moshfegh, R|Nilsson, L|Larsson, M",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,stochastical analysis sensitivity indicator admissible process parameter variation finite element method sheet-metal forming,10.1007/s00158-007-0136-0
52,WOS:000295029800004,2011,A simulation of the fate of nitrogen in an on-site wastewater treatment system,DYNAMIC-MODEL NITRIFICATION VALIDATION,"An experiment to study and to build a mathematical model to reproduce the behavior of nitrogen in a residential wastewater treatment process was performed. A pilot unit receiving grey and black water was used. The pilot consisted in a septic tank followed by a fixed-film, partly aerated bioreactor with effluent recirculation operated under two different scenarios: normal operating conditions and increased influent flow. Modeling was performed with the GPS-X (TM) software. Following a sensitivity analysis, the model was calibrated by comparing results from the pilot experiment and those of the simulation predictions. Obtained results show that the studied pilot unit is able to eliminate most of the ammonia contained in the influent, except for days with exceptionally high influent concentrations. Chemical oxygen demand (COD) and total suspended solids (TSS) reduction is also near complete and a partial denitrification is present. The calibrated model shows a good agreement with results obtained in the pilot unit, although work remains to be done for nitrates. Modeling of residential wastewater treatment appears to be a useful tool for understanding, optimizing and predicting the expected performances of such a system.",,"Bernier, J|Lessard, P|Girard, M",WATER QUALITY RESEARCH JOURNAL OF CANADA,biological treatment modeling nitrogen removal on-site treatment,10.2166/wqrjc.2011.028
53,WOS:000294704000006,2011,Multi-parametric sensitivity analysis of CCHE2D for channel flow simulations in Nile River,MODEL,"Multi-Parametric Sensitivity Analysis (MPSA) is proposed to determine the relative importance of the different empirical parameters controlling flow field. CCHED which is one of the famous public unsteady-flow simulation software is applied to simulate the flow field at Elbogdady reach, at the south of Luxor staff gauge, . km upstream of Roda's staff gauge on the Nile River. The main purpose of the paper is to apply the model in the considered river reach and to assess the capabilities of CCHED. Moreover, the paper presents findings of the authors in determining the model's sensitivity analysis. Multi-Parametric Sensitivity Analysis results show that flow field in Nile River is controlled mainly by the bed roughness coefficient. In addition, the empirical formula of Van Rijn () is found to be more efficient to calculate bed roughness. (C)  International Association of Hydro-environment Engineering and Research, Asia Pacific Division.", Published by Elsevier B.V. All rights reserved.,"Nassar, MA",JOURNAL OF HYDRO-ENVIRONMENT RESEARCH,2d-modling cche2d nile river numerical and sensitivity analysis,10.1016/j.jher.2010.12.002
54,WOS:000087626100009,2000,ECOP: an economic model to assess the willow short rotation coppice global profitability in a case of small scale gasification pathway in Belgium,WOOD,"This paper presents a software package developed to assess the economic profitability of an original way to produce renewable energy: the small scale gasification of willow cultivated as short rotation coppice (SRC) in Belgium. The theoretical bases of the model (process hypotheses and economic indicators) are firstly presented together with the most relevant characteristics of the energy production route (SRC management and wood production, storage and conversion). A reference case is then defined which combines the most influencing parameters (reference interest rate, rotation length, subsidies, harvest mode, SRC yield, power of the electricity generator and annual production of electricity). A sensitivity analysis on these parameters highlighted that the project profitability, from the net present value point of view, is very sensitive to the reference interest rate, to the subsidies (of the conversion unit but probably also of any other kind of subsidies), to the SRC yield and to the generator power, all other parameters remaining constant. The rotation length has only a low influence, at least in the range of classic values ( to  years). To harvest the wood in stems (with delayed chipping) seems also to be the most interesting option.", (C) 2000 Elsevier Science Ltd. All rights reserved.,"Goor, F|Jossart, JM|Ledent, JF",ENVIRONMENTAL MODELLING & SOFTWARE,,10.1016/S1364-8152(00)00014-1
55,WOS:000256840200073,2007,"MEDOR, a didactic tool to support interpretation of bioassay data after internal contamination by actinides",UNCERTAINTIES INHALATION (PUO2)-PU-239 WORKERS MODEL LUNG,"A didactic software, MEthodes DOsimetriques de REference (MEDOR), is being developed to provide help in the interpretation of biological data. Its main purpose is to evaluate the pertinence of the application of different models. This paper describes its first version that is focused on inhalation exposure to actinide aerosols. With this tool, sensitivity analysis on different parameters of the ICRP models can be easily done for aerosol deposition, in terms of activity and particle number, actinide biokinetics and doses. The user can analyse different inhalation cases showing either that dose per unit intake cannot be applied if the aerosol contains a low number of particles or that an inhibition of the late pulmonary clearance by particle transport can occur which contributes to a - fold increase in effective dose as compared with application of default parameters. This underlines the need to estimate systematically the number of deposited particles, as well as to do chest monitoring as long as possible.",,"Miele, A|Blanchin, N|Raynaud, P|Quesne, B|Giraud, JM|Fottorino, R|Berard, P|Ansoborlo, E|Franck, D|Blanchardon, E|Vathaire, CCD|Lebaron-Jacobs, L|Poncy, JL|Piechowski, J|Fritsch, P",RADIATION PROTECTION DOSIMETRY,,10.1093/rpd/ncm288
56,WOS:000244170000035,2007,Economic modelling of price support mechanisms for renewable energy: Case study on Ireland,,"The Irish Government is considering its future targets, policy and programmes for renewable energy for the period beyond . This follows a review in  of policy options that identified a number of different measures to stimulate increased deployment of renewable energy generation capacity. This paper expands this review with an economic analysis of renewable energy price support mechanisms in the Irish electricity generation sector. The focus is on three primary price support mechanisms quota obligations, feed in tariffs and competitive tender schemes. The Green-X computer model is utilised to characterise the RES-E potential and costs in Ireland up until, and including, . The results from this dynamic software tool are used to compare the different support mechanisms in terms of total costs to society and the average premium costs relative to the market price for electricity. The results indicate that in achieving a % RES-E proportion of gross electricity consumption by , a tender scheme provides the least costs to society over the period - but only in case there is limited or no strategic bidding. Considering, however, strategic bidding, a feed-in tariff can be the more efficient solution. Between the other two support mechanisms, the total costs to society are highest for feed-in-tariffs (FIT) until , at which point the costs for the quota system begin to rise rapidly and overtake FIT in -. The paper also provides a sensitivity analysis of the support mechanism calculations by varying default parameters such as the interim () target, the assumed investment risk levels and the amount of biomass co-firing. This analysis shows that a  target of % rather than .% generates lower costs for society over the whole period -, but higher costs for the RES-E strategy over the period -.", (c) 2006 Published by Elsevier Ltd.,"Huber, C|Ryan, L|O Gallachoir, B|Resch, G|Polaski, K|Bazilian, M",ENERGY POLICY,renewable energy policy modelling energy economics,10.1016/j.enpol.2006.01.025
57,WOS:000402819500015,2017,Investigation on the effect of geometrical and geotechnical parameters on elongated offshore piles using fuzzy inference systems,PLATFORMS BEHAVIOR,"Among numerous offshore structures used in oil extraction, jacket platforms are still the most favorable ones in shallow waters. In such structures, log piles are used to pin the substructure of the platform to the seabed. The pile's geometrical and geotechnical properties are considered as the main parameters in designing these structures. In this study, ANSYS was used as the FE modeling software to study the geometrical and geotechnical properties of the offshore piles and their effects on supporting jacket platforms. For this purpose, the FE analysis has been done to provide the preliminary data for the fuzzy-logic post-process. The resulting data were implemented to create Fuzzy Inference System (FIS) classifications. The resultant data of the sensitivity analysis suggested that the orientation degree is the main factor in the pile's geometrical behavior because piles which had the optimal operational degree of about A degrees are more sustained. Finally, the results showed that the related fuzzified data supported the FE model and provided an insight for extended offshore pile designs.",,"Aminfar, A|Mojtahedi, A|Ahmadi, H|Aminfar, MH",CHINA OCEAN ENGINEERING,pile soil fem offshore jacket platform pile-soil interaction fuzzy-logic fuzzification,10.1007/s13344-017-0044-z
58,WOS:000389785300007,2017,An automated decision support system for aided assessment of variogram models,GLOBAL SENSITIVITY-ANALYSIS ENVIRONMENTAL-MODELS VALIDATION FRAMEWORK NETWORKS ROBUST SOIL,"In the present paper, an extensive cross-validation procedure, based on the analysis of numerical indices and graphical tools, is described and discussed. The procedure has been implemented in a software application designed to support practitioners in the variogram model assessment. It provides an extensive report, which summarizes a large post-processing stage and suggests how to interpret the performed analysis to rate the model to be validated. Besides classical accuracy indices, two new integrated tools based on the variogram of residuals are introduced, which take the spatial nature of the dataset into account. Finally, inspecting the summary report, the user can decide whether the considered model is satisfactory for his/her goals or it needs to be improved. Finally, a case study is presented related to the variogram assessment of groundwater level measured in a porous shallow aquifer of the Apulia Region (South -Italy). (C) ", Elsevier Ltd. All rights reserved.,"Barca, E|Porcu, E|Bruno, D|Passarella, G",ENVIRONMENTAL MODELLING & SOFTWARE,geostatistics extensive cross-validation rank coefficient of spatial correlation decision-support system,10.1016/j.envsoft.2016.11.004
59,WOS:000321439500015,2013,Application of Bayesian Networks in Quantitative Risk Assessment of Subsea Blowout Preventer Operations,OFFSHORE SAFETY ASSESSMENT RELIABILITY-ANALYSIS ORGANIZATIONAL-FACTORS FAULT-TREES SYSTEMS METHODOLOGY FACILITIES ACCIDENTS SECURITY PLATFORM,"This article proposes a methodology for the application of Bayesian networks in conducting quantitative risk assessment of operations in offshore oil and gas industry. The method involves translating a flow chart of operations into the Bayesian network directly. The proposed methodology consists of five steps. First, the flow chart is translated into a Bayesian network. Second, the influencing factors of the network nodes are classified. Third, the Bayesian network for each factor is established. Fourth, the entire Bayesian network model is established. Lastly, the Bayesian network model is analyzed. Subsequently, five categories of influencing factors, namely, human, hardware, software, mechanical, and hydraulic, are modeled and then added to the main Bayesian network. The methodology is demonstrated through the evaluation of a case study that shows the probability of failure on demand in closing subsea ram blowout preventer operations. The results show that mechanical and hydraulic factors have the most important effects on operation safety. Software and hardware factors have almost no influence, whereas human factors are in between. The results of the sensitivity analysis agree with the findings of the quantitative analysis. The three-axiom-based analysis partially validates the correctness and rationality of the proposed Bayesian network model.",,"Cai, BP|Liu, YH|Liu, ZK|Tian, XJ|Zhang, YZ|Ji, RJ",RISK ANALYSIS,bayesian networks quantitative risk assessment subsea blowout preventer,10.1111/j.1539-6924.2012.01918.x
60,WOS:000243742700004,2007,The Data Uncertainty Engine (DUE): A software tool for assessing and simulating uncertain environmental variables,MODELING ERROR REJECTION BETA PROPAGATION PREDICTION POISSON GAMMA GIS,"This paper describes a software tool for: () assessing uncertainties in environmental data; and () generating realisations of uncertain data for use in uncertainty propagation analyses: the ""Data Uncertainty Engine (DUE)"". Data may be imported into DUE from file or from a database, and are represented in DUE as objects whose positions and attribute values may be uncertain. Objects supported by DUE include spatial vectors, spatial rasters, time-series of spatial data, simple time-series and objects that are constant in space and time. Attributes supported by DUE include continuous numerical variables (e.g. rainfall), discrete numerical variables (e.g. bird counts) and categorical variables (e.g. land-cover). Once data are imported, an uncertainty model can be developed for the positional and attribute uncertainties of environmental objects. This is currently limited to probability models, but confidence intervals and scenarios will be provided in the future. Using DUE, the spatial and temporal patterns of uncertainty (autocorrelation), as well as cross-correlations between related inputs, can be incorporated into an uncertainty analysis. Alongside expert judgement, sample data may be used to help estimate uncertainties, and to reduce the uncertainty of the simulated output by ensuring each realisation reproduces the sample data. Most importantly, DUE provides a conceptual framework for structuring an uncertainty analysis, allowing users without direct experience of uncertainty methods to develop realistic uncertainty models for their data. (c) ", Elsevier Ltd. All rights reserved.,"Brown, JD|Heuvelink, GBM",COMPUTERS & GEOSCIENCES,uncertainty analysis monte carlo uncertainty propagation java,10.1016/j.cageo.2006.06.015
61,WOS:000317821300006,2013,Sensitivity Analysis of Multiple Informant Models When Data Are Not Missing at Random,MARITAL SATISFACTION DROP-OUT SYMPTOMS ADOPTION PARENT CHILD,"Missing data are common in studies that rely on multiple informant data to evaluate relationships among variables for distinguishable individuals clustered within groups. Estimation of structural equation models using raw data allows for incomplete data, and so all groups can be retained for analysis even if only  member of a group contributes data. Statistical inference is based on the assumption that data are missing completely at random or missing at random. Importantly, whether or not data are missing is assumed to be independent of the missing data. A saturated correlates model that incorporates correlates of the missingness or the missing data into an analysis and multiple imputation that might also use such correlates offer advantages over the standard implementation of SEM when data are not missing at random because these approaches could result in a data analysis problem for which the missingness is ignorable. This article considers these approaches in an analysis of family data to assess the sensitivity of parameter estimates and statistical inferences to assumptions about missing data, a strategy that could be easily implemented using SEM software.",,"Blozis, SA|Ge, XJ|Xu, S|Natsuaki, MN|Shaw, DS|Neiderhiser, JM|Scaramella, LV|Leve, LD|Reiss, D",STRUCTURAL EQUATION MODELING-A MULTIDISCIPLINARY JOURNAL,auxiliary variables missing data missing not at random multiple imputation multiple informant data,10.1080/10705511.2013.769393
62,WOS:000264918900016,2009,Modelling of anaerobic treatment of evaporator condensate (EC) from a sulphite pulp mill using the IWA anaerobic digestion model no. 1 (ADM1),WASTE-WATER MOLASSES REACTOR,"This paper presents the application of the ADM model to simulate the dynamic behaviour of an anaerobic reactor treating the condensate effluent (EC) generated in a sulphite pulp mill. The model was implemented in the simulation software AQUASIM (R) .d and its predictions were compared to experimental data obtained in lab-scale semi-continuous assays treating the industrial effluent. Sensitivity analysis revealed high influence of kinetic parameters on the process behaviour, which were further estimated: maximum specific uptake rate (k(m) = . d(-)) and half-saturation constant (K-s = . kg COD m(-)). The accuracy of the optimised parameters was assessed against experimental data from a second lab-scale reactor treating EC effluent with an additional carbon source (molasses). It was concluded that the model predicted reasonably the dynamic behaviour of the anaerobic reactor under different loading rates. In addition, simulations successfully predicted a better stability and performance of the process (lower VIA accumulation and higher COD removal and methane production) for the EC treatment when an external carbon source is added to the reactor, specifically at high organic loads ( kg COD m(-) d(-) or higher). The model was not able to describe adequately the reactor behaviour at high organic loads when molasses was not added, thus application of the model for the anaerobic treatment of EC effluent needs to be further evaluated.", (C) 2008 Elsevier B.V. All rights reserved.,"Silva, F|Nadais, H|Prates, A|Arroja, L|Capela, I",CHEMICAL ENGINEERING JOURNAL,dynamic simulation adm1 anaerobic reactor acetic acid sulphite pulping process semi-continuous assay,10.1016/j.cej.2008.09.002
63,WOS:000314802700001,2013,An educational model for ensemble streamflow simulation and uncertainty analysis,CLIMATE-CHANGE SENSITIVITY HYDROLOGY FUTURE TOOL,"This paper presents the hands-on modeling toolbox, HBV-Ensemble, designed as a complement to theoretical hydrology lectures, to teach hydrological processes and their uncertainties. The HBV-Ensemble can be used for in-class lab practices and homework assignments, and assessment of students' understanding of hydrological processes. Using this modeling toolbox, students can gain more insights into how hydrological processes (e.g., precipitation, snowmelt and snow accumulation, soil moisture, evapotranspiration and runoff generation) are interconnected. The educational toolbox includes a MATLAB Graphical User Interface (GUI) and an ensemble simulation scheme that can be used for teaching uncertainty analysis, parameter estimation, ensemble simulation and model sensitivity. HBV-Ensemble was administered in a class for both in-class instruction and a final project, and students submitted their feedback about the toolbox. The results indicate that this educational software had a positive impact on students understanding and knowledge of uncertainty in hydrological modeling.",,"AghaKouchak, A|Nakhjiri, N|Habib, E",HYDROLOGY AND EARTH SYSTEM SCIENCES,,10.5194/hess-17-445-2013
64,WOS:000296919500013,2012,Modeling and quantitatively predicting software security based on stochastic Petri nets,RELIABILITY SYSTEMS TOOLS,"To quantitatively predict software security in the design phase, hierarchical software security modeling and evaluation methods are proposed based on Stochastic Petri Nets (SPNs). Hierarchical methods mitigate the state-space explosion problem in SPNs. An isomorphic Markov Chain (MC) is obtained from the component SPN model. The security prediction value is calculated based on the probability distribution of the MC in the steady state. A sensitivity analysis method is proposed through evaluating the derivative of the security evaluation prediction equation. It provides a means to identify and trace back to the critical components for security enhancing. Security prediction and sensitivity analysis in the design phase provide the possibility to investigate and compare different solutions to the target system before realization. A case study shows the applicability and feasibility of our method. (C) ", Elsevier Ltd. All rights reserved.,"Yang, NAH|Yu, HQ|Qian, ZL|Sun, H",MATHEMATICAL AND COMPUTER MODELLING,model quantify software security sensitivity analysis hierarchical stochastic petri net,10.1016/j.mcm.2011.01.055
65,WOS:000311188300025,2012,"Integrating environmental gap analysis with spatial conservation prioritization: A case study from Victoria, Australia",DIGITAL ELEVATION MODELS RESERVE SELECTION PROTECTED AREAS SOUTH-AFRICA BIODIVERSITY CONSERVATION VEGETATION CONDITION UNCERTAINTY ANALYSIS QUANTITATIVE METHOD PLANNING TOOLS CLIMATE-CHANGE,"Gap analysis is used to analyse reserve networks and their coverage of biodiversity, thus identifying gaps in biodiversity representation that may be filled by additional conservation measures. Gap analysis has been used to identify priorities for species and habitat types. When it is applied to identify gaps in the coverage of environmental variables, it embodies the assumption that combinations of environmental variables are effective surrogates for biodiversity attributes. The question remains of how to fill gaps in conservation systems efficiently. Conservation prioritization software can identify those areas outside existing conservation areas that contribute to the efficient covering of gaps in biodiversity features. We show how environmental gap analysis can be implemented using high-resolution information about environmental variables and ecosystem condition with the publicly available conservation prioritization software, Zonation. Our method is based on the conversion of combinations of environmental variables into biodiversity features. We also replicated the analysis by using Species Distribution Models (SDMs) as biodiversity features to evaluate the robustness and utility of our environment-based analysis. We apply the technique to a planning case study of the state of Victoria. Australia. (C) ", Elsevier Ltd. All rights reserved.,"Sharafi, SM|Moilanen, A|White, M|Burgman, M",JOURNAL OF ENVIRONMENTAL MANAGEMENT,reserve selection zonation software conservation prioritization spatial optimization environmental variables habitat condition,10.1016/j.jenvman.2012.07.010
66,WOS:000384855300018,2016,On ISSM and leveraging the Cloud towards faster quantification of the uncertainty in ice-sheet mass balance projections,NORTHEAST GREENLAND MODEL FLOW SENSITIVITY CREEP,"With the Amazon EC Cloud becoming available as a viable platform for parallel computing, Earth System Models are increasingly interested in leveraging its capabilities towards improving climate projections. In particular, faced with long wait periods on high-end clusters, the elasticity of the Cloud presents a unique opportunity of potentially ""infinite"" availability of small-sized clusters running on high-performance instances. Among specific applications of this new paradigm, we show here how uncertainty quantification in climate projections of polar ice sheets (Antarctica and Greenland) can be significantly accelerated using the Cloud. Indeed, small-sized clusters are very efficient at delivering sensitivity and sampling analysis, core tools of uncertainty quantification. We demonstrate how this approach was used to carry out an extensive analysis of ice-flow projections on one of the largest basins in Greenland, the North-East Greenland Glacier, using the Ice Sheet System Model, the public-domain NASA-funded ice-flow modeling software. We show how errors in the projections were accurately quantified using Monte-Carlo sampling analysis on the EC Cloud, and how a judicious mix of high-end parallel computing and Cloud use can best leverage existing infrastructures, and significantly accelerate delivery of potentially ground-breaking climate projections, and in particular, enable uncertainty quantification that were previously impossible to achieve. (C) ", Elsevier Ltd. All rights reserved.,"Larour, E|Schlegel, N",COMPUTERS & GEOSCIENCES,polar ice sheet modeling cloud uncertainty quantification,10.1016/j.cageo.2016.08.007
67,WOS:000388621000001,2016,Predictive Simulation of Seawater Intrusion in a Tropical Coastal Aquifer,WATER-INTRUSION CLIMATE-CHANGE FLORIDA ISLAND MODEL FLOW,"The solute transport in a tropical, coastal aquifer of southern India is numerically simulated considering the possible cases of aquifer recharge, freshwater draft, and seawater intrusion using numerical modeling software. The aquifer considered for the study is a shallow, unconfined aquifer with lateritic formations having good monsoon rains up to about ,mm during June to September and the rest of the months almost dry. The model is calibrated for a two-year period and validated against the available dataset, which gave satisfactory results. The groundwater flow pattern during the calibration period shows that for the month of May a depleted water table and during the monsoon month of August a saturated water table was predicted. The sensitivity analysis of model parameters reveals that the hydraulic conductivity and recharge rate are the most sensitive parameters. Based on seasonal investigation, the seawater intrusion is found to be more sensitive to pumping and recharge rates compared to the aquifer properties. The water balance study confirms that river seepage and rainfall recharge are the major input to the aquifer. The model is used to forecast the landward movement of seawater intrusion because of the anticipated increase in freshwater draft scenarios in combination with the decreased recharge rate over a longer period. The results of the predictive simulations indicate that seawater intrusion may still confine up to a distance of approximately -m landward for the scenarios considered and thus are sustainable.", (C) 2015 American Society of Civil Engineers.,"Lathashri, UA|Mahesha, A",JOURNAL OF ENVIRONMENTAL ENGINEERING,coastal aquifer freshwater modflow seawater intrusion seawat solute transport,10.1061/(ASCE)EE.1943-7870.0001037
68,WOS:000086420500002,2000,Assessing the impact of managed-care on the distribution of length-of-stay using Bayesian hierarchical models,CANCER CLINICAL-TRIAL SURVIVAL-DATA FRAILTY,"Hierarchical models provide a useful framework for the complexities encountered in policy-relevant research in which the impact of social programs is being assessed. Such complexities include multi-site data, censored data and over-dispersion. In this paper, Bayesian inference through Markov Chain Monte Carlo methods is used for the analysis of a complex hierarchical log-normal model that shows the impact of a managed care strategy aimed at limiting length of hospital stays. Parameters in this model allow for variability in baseline length-of-stay as well as the program effect across hospitals. The authors demonstrate elicitation and sensitivity analysis with respect to prior distributions. All calculations for the posterior and predictive distributions were obtained using the software BUGS.",,"Stangl, D|Huerta, G",LIFETIME DATA ANALYSIS,hierarchical log-normal model prior elicitation gibbs sampling predictive distribution health policy,10.1023/A:1009691326989
69,WOS:000392165500023,2017,Shale gas flowback water desalination: Single vs multiple-effect evaporation with vapor recompression cycle and thermal integration,PERFORMANCE EVALUATION MEMBRANE DISTILLATION COMPRESSION SYSTEM MANAGEMENT OPTIMIZATION DESIGN UNCERTAINTY RESOURCES SELECTION,"This paper introduces a new optimization model for the single and multiple-effect evaporation (SEE/MEE) systems design, including vapor recompression cycle and thermal integration. The SEE/MEE model is specially developed for shale gas flowback water desalination. A superstructure is proposed to solve the problem, comprising several evaporation effects coupled with intermediate flashing tanks that are used to enhance thermal integration by recovering condensate vapor. Multistage equipment with intercooling is used to compress the vapor formed by flashing and evaporation. The compression cycle is driven by electricity to operate on the vapor originating from the SEE/MEE system, providing all the energy needed in the process. The mathematical model is formulated as a nonlinear programming (NLP) problem optimized under GAMS software by minimizing the total annualized cost. The SEE/MEE system application for zero liquid discharge (ZLD) is investigated by allowing brine salinity discharge near to salt saturation conditions. Additionally, sensitivity analysis is carried out to evaluate the optimal process configuration and performance under distinct feed water salinity conditions. The results highlight the potential of the proposed model to cost-effectively optimize SEE/MEE systems by producing fresh water and reducing brine discharges and associated environmental impacts. (C)  The Authors.", Published by Elsevier B.V.,"Onishi, VC|Carrero-Parreno, A|Reyes-Labarta, JA|Ruiz-Femenia, R|Salcedo-Diaz, R|Fraga, ES|Caballero, JA",DESALINATION,shale gas single-effect evaporation (see) multiple-effect evaporation (mee) mechanical vapor recompression (mvr) thermal integration zero liquid discharge (zld),10.1016/j.desal.2016.11.003
70,WOS:000330675000001,2014,Intelligent Platform for Model Updating in a Structural Health Monitoring System,PARAMETER SELECTION DYNAMICS,"The main aim of this study is to develop an automated smart software platform to improve the time-consuming and laborious process of model updating. We investigate the key techniques of model updating based on intelligent optimization algorithms, that is, accuracy judgment methods for basic finite element model, parameter choice theory based on sensitivity analysis, commonly used objective functions and their construction methods, particle swarm optimization, and other intelligent optimization algorithms. An intelligent model updating prototype software framework is developed using the commercial software systems ANSYS and MATLAB. A parameterized finite element modeling technique is proposed to suit different bridge types and different model updating requirements. An objective function library is built to fit different updating targets. Finally, two case studies are conducted to verify the feasibility of the techniques used by the proposed software platform.",,"Dan, DH|Yang, T|Gong, JX",MATHEMATICAL PROBLEMS IN ENGINEERING,,10.1155/2014/628619
71,WOS:000237124500022,2006,Automatic calibration and predictive uncertainty analysis of a semidistributed watershed model,RAINFALL-RUNOFF MODELS GLOBAL OPTIMIZATION HYDROLOGIC-MODELS PARAMETER-ESTIMATION ALGORITHM MULTIPLE SCHEME,"Semidistributed models are commonly calibrated manually, but software for automatic calibration is now available. We present a two-stage routine for automatic calibration of the semidistributed watershed model Soil and Water Assessment Tool ( SWAT) that finds the best values for the model parameters, preserves spatial variability in essential parameters, and leads to a measure of the model prediction uncertainty. In the first stage, a modified global Shuffled Complex Evolution (SCE-UA) method was employed to find the ""best'' values for the lumped model parameters. In the second stage, the spatial variability of the original model parameters was restored and a local search method ( a variant of Levenberg - Marquart method) was used to find a more distributed set of parameters using the results of the previous stage as starting values. A method called ""regularization'' was adopted to prevent the parameters from taking extreme values. In addition, we applied a nonlinear calibration-constrained method to develop confidence intervals for annual and -d average flow predictions. We calibrated stream flow in the Etowah River measured at Canton, GA ( a watershed area of  km()) for the years  to  and used the years  to  for validation. The Parameter Estimator ( PEST) software was used to conduct the two-stage automatic calibration and prediction uncertainty analysis. Calibration for daily and monthly flow produced a very good fit to the measured data. Nash-Sutcliffe coefficients for daily and monthly flow over the calibration period were . and ., respectively. They were . and ., respectively, for the validation period. The nonlinear prediction uncertainty analysis worked well for long-term ( annual) flow in that our prediction confidence intervals included or were very near to the observed flow for most years. It did not work well for short-term (-d average) flows in that the prediction confidence intervals did not include the observed flow, especially for low and high flow conditions.",,"Lin, ZL|Radcliffe, DE",VADOSE ZONE JOURNAL,,10.2136/vzj2005.0025
72,WOS:000352642900010,2015,"A computational framework for dynamic data-driven material damage control, based on Bayesian inference and model selection",APPLICATIONS SYSTEMS CREEP DAMAGE SIMULATIONS MECHANICS GROWTH,"In the present study, a general dynamic data-driven application system (DDDAS) is developed for real-time monitoring of damage in composite materials using methods and models that account for uncertainty in experimental data, model parameters, and in the selection of the model itself. The methodology involves (i) data data from uniaxial tensile experiments conducted on a composite material; (ii) continuum damage mechanics based material constitutive models; (iii) a Bayesian framework for uncertainty quantification, calibration, validation, and selection of models; and (iv) general Bayesian filtering, as well as Kalman and extended Kalman filters. A software infrastructure is developed and implemented in order to integrate the various parts of the DDDAS. The outcomes of computational analyses using the experimental data prove the feasibility of the Bayesian-based methods for model calibration, validation, and selection. Moreover, using such DDDAS infrastructure for real-time monitoring of the damage and degradation in materials results in results in an improved prediction of failure in the system."," Copyright (C) 2014 John Wiley & Sons, Ltd.","Prudencio, EE|Bauman, PT|Faghihi, D|Ravi-Chandar, K|Oden, JT",INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,bayesian model selection extended kalman filter dynamic data-driven application systems material damage,10.1002/nme.4669
73,WOS:000283094800003,2010,Uncertainty analysis for estimation of landfill emissions and data sensitivity for the input variation,,"Results of research and practical experience confirm that stabilization of GHG concentrations will require a tremendous effort. One of the sectors identified as a significant source of methane (CH()) emissions are solid waste disposal sites (SWDS). Landfills are the key source of CH() emissions in the emissions inventory of Slovakia, and the actual emission factors are estimated with a high uncertainty level. The calculation of emission uncertainty of the landfills using the more sophisticated Tier  Monte Carlo method is evaluated in this article. The software package that works with the probabilistic distributions and their combination was developed with this purpose in mind. The results, sensitivity analysis, and computational methodology of the CH() emissions from SWDS are presented in this paper.",,"Szemesova, J|Gera, M",CLIMATIC CHANGE,,10.1007/s10584-010-9919-1
74,WOS:000383683800015,2016,Scalable subsurface inverse modeling of huge data sets with an application to tracer concentration breakthrough data from magnetic resonance imaging,COMPONENT GEOSTATISTICAL APPROACH GENERALIZED COVARIANCE FUNCTIONS HETEROGENEOUS POROUS-MEDIA PARAMETER-ESTIMATION UNCERTAINTY QUANTIFICATION HYDRAULIC CONDUCTIVITY TEMPORAL MOMENTS EQUATIONS TRANSPORT SYSTEMS,"Characterizing subsurface properties is crucial for reliable and cost-effective groundwater supply management and contaminant remediation. With recent advances in sensor technology, large volumes of hydrogeophysical and geochemical data can be obtained to achieve high-resolution images of subsurface properties. However, characterization with such a large amount of information requires prohibitive computational costs associated with ""big data'' processing and numerous large-scale numerical simulations. To tackle such difficulties, the principal component geostatistical approach (PCGA) has been proposed as a ""Jacobian-free'' inversion method that requires much smaller forward simulation runs for each iteration than the number of unknown parameters and measurements needed in the traditional inversion methods. PCGA can be conveniently linked to any multiphysics simulation software with independent parallel executions. In this paper, we extend PCGA to handle a large number of measurements (e.g.,  or more) by constructing a fast preconditioner whose computational cost scales linearly with the data size. For illustration, we characterize the heterogeneous hydraulic conductivity (K) distribution in a laboratory-scale -D sand box using about  million transient tracer concentration measurements obtained using magnetic resonance imaging. Since each individual observation has little information on the K distribution, the data were compressed by the zeroth temporal moment of breakthrough curves, which is equivalent to the mean travel time under the experimental setting. Only about  forward simulations in total were required to obtain the best estimate with corresponding estimation uncertainty, and the estimated K field captured key patterns of the original packing design, showing the efficiency and effectiveness of the proposed method.",,"Lee, JH|Yoon, HK|Kitanidis, PK|Werth, CJ|Valocchi, AJ",WATER RESOURCES RESEARCH,,10.1002/2015WR018483
75,WOS:000404559900070,2017,"Two-Dimensional Dam-Break Flood Analysis in Data-Scarce Regions: The Case Study of Chipembe Dam, Mozambique",DIGITAL ELEVATION MODELS INUNDATION MODELS SHUTTLE RADAR SIMULATION RESOLUTION UNCERTAINTY PREDICTIONS HYDRAULICS TOPOGRAPHY PARAMETERS,"This paper presents the results of a modeling study of the hypothetical dam break of Chipembe dam in Mozambique. The modeling approach is based on the software Iber, a freely available dam break and two-dimensional finite volume shallow water model. The shuttle radar topography mission (SRTM) online digital elevation model (DEM) is used as main source of topographic data. Two different DEMs are considered as input for the hydraulic model: a DEM based on the original SRTM data and a hydrologically-conditioned DEM. A sensitivity analysis on the Manning roughness coefficient is performed. The results demonstrate the relevant impact of the DEM used on the predicted flood wave propagation, and a lower influence of the roughness value. The low cost modeling approach proposed in this paper can be an attractive option for modeling exceptional flood caused by dam break, when limited data and resources are available, as in the presented case. The resulting flood-inundation and hazard maps will enable the Regional Water Management Administration of Mozambique (ARA) to develop early warning systems.",,"Alvarez, M|Puertas, J|Pena, E|Bermudez, M",WATER,dam-break flood modeling hazard mapping iber model mozambique,10.3390/w9060432
76,WOS:000174593500001,2002,Use of the most likely failure point method for risk estimation and risk uncertainty analysis,,"The most likely failure point (MLFP) method, developed within the field of structural reliability analysis (where it is known as the FORM/SORM method) is a technique for estimating the risk (probability) that a calculated quantity Q exceeds a set limit Q(lim) when some or all of the inputs to the calculation are uncertain. It can be used as an efficient stand-alone method for this type of risk calculation. However, for application within the field of toxic hazards, it is proposed as a means for performing sensitivity analyses, possibly in parallel with a risk calculation carried out by conventional methods. The basis of the method is outlined and its use is demonstrated by means of an example calculation of the risk arising froth an installation containing chlorine. The calculation uses, as a consequence model, commercial software for the prediction of dense gas transport. The risk estimate is shown to be acceptably close to that obtained by the Monte Carlo method. The use of a proposed screening procedure utilising the sensitivity formulas that the method provides, in order to identify the most significant uncertainties, is demonstrated. The identification of a single set of input values containing sufficient information to summarise (at least approximately) the entire risk analysis is considered to be an important feature of the method and is proposed as the basis of a means for assessing the validity of the consequence model.", (C) 2002 Elsevier Science B.V All rights reserved.,"Mitchell, B",JOURNAL OF HAZARDOUS MATERIALS,risk toxic hazard uncertainty sensitivity,10.1016/S0304-3894(01)00378-8
77,WOS:000345470700014,2014,Oxygen blast furnace with CO2 capture and storage at an integrated steel mill-Part I: Technical concept analysis,,"In this study application of OBF with and without CCS to an integrated steel mill is investigated. The study is based on the real, Ruukki Metals Ltd.'s existing steel mill, located in the city of Raahe, Finland. Implications of application of OBF to energy and mass balances at the site are studied. Based on the technical evaluation, costs and feasibility for carbon capture are estimated. The energy and mass balance basis is presented in this first part of the series of two papers. Costs, feasibility and sensitivity analysis are assessed in the second part of the series (Tsupari et al. . Int. J. Greenhouse Gas Control). The impact of applying OBF at an integrated steel mill is evaluated based on a consequential assessment following the methodology of Arasto et al. (). Int. J. Greenhouse Gas Control  (August) pp. - concentrating only on the parts of the steelmaking processes affected by the deployment of OBF and CO capture. The technical processes, CO capture and the steelmaking processes affected were modelled using Aspen Plus process modelling software and the results were used to estimate the CO emission reduction potential with OBF technology at an integrated steel mill. The results show that the CO emission from an iron and steel mill can be significantly reduced by application of an oxygen blast furnace and CCS. By applying only the blast furnace process, the emissions can already be reduced by . Mt/a without storing the separated CO. If captured CO is also purified and stored permanently, the emission can be further reduced by an additional . Mt/a. This is a significant reduction considering that the production of the mill stays the same as in the reference case. In addition to carbon footprint of the production, application of oxygen blast furnace also has significant impact on coke consumption and energy balance on the site. (C) ", Elsevier Ltd. All rights reserved.,"Arasto, A|Tsupari, E|Karki, J|Lilja, J|Sihvonen, M",INTERNATIONAL JOURNAL OF GREENHOUSE GAS CONTROL,ccs iron and steel industry oxygen blast furnace obf concept evaluation vacuum pressure swing adsorption aspen plus,10.1016/j.ijggc.2014.09.004
78,WOS:000240365500002,2006,Environmental and ecological hydroinformatics to support the implementation of the European Water Framework Directive for river basin management,DECISION-SUPPORT MODELING APPROACH SWAT MODEL SPECIES RICHNESS QUALITY MODELS LANDSCAPE INTEGRATION DESIGN SYSTEM UNCERTAINTY,"Research and development in hydroinformatics can play an important role in environmental impact assessment by integrating physically-based models, data-driven models and other information and Communication Tools (ICT). An illustration is given in this paper describing the developments around the Soil and Water Assessment Tool (SWAT) to support the implementation of the EU water Framework Directive. SWAT operates on the river basin scale and includes processes for the assessment of complex diffuse pollution; it is open-source software, which allows for site-specific modifications to the source and easy linkage to other hydroinformatics tools. A crucial step in the world-wide applicability of SWAT was the integration of the model into a GIS environment, allowing for a quick model set-up using digital information on terrain elevation, land use and management, soil properties and weather conditions. Model analysis tools can be integrated with SWAT to assist in the tedious tasks of model calibration, parameter optimisation, sensitivity and uncertainty analysis and allows better understanding of the model before addressing scientific and societal questions. Finally, further linkage of SWAT to ecological assessment tools, Land Use prediction tools and tools for optimal Experimental Design shows that SWAT can play an important role in multi-disciplinary eco-environmental impact assessment studies.",,"van Griensven, A|Breuer, L|Di Luzio, M|Vandenberghe, V|Goethals, P|Meixner, T|Arnold, J|Srinivasan, R",JOURNAL OF HYDROINFORMATICS,catchment modelling eco-hydrology environmental hydroinformatics eu water framework directive model integration swat,10.2166/hydro.2006.010
79,WOS:000287437100013,2011,"NCNA: Integrated platform for constructing, visualizing, analyzing and sharing human-mediated nitrogen biogeochemical networks",,"Human alterations to the nitrogen (N) cycle are closely associated with global environmental and climate change. New tools are necessary to model and analyze the highly complex N cycles emerging from human-mediated ecosystems. We developed a new software. NCNA, to provide three functions: a) rigorous reconstruction of quasi-empirical models (QEMs), b) computer-aided interface for data collection and automatic sensitivity analysis, and c) automatic generation, visualization and network environ analysis (NEA) of N cycling networks. (c) ", Elsevier Ltd. All rights reserved.,"Min, Y|Gong, W|Jin, XG|Chang, J|Gu, BJ|Han, Z|Ge, Y",ENVIRONMENTAL MODELLING & SOFTWARE,quasi-empirical model reconstruction visualization network environ analysis urbanization,10.1016/j.envsoft.2010.11.002
80,WOS:000340951200003,2014,Numerical and intelligent modeling of triaxial strength of anisotropic jointed rock specimens,FUZZY INFERENCE SYSTEM INTACT ROCKS CONTINUUM MASSES,"The strength of anisotropic rock masses can be evaluated through either theoretical or experimental methods. The latter is more precise but also more expensive and time-consuming especially due to difficulties of preparing high-quality samples. Numerical methods, such as finite element method (FEM), finite difference method (FDM), distinct element method (DEM), etc. have been regarded as precise and low-cost theoretical approaches in different fields of rock engineering. On the other hand, applicability of intelligent approaches such as fuzzy systems, neural networks and decision trees in rock mechanics problems has been recognized through numerous published papers. In current study, it is aimed to theoretically evaluate the strength of anisotropic rocks with through-going discontinuity using numerical and intelligent methods. In order to do this, first, strength data of such rocks are collected from the literature. Then FlAC, a commercially well-known software for FDM analysis, is applied to simulate the situation of triaxial test on anisotropic jointed specimens. Reliability of this simulation in predicting the strength of jointed specimens has been verified by previous researches. Therefore, the few gaps of the experimental data are filled by numerical simulation to prevent unexpected learning errors. Furthermore, a sensitivity analysis is carried out based on the numerical process applied herein. Finally, two intelligent methods namely feed forward neural network and a newly developed fuzzy modeling approach are utilized to predict the strength of above-mentioned specimens. Comparison of the results with experimental data demonstrates that the intelligent models result in desirable prediction accuracy.",,"Asadi, M|Bagheripour, MH",EARTH SCIENCE INFORMATICS,numerical modeling artificial neural networks fuzzy systems strength anisotropy jointed rock,10.1007/s12145-013-0137-z
81,WOS:000265171300023,2009,Artificial neural networks to predict daylight illuminance in office buildings,CONTROL-SYSTEMS COST ESTIMATION PERFORMANCE COEFFICIENT SAVINGS DESIGN MODELS FUZZY,"A prediction model was developed to determine daylight illuminance for the office buildings by using artificial neural networks (ANNs). Illuminance data were collected for  months by applying a field measuring method. Utilizing weather data from the local weather station and building parameters from the architectural drawings, a three-layer ANN model of feed-forward type (with one output node) was constructed. Two variables for time (date, hour),  weather determinants (outdoor temperature, solar radiation, humidity, UV index and UV dose) and  building parameters (distance to windows. number of windows, orientation of rooms, floor identification, room dimensions and point identification) were considered as input variables. Illuminance was used as the output variable. In ANN modeling, the data were divided into two groups; the first  of these data sets were used for training and the remaining  for testing. Microsoft Excel Solver used simplex optimization method for the optimal weights. The model's performance was then measured by using the illuminance percentage error. As the prediction power of the model was almost %, predicted data had close matches with the measured data. The prediction results were successful within the sample measurements. The model was then subjected to sensitivity analysis to determine the relationship between the input and output variables. NeuroSolutions Software by NeuroDimensions Inc., was adopted for this application. Researchers and designers will benefit from this model in daylighting performance assessment of buildings by making predictions and comparisons and in the daylighting design process by determining illuminance. (C) ", Elsevier Ltd. All rights reserved.,"Kazanasmaz, T|Gunaydin, M|Binol, S",BUILDING AND ENVIRONMENT,modeling building daylighting artificial neural networks,10.1016/j.buildenv.2008.11.012
82,WOS:000378366700024,2016,Thermal comfort and energy performance: Sensitivity analysis to apply the Passive House concept to the Portuguese climate,SIMULATION,"The need to apply the Passive House concept to Mediterranean countries climate is regarded as being of great importance to support countries such as Portugal to reduce its primary energy demand associated to buildings consumption and thus, devising a cost-efficient strategy to meet the targets pointed out by the recast of the EPBD //EU. In this sense, the present research intends to contribute to the implementation of the Passive House concept in Portugal, by means of a detailed study for the Aveiro region and a more broad analysis examination for different district capitals of Portugal mainland. A detached two-storey lightweight steel structure of contemporary architecture was modelled as case study for the Portuguese climate, based on its original design solutions and resorting to the EnergyPlus((R)) software. From this original model, sensitivity analyses were carried out in order to meet the parameters defined by PH standards. The improved results from the climate region of Aveiro, in Portugal, have led to a reduction of the %, % and .% for the heating demand, cooling demand and overheating rate, respectively (comparing the improved solution with the original as reference). It was therefore possible to meet the PH requirements, proving its applicability to the Portuguese climate and for this particular building technology.", (c) 2016 Elsevier Ltd.,"Figueiredo, A|Figueira, J|Vicente, R|Maio, R",BUILDING AND ENVIRONMENT,energy efficiency passive house dynamic building simulation,10.1016/j.buildenv.2016.03.031
83,WOS:000344676000001,2014,Kinetic Study of Nonequilibrium Plasma-Assisted Methane Steam Reforming,DIELECTRIC-BARRIER DISCHARGE CONVERSION TRANSPORT MECHANISM REACTOR,"To develop a detailed reaction mechanism for plasma-assisted methane steam reforming, a comprehensive numerical and experimental study of effect laws on methane conversion and products yield is performed at different steam to methane molar ratio (S/C), residence time s, and reaction temperatures. A CHEMKIN-PRO software with sensitivity analysis module and path flux analysis module was used for simulations. A set of comparisons show that the developed reaction mechanism can accurately predict methane conversion and the trend of products yield in different operating conditions. Using the developed reaction mechanism in plasma-assisted kinetic model, the reaction path flux analysis was carried out. The result shows that CH recombination is the limiting reaction for CO production and O is the critical species for CO production. Adding wt.% Ni/SiO in discharge region has significantly promoted the yield of H-, CO, or CO in dielectric packed bed (DPB) reactor. Plasma catalytic hybrid reforming experiment verifies the reaction path flux analysis tentatively.",,"Zheng, HT|Liu, Q",MATHEMATICAL PROBLEMS IN ENGINEERING,,10.1155/2014/938618
84,WOS:000308971400008,2012,Stochastic cost optimization of DNAPL remediation - Method description and sensitivity study,GROUNDWATER MONITORING DESIGN AQUIFER REMEDIATION GENETIC ALGORITHM TRANSPORT UNCERTAINTY MANAGEMENT MODEL NETWORK SYSTEMS PUMP,"A modeling approach is described for optimizing the design and operation of groundwater remediation at DNAPL sites that considers uncertainty in site and remediation system characteristics, performance and cost model limitations, and measurement uncertainties that affect predictions of remediation performance and cost. The performance model simulates performance and costs for thermal source zone treatment and enhanced bioremediation with statistical compliance rules and real-time operational system monitoring. An inverse solution is employed to estimate model parameters, parameter covariances, and residual prediction error from site data and a stochastic cost optimization algorithm determines design and operation variables that minimize expected net present value cost over Monte Carlo realizations. The method is implemented in the program SCOToolkit. A series of applications to a hypothetical problem yielded expected cost reductions for site remediation as much as % compared to conventional non-optimized approaches, while also increasing the probability of achieving ""no further action"" status in a specified timeframe by more than %. Optimizing monitoring frequency for compliance wells used to make no further action determinations as well as operational monitoring used to make decisions on individual remediation system components reveals tradeoffs between increased direct costs for sampling and analysis versus decreased construction and operating costs that arise because more data increases decision reliability. Optimizing protocols for operational monitoring and heating unit shutdown protocols for thermal source treatment (incremental versus all-or-none shutdown, soil versus groundwater sampling, number and frequency of samples) produced cost savings of more than %. Defining compliance based on confidence limits of a moving time window regression decreased expected cost and lowered failure probability compared to using measured extreme values over a lookback period. Uncertainty in DNAPL source delineation was found to have a large effect on the cost and probability of achieving remediation objectives for thermal source remediation. (C) ", Elsevier Ltd. All rights reserved.,"Parker, J|Kim, U|Kitanidis, P|Cardiff, M|Liu, XY|Beyke, G",ENVIRONMENTAL MODELLING & SOFTWARE,stochastic optimization uncertainty analysis dnapl model calibration thermal source treatment enhanced bioremediation remediation cost,10.1016/j.envsoft.2012.05.002
85,WOS:000401544900015,2016,"Synergies between biodiversity conservation and ecosystem service provision: Lessons on integrated ecosystem service valuation from a Himalayan protected area, Nepal",TRADE-OFFS POVERTY TARGETS,"We utilised a practical approach to integrated ecosystem service valuation to inform decision-making at Shivapuri-Nagarjun National Park in Nepal. The Toolkit for Ecosystem Service Site-based Assessment (TESSA) was used to compare ecosystem services between two alternative states of the site (protection or lack of protection with consequent changed land use) to estimate the net consequences of protection. We estimated that lack of protection would have substantially reduced the annual ecosystem service flow, including a % reduction in the value of greenhouse gas sequestration, % reduction in carbon storage, % reduction in nature-based recreation, and % reduction in water quality. The net monetary benefit of the park was estimated at $ million year(-). We conclude that: () simplified cost-benefit analysis between alternative states can be usefully employed to determine the ecosystem service consequences of land-use change, but monetary benefits should be subject to additional sensitivity analysis; () both biophysical indicators and monetary values can be standardised using rose plots, to illustrate the magnitude of synergies and trade-offs among the services; and () continued biodiversity protection measures can preserve carbon stock, although the benefit of doing so remains virtual unless an effective governance option is established to realise the monetary values.", (C) 2016 Elsevier B.V. All rights reserved.,"Peh, KSH|Thapa, I|Basnyat, M|Balmford, A|Bhattarai, GP|Bradbury, RB|Brown, C|Butchart, SHM|Dhakal, M|Gurung, H|Hughes, FMR|Mulligan, M|Pandeya, B|Stattersfield, AJ|Thomas, DHL|Walpole, M|Merriman, JC",ECOSYSTEM SERVICES,alternative state decision-making integrated valuation rapid assessment trade-off tessa,10.1016/j.ecoser.2016.05.003
86,WOS:000266820000004,2009,Finite element response sensitivity analysis of multi-yield-surface J(2) plasticity model by direct differentiation method,ELASTOPLASTIC SEISMIC RESPONSE FOUNDATION-GROUND SYSTEM STRESS-STRAIN BEHAVIOR 3-D EARTH DAMS DYNAMIC-RESPONSE SOFTWARE RELIABILITY,"Finite element (FE) response sensitivity analysis is an essential tool for gradient-based optimization methods used in various sub-fields of civil engineering such as structural optimization, reliability analysis, system identification, and finite element model updating. Furthermore, stand-alone sensitivity analysis is invaluable for gaining insight into the effects and relative importance of various system and loading parameters on system response. The direct differentiation method (DDM) is a general, accurate and efficient method to compute FE response sensitivities to FE model parameters. In this paper, the DDM-based response sensitivity analysis methodology is applied to a pressure independent multi-yield-surface J() plasticity material model, which has been used extensively to simulate the nonlinear undrained shear behavior of cohesive soils subjected to static and dynamic loading conditions. The complete derivation of the DDM-based response sensitivity algorithm is presented. This algorithm is implemented in a general-purpose nonlinear finite element analysis program. The work presented in this paper extends significantly the framework of DDM-based response sensitivity analysis, since it enables numerous applications involving the use of the multi-yield-surface J() plasticity material model. The new algorithm and its software implementation are validated through two application examples, in which DDM-based response sensitivities are compared with their counterparts obtained using forward finite difference (FFD) analysis. The normalized response sensitivity analysis results are then used to measure the relative importance of the soil constitutive parameters on the system response.", (C) 2009 Elsevier B.V. All rights reserved.,"Quan, G|Conte, JP|Elgamal, A|Yang, ZH",COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,nonlinear finite element analysis response sensitivity analysis multi-yield-surface plasticity model direct differentiation method soil material model,10.1016/j.cma.2009.02.030
87,WOS:000170761700007,2001,Integration of topology and shape optimization for design of structural components,,"This paper presents an integrated approach that supports the topology optimization and CAD-based shape optimization. The main contribution of the paper is using the geometric reconstruction technique that is mathematically sound and error bounded for creating solid models of the topologically optimized structures with smooth geometric boundary. This geometric reconstruction method extends the integration to -D applications. In addition, commercial Computer-Aided Design (CAD), finite element analysis (FEA), optimization, and application software tools are incorporated to support the integrated optimization process. The integration is carried out by first converting the geometry of the topologically optimized structure into smooth and parametric B-spline curves and surfaces. The B-spline curves and surfaces are then imported into a parametric CAD environment to build solid models of the structure. The control point movements of the B-spline curves or surfaces are defined as design variables for shape optimization, in which CAD-based design velocity field computations, design sensitivity analysis (DSA), and nonlinear programming are performed. Both -D plane stress and -D solid examples are presented to demonstrate the proposed approach.",,"Tang, PS|Chang, KH",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,cad design sensitivity analysis fea shape optimization topology optimization,10.1007/PL00013282
88,WOS:000281237800007,2010,Sensitivity Analysis of Land Unit Suitability for Conservation Using a Knowledge-Based System,FOREST ECOSYSTEM SUSTAINABILITY COLUMBIA RIVER BASIN RESERVE SELECTION NETWORKS MANAGEMENT ECOLOGY SCALES AREAS MODEL,"The availability of spatially continuous data layers can have a strong impact on selection of land units for conservation purposes. The suitability of ecological conditions for sustaining the targets of conservation is an important consideration in evaluating candidate conservation sites. We constructed two fuzzy logic-based knowledge bases to determine the conservation suitability of land units in the interior Columbia River basin using NetWeaver software in the Ecosystem Management Decision Support application framework. Our objective was to assess the sensitivity of suitability ratings, derived from evaluating the knowledge bases, to fuzzy logic function parameters and to the removal of data layers (land use condition, road density, disturbance regime change index, vegetation change index, land unit size, cover type size, and cover type change index). The amount and geographic distribution of suitable land polygons was most strongly altered by the removal of land use condition, road density, and land polygon size. Removal of land use condition changed suitability primarily on private or intensively-used public land. Removal of either road density or land polygon size most strongly affected suitability on higher-elevation US Forest Service land containing small-area biophysical environments. Data layers with the greatest influence differed in rank between the two knowledge bases. Our results reinforce the importance of including both biophysical and socio-economic attributes to determine the suitability of land units for conservation. The sensitivity tests provided information about knowledge base structuring and parameterization as well as prioritization for future data needs.",,"Humphries, HC|Bourgeron, PS|Reynolds, KM",ENVIRONMENTAL MANAGEMENT,sensitivity analysis map removal knowledge base conservation suitability land suitability fuzzy logic ecosystem management decision support,10.1007/s00267-010-9520-4
89,WOS:000302118200009,2012,GIS-based applications of sensitivity analysis for sewer models,URBAN DRAINAGE SYSTEMS CALIBRATION PERFORMANCE INFRASTRUCTURE IDENTIFICATION UNCERTAINTIES VULNERABILITY IMPACT,"Sensitivity analysis (SA) evaluates the impact of changes in model parameters on model predictions. Such an analysis is commonly used when developing or applying environmental models to improve the understanding of underlying system behaviours and the impact and interactions of model parameters. The novelty of this paper is a geo-referenced visualization of sensitivity indices for model parameters in a combined sewer model using geographic information system (GIS) software. The result is a collection of maps for each analysis, where sensitivity indices (calculated for model parameters of interest) are illustrated according to a predefined symbology. In this paper, four types of maps (an uncertainty map, calibration map, vulnerability map, and design map) are created for an example case study. This article highlights the advantages and limitations of GIS-based SA of sewer models. The conclusion shows that for all analyzed applications, GIS-based SA is useful for analyzing, discussing and interpreting the model parameter sensitivity and its spatial dimension. The method can lead to a comprehensive view of the sewer system.",,"Mair, M|Sitzenfrei, R|Kleidorfer, M|Moderl, M|Rauch, W",WATER SCIENCE AND TECHNOLOGY,capacity design combined sewer system gis applications model calibration uncertainty assessment vulnerability assessment,10.2166/wst.2012.954
90,WOS:000307721100006,2012,Evolutionary topology optimization of periodic composites for extremal magnetic permeability and electrical permittivity,LEVEL-SET STRUCTURAL OPTIMIZATION DESIGN HOMOGENIZATION METAMATERIALS SHAPE MICROSTRUCTURES,"This paper presents a bidirectional evolutionary structural optimization (BESO) method for designing periodic microstructures of two-phase composites with extremal electromagnetic permeability and permittivity. The effective permeability and effective permittivity of the composite are obtained by applying the homogenization technique to the representative periodic base cell (PBC). Single or multiple objectives are defined to maximize or minimize the electromagnetic properties separately or simultaneously. The sensitivity analysis of the objective function is conducted using the adjoint method. Based on the established sensitivity number, BESO gradually evolves the topology of the PBC to an optimum. Numerical examples demonstrate that the electromagnetic properties of the resulting D and D microstructures are very close to the theoretical Hashin-Shtrikman (HS) bounds. The proposed BESO algorithm is computationally efficient as the solution usually converges in less than  iterations. The proposed BESO method can be implemented easily as a post-processor to standard commercial finite element analysis software packages, e.g. ANSYS which has been used in this study. The resulting topologies are clear black-and-white solutions (with no grey areas). Some interesting topological patterns such as Vigdergauz-type structure and Schwarz primitive structure have been found which will be useful for the design of electromagnetic materials.",,"Huang, X|Xie, YM|Jia, B|Li, Q|Zhou, SW",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,topology optimization bidirectional evolutionary structural optimization (beso) homogenization effective permeability effective permittivity,10.1007/s00158-012-0766-8
91,WOS:000275621700002,2010,"Controlling setup cost in (Q, r, L) inventory model with defective items",LEAD TIME REDUCTION,"This study discusses a mixture inventory model with back orders and lost sales in which the order quantity, reorder point, lead time and setup cost are decision variables. It is assumed that an arrival order lot may contain some defective items and the number of defective items is a random variable. There are two inventory models proposed in this paper, one with normally distributed demand and another with distribution free demand. Finally we develop two computational algorithms to obtain the optimal ordering policy, A computer code using the software Matlab is developed to derive the optimal solution and present numerical examples to illustrate the models. Additionally, sensitivity analysis is conducted with respect to the various system parameters.", (C) 2009 Elsevier Inc. All rights reserved.,"Annadurai, K|Uthayakumar, R",APPLIED MATHEMATICAL MODELLING,setup cost lead time defective items minimax distribution-free procedure computational algorithm optimization,10.1016/j.apm.2009.04.010
92,WOS:000367774700005,2015,Chaospy: An open source tool for designing methods of uncertainty quantification,,"The paper describes the philosophy, design, functionality, and usage of the Python software toolbox Chaospy for performing uncertainty quantification via polynomial chaos expansions and Monte Carlo simulation. The paper compares Chaospy to similar packages and demonstrates a stronger focus on defining reusable software building blocks that can easily be assembled to construct new, tailored algorithms for uncertainty quantification. For example, a Chaospy user can in a few lines of high-level computer code define custom distributions, polynomials, integration rules, sampling schemes, and statistical metrics for uncertainty analysis. In addition, the software introduces some novel methodological advances, like a framework for computing Rosenblatt transformations and a new approach for creating polynomial chaos expansions with dependent stochastic variables. (C)  The Authors.", Published by Elsevier B.V.,"Feinberg, J|Langtangen, HP",JOURNAL OF COMPUTATIONAL SCIENCE,uncertainty quantification polynomial chaos expansions monte carlo simulation rosenblatt transformations python package,10.1016/j.jocs.2015.08.008
93,WOS:000299955600008,2012,Preliminary flood risk assessment: the case of Athens,MULTICRITERIA EVALUATION DECISION-MAKING URBAN AREAS MANAGEMENT SCENARIOS SYSTEMS GIS,"Flood mapping, especially in urban areas, is a demanding task requiring substantial (and usually unavailable) data. However, with the recent introduction of the EU Floods Directive (//EC), the need for reliable, but cost effective, risk mapping at the regional scale is rising in the policy agenda. Methods are therefore required to allow for efficiently undertaking what the Directive terms ""preliminary flood risk assessment,"" in other words a screening of areas that could potentially be at risk of flooding and that consequently merit more detailed attention and analysis. Such methods cannot rely on modeling, as this would require more data and effort that is reasonable for this high-level, screening phase. This is especially true in urban areas, where modeling requires knowledge of the detailed urban terrain, the drainage networks, and their interactions. A GIS-based multicriteria flood risk assessment methodology was therefore developed and applied for the mapping of flood risk in urban areas. This approach quantifies the spatial distribution of flood risk and is able to deal with uncertainties in criteria values and to examine their influence on the overall flood risk assessment. It can further assess the spatially variable reliability of the resulting maps on the basis of the choice of method used to develop the maps. The approach is applied to the Greater Athens area and validated for its central and most urban part. A GIS database of economic, social, and environmental criteria contributing to flood risk was created. Three different multicriteria decision rules (Analytical Hierarchy Process, Weighted Linear Combination and Ordered Weighting Averaging) were applied, to produce the overall flood risk map of the area. To implement this methodology, the IDRISI Andes GIS software was customized and used. It is concluded that the results of the analysis are a reasonable representation of actual flood risk, on the basis of their comparison with historical flood events.",,"Kandilioti, G|Makropoulos, C",NATURAL HAZARDS,floods gis multicriteria evaluation (mce) sensitivity analysis uncertainty vulnerability,10.1007/s11069-011-9930-5
94,WOS:000321813200001,2013,Modelling of groundwater infiltration into sewer systems,,"Groundwater infiltration into urban sewers represents a problem that influences costs and management of technical systems. The hydrodynamic groundwater software MODFLOW is used to analyse the influencing variables of the infiltration processes. Besides the hydraulic conductivity of the soil and the piezometric head in the vicinity of the sewer pipe, properties of the sewer trench, the shape and the size of leaks are important influencing factors. A non-linear-regression method is applied to develop a one-dimensional approach in accordance with the MODFLOW results and Darcy's law. Monte Carlo simulations and the developed one-dimensional model are used to assess the leak area and the range of pressure loss in the vicinity of the pipe leaks. By additional sensitivity analysis it was found that the infiltration factor and the conductivity of the backfill are very important for the calculation of the leak area.",,"Karpf, C|Krebs, P",URBAN WATER JOURNAL,infiltration sewer modelling parameter sensitivity monte carlo simulations,10.1080/1573062X.2012.724077
95,WOS:000332448800030,2014,PERSiST: a flexible rainfall-runoff modelling toolkit for use with the INCA family of models,MEDITERRANEAN FORESTED CATCHMENT CLIMATE-CHANGE IMPACTS WATER-QUALITY MULTISITE CALIBRATION NITROGEN MODEL HYDROLOGY UNCERTAINTY DYNAMICS FLOW ACIDIFICATION,"Runoff generation processes and pathways vary widely between catchments. Credible simulations of solute and pollutant transport in surface waters are dependent on models which facilitate appropriate, catchment-specific representations of perceptual models of the runoff generation process. Here, we present a flexible, semi-distributed landscape-scale rainfall-runoff modelling toolkit suitable for simulating a broad range of user-specified perceptual models of runoff generation and stream flow occurring in different climatic regions and landscape types. PERSiST (the Precipitation, Evapotranspiration and Runoff Simulator for Solute Transport) is designed for simulating present-day hydrology; projecting possible future effects of climate or land use change on runoff and catchment water storage; and generating hydrologic inputs for the Integrated Catchments (INCA) family of models. PERSiST has limited data requirements and is calibrated using observed time series of precipitation, air temperature and runoff at one or more points in a river network. Here, we apply PERSiST to the river Thames in the UK and describe a Monte Carlo tool for model calibration, sensitivity and uncertainty analysis.",,"Futter, MN|Erlandsson, MA|Butterfield, D|Whitehead, PG|Oni, SK|Wade, AJ",HYDROLOGY AND EARTH SYSTEM SCIENCES,,10.5194/hess-18-855-2014
96,WOS:000295765200012,2011,Modelling nitrogen transformations in waters receiving mine effluents,STABILIZATION PONDS WASTE-WATER POTENTIAL DENITRIFICATION BIOLOGICAL TREATMENT N-TRANSFORMATION ORGANIC-MATTER LAKE ECOSYSTEM SHALLOW LAKES DUCKWEED POND RIVER-BASINS,"This paper presents a biogeochemical model developed for a clarification pond receiving ammonium nitrogen rich discharge water from the Boliden concentration plant located in northern Sweden. Present knowledge about nitrogen (N) transformations in lakes is compiled in a dynamic model that calculates concentrations of the six N species (state variables) ammonium-N (N(am)), nitrate-N (N(ox)), dissolved organic N in water (N(org)), N in phytoplankton (N(pp)), in macrophytes (N(mp)) and in sediment (N(sed)). It also simulates the rate of  N transformation processes occurring in the water column and sediment as well as water-sediment and water-atmosphere interactions. The model was programmed in the software Powersim using  data, whilst validation was performed using data from  to . The sensitivity analysis showed that the state variables are most sensitive to changes in the coefficients related to the temperature dependence of the transformation processes. A six-year simulation of N(am) showed stable behaviour over time. The calibrated model rendered coefficients of determination (R()) of ., . and . for N(am), N(ox) and N(org) respectively. Performance measures quantitatively expressing the deviation between modelled and measured data resulted in values close to zero, indicating a stable model structure. The simulated denitrification rate was on average five times higher than the ammonia volatilisation rate and about three times higher than the permanent burial of N(sed) and, hence, the most important process for the permanent removal of N. The model can be used to simulate possible measures to reduce the nitrogen load and, after some modification and recalibration, it can be applied at other mine sites affected by N rich effluents.", (C) 2011 Elsevier B.V. All rights reserved.,"Chlot, S|Widerlund, A|Siergieiev, D|Ecke, F|Husson, E|Ohlander, B",SCIENCE OF THE TOTAL ENVIRONMENT,process water ammonium natural removal biogeochemical modelling northern sweden boliden,10.1016/j.scitotenv.2011.07.024
97,WOS:000358060800012,2015,A software development framework for structural optimization considering non linear static responses,DYNAMIC TOPOLOGY OPTIMIZATION EQUIVALENT LOADS,"In the real world, structural systems may not have linear static characteristics. However, structural optimization has been developed based on static responses because sensitivity analysis regarding static finite element analysis is developed quite well. Analyses other than static analysis are heavily required in the engineering community these days. Techniques for such analyses have been extensively developed and many software systems using the finite element method are easily available in the market. On the other hand, development of structural optimization using such analyses is fairly slow due to many obstacles. One obstacle is that it is very difficult and expensive to consider the nonlinearities or dynamic effects in the way of conventional optimization. Recently, the equivalent static loads method for non linear static response structural optimization (ESLSO) has been proposed for structural optimization with various responses: linear dynamic response, nonlinear static response, and nonlinear dynamic response. In ESLSO, finite element analysis other than static analysis is performed, equivalent static loads (ESLs) are generated, linear static response structural optimization is carried out with the ESLs and the process iterates. A software system for the automatic use of ESLSO is developed and described. One of the advantages of ESLSO is that it can use well developed commercial software systems for structural analysis and linear static response structural optimization. Various analysis and optimization systems are integrated in the developed system. The structure of the system is systematically defined and the software is developed by the C++ language on the Windows operating system.",,"Lee, HA|Park, GJ",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,structural optimization equivalent static loads (esls) equivalent static loads method for non linear static response structural optimization (eslso),10.1007/s00158-015-1228-x
98,WOS:000334298800035,2014,Direct Production of Gasoline and Diesel Fuels from Biomass via Integrated Hydropyrolysis and Hydroconversion Process-A Techno- economic Analysis,LIGNOCELLULOSIC BIOMASS FAST PYROLYSIS TECHNOECONOMICS GASIFICATION,"A techno-economic analysis (TEA) is performed to investigate the production of gasoline and diesel range hydrocarbon fuels by conversion of woody biomass via Gas Technology Institute (GTI)'s integrated hydropyrolysis plus hydroconversion (IH) process. The processing capacity is  dry metric tonnes ( dry US tons) of woody biomass per day. Major process areas include catalytic hydropyrolysis, catalytic hydroconversion, on-site hydrogen production, feedstock handling and storage, hydrocarbon absorber, sour water stripper, hydrogen sulfide scrubber, distillation tower, and all other operations support utilities. The TEA incorporates applicable commercial technologies, process modeling using Aspen HYSYS software, equipment cost estimation, and discounted cash flow analysis. The resulting minimum fuel selling price is $. per gallon (or $. per gallon of gasoline equivalent) in  US dollars. The process yields  gallons of liquid fuels per dry US ton of woody biomass feedstock, for an annual fuel production rate of  million gallons at % on-stream time. The estimated total capital investment for an nth-plant is $ million. A sensitivity analysis captures uncertainties in costs and plant performance. Results from this TEA can serve as the baseline for future comparison and as a basis for comparing this process to other biomass-to-liquid fuel pathways."," (c) 2013 American Institute of Chemical Engineers Environ Prog, 33: 609-617, 2014","Tan, ECD|Marker, TL|Roberts, MJ",ENVIRONMENTAL PROGRESS & SUSTAINABLE ENERGY,biomass to fuels hydropyrolysis gasoline diesel techno-economic analysis process modeling,10.1002/ep.11791
99,WOS:000416715900003,2017,Simulation of a workflow execution as a real Cloud by adding noise,TOOLKIT,"Cloud computing provides a cheap and elastic platform for executing large scientific workflow applications, but it rises two challenges in prediction of makespan (total execution time): performance instability of Cloud instances and variant scheduling of dynamic schedulers. Estimating the makespan is necessary for IT managers in order to calculate the cost of execution, for which they can use Cloud simulators. However, the ideal simulated environment produces the same output for the same workflow schedule and input parameters and thus can not reproduce the Cloud variant behavior. In this paper, we define a model and a methodology to add a noise to the simulation in order to equalise its behavior with the Clouds' one. We propose several metrics to model a Cloud fluctuating behavior and then by injecting them within the simulator, it starts to behave as close as the real Cloud. Instead of using a normal distribution naively by using mean value and standard deviation of workflow tasks' runtime, we inject two noises in the tasks' runtime: noisiness of tasks within a workflow (defined as average runtime deviation) and noisiness provoked by the environment over the whole workflow (defined as average environmental deviation). In order to measure the quality of simulation by quantifying the relative difference between the simulated and measured values, we introduce the parameter inaccuracy. A series of experiments with different workflows and Cloud resources were conducted in order to evaluate our model and methodology. The results show that the inaccuracy of the makespan's mean value was reduced up to  times compared to naively using the normal distribution. Additionally, we analyse the impact of particular workflow and Cloud parameters, which shows that the Cloud performance instability is simulated more correctly for small instance type (inaccuracy of up to .%), instead of medium (inaccuracy of up to %), regardless of the workflow. Since our approach requires collecting data by executing the workflow in the Cloud in order to learn its behavior, we conduct a comprehensive sensitivity analysis. We determine the minimum amount of data that needs to be collected or minimum number of test cases that needs to be repeated for each experiment in order to get less than % inaccuracy for our noising parameter. Additionally, in order to reduce the number of experiments and determine the dependency of our model against Cloud resource and workflow parameters, the conducted comprehensive sensitivity analysis shows that the correctness of our model is independent of workflow parallel section size. With our sensitivity anal- ysis, we show that we can reduce the inaccuracy of the naive approach with only % of total number of executions per experiment in the learning phase. In our case,  executions per experiment instead of , and only half of all experiments, which means down to %, i.e.  test cases instead of .", (C) 2017 Elsevier B.V. All rights reserved.,"Matha, R|Ristov, S|Prodan, R",SIMULATION MODELLING PRACTICE AND THEORY,inaccuracy makespan metric modelling precision simulator,10.1016/j.simpat.2017.09.003
100,WOS:000266765700013,2009,Nitritation performance and biofilm development of co- and counter-diffusion biofilm reactors: Modeling and experimental comparison,MEMBRANE-AERATED BIOFILM AUTOTROPHIC NITROGEN REMOVAL WASTE-WATER TREATMENT PARTIAL NITRIFICATION NITRIFYING BIOFILM DENITRIFICATION BIOREACTOR AMMONIA STRATIFICATION OXIDATION,"A comparative study was conducted on the start-up performance and biofilm development in two different biofilm reactors with aim of obtaining partial nitritation. The reactors were both operated under oxygen limited conditions, but differed in geometry. While substrates (O-, NH) co-diffused in one geometry, they counter-diffused in the other. Mathematical simulations of these two geometries were implemented in two -D multispecies biofilm models using the AQUASIM software. Sensitivity analysis results showed that the oxygen mass transfer coefficient (K-i) and maximum specific growth rate of ammonia-oxidizing (AOB) and nitrite-oxidizing bacteria (NOB) were the determinant parameters in nitrogen conversion simulations. The modeling simulations demonstrated that Ki had stronger effects on nitrogen conversion at lower (- m d(-)) than at the higher values (>  m d(-)). The experimental results showed that the counter-diffusion biofilms developed faster and attained a larger maximum biofilm thickness than the co-diffusion biofilms. Under oxygen limited condition (DO < . mg L-) and high pH (.-.), nitrite accumulation was triggered more significantly in co-diffusion than counter-diffusion biofilms by increasing the applied ammonia loading from . to . g NH+-N L- d(-). The co- and counter-diffusion biofilms displayed very different spatial structures and population distributions after  days of operation. AOB were dominant throughout the biofilm depth in co-diffusion biofilms, while the counter-diffusion biofilms presented a stratified structure with an abundance of AOB and NOB at the base and putative heterotrophs at the surface of the biofilm, respectively. (C) ", Elsevier Ltd. All rights reserved.,"Wang, RC|Terada, A|Lackner, S|Smets, BF|Henze, M|Xia, SQ|Zhao, JF",WATER RESEARCH,nitritation co-diffusion counter-diffusion biofilm development fluorescence in situ hybridization membrane-aerated biofilm reactor,10.1016/j.watres.2009.03.017
101,WOS:000386560600005,2016,SOFTWARE RELIABILITY GROWTH MODEL WITH TEMPORAL CORRELATION IN A NETWORK ENVIRONMENT,CHANGE-POINT NOISE,"Increasingly software systems are developed to provide great flexibility to customers but also introduce great uncertainty for system development. The uncertain behavior of fault-detection rate has irregular fluctuation and is described as a Markovian stochastic processes (white noise). However, in many cases the white noise idealization is insufficient, and real fluctuations are always correlated and correlated fluctuations (or colored noise) are non-Markovian stochastic processes. We develop a new model to quantify the uncertainties within non-homogeneous Poisson process (NHPP) in the noisy environment. Based on a stochastic model of the software fault detection process, the environmental uncertainties collectively are treated as a noise of arbitrary distribution and correlation structure. Based on the stochastic model, the analytical solution can be derived. To validate our model, we consider five particular scenarios with distinct environmental uncertainty. Experimental comparisons with existing methods demonstrate that the new framework shows a closer fitting to actual data and exhibits a more accurately predictive power.",,"Xu, JJ|Yao, SZ|Yang, SK|Wang, P",INTERNATIONAL JOURNAL FOR UNCERTAINTY QUANTIFICATION,uncertainty quantification reliability nhpp noise correlation,10.1615/Int.J.UncertaintyQuantification.2016016194
102,WOS:000237701400007,2006,Kinetic model for the degradation of MTBE by Fenton's oxidation,TERT-BUTYL ETHER HABER-WEISS REACTION HYDROGEN-PEROXIDE AQUEOUS-SOLUTION HYDROXYL RADICALS RATE CONSTANTS ULTRASONIC IRRADIATION SONOLYTIC DESTRUCTION ORGANIC-COMPOUNDS PULSE RADIOLYSIS,"A kinetic model for the degradation of methyl tert-butyl ether (MTBE) in batch reactors with Fenton's oxidation (Fe+/HO) in aqueous solutions was developed. This kinetic model consists of equations accounting for () hydrogen peroxide chemistry in aqueous solution, () iron speciation, and () MTBE oxidation. The mechanisms of MTBE degradation, and the resultant pathways for the formation and degradation of the byproducts, were proposed on the basis of previous studies. A set of stiff nonlinear ordinary differential equations that describe the rate of formation of each species in this batch system was solved using Matlab (R) software. The kinetic model was validated with published experimental data. The degradation of MTBE by Fenton's oxidation is predicted well by the model, as are the formation and degradation of byproducts, especially methyl acetate (MA) and tert-butyl alcohol (TBA). Finally, a sensitivity analysis based on calculating the sum of the squares of the residuals (SSR) after making a perturbation of one rate constant at a time was applied to discern the effect of each reaction on MTBE disappearance.",,"Al Ananzeh, N|Bergendahl, JA|Thompson, RW",ENVIRONMENTAL CHEMISTRY,oxidation fenton's reagent kinetic modeling mtbe water,10.1071/EN05044
103,WOS:000330079500009,2014,Exploring incomplete information in maintenance materials inventory optimization,SPARE PARTS INTERMITTENT DEMAND STOCK CONTROL CONTROL PERFORMANCE REORDER POINT PARAMETERS SYSTEM MODEL,"Purpose Ensuring the sufficient service level is essential for critical materials in industrial maintenance. This study aims to evaluate the use of statistically imperfect data in a stochastic simulation-based inventory optimization where items' failure characteristics are derived from historical consumption data, which represents a real-life situation in the implementation of such an optimization model. Design/methodology/approach - The risks of undesired shortages were evaluated through a service-level sensitivity analysis. The service levels were simulated within the error of margin of the key input variables by using StockOptim optimization software and real data from a Finnish steel mill. A random sample of  inventory items was selected. Findings - Service-level sensitivity is item specific, but, for many items, statistical imprecision in the input data causes significant uncertainty in the service level. On the other hand, some items seem to be more resistant to variations in the input data than others. Research limitations/implications - The case approach, with one simulation model, limits the generalization of the results. The possibility that the simulation model is not totally realistic exists, due to the model's normality assumptions. Practical implications - Margin of error in input data estimation causes a significant risk of not achieving the required service level. It is proposed that managers work to improve the preciseness of the data, while the sensitivity analysis against statistical uncertainty, and a correction mechanism if necessary, should be integrated into optimization models. Originality/value - The output limitations in the optimization, i.e. service level, are typically stated precisely, but the capabilities of the input data have not been addressed adequately. This study provides valuable insights into ensuring the availability of-critical materials.",,"Puurunen, A|Majava, J|Kess, P",INDUSTRIAL MANAGEMENT & DATA SYSTEMS,inventory optimization maintenance materials service-level sensitivity spare parts stockoptim,10.1108/IMDS-01-2013-0025
104,WOS:000249895700008,2007,Automatic concept model generation for optimisation and robust design of passenger cars,,"A fully automated method of structural optimisation for the body in white structure is presented. The body in white is a technical term for the car body without windows and closures. The iterations in the optimisation loop comprise the following steps: fully parameterised design creation, automated meshing and model assembly, parallel computation and evaluation. For this purpose several free and commercially available software applications were combined, including: SFE concept, Hypermesh, Perl, Matlab, and Radioss. The optimisation was conducted using Genetic Algorithms (GA), which are ideally suited to solve problems with solution spaces that are too large to be exhaustively searched. The viability of the method is demonstrated for a vehicle component model of a front bumper system utilizing both material and geometry related properties as design variables. (c) ", Elsevier Ltd. and Civil-Comp Ltd. All rights reserved.,"Hilmann, J|Paas, M|Haenschke, A|Vietor, T",ADVANCES IN ENGINEERING SOFTWARE,vehicle engineering structural optimisation sfe concept genetic algorithms finite element method parametric modelling sensitivity analysis,10.1016/j.advengsoft.2006.08.031
105,WOS:000291785400002,2011,Concurrent Decisions on Design Concept and Material Using Analytical Hierarchy Process at the Conceptual Design Stage,MATERIALS SELECTION SYSTEM,"There is an increased study for considering the precise decisions on the design concept (DC) and material concurrently at the early stage of development of product. Inappropriate decisions on DC and material always lead to huge cost involvement and ultimately drive toward premature component or product failure. To overcome this problem, concurrent engineering (CE) is an approach which allows designers to consider early decision making (EDM) need to be implemented. To illustrate the use of CE principle at the early stage of design process, a concept selection framework called concurrent DC selection and materials selection (CDCSMS) was proposed. In order to demonstrate the proposed CDCSMS framework, eight DC s and six different types of composite materials of automotive bumper beam have been considered. Both of these decisions were then verified by performing various scenarios of sensitivity analysis by using analytical hierarchy process through utilizing Expert Choice software.",,"Hambali, A|Sapuan, SM|Rahim, AS|Ismail, N|Nukman, Y",CONCURRENT ENGINEERING-RESEARCH AND APPLICATIONS,concurrent engineering analytical hierarchy process early decision making,10.1177/1063293X11408138
106,WOS:000362007900004,2015,Estimation of evapotranspiration from ground-based meteorological data and global land data assimilation system (GLDAS),ALFALFA-REFERENCE EVAPOTRANSPIRATION REFERENCE CROP EVAPOTRANSPIRATION KEY CLIMATIC VARIABLES PENMAN-MONTEITH POTENTIAL EVAPOTRANSPIRATION RIVER-BASIN SENSITIVITY ANALYSES EVAPO-TRANSPIRATION UNITED-STATES CHINA,"Evapotranspiration (ET) is one of the most significant factors in understanding global hydrological budgets, and its accurate estimation is crucial for understanding water balance and developing efficient water resource management plans. For calculation of reference ET (ETref), the meteorological data from weather stations have been widely used for estimation at the point scale; however, meteorological data from the global land data assimilation system (GLDAS) at the regional scale are rarely used for the estimation of ET. In this study,  different equations provided in the Reference Evapotranspiration Calculator Software (REF-ET) were utilized for estimating ETref with GLDAS and point scale data collected at  observation sites in the Korean Peninsula during . Using ETref calculated from observation and GLDAS,  equations were evaluated by estimating the overall rank number, as determined by the correlation coefficient, normalized standard deviation, bias, and root mean square error (RMSE). Results showed that the Penman (Proc R Soc Lond Ser A Math Phys Sci :-, ) FAO- Penman-Monteith,  Kpen equation (combination equations), the  Makkink, Priestley-Taylor equation (radiation based equation), and the  Hargreaves equation had a good overall rank. Using the six selected equations, seasonal analysis was conducted and evaluated using the bias and RMSE. Comparison of the ETref gathered from observation and GLDAS revealed that both of them showed similar seasonal variation, although ETref calculated from GLDAS were underestimated. Sensitivity analysis conducted by changing three main climatic variables (i.e., temperature, wind speed, and sunshine hours) by +/- , +/- , +/- , +/- , and +/-  % with one variable fixed also revealed that ETref was more affected by air temperature than sunshine hours and wind speed throughout the  selected stations.",,"Park, J|Choi, M",STOCHASTIC ENVIRONMENTAL RESEARCH AND RISK ASSESSMENT,evapotranspiration reference evapotranspiration gldas sensitivity analysis,10.1007/s00477-014-1004-2
107,WOS:000276253100012,2010,Numerical analysis of the unsteady flow in the near-tongue region in a volute-type centrifugal pump for different operating points,PRESSURE-FLUCTUATIONS IMPELLER PULSATIONS FREQUENCY,"An investigation is presented on the unsteady flow behaviour near the tongue region of a single-suction volute-type centrifugal pump with a specific speed of .. For this study, the flow through the test pump, which was available at laboratory, was simulated by means of a commercial CFD software that solved the Navier-Stokes equations for three-dimensional unsteady flow (D-URANS). A sensitivity analysis of the numerical model was performed in order to impose appropriate parameters regarding grid size, time step size and turbulence model. The predictions of the numerical model were contrasted with experimental results of both global (flow-head curve and static pressure distribution at volute front side) and unsteady variables (unsteady pressure distribution at the volute front side filtered at the blade-passing frequency). Once validated, the model was used to study the flow pulsations associated to the interaction between the impeller blades and the volute tongue as a function of the flow rate, for several flow rates ranging from % to % of the nominal flow rate. The study allowed relating the blade passage with the pulsations of pressure and tangential and radial velocity at a number of reference locations in the near-tongue region. The numerical model was also used to evaluate the evolution of the leakage flow between the impeller-tongue gap and of the flow exiting the impeller through some specific angular intervals, during one single-blade passage. (C) ", Elsevier Ltd. All rights reserved.,"Barrio, R|Parrondo, J|Blanco, E",COMPUTERS & FLUIDS,centrifugal pump numerical simulation unsteady flow rotor-stator interaction blade-passing frequency,10.1016/j.compfluid.2010.01.001
108,WOS:000388155500003,2016,GTApprox: Surrogate modeling for industrial design,SENSITIVITY-ANALYSIS GAUSSIAN-PROCESSES REGRESSION REGULARIZATION ALGORITHM SELECTION MACHINE SPLINES SAMPLES EXPERTS,"We describe GTApprox - a new tool for medium-scale surrogate modeling in industrial design. Compared to existing software, GTApprox brings several innovations: a few novel approximation algorithms, several advanced methods of automated model selection, novel options in the form of hints. We demonstrate the efficiency of GTApprox on a large collection of test problems. In addition, we describe several applications of GTApprox to real engineering problems. (C) ", Elsevier Ltd. All rights reserved.,"Belyaev, M|Burnaev, E|Kapushev, E|Panov, M|Prikhodko, P|Vetrov, D|Yarotsky, D",ADVANCES IN ENGINEERING SOFTWARE,approximation surrogate model surrogate-based optimization,10.1016/j.advengsoft.2016.09.001
109,WOS:000236055400006,2006,Simulation of yield decline as a result of water stress with a robust soil water balance model,DEFICIT IRRIGATION CROP MODELS GROWTH WHEAT MANAGEMENT,"The relative yield decline that is expected under specific levels of water stress at different moments in the growing period is estimated by integrating the FAO K, approach [Doorenbos, J., Kassam, A.H., . Yield response to water. FAO Irrigation and Drainage Paper No. . Rome, Italy] in the soil water balance model BUDGET. The water stored in the root zone is determined in the soil water balance model on a daily basis by keeping track of incoming and outgoing water fluxes at its boundary. Given the simulated soil water content in the root zone, the corresponding crop water stress is deter-mined. Subsequently, the yield decline is estimated with the Ky approach. In the Ky approach the relation between water stress in a particular growth stage and the corresponding expected yield is described by a linear function. To account for the effect of water stresses in the various growth stages, the multiplicative, seasonal and minimal approach are integrated in the model. To evaluate the model, the simulated yields for two crops under various levels of water stress in two different environments were compared with observed yields: winter wheat under three different water application levels in the North of Tunisia, and maize in three different farmers' fields in different years in the South West of Burkina Faso. Simulated crop yields agreed well with observed yields for both locations using the multiplicative approach. The correlation value (R ) between observed and simulated yields ranged from . to . with very high modeling efficiencies. The root mean square error values are relatively small and ranged between  and %. The minimal and seasonal approaches performed significantly less accurately in both of the study areas. Estimation of yields on basis of relative transpiration performed significantly better than estimations on basis of relative evapotranspiration in Burkina Faso. A sensitivity analysis showed that the model is robust and that good estimates can be obtained in both regions even by using indicative values for the required crop and soil parameters. The minimal input requirement, the robustness of the model and its ability to describe the effect on seasonal yield of water stress occurring at particular moments in the growing period, make the model very useful for the design of deficit irrigation strategies. BUDGET is public domain software and hence freely available. An installation disk and manual can be downloaded from the web.", (c) 2005 Elsevier B.V. All rights reserved.,"Raes, D|Geerts, S|Kipkorir, E|Wellens, J|Sahli, A",AGRICULTURAL WATER MANAGEMENT,water productivity maize winter wheat k-y approach yield estimation soil water balance,10.1016/j.agwat.2005.04.006
110,WOS:000186310600006,2003,Direct and adjoint sensitivity analysis of chemical kinetic systems with KPP: Part I - theory and software tools,VARIATIONAL DATA ASSIMILATION ATMOSPHERIC CHEMISTRY PROBLEMS GREENS-FUNCTION METHOD NON-LINEAR SYSTEMS STIFF ODE SOLVERS ROSENBROCK METHOD MODELS IMPLEMENTATION IMPLICIT EXPLICIT,"The analysis of comprehensive chemical reactions mechanisms, parameter estimation techniques, and variational chemical data assimilation applications require the development of efficient sensitivity methods for chemical kinetics systems. The new release (KPP-.) of the kinetic preprocessor (KPP) contains software tools that facilitate direct and adjoint sensitivity analysis. The direct-decoupled method, built using BDF formulas, has been the method of choice for direct sensitivity studies. In this work, we extend the direct-decoupled approach to Rosenbrock stiff integration methods. The need for Jacobian derivatives prevented Rosenbrock methods to be used extensively in direct sensitivity calculations; however, the new automatic and symbolic differentiation technologies make the computation of these derivatives feasible. The direct-decoupled method is known to be efficient for computing the sensitivities of a large number of output parameters with respect to a small number of input parameters. The adjoint modeling is presented as an efficient tool to evaluate the sensitivity of a scalar response function with respect to the initial conditions and model parameters. In addition, sensitivity with respect to time-dependent model parameters may be obtained through a single backward integration of the adjoint model. KPP software may be used to completely generate the continuous and discrete adjoint models taking full advantage of the sparsity of the chemical mechanism. Flexible direct-decoupled and adjoint sensitivity code implementations are achieved with minimal user intervention. In a companion paper, we present an extensive set of numerical experiments that validate the KPP software tools for several direct/adjoint sensitivity applications, and demonstrate the efficiency of KPP-generated sensitivity code implementations. (C) ", Elsevier Ltd. All rights reserved.,"Sandu, A|Daescu, DN|Carmichael, GR",ATMOSPHERIC ENVIRONMENT,chemical kinetics sensitivity analysis direct-decoupled method adjoint model,10.1016/j.atmosenv.2003.08.019
111,WOS:000239980800018,2006,"Series of experiments for empirical validation of solar gain modeling in building energy simulation codes - Experimental setup, test cell characterization, specifications and uncertainty analysis",SPACE ANALYSIS TOOLS PROGRAMS,"Empirical validation of building energy simulation codes is an important component in understanding the capacity and limitations of the software. Within the framework of Task /Annex  of the International Energy Agency (IEA), a series of experiments was performed in an outdoor test cell. The objective of these experiments was to provide a high-quality data set for code developers and modelers to validate their solar gain models for windows with and without shading devices. A description of the necessary specifications for modeling these experiments is provided in this paper, which includes information about the test site location, experimental setup, geometrical and thermophysical cell properties including estimated uncertainties. Computed overall thermal cell properties were confirmed by conducting a steady-state experiment without solar gains. A transient experiment, also without solar gains, and corresponding simulations from four different building energy simulation codes showed that the provided specifications result in accurate thermal cell modeling. A good foundation for the following experiments with solar gains was therefore accomplished. (c) ", Elsevier Ltd. All rights reserved.,"Manz, H|Loutzenhiser, P|Frank, T|Strachan, PA|Bundi, R|Maxwell, G",BUILDING AND ENVIRONMENT,building energy simulation empirical validation test cell specification,10.1016/j.buildenv.2005.07.020
112,WOS:000252516900010,2008,Self-adjoint sensitivity analysis of lossy dielectric structures with electromagnetic time-domain simulators,MICROWAVE IMAGE-RECONSTRUCTION OPTIMAL-DESIGN METHOD GRIDS,"We present an efficient self-adjoint approach for the computation of response derivatives in lossy inhomogencous structures with time-domain electromagnetic solvers. Our approach yields the responses and their derivatives with only one system analysis regardless of the number of optimizable parameters. The only requirement is to access the field solution at the perturbation grid points. The computation is performed as an independent post-process outside the solver. This makes our approach easy to implement as stand-alone software, which aids microwave design based on commercial computer-aided design packages. We show that our sensitivity analysis approach yields Jacobians of second-order accuracy for lossy dielectric structures. The approach is verified through -D, -D and -D examples using the time-domain field solutions obtained with solvers based on the finite-difference time-domain (FDTD) and transmission line modeling (TLM) methods."," Copyright (c) 2007 John Wiley & Sons, Ltd.","Song, YP|Li, Y|Nikolova, NK|Bakr, MH",INTERNATIONAL JOURNAL OF NUMERICAL MODELLING-ELECTRONIC NETWORKS DEVICES AND FIELDS,time-domain analysis sensitivity analysis adjoint-variable methods tlm method fdtd method,10.1002/jnm.659
113,WOS:000378360600027,2016,Operational snow mapping with simplified data assimilation using the seNorge snow model,WATER EQUIVALENT COVERED AREA SWISS ALPS DEPTH PREDICTION CALIBRATION CHALLENGES RADIATION NORWAY SCHEME,"Frequently updated maps of snow conditions are useful for many applications, e.g., for avalanche and flood forecasting services, hydropower energy situation analysis, as well as for the general public. Numerical snow models are often applied in snow map production for operational hydrological services. However, inaccuracies in the simulated snow maps due to model uncertainties and the lack of suitable data assimilation techniques to correct them in near-real time may often reduce the usefulness of the snow maps in operational use. In this paper the revised seNorge snow model (v...) for snow mapping is described, and a simplified data assimilation procedure is introduced to correct detected snow model biases in near real-time. The data assimilation procedure is theoretically based on the Bayesian updating paradigm and is meant to be pragmatic with modest computational and input data requirements. Moreover, it is flexible and can utilize both point-based snow depth and satellite-based areal snow-covered area observations, which are generally the most common data-sources of snow observations. The model and analysis codes as well as the ""R"" statistical software are freely available. All these features should help to lower the challenges and hurdles hampering the application of data-assimilation techniques in operational hydrological modeling. The steps of the data assimilation procedure (evaluation, sensitivity analysis, optimization) and their contribution to significantly increased accuracy of the snow maps are demonstrated with a case from eastern Norway in winter /.", (C) 2016 Elsevier B.V. All rights reserved.,"Saloranta, TM",JOURNAL OF HYDROLOGY,snow modeling snow mapping data assimilation,10.1016/j.jhydrol.2016.03.061
114,WOS:000229363800003,2005,Modeling distributed software defect removal effectiveness in the presence of code churn,,"Two types of discrete defect removal models that consider the dynamics of code churn behavior during software testing phases under distributed software development environment are proposed. The first model is based on a sequential debugging process, while the second model is based on an iterative debugging process during each testing phase. The mathematical relationship between the number of defects detected during a testing phase and the total estimated remaining defects at the end of the same testing phase for both models are elaborated in detail. The defect detection ratio is identified to have the greatest contribution to the variance of the estimated number of remaining defects based on the sensitivity analysis using Monte-Carlo simulation. Using the proposed models, we quantitatively show how to estimate the number of defects by varying the defect detection ratio, defect correction ratio, the percentage of added code, and the percentage of deleted code. (c) ", Elsevier Ltd. All rights reserved.,"Tian, L|Noore, A",MATHEMATICAL AND COMPUTER MODELLING,defect removal distributed environment imperfect debugging,10.1016/j.mcm.2004.10.021
115,WOS:000262565100007,2009,More efficient PEST compatible model independent model calibration,GLOBAL OPTIMIZATION METHODS RAINFALL-RUNOFF MODELS WATERSHED MODEL MULTIOBJECTIVE CALIBRATION AUTOMATIC CALIBRATION PARAMETER-ESTIMATION SENSITIVITY CATCHMENT SCALE HSPF,"This article describes some of the capabilities encapsulated within the Model Independent Calibration and Uncertainty Analysis Toolbox (MICUT), which was written to support the popular PEST model independent interface. We have implemented a secant version of the Levenberg-Marquardt (LM) method that requires far fewer model calls for local search than the PEST LM methodology. Efficiency studies on three distinct environmental model structures (HSPF, FASST and GSSHA) show that we can find comparable local minima with -% fewer model calls than a conventional model independent LM application. Using the secant LM method for local search, MICUT also supports global optimization through the use of a slightly modified version of a stochastic global search technique called Multi-Level Single Linkage [Rinnooy Kan, A.H.G., Timmer, G., a. Stochastic global optimization methods, part : clustering methods. Math. Program. , -; Rinnooy Kan, A.H.G., Timmer, G., b. Stochastic global optimization methods, part ii: multi level methods. Math. Program. , -.]. Comparison studies with three environmental models suggest that the stochastic global optimization algorithm in MICUT is at least as, and sometimes more efficient and reliable than the global optimization algorithms available in PEST.", Published by Elsevier Ltd.,"Skahill, BE|Baggett, JS|Frankenstein, S|Downer, CW",ENVIRONMENTAL MODELLING & SOFTWARE,calibration efficiency secant version of levenberg-marquardt multi-level single linkage,10.1016/j.envsoft.2008.09.011
116,WOS:000306043900002,2012,MVC3: A MATLAB graphical interface toolbox for third-order multivariate calibration,PARALLEL FACTOR-ANALYSIS TRILINEAR LEAST-SQUARES RESIDUAL TRILINEARIZATION CURVE RESOLUTION FOLIC-ACID 4-WAY CALIBRATION MASS-SPECTROMETRY 2ND-ORDER METHOTREXATE SAMPLES,"A new MATIAB graphical interface toolbox for implementing third-order multivariate calibration methodologies is discussed. Multivariate calibration  (MVC) is a sequel of the already described first-order (MVC) and second-order (MVC) toolboxes. MVC accepts a variety of ASCII data for input, depending on whether the third-order data are vectorized or matricized. If required, data for sample sets are arranged into four-way arrays for processing with several quadrilinear and non-quadrilinear algorithms. Quadrilinear decomposition techniques and latent structured models based on partial least-squares regression and residual trilinearization are included in the software. Appropriate working sensor regions in the three data dimensions can be selected. Model development and its subsequent application to unknown samples are straightforward from the interface. Prediction results are provided along with analytical figures of merit and standard concentration errors, as calculated by modern concepts of uncertainty propagation.", (C) 2012 Elsevier B.V. All rights reserved.,"Olivieri, AC|Wu, HL|Yu, RQ",CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS,third-order multivariate calibration matlab program graphical interface figures of merit,10.1016/j.chemolab.2012.03.018
117,WOS:000399586700038,2017,Accelerating Monte Carlo estimation with derivatives of high-level finite element models,SENSITIVITY DERIVATIVES CHAOS,In this paper we demonstrate the ability of a derivative-driven Monte Carlo estimator to accelerate the propagation of uncertainty through two high-level non-linear finite element models. The use of derivative information amounts to a correction to the standard Monte Carlo estimation procedure that reduces the variance under certain conditions. We express the finite element models in variational form using the high-level Unified Form Language (UFL). We derive the tangent linear model automatically from this high-level description and use it to efficiently calculate the required derivative information. To study the effectiveness of the derivative-driven method we consider two stochastic PDEs; a one-dimensional Burgers equation with stochastic viscosity and a three-dimensional geometrically non-linear Mooney-Rivlin hyperelastic equation with stochastic density and volumetric material parameter. Our results show that for these problems the first-order derivative-driven Monte Carlo method is around one order of magnitude faster than the standard Monte Carlo method and at the cost of only one extra tangent linear solution per estimation problem. We find similar trends when comparing with a modern non-intrusive multi-level polynomial chaos expansion method. We parallelise the task of the repeated forward model evaluations across a cluster using the ipyparallel and mpipy software tools. A complete working example showing the solution of the stochastic viscous Burgers equation is included as supplementary material., (C) 2017 Published by Elsevier B.V.,"Hauseux, P|Hale, JS|Bordas, SPA",COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,monte carlo methods uncertainty propagation tangent linear models partially intrusive methods polynomial chaos expansion parallel computing,10.1016/j.cma.2017.01.041
118,WOS:000418314300026,2017,Optimal design of a multi-echelon supply chain in a system thinking framework: An integrated financial-operational approach,ARCHITECTURE MODULARITY IMPLICATIONS NETWORK DESIGN BENDERS DECOMPOSITION STOCHASTIC OPTIMIZATION DEMAND UNCERTAINTIES INVENTORY SYSTEM PROCESS INDUSTRY TRADE CREDIT MANAGEMENT ROBUST,"The purpose of this study is to design a four-echelon supply chain network in which operational and financial dimensions have been considered by a holistic, comprehensive and rigorous viewpoint within tactical and. strategic decision-making levels. In this process, the aforementioned dimensions are extended and integrated in mathematical modeling framework. The research modeling is compared with traditional approaches, which merely rely on operational dimension to optimize profit and due to problems in profit, the research objectives changed to the multiple objectives of corporate value, change in equity and economic value added. Within this framework, a comparison is primarily made between profit and each of corporate value, change in equity and economic value added two by two (in the form of traditional approach (scenario A) and new approach (scenario B)). Then the objectives are simultaneously assessed and a compromise is gained among them through fuzzy goal programming. In this process, the effectiveness and efficiency of the scenario B is analyzed and assessed. Furthermore, a sensitivity analysis is performed on such factors as demand, return of equity rate, tax rate, production capacity and supplier capacity while the impacts of their changes on multi-objective function and satisfaction coefficient are simultaneously calculated. The mathematical model in the study is multi-product and multi-period while the values of parameters are determined under definite circumstances. For modeling and solving the mathematical model, GAMS  software and CPLEX solver are employed. To test the model, data of an Iranian petrochemical company are used. Besides highlighting the significance of the financial dimension and its integration with the operational dimension in gaining sustainable competitive advantage, the research results revealed that CVM- in contrast to change in equity and EVA- is much more favorable than the other objectives. Similarly, changes in demand had more favorable effects on multi-objective analysis.",,"Mohammadi, A|Abbasi, A|Alimohammadlou, M|Eghtesadifard, M|Khalifeh, M",COMPUTERS & INDUSTRIAL ENGINEERING,multi-echelon supply chain system mathematical modeling integrated financial/operational approach fuzzy goal programming,10.1016/j.cie.2017.10.019
119,WOS:000246812000012,2007,Comparing sensitivity analysis methods to advance lumped watershed model identification and evaluation,SURFACE PARAMETERIZATION SCHEMES DISTRIBUTED MODEL MULTIOBJECTIVE OPTIMIZATION ENVIRONMENTAL SYSTEMS HYDROLOGICAL MODELS UNCERTAINTY CALIBRATION EFFICIENT PROJECT OUTPUT,"This study seeks to identify sensitivity tools that will advance our understanding of lumped hydrologic models for the purposes of model improvement, calibration efficiency and improved measurement schemes. Four sensitivity analysis methods were tested: () local analysis using parameter estimation software (PEST), () regional sensitivity analysis (RSA), () analysis of variance (ANOVA), and () Sobol's method. The methods' relative efficiencies and effectiveness have been analyzed and compared. These four sensitivity methods were applied to the lumped Sacramento soil moisture accounting model (SAC-SMA) coupled with SNOW-. Results from this study characterize model sensitivities for two medium sized watersheds within the Juniata River Basin in Pennsylvania, USA. Comparative results for the  sensitivity methods are presented for a -year time series with  h,  h, and  h time intervals. The results of this study show that model parameter sensitivities are heavily impacted by the choice of analysis method as well as the model time interval. Differences between the two adjacent watersheds also suggest strong influences of local physical characteristics on the sensitivity methods' results. This study also contributes a comprehensive assessment of the repeatability, robustness, efficiency, and ease-of-implementation of the four sensitivity methods. Overall ANOVA and Sobol's method were shown to be superior to RSA and PEST. Relative to one another, ANOVA has reduced computational requirements and Sobol's method yielded more robust sensitivity rankings.",,"Tang, Y|Reed, P|Wagener, T|van Werkhoven, K",HYDROLOGY AND EARTH SYSTEM SCIENCES,,10.5194/hess-11-793-2007
120,WOS:000223572800017,2004,Radionuclide migration modeling through the soil-plant system as adapted for Hungarian environment,FOOD-CHAIN MODEL DYNAMIC-MODEL FALLOUT,"The migration of radionuclides released as fallout through the food-chain to humans was modelled using the MODELMAKER software. In the established dynamic environmental transfer model ETM- with compartmental structure, the principal pathways of vegetable contamination were studied specifically for the Hungarian environment. These pathways were: direct deposition on plant surface, root uptake and deposition after resuspension from the soil surface. As result of the modeling the variation of activity-concentration with time was obtained in the compartments. The validation of the model was done by comparing the calculated results with those obtained in field experiments. A sensitivity analysis of the input parameters was also carried out and the parameters were categorized by their sensitivity index (SI). According to this study, the most sensitive parameters are the daily human intake of vegetable, the distribution coefficient, the transfer factor from soil to plant and the weathering half-time. The most probable distribution types for the parameter values were also determined based on Monte Carlo simulations.", (C) 2004 Elsevier B.V. All rights reserved.,"Kabai, E|Zagyvai, P|Lang-Lazi, M|Oncsik, MB",SCIENCE OF THE TOTAL ENVIRONMENT,environmental model plants sensitivity analysis sensitivity index,10.1016/j.scitotenv.2004.03.039
121,WOS:000223741600032,2004,Technical and economic survey of low enthalpy solar installations for heating sanitary water,,"For the last decade, the energy supplies has become a very preoccupying problem, not only because of the increasing difficulties bound to oil production, but also because it is necessary today to admit that at the scale of our planet the energy resources, fossil-fuel or other, are limited. The solar energy is the only outside energy whose contribution is permanent to the global scale. However, real progress of solar field depends widely on its economic cost. In this context, this investigation focuses on a techno-economic study of a solar power unit for heating sanitary water at low enthalpy. A software program, based on a mathematical model, was elaborated, in order to identify criteria of an optimum use. These criteria take into account, in particular, the technical models, the recent technological innovations and an accurate analysis of the commercial balance (investment costs, actualised values of energy savings, profitability ratio, etc.). The solar installation production is determined by climate conditions, energy demand, collector area, material type and load capacity A sensitivity analysis is carried out to determine the influence of these parameters, and many others, on the yearly capacity. This paper presents the results of a program concerning water heating in a partial solar central power station. The study aims, on the one hand, to help the fitters and the industrials in the calculation and the design of such plants, and on the other hand, the economists will be allowed to compute the different financial and economic parameters in deal with the solar systems.",,"Khiari, B|Mabrouk, SB",DESALINATION,heating sanitary water solar central power installation economic cost mathematical model software survey profitability,10.1016/j.desal.2004.06.030
122,WOS:000371559900011,2016,Prediction of fixed-bed breakthrough curves for H2S adsorption from biogas: Importance of axial dispersion for design,,"An axially dispersed plug flow model with non-linear isotherm based on the linear driving force (LDF) approximation was used to predict the fixed-bed breakthrough curves for HS adsorption from biogas on sewage sludge thermally treated. The model was implemented and solved numerically by Comsol Multiphysics software. The predicted breakthrough curves matched very well the experimental data and were clearly better than those predictions obtained in our previous work by Aspen Adsorption assuming ideal plug flow. The comparison between the present and previous models, as well as a sensitivity analysis of the model and operational parameters, revealed that the overall mass transfer coefficient is usually underestimated when axial dispersion is neglected in a scale-up from lab scale, and hence, the importance of axial dispersion for design purposes of HS fixed-bed adsorption.", (C) 2015 Elsevier B.V. All rights reserved.,"Aguilera, PG|Ortiz, FJG",CHEMICAL ENGINEERING JOURNAL,breakthrough curves fixed bed column reactor h2s adsorption comsol modeling,10.1016/j.cej.2015.12.075
123,WOS:000260992800007,2009,The OECD software tool for screening chemicals for persistence and long-range transport potential,ORGANIC-CHEMICALS MULTIMEDIA MODELS SPATIAL RANGE UNCERTAINTY BEHAVIOR FATE,"We present the software implementation of The OECD P-OV & LRTP Screening Tool (The Tool) that is used to assess the environmental hazard of organic chemicals using metrics of overall persistence (P-OV) and long-range transport potential (LRTP). The Tool is designed to support decision making for chemical management and includes features that are recommended by the Organization for Economic Cooperation and Development (OECD) expert group on multimedia modeling. The Tool is useful for screening the environmental hazard potential of non-ionizing organic chemicals whose environmental partitioning can be described by absorptive capacities of environmental media estimated from partitioning between air, water and octanol in the laboratory. The software includes data storage functionality, and a user interface that is designed to facilitate simple data input and straightforward interpretation of the model results. The effect of uncertainties in input properties describing chemicals can be assessed with a Monte Carol analysis. The software is evaluated and illustrated by comparing results from The Tool with those from other models and by evaluating four substances that are candidates for regulation or ban under the Stockholm convention on Persistent Organic Pollutants. (C) ", Elsevier Ltd. All rights reserved.,"Wegmann, F|Cavin, L|MacLeod, M|Scheringer, M|Hungerbuhler, K",ENVIRONMENTAL MODELLING & SOFTWARE,chemicals persistent organic pollutants multimedia modeling hazard assessment uncertainty analysis decision support,10.1016/j.envsoft.2008.06.014
124,WOS:000361906900013,2015,A bootstrap method for estimating uncertainty of water quality trends,LOAD ESTIMATION REGRESSION STREAM VARIABLES MODELS TESTS,"Estimation of the direction and magnitude of trends in surface water quality remains a problem of great scientific and practical interest. The Weighted Regressions on Time, Discharge, and Season (WRTDS) method was recently introduced as an exploratory data analysis tool to provide flexible and robust estimates of water quality trends. This paper enhances the WRTDS method through the introduction of the WRTDS Bootstrap Test (WBT), an extension of WRTDS that quantifies the uncertainty in WRTDS-estimates of water quality trends and offers various ways to visualize and communicate these uncertainties. Monte Carlo experiments are applied to estimate the Type I error probabilities for this method. WBT is compared to other water-quality trend-testing methods appropriate for data sets of one to three decades in length with sampling frequencies of - observations per year. The software to conduct the test is in the EGRETci R-package.", Published by Elsevier Ltd.,"Hirsch, RM|Archfield, SA|De Cicco, LA",ENVIRONMENTAL MODELLING & SOFTWARE,water quality bootstrap trend uncertainty analysis,10.1016/j.envsoft.2015.07.017
125,WOS:000282087300021,2010,TAMkin: A Versatile Package for Vibrational Analysis and Chemical Kinetics,MOLECULAR-ORBITAL METHODS FREE-RADICAL POLYMERIZATIONS BLOCK HESSIAN APPROACH FREQUENCY NORMAL-MODES AB-INITIO CALCULATION PHASE N-ALKANES HARMONIC-ANALYSIS LARGE SYSTEMS BASIS-SET THERMOCHEMICAL KINETICS,"TAMkin is a program for the calculation and analysis of normal modes, thermochemical properties and chemical reaction rates. At present, the output from the frequently applied software programs ADF, CHARMM, CPMD, CPK, Gaussian, Q-Chem, and VASP can be analyzed. The normal-mode analysis can be performed using a broad variety of advanced models, including the standard full Hessian, the Mobile Block Hessian, the Partial Hessian Vibrational approach, the Vibrational Subsystem Analysis with or without mass matrix correction, the Elastic Network Model, and other combinations. TAMkin is readily extensible because of its modular structure. Chemical kinetics of unimolecular and bimolecular reactions can be analyzed in a straightforward way using conventional transition state theory, including tunneling corrections and internal rotor refinements. A sensitivity analysis can also be performed, providing important insight into the theoretical error margins on the kinetic parameters. Two extensive examples demonstrate the capabilities of TAMkin: the conformational change of the biological system adenylate kinase is studied, as well as the reaction kinetics of the addition of ethene to the ethyl radical. The important feature of batch processing large amounts of data is highlighted by performing an extended level of theory study, which TAMkin can automate significantly.",,"Ghysels, A|Verstraelen, T|Hemelsoet, K|Waroquier, M|Van Speybroeck, V",JOURNAL OF CHEMICAL INFORMATION AND MODELING,,10.1021/ci100099g
126,WOS:000346334900006,2015,Context-Aware Framework for Highway Bridge Inspections,,"Bridge inspections are tedious, time consuming, and complex tasks in the field that require highly specific information pertinent to the decisions at hand. The use of a centralized inspection database and bridge inspection reporting software has been explored by several state DOTs in recent years. During an inspection routine, the inspector visually assesses the condition of a particular bridge component. Based on a priori knowledge of the bridge components' taxonomic hierarchy and ontology, the inspector navigates to the form corresponding to the component. The inspector then reports the component's condition to the database. Context-aware computing offers the possibility to make inspections more efficient by reducing the time required to navigate the software and the effort spent by inspectors to learn, remember, and recall the taxonomic hierarchy and ontology of bridge components. Context-aware computing leverages environmental variables, which define the inspector's context, and delivers streamlined information, pertinent to the task at hand, to assist decision making. This paper presents a computing framework that identifies the component of interest to the inspector and automatically queries the inspection database to retrieve information relevant to the component being assessed. The framework's run-time and space complexity are analyzed and presented. The uncertainty in sensing the inspector's location and line of sight are translated into errors in identifying the component of interest. Using a case study bridge, sensitivity analysis is performed to evaluate and characterize the errors in identifying the component of interest due to the errors in tracking technologies through simulation studies and field testing. The sensitivity analysis is used to evaluate the feasibility of employing global positioning systems (GPS) and magnetic compass technologies for location and line-of-sight tracking. Finally, the authors suggest a workflow design for integrating the framework into bridge inspection reporting software.", (C) 2014 American Society of Civil Engineers.,"Akula, M|Sandur, A|Kamat, VR|Prakash, A",JOURNAL OF COMPUTING IN CIVIL ENGINEERING,bridge inspection context-aware computing contextual object identification gps coordinates simulation uncertainty,10.1061/(ASCE)CP.1943-5487.0000292
127,WOS:000251764200014,2007,Conservation planning in forest landscapes of Fennoscandia and an approach to the challenge of Countdown 2010,MODELING DEAD WOOD BOREAL FOREST BIODIVERSITY CONSERVATION SUSTAINABLE FORESTRY UNCERTAINTY ANALYSIS RESERVE SELECTION SOUTHERN FINLAND NORWAY SPRUCE MANAGEMENT EUROPE,"Effective management of biodiversity in production landscapes requires a conservation approach that acknowledges the complexity of ecological and cultural systems in time and space. Fennoscandia has experienced major loss of forest biodiversity caused by intensive forestry. Therefore, the Countdown  initiative to halt the loss of biodiversity in Europe is highly relevant to forest management in this part of the continent. As a contribution to meeting the challenge posed by Countdown , we developed a spatially explicit conservation-planning exercise that used regional knowledge on forest biodiversity to provide support for managers attempting to halt further loss of biological diversity in the region. We used current data on the distribution of  species (including  red-listed species) representing different forest habitats and ecologies along with forest data within the frame of modern conservation software to devise a map of priority areas for conservation. The top % of priority areas contained over % of red-listed species locations and % of existing protected forest areas, but only % of these top priorities overlapped with core areas identified previously in a regional strategy that used more qualitative methods. We argue for aggregating present and future habitat value of single management units to landscape and regional scales to identify potential bottlenecks in habitat availability linked to landscape dynamics. To address the challenge of Countdown , a general framework for forest conservation planning in Fennoscandia needs to cover different conservation issues, tools, and data needs.",,"Mikusinski, G|Pressey, RL|Edenius, L|Kujala, H|Moilanen, A|Niemela, J|Ranius, T",CONSERVATION BIOLOGY,boreal forests reserve selection spatial conservation planning zonation software,10.1111/j.1523-1739.2007.00833.x
128,WOS:000241177900002,2006,Design parameterization and tool integration for CAD-based mechanism optimization,,"This paper presents an open and integrated tool environment that enables engineers to effectively search, in a CAD solid model form, for a mechanism design with optimal kinematic and dynamic performance. In order to demonstrate the feasibility of such an environment, design parameterization that supports capturing design intents in product solid models must be available, and advanced modeling, simulation, and optimization technologies implemented in engineering software tools must be incorporated. In this paper, the design parameterization capabilities developed previously have been applied to support design optimization of engineering products, including a High Mobility Multi-purpose Wheeled Vehicle (HMMWV). In the proposed environment, Pro/ENGINEER and SolidWorks are supported for product model representation, DADS (Dynamic Analysis and Design System) is employed for dynamic simulation of mechanical systems including ground vehicles, and DOT (Design Optimization Tool) is included for a batch mode design optimization. In addition to the commercial tools, a number of software modules have been implemented to support the integration; e.g., interface modules for data retrieval, and model update modules for updating CAD and simulation models in accordance with design changes. Note that in this research, the overall finite difference method has been adopted to support design sensitivity analysis. (C) ", Elsevier Ltd. All rights reserved.,"Chang, KH|Joo, SH",ADVANCES IN ENGINEERING SOFTWARE,design optimization design parameterization computer-aided design dynamic simulation tool integration,10.1016/j.advengsoft.2006.05.005
129,WOS:000363949300025,2015,Feasibility analysis of a hybrid off-grid wind-DG-battery energy system for the eco-tourism remote areas,POWER PINCH ANALYSIS MALAYSIA PERFORMANCE GENERATION BANGLADESH SIMULATION STORAGE DESIGN PLANT,"The electrification process of the remote areas and decentralized areas is being a vital fact for the improvement of its eco-tourism issues such as the Cameron Highland of Malaysia. Renewable energy (RE) resources can be used extensively to support and fulfill the demand of the expected loads of these areas. This article presents an analysis of a complete off-grid wind-diesel-battery hybrid RE model. The main objective of the present analysis is to visualize the optimum volume of systems capable of fulfilling the requirements of  kWh/day primary load in coupled with . kW peak for  residential hotels of Cameron Highlands. The hybrid power system can be effective for the tourists of that area as it is a decentralized region of Malaysia. The main motto of this analysis is to minimize the electricity unit cost and ensure the most reliable and feasible system to fulfill the requirements of the desired or expected energy system using HOMER software. From the simulation result, it can be seen that  wind turbines ( kW),  diesel generator ( kW), and  battery (Hoppecke  OPzS) hybrid RE system is the most economically feasible and lowest cost of energy is nearing USD ./kWh and net present cost is USD , . The decrement of the CO emissions also can be identified from the simulation results using that most feasible RE system including the renewable fraction value which is about ., . % capacity shortage and . % electricity as storage as compared to the other energy system.",,"Shezan, SKA|Saidur, R|Ullah, KR|Hossain, A|Chong, WT|Julai, S",CLEAN TECHNOLOGIES AND ENVIRONMENTAL POLICY,renewable energy wind energy wind turbines homer diesel generator sensitivity analysis optimization hybrid model,10.1007/s10098-015-0983-0
130,WOS:000256243200015,2008,Sensitivity of population viability to spatial and nonspatial parameters using grip,METAPOPULATION DYNAMICS PREDICTIVE ABILITY PVA MODELS MATRIX REINTRODUCTION,"Metapopulation dynamics are influenced by spatial parameters including the amount and arrangement of suitable habitat, yet these parameters may be uncertain when deciding how to manage species or their habitats. Sensitivity analyses of population viability analysis (PVA) models can help measure relative parameter influences on predictions, identify research priorities for reducing uncertainty, and evaluate management strategies. Few spatial PVAs, however, include sensitivity analyses of both spatial and nonspatial parameters, perhaps because computationally efficient tools for such analyses are lacking or inaccessible. We developed GRIP, a program to facilitate sensitivity analysis of spatial and nonspatial input parameters for PVAs created in RAMAS Metapop, a widely applied software program. GRIP creates random sets of input files by varying parameters specified in the PVA model including vital rates and their correlations among populations, the number and configuration of populations, dispersal rates, dispersal survival, initial population abundances, carrying capacities, and the probability, intensity, and spatial extent of catastrophes, while drawing on specified parameter distributions. We evaluated GRIP's performance as a tool for sensitivity analysis of spatial PVAs and explored the consequences of varying spatial input parameters for predictions of a published PVA model of the sand lizard (Lacerta agilis). We used GRIP output to generate standardized regression coefficients (SRCs) and nonparametric correlation coefficients as indices of the relative sensitivity of predicted conservation status to input parameters. GRIP performed well; with a single analysis we were able to rank the relative influence of input parameters identified as influential by the PVA's original author, S. A. Berglind, who used three separate forms of sensitivity analysis. Our analysis, however, also underscored the value of exploring the relative influence of spatial parameters on PVA predictions; both SRCs and correlation coefficients indicated that the most influential parameters in the sand lizard model were spatial in nature. We provide annotated code so that GRIP may be modified to reflect particular species biology, customized for more complex spatial PVA models, upgraded to incorporate features added in newer versions of RAMAS Metapop, used as a template to develop similar programs, or used as it is for computationally efficient sensitivity analyses in support of conservation planning.",,"Curtis, JMR|Naujokaitis-Lewis, I",ECOLOGICAL APPLICATIONS,decision-support tool lacerta agilis recovery planning sand lizard sensitivity analysis spatial population viability analysis uncertainty,10.1890/07-1306.1
131,WOS:000256623900025,2008,Development of a open-vessel single-stage respirometer,ACTIVATED-SLUDGE WASTE-WATER PARAMETERS MODEL,"This paper describes the development and accuracy analysis of a single-stage respirometer which can be used both in the laboratory for wastewater characterization and in the plant as a process instrument. It is based on an accurate model of parasitic aeration, making the two-stage assumption unnecessary. Its operation is supervised by a real-time software, written in Lab View, managing the various measurement procedures and estimating the wastewater characteristics. Its accuracy is assessed through sensitivity and error propagation analysis, proving superior to the conventional model. A laboratory implementation of the instrument was tested with readily degradable substrate, yielding consistent and accurate respirograms.",,"Marsili-Libelli, S|D'Ardes, V|Bondi, C",WATER SCIENCE AND TECHNOLOGY,on-line process control parameter estimation respirometry sensitivity analysis sensors,10.2166/wst.2008.149
132,WOS:000286284700004,2011,A methodology for the design and development of integrated models for policy support,RIVER-BASIN MANAGEMENT 10 ITERATIVE STEPS LAND-USE PATTERNS PLANNING-SUPPORT OPTIMIZATION METHODOLOGY ORGANIZATIONAL-CHANGE ENVIRONMENTAL-MODELS SENSITIVITY-ANALYSIS CELLULAR-AUTOMATA SCENARIO ANALYSIS,"The development of Decision Support Systems (DSS) to inform policy making has been increasing rapidly. This paper aims to provide insight into the design and development process of policy support systems that incorporate integrated models. It will provide a methodology for the development of such systems that attempts to synthesize knowledge and experience gained over the past - years from developing a suite of these DSSs for a number of users in different geographical contexts worldwide. The methodology focuses on the overall iterative development process that includes policy makers, scientists and IT-specialists. The paper highlights important tasks in model integration and system development and illustrates these with some practical examples from DSS that have dynamic, spatial and integrative attributes. Crucial integrative features of modelling systems that aim to provide support to policy processes, and to which we refer as integrated Decision Support Systems, are: Synthesis of relevant drivers, processes and characteristics of the real world system at relevant spatial and temporal scales. An integrated approach linking economic, environmental and social domains. Connection to the policy context, interest groups and end-users. Engagement with the policy process. Ability to provide added value to the current decision-making practice. With this paper we aim to provide a methodology for the design and development of these integrated Decision Support Systems that includes the 'hard' elements of model integration and software development as well as the 'softer' elements related to the user-developer interaction and social learning of all groups involved in the process. (C) ", Elsevier Ltd. All rights reserved.,"van Delden, H|Seppelt, R|White, R|Jakeman, AJ",ENVIRONMENTAL MODELLING & SOFTWARE,decision support system (dss) model integration design and development process iterative process social learning policy support,10.1016/j.envsoft.2010.03.021
133,WOS:000411574400095,2017,The Effect of Vitamin A on Fracture Risk: A Meta-Analysis of Cohort Studies,BONE-MINERAL DENSITY HIP FRACTURE RETINOIC ACID BETA-CAROTENE SERUM RETINOL POSTMENOPAUSAL WOMEN OSTEOCLAST FORMATION HYPERVITAMINOSIS-A CLINICAL-TRIALS FOLLOW-UP,"This meta-analysis evaluated the influence of dietary intake and blood level of vitamin A (total vitamin A, retinol or beta-carotene) on total and hip fracture risk. Cohort studies published before July  were selected through English-language literature searches in several databases. Relative risk (RR) with corresponding % confidence interval (CI) was used to evaluate the risk. Heterogeneity was checked by Chi-square and I- test. Sensitivity analysis and publication bias were also performed. For the association between retinol intake and total fracture risk, we performed subgroup analysis by sex, region, case ascertainment, education level, age at menopause and vitamin D intake. R software was used to complete all statistical analyses. A total of , participants over the age of  years were included. Higher dietary intake of retinol and total vitamin A may slightly decrease total fracture risk (RR with % CI: . (., .) and . (., .), respectively), and increase hip fracture risk (RR with % CI: . (., .) and . (., .), respectively). Lower blood level of retinol may slightly increase total fracture risk (RR with % CI: . (., .)) and hip fracture risk (RR with % CI: . (., .)). In addition, higher beta-carotene intake was weakly associated with the increased risk of total fracture (RR with % CI: . (., .)). Our data suggest that vitamin A intake and level may differentially influence the risks of total and hip fractures. Clinical trials are warranted to confirm these results and assess the clinical applicability.",,"Zhang, XG|Zhang, R|Moore, JB|Wang, YG|Yan, HY|Wu, YR|Tan, AR|Fu, JL|Shen, ZG|Qin, GY|Li, R|Chen, GX",INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH AND PUBLIC HEALTH,vitamin a retinol beta-carotene hip fracture total fracture,10.3390/ijerph14091043
134,WOS:000236011600030,2006,A standard interface between simulation programs and systems analysis software,TRANSPORT PARAMETERS AQUATIC SYSTEMS UNCERTAINTY MODELS MINIMIZATION FLOW,"A simple interface between simulation programs and systems analytical software is proposed. This interface is designed to facilitate linkage of environmental simulation programs with systems analytical software and thus can contribute to remedying the deficiency in applying systems analytical techniques to environmental modelling studies. The proposed concept, consisting of a text file interface combined with a batch mode simulation program call, is independent of model structure, operating system and programming language. It is open for implementation by academic and commercial simulation and systems analytical software developers and is very simple to implement. Its practicability is demonstrated by implementations for three environmental simulation packages (AQUASIM, SWAT and LEACHM) and two systems analytical program packages (UNCSIM, SUR). The properties listed above and the demonstration of the ease of implementation of the approach are prerequisites for the stimulation of a widespread implementation of the proposed interface that would be beneficial for the dissemination of systems analytical techniques in the environmental and engineering sciences. Furthermore, such a development could stimulate the transfer of systems analytical techniques between different fields of application.",,"Reichert, P",WATER SCIENCE AND TECHNOLOGY,systems analytical techniques environmental simulation programs statistical inference sensitivity analysis identifiability analysis uncertainty analysis,10.2166/wst.2006.029
135,WOS:000343415200006,2013,UNCERTAINTY IN THE DEVELOPMENT AND USE OF EQUATION OF STATE MODELS,,"In this paper we present the results from a series of focus groups on the visualization of uncertainty in equation-of-state (EOS) models. The initial goal was to identify the most effective ways to present EOS uncertainty to analysts, code developers, and material modelers. Four prototype visualizations were developed to present EOS surfaces in a three-dimensional, thermodynamic space. Focus group participants, primarily from Sandia National Laboratories, evaluated particular features of the various techniques for different use cases and discussed their individual workflow processes, experiences with other visualization tools, and the impact of uncertainty on their work. Related to our prototypes, we found the D presentations to be helpful for seeing a large amount of information at once and for a big-picture view; however, participants also desired relatively simple, two-dimensional graphics for better quantitative understanding and because these plots are part of the existing visual language for material models. In addition to feedback on the prototypes, several themes and issues emerged that are as compelling as the original goal and will eventually serve as a starting point for further development of visualization and analysis tools. In particular, a distributed workflow centered around material models was identified. Material model stakeholders contribute and extract information at different points in this workflow depending on their role, but encounter various institutional and technical barriers which restrict the flow of information. An effective software tool for this community must be cognizant of this workflow and alleviate the bottlenecks and barriers within it. Uncertainty in EOS models is defined and interpreted differently at the various stages of the workflow. In this context, uncertainty propagation is difficult to reduce to the mathematical problem of estimating the uncertainty of an output from uncertain inputs.",,"Weirs, VG|Fabian, N|Potter, K|McNamara, L|Otahal, T",INTERNATIONAL JOURNAL FOR UNCERTAINTY QUANTIFICATION,materials uncertainty quantification representation of uncertainty model validation and verification continnum mechanics,10.1615/Int.J.UncertaintyQuantification.2012003960
136,WOS:000414081000003,2017,Multi-scale equation of state computations for confined fluids,EQUILIBRIUM,"Fluid properties of five binary mixtures relevant to shale gas and light tight oil in confined nano-channels are studied. Canonical (NVT) Monte Carlo simulations are used to determine internal energies of departure of pure fluids using the RASPA software system (Dubbeldam et al., ). The linear mixing rule proposed by Lucia et al. () is used to determine internal energies of departure for mixtures, U-M(D), in confined spaces and compared to U-M(D) from direct NVT Monte Carlo simulation. The sensitivity of the mixture energy parameter, a(M), for the Gibbs-Helmholtz constrained (GHC) equation, confined fluid molar volume, V-M, and bubble point pressure are studied as a function of uncertainty in U-M(D). Results show that the sensitivity of confined fluid molar volume to % uncertainty in U-M(D) is less than % and that the GHC equation predicts physically meaningful reductions in bubble point pressure for light tight oils. (C) ", Elsevier Ltd. All rights reserved.,"Thomas, E|Lucia, A",COMPUTERS & CHEMICAL ENGINEERING,confined fluids monte carlo simulation ghc equation of state sensitivity analysis bubble point pressure,10.1016/j.compchemeng.2017.05.028
137,WOS:000246956100013,2007,Evaluation of urban stormwater quality models,,"The use of urban stormwater quality models necessitates the estimation of their outputs uncertainty level. The results of the application of a Monte Carlo Markov Chain method based on the Bayesian theory for the calibration and uncertainty analysis of a storm water quality model commonly used in available software are presented in this paper. The tested model estimates the accumulation, erosion and transport of pollutants on surfaces and in sewers using a hydrologic/hydrodynamic scheme. The model was calibrated for  different initial conditions of in-sewer deposits. Calibration results showed a large variability in the model outputs in function of the initial conditions and demonstrated that the tested model predictive capacity is very low.",,"Kanso, A|Tassin, B|Chebbo, G",HOUILLE BLANCHE-REVUE INTERNATIONALE DE L EAU,,10.1051/lhb:2007025
138,WOS:000354547700014,2015,Identification of Critical Erosion Watersheds for Control Management in Data Scarce Condition Using the SWAT Model,LEAST-SQUARES REGRESSION CRITICAL SUBWATERSHEDS SENSITIVITY-ANALYSIS SEDIMENT YIELD SOIL-EROSION PRONE AREAS CLIMATE CATCHMENT INDIA REQUIREMENTS,"Identification of critical watersheds prone to soil erosion has been performed by using a hydrological model in data scarce Damodar River catchment, located in Jharkhand state of India. Model is calibrated and validated for two watersheds, i.e.,()Nagwan, .km; and ()Banikdih, .km, nested within the catchment. The achieved R values of predicted monthly runoff and sediment yield varies, respectively, .-. and .-., for both the watersheds during calibration and validation period. Calibration and validation results revealed that model is predicting monthly runoff and sediment yield satisfactory for the two watersheds of the Damodar River catchment. The validated model parameters were then up-scaled to the whole catchment and model was run from - to identify the critical watersheds. Model was successfully used for prioritization of  watersheds delineated using the computer software model within the catchment. In delineation process, the boundaries of  watersheds matched exactly with the watersheds delineated manually by Damodar Valley Corporation. Out of these  watersheds, erosion classes of  exactly matched with the manually described erosion class. For remaining  watersheds, priority of  watersheds was either one class higher or lower, whereas eight watersheds showed complete mismatch. Overall results showed that the hydrological model used in this paper may be helpful in prioritization of management strategies to manage the resources where availability of data is a big concern. Such approach for managing resources is particularly needed in developing countries for better utilization of limited resources.",,"Kumar, S|Mishra, A|Raghuwanshi, NS",JOURNAL OF HYDROLOGIC ENGINEERING,soil and water assessment tool (swat) calibration validation watershed critical erosion identification prioritization,10.1061/(ASCE)HE.1943-5584.0001093
139,WOS:000330720400013,2014,Effect of thermal-hydrogeological and borehole heat exchanger properties on performance and impact of vertical closed-loop geothermal heat pump systems,ENERGY-STORAGE SENSITIVITY-ANALYSIS POROUS-MEDIA GROUND-WATER TRANSPORT SIMULATION AQUIFER CANADA MODEL,"Ground-source geothermal systems are drawing increasing attention and popularity due to their efficiency, sustainability and being implementable worldwide. Consequently, design software and regulatory guidelines have been developed. Interaction with the subsurface significantly affects the thermal performance, sustainability, and impacts of such systems. Reviewing the related guidelines and the design software, room for improvement is evident, especially in regards to interaction with groundwater movement. In order to accurately evaluate the thermal effect of system and hydrogeological properties on a borehole heat exchanger, a fully discretized finite-element model is used. Sensitivity of the loop outlet temperatures and heat exchange rates to hydrogeological, system and meteorological factors (i.e. groundwater flux, thermal conductivity and volumetric heat capacity of solids, porosity, thermal dispersivity, grout thermal conductivity, background and inlet temperatures) are analyzed over -month and -year operation periods. Furthermore, thermal recovery during  years after system decommissioning has been modeled. The thermal plume development, transport and dissipation are also assessed. This study shows the importance of subsurface thermal conductivity, groundwater flow (flux > (-) m/s), and background and inlet temperature on system performance and impact. It also shows the importance of groundwater flow (flux > (-) m/s) on thermal recovery of the ground over other factors.",,"Dehkordi, SE|Schincariol, RA",HYDROGEOLOGY JOURNAL,groundwater flow temperature thermal conditions thermal conductivity thermal recovery,10.1007/s10040-013-1060-6
140,WOS:000281060100019,2010,Prediction of performance of sea water reverse osmosis units,SEAWATER RO DESALINATION REGRESSION OPTIMIZATION SIMULATION DESIGN,"This paper presents a methodology for the prediction of the performance of sea water reverse osmosis units acquired by salient mathematical correlations which involve the important aspects of membrane input and output variables. Feed water conditions pertinent to flow rate, TDS (total dissolved solids), pressure, pH and temperature are correlated with recovery ratio, permeate TDS and power consumption via regression analysis. Satisfactory results are obtained by comparing the resulting correlations against the software ROSA predictions. The mathematical correlations and regression assessment led to the derivation of simple polynomial expressions which were used for sensitivity analysis and may be useful for the design, monitoring and control of this important desalination process.", (C) 2010 Elsevier B.V. All rights reserved.,"Alahmad, M",DESALINATION,correlations performance predictions reverse osmosis rosa,10.1016/j.desal.2010.05.018
141,WOS:000256840200114,2007,Estimating uncertainty on internal dose assessments,RADON PROGENY UNIT EXPOSURE DOSIMETRY,"The estimation of uncertainty on doses broadly falls into three categories. () Estimating the uncertainty on prospective doses. Here, the intake is known and the uncertainties in individual parameter values must be propagated through the calculated dose. () Estimating the error or uncertainty on dose assessments made from single measurements. Here, intake, model parameter and measurement uncertainties are propagated into the measurement, but default ICRP parameter values are used to estimate the intake and dose from the measurement. ()Estimating the probability distribution of an individual's dose from a set of monitoring data. Here, Bayesian inference methods must be used to estimate the uncertainty on the estimated dose. A computer code is being developed that performs all three types of uncertainty analysis using Monte Carlo simulation. The software samples biokinetic parameters from probability density functions and then calculates doses from these parameters by calling the dosimetry code IMBA Professional Plus. A description of the methodology, together with an example application of the software, is included in this paper.",,"Puncher, M|Birchall, A",RADIATION PROTECTION DOSIMETRY,,10.1093/rpd/ncm361
142,WOS:000347582800052,2015,Development of the integrated fuzzy analytical hierarchy process with multidimensional scaling in selection of natural wastewater treatment alternatives,TREATMENT SYSTEMS NETWORK PROCESS OPTIMIZATION MANAGEMENT WETLANDS AHP,"Multi-criteria decision-making in selection of wastewater treatment alternatives has been explored widely, while few past studies comprehensively addressed the integration of various aspects (e.g., environmental, economical, ecological and management, and technical factors), which is a priority for decision-makers. This paper develops the integrated fuzzy analytical hierarchy process (AHP) with multidimensional scaling (MDS) approach to improve current methods for determining the optimal alternative. The integrated method incorporates the weights computed by AHP into the fuzzy matter-element, and allows evaluators to understand the relative importance of each index or criterion at a high level. This is followed by the MDS method to determine the optimal alternative directly through the coordinates associated with each alternative in a two-dimensional configuration. The method was evaluated via specific programming language software packages, and was applied to select natural wastewater treatment alternatives in a case study. Results indicate the stabilization pond was the optimal alternative among five natural wastewater treatment systems. Sensitivity analysis was conducted and reflects the importance of weighing on alternative selection.", (C) 2014 Elsevier B.V. All rights reserved.,"Ouyang, XG|Guo, F|Shan, D|Yu, HY|Wang, J",ECOLOGICAL ENGINEERING,analytical hierarchy process fuzzy matter-element multidimensional scaling wastewater treatment alternative sensitivity analysis,10.1016/j.ecoleng.2014.11.006
143,WOS:000349876500012,2015,"Understanding the Day Cent model: Calibration, sensitivity, and identifiability through inverse modeling",NITROUS-OXIDE EMISSIONS EVALUATING PARAMETER IDENTIFIABILITY DAYCENT ECOSYSTEM MODEL SOIL ORGANIC-MATTER ERROR REDUCTION 2 STATISTICS AUTOMATIC CALIBRATION PRODUCTION SYSTEMS N2O EMISSIONS WATER-FLOW,"The ability of biogeochemical ecosystem models to represent agro-ecosystems depends on their correct integration with field observations. We report simultaneous calibration of  DayCent model parameters using multiple observation types through inverse modeling using the PEST parameter estimation software. Parameter estimation reduced the total sum of weighted squared residuals by % and improved model fit to crop productivity, soil carbon, volumetric soil water content, soil temperature, NO, and soil NO- compared to the default simulation. Inverse modeling substantially reduced predictive model error relative to the default model for all model predictions, except for soil NO- and NH+. Post-processing analyses provided insights into parameter-observation relationships based on parameter correlations, sensitivity and identifiability. Inverse modeling tools are shown to be a powerful way to systematize and accelerate the process of biogeochemical model interrogation, improving our understanding of model function and the underlying ecosystem biogeochemical processes that they represent. (C)  The Authors. Published by", Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).,"Necpalova, M|Anex, RP|Fienen, MN|Del Grosso, SJ|Castellano, MJ|Sawyer, JE|Iqbal, J|Pantoja, JL|Barker, DW",ENVIRONMENTAL MODELLING & SOFTWARE,daycent model inverse modeling pest sensitivity analysis parameter identifiability parameter correlations,10.1016/j.envsoft.2014.12.011
144,WOS:000238960100008,2006,Variance-based sensitivity analysis of the probability of hydrologically induced slope instability,STABILITY MODEL UNSATURATED SOILS UNCERTAINTY DESIGN,"Analysis of the sensitivity of predictions of slope instability to input data and model uncertainties provides a rationale for targeted site investigation and iterative refinement of geotechnical models. However, sensitivity methods based on local derivatives do not reflect model behaviour over the whole range of input variables. whereas methods based on standardised regression or correlation coefficients cannot detect non-linear and non-monotonic relationships between model input and output. Variance-based sensitivity analysis (VBSA) provides a global, model-independent sensitivity measure. The approach is demonstrated using the Combined Hydrology and Stability Model (CHASM) and is applicable to a wide variety of computer models. The method of Sobol', assuming independence between input variables, was used to identify interactions between model input variables, whilst replicated Latin Hypercube Sampling (LHS) is used to investigate the effects of statistical dependence between the input variables. The SIMLAB software was used, both to generate the input sample and to calculate the sensitivity indices. The analysis provided quantified evidence of well-known sensitivities as well demonstrating how uncertainty in slope failure during rainfall is, for the examples tested here. more attributable to uncertainty in the soil strength than to uncertainty in the rainfall. (c) ", Elsevier Ltd. All rights reserved.,"Hamm, NAS|Hall, JW|Anderson, MG",COMPUTERS & GEOSCIENCES,site investigation slope stability analysis statistical analysis sensitivity analysis uncertainty analysis,10.1016/j.cageo.2005.10.007
145,WOS:000295845300022,2011,Modeling the effects of completion techniques and formation heterogeneity on CO2 sequestration in shallow and deep saline aquifers,STORAGE,"This work studied the effect of completion techniques and reservoir heterogeneity on CO storage and injectivity in saline aquifers using a compositional reservoir simulator, CMG-GEM. Two reservoir models were built based on the published data to represent a deep saline aquifer and a shallow aquifer. The effect of various completion conditions on CO storage was then discussed, including partial perforation of the reservoir net pay (partial completion), well geometry, orientation, location, and length. The heterogeneity effect was addressed by considering three parameters: mean permeability, the vertical to horizontal permeability ratio, and permeability variation. Sensitivity analysis was carried out using iSIGHT software (design of experiments) to determine the dominant factors affecting CO storage capacity and injectivity. Simulation results show that the most favorable option is the perforation of all layers with horizontal wells - m long set in the upper layers. Mean permeability has the most effect on CO storage capacity and injectivity; k(v)/k(h) affects CO injectivity storage capacity more than permeability variation, V-k. More CO can be stored in the heterogeneous reservoirs with low mean permeability; however, high injectivity can be achieved in the uniform reservoirs with high mean permeability.",,"Yang, F|Bai, BJ|Dunn-Norman, S",ENVIRONMENTAL EARTH SCIENCES,co2 sequestration saline aquifer completion heterogeneity reservoir simulation,10.1007/s12665-011-0908-0
146,WOS:000261743900026,2008,Modeling of radionuclide transport through repository components using finite volume finite element and multidomain methods,,"This paper presents D saturated flow and transport calculations performed using numerical methods developed within the MELODIE software in the framework of assessing the performance of a radioactive waste geological repository. This type of computational modeling is a challenging task due to the presence of strong physical, chemical and hydrogeological heterogeneities, as well as the very different geometrical scales (from dm to km) to be handled simultaneously. First, the software is briefly described, with a particular focus on the specific features that help for dealing with such contrasted system, notably the finite volume finite element and the Schwarz domain decomposition methods. IRSN calculations based on the French ""spent fuel/iron canister/clay"" concept designed by Andra in the framework of its feasibility study of a deep geological repository are then presented. Through sensitivity analyses based on various evolution scenarios, these calculations aim at assessing the possible influence of design features, waste degradation mechanisms or radionuclide transfer properties of repository components on the behavior of the containment system as a whole. The results of the simulation have shown the transport regime in the drifts depends on the efficiency of the seals. When seals are efficient, velocities are low and transfer regime in the drifts is dominated by diffusion, whereas advection is dominant when seals are poorly sealed. However, whatever the dominant transport regime (diffusion or advection) in the drifts, the activity released out from the canisters was shown to be mainly transferred by diffusion to the host rock exits. This transfer through clayey components (bentonite and host rock) highlights the importance of chemical properties of the three types of long-lived radionuclides considered in the present study. The sensitivity analysis has shown that strongly sorbed and/or weakly soluble radionuclides are less influenced by activity release mechanisms and by seal efficiency. Concerning the strongly sorbed radionuclide ()Nb, the activity is hardly transferred beyond the vicinity of the disposal cells: the sorption properties allow limiting and delaying efficiently radionuclide migration. Concerning the weakly soluble radionuclide ()Se, a part of the activity is instantaneously precipitated in the bentonite surrounding the canisters and the activity release in the system is controlled by the dissolution of this precipitated activity. Such radionuclides are thus hardly concerned with canister dissolution mechanisms. On the contrary, the migration and release at the host rock exits of ()I, which is a non-sorbed and soluble radionuclide, is shown to be influenced by the radionuclide source term, i.e. dissolution mechanism and instantaneous released fraction (IRF). The IRF, which corresponds to % of the total activity contained in the canisters impacts significantly the activity released out of the host rock when UO() matrix dissolution controlled by uranium solubility lasts up to several million years, whereas its contribution is far less marked when matrix dissolves rapidly by alpha-radiolysis. (C) ", Elsevier Ltd. All rights reserved.,"Mathieu, G|Dymitrowska, M|Bourgeois, M",PHYSICS AND CHEMISTRY OF THE EARTH,radionuclide transport radioactive waste geological repository finite volume finite element method domain decomposition,10.1016/j.pce.2008.10.041
147,WOS:000356741300007,2015,A Matlab toolbox for Global Sensitivity Analysis,IDENTIFICATION UNCERTAINTIES MODELS,"Global Sensitivity Analysis (GSA) is increasingly used in the development and assessment of environmental models. Here we present a Matlab/Octave toolbox for the application of GSA, called SAFE (Sensitivity Analysis For Everybody). It implements several established GSA methods and allows for easily integrating others. All methods implemented in SAFE support the assessment of the robustness and convergence of sensitivity indices. Furthermore, SAFE includes numerous visualisation tools for the effective investigation and communication of GSA results. The toolbox is designed to make GSA accessible to non-specialist users, and to provide a fully commented code for more experienced users to complement their own tools. The documentation includes a set of workflow scripts with practical guidelines on how to apply GSA and how to use the toolbox. SAFE is open source and freely available for academic and non-commercial purpose. Ultimately, SAFE aims at contributing towards improving the diffusion and quality of GSA practice in the environmental modelling community. (C)  The Authors.", Published by Elsevier Ltd.,"Pianosi, F|Sarrazin, F|Wagener, T",ENVIRONMENTAL MODELLING & SOFTWARE,global sensitivity analysis matlab octave open-source software,10.1016/j.envsoft.2015.04.009
148,WOS:000309958100005,2012,A Simulation Study to Assess a Variable Selection Method for Selecting Single Nucleotide Polymorphisms Associated with Disease,GENOME-WIDE ASSOCIATION ORACLE PROPERTIES REGRESSION LASSO LIKELIHOOD,"In genome-wide association studies, where hundreds of thousands of single nucleotide polymorphisms (SNPs) are genotyped, the potential for false positives is high and methods for selecting models with only a few SNPs are required. Methods for variable selection giving sets of SNPs associated with disease have been developed, but are still less common than evaluation of individual SNPs one at a time. To assess the potential improvement available from multi-SNP approaches, we examined the performance of the software GeneRaVE as a variable selection method when applied to SNP data in case-control studies. The method was assessed via simulations, in which a haplotype identified by three SNPs was taken to be associated with the disease. Simulated data sets reflecting different levels and patterns of genetic association with the disease were generated. In order to have a baseline level of performance to assess the method against, we used a generalized linear model using only the three disease susceptibility SNPs to provide an upper bound on the possible performance of the selection methods. To investigate the advantage of using variable selection method as a multivariate method over a single SNP approach, we used chi-squared tests for each of the disease susceptibility (DS) SNPs with correction for multiple testing. Simulation results showed that GeneRaVE performed well and outperformed single SNP analysis using the chi-squared method in identifying disease-related SNPs. In application to a large dataset, it identified SNPs known to be associated with disease that were not identified by single SNP methods.",,"Rabie, HS|Saunders, IW",JOURNAL OF COMPUTATIONAL BIOLOGY,bayesian generalized linear models (glm) lasso logistic regression risk ratio,10.1089/cmb.2011.0105
149,WOS:000330087600012,2013,COMPARATIVE ANALYSIS OF MUNICIPAL SOLID WASTE SYSTEMS: CRACOW CASE STUDY,MANAGEMENT TOOL,"The evaluation method to compare municipal solid waste management (MSWM) systems has been presented. The results of the integrated waste management model (IWM-), were used as the input data for the analysis. The results were integrated into life cycle analysis (LCA) impact categories. The authors present possible to calculate categories, and calculate them for the two MSWM scenarios. Next, the system performance was compared using a multicriteria method, called analytic hierarchy process (AHP). The hierarchical preference analysis on the World Wide Web software (Web-HIPRE) was applied to conduct the analysis. The criteria ratios for the AHP were assumed arbitrarily based on the best knowledge of the authors. Finally, the presented sensitivity analysis showed the confidence of the obtained results and pointed out the most important assumptions of the whole analysis. The two Cracow MSWM systems were used as a case study.",,"Stypka, T|Flaga-Maryanczyk, A",ENVIRONMENT PROTECTION ENGINEERING,,10.5277/epe130412
150,WOS:000384333000001,2016,"Gulf war contamination assessment for optimal monitoring and remediation cost-benefit analysis, Kuwait",RAUDHATAIN,"Site characterization was performed on an area of  km() around the strategically vital freshwater aquifers of the Al-Rawdhatain and Umm Al-Aish to assess the status of groundwater pollution as the result of Iraq invasion to Kuwait in . Advanced data analysis and visualization software (EVS-Pro) was used for groundwater contamination assessment analytes: total petroleum hydrocarbon (TPH) and total dissolved solids (TDS). This will reduce the number of samples needed (saves time and money) and provide a superior assessment of the analytes distribution. Based on the ""minimum-maximum plume technology'' analysis, the nominal plume area with a threshold of . mg/kg TPH is estimated at about . km . This is the difference between the maximum and minimum predicted plume sizes. EVS-Pro also computed . x () and . x () for the plume volumes and masses (dollars per volume and mass), respectively. Also, new sampling locations were determined for further detailed site assessments based on the confidence and uncertainty analysis, which is more defensible and cost-optimized approach. This will reduce the number of samples needed (saves time and money) and provide a superior assessment of the analytes distribution. These tools prove to be effective in assessing remediation costs of clean-up versus benefits obtained and in developing a cost-effective monitoring programme for insights into processes controlling subsurface contaminant transport that impact water quality.",,"Yihdego, Y|Al-Weshah, RA",ENVIRONMENTAL EARTH SCIENCES,visualization data analysis site investigation plume monitoring remediation hydrocarbon clean-up cost pollution,10.1007/s12665-016-6025-3
151,WOS:000392568100009,2017,Uncertainty quantification in littoral erosion,SHALLOW-WATER FLOWS GEOMETRIC CHARACTERIZATION SHAPE OPTIMIZATION SENSITIVITY SPACES EVOLUTION SCHEME RISK,We aim at quantifying the impact of flow state uncertainties in littoral erosion to provide confidence bounds on deterministic predictions of bottom morphodynamics. Two constructions of the bathymetry standard deviation are discussed. The first construction involves directional quantile-based extreme scenarios using what is known on the flow state Probability Density Function (PDF) from on site observations. We compare this construction to a second cumulative one using the gradient by adjoint of a functional involving the energy of the system. These ingredients are illustrated for two models for the interaction between a soft bed and a flow in a shallow domain. Our aim is to keep the computational complexity comparable to the deterministic simulations taking advantage of what already available in our simulation toolbox. (C) , Elsevier Ltd. All rights reserved.,"Mohammadi, B",COMPUTERS & FLUIDS,backward propagation quantile uncertainty littoral morphodynamics shallow water equations sensitivity analysis worst-case analysis,10.1016/j.compfluid.2016.10.017
152,WOS:000401556800003,2017,Life cycle analysis of pistachio production in Greece,GREENHOUSE-GAS EMISSIONS ENVIRONMENTAL BENEFITS PHOSPHORUS RECOVERY CROPPING SYSTEMS BIOCHAR SOIL COMPOST IRRIGATION FRAMEWORK ENERGY,"In the present paper, a life cycle assessment (LCA) study regarding pistachio (Pistacia vera L.) cultivation in Aegina island, Greece, was performed to evaluate the energy use footprint and the associated environmental impacts. In this context, a detailed life cycle inventory was created based on site-survey data and used for a holistic cradle-to-farm gate LCA analysis using the GaBi . software. The main impact categories assessed were acidification potential (AP), eutrophication potential (EP), global warming potential (GWP), ozone depletion potential (ODP), photochemical ozone creation potential (POCP) and cumulative energy demand (CED). In order to reveal the main environmental concerns pertinent to pistachio production and in turn propose measures for the reduction of environmental and energetic impacts, three scenarios were compared, namely the Baseline scenario (BS) that involves current cultivation practices, the Green Energy (GE) scenario that involves the use of biological fertilizers i.e. compost, and the Waste Utilization (WU) scenario that involves the production of biochar from pistachio and other agricultural wastes and its subsequent soil application to promote carbon sequestration and improve soil quality. Based on the results of this study, the use of compost for fertilization (GE scenario), which results in approximately % savings in terms of energy consumption and the five environmental impact categories studied compared to BS scenario, is considered a promising alternative cultivation strategy. Slightly higher savings (% on average) in terms of the five calculated environmental impact categories, compared to the BS scenario, were indicated when the WU scenario was considered. Regarding energy consumption, the WU scenario results in minor increase, %, compared to the BS scenario. Results of uncertainty analysis performed using the Monte Carlo technique and contribution analysis showed that GE and WU scenarios offer reliable and significant eco-profile improvements for pistachio production in the study area compared to the current situation.", (C) 2017 Elsevier B.V. All rights reserved.,"Bartzas, G|Komnitsas, K",SCIENCE OF THE TOTAL ENVIRONMENT,life cycle assessment (lca) pistachios aegina waste management compost biochar,10.1016/j.scitotenv.2017.03.251
153,WOS:000383298800020,2016,Designing and implementing a multi-core capable integrated urban drainage modelling Toolkit:Lessons from CityDrain3,WASTE-WATER SYSTEM CLIMATE-CHANGE IDENTIFIABILITY ANALYSIS SENSITIVITY-ANALYSIS STORMWATER MODELS STORAGE TANK SEWER SYSTEM CITY DRAIN MANAGEMENT UNCERTAINTY,"Integrated urban drainage modelling combines different aspects of the urban water system into a common framework. With increasing pressures of a changing climate, urban growth and economic constraints, the need for wider spread integration is necessary in the interest of a sustainable future. Greater complexity results in greater computational burden but modelling packages will, likewise, need to be flexible enough to allow incorporation of new algorithms. With advancements in modern information technology, a parallel implementation of such a modelling toolkit is mandatory while still leaving its users the flexibility of extensions. The design and implementation of the integrated modelling framework CityDrain shows that it is possible to write research code that is high-performance and extensible by many research projects. Three use case scenarios are presented to showcase the application of CityDrain. The performance advantage of parallelization (up to  times compared to its predecessor) and the scalability of the framework are also demonstrated. (C) ", Elsevier Ltd. All rights reserved.,"Burger, G|Bach, PM|Urich, C|Leonhardt, G|Kleidorfer, M|Rauch, W",ADVANCES IN ENGINEERING SOFTWARE,integrated urban drainage modelling simulation framework object-oriented design multi-core parallel computing,10.1016/j.advengsoft.2016.08.004
154,WOS:000270236100003,2009,Estimating stream metabolism from oxygen concentrations: Effect of spatial heterogeneity,ORGANIC-MATTER ECOSYSTEM METABOLISM GAS-EXCHANGE PRIMARY PRODUCTIVITY SENSITIVITY-ANALYSIS RIVER RESPIRATION REAERATION VARIABILITY TURNOVER,"Rivers are heterogeneous at various scales. River metabolism estimators based on oxygen time series provide average estimates of net oxygen production at the scale of a river reach. These estimators are derived for homogeneous river reaches. For this reason, they cannot be used to analyze how exactly they average over longitudinal variations in net production, reaeration, oxygen saturation concentration and flow velocity. We try to fill this gap by using a general analytical solution of the transport-reaction equation to () demonstrate how downstream oxygen concentration is affected by upstream concentration and (possible) longitudinally varying values of net production, reaeration, oxygen saturation concentration and flow velocity within a reach, and () derive how the net production estimate depends on varying upstream river parameters. In addition, we derive a new net production estimator that extends previously suggested estimators. The equations derived in this paper provide a general framework for understanding the assumptions underlying net production estimators. They are used to derive recommendations on the use of single station or two stations measurement layouts to get accurate river metabolism estimates. The estimator is implemented in the freely available statistics and graphics software package R (http://www.r-project.org). This makes it easily applicable to observed oxygen time series. Empirical evidence of the significance of heterogeneity in rivers is demonstrated by applying the estimator to four subsequent reaches of a river using oxygen measurements from the ends of all reaches.",,"Reichert, P|Uehlinger, U|Acuna, V",JOURNAL OF GEOPHYSICAL RESEARCH-BIOGEOSCIENCES,,10.1029/2008JG000917
155,WOS:000335783500001,2014,Verification of computational models of cardiac electro-physiology,BIDOMAIN EQUATIONS EXCITABLE MEDIA ELECTROPHYSIOLOGY SIMULATIONS CELLML ADAPTIVITY DYNAMICS PROJECT TISSUE TOOLS,"For computational models of cardiac activity to be used in safety-critical clinical decision-making, thorough and rigorous testing of the accuracy of predictions is required. The field of 'verification, validation and uncertainty quantification' has been developed to evaluate the credibility of computational predictions. The first stage, verification, is the evaluation of how well computational software correctly solves the underlying mathematical equations. The aim of this paper is to introduce novel methods for verifying multi-cellular electro-physiological solvers, a crucial first stage for solvers to be used with confidence in clinical applications. We define D-D model problems with exact solutions for each of the monodomain, bidomain, and bidomain-with-perfusing-bath formulations of cardiac electro-physiology, which allow for the first time the testing of cardiac solvers against exact errors on fully coupled problems in all dimensions. These problems are carefully constructed so that they can be easily run using a general solver and can be used to greatly increase confidence that an implementation is correct, which we illustrate by testing one major solver, 'Chaste', on the problems. We then perform case studies on calculation verification (also known as solution verification) for two specific applications. We conclude by making several recommendations regarding verification in cardiac modelling."," Copyright (c) 2013 John Wiley & Sons, Ltd.","Pathmanathan, P|Gray, RA",INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN BIOMEDICAL ENGINEERING,software vvuq cardiac modelling,10.1002/cnm.2615
156,WOS:000366074400041,2016,Soil solution concentrations and chemical species of copper and zinc in a soil with a history of pig slurry application and plant cultivation,DISSOLVED ORGANIC-MATTER SANDY TYPIC HAPLUDALF CONTAMINATED SOILS HEAVY-METALS VINEYARD SOILS DEEP-LITTER ZN CU COMPLEXATION PHOSPHORUS,"Successive pig slurry applications may increase soil copper(Cu) and zinc (Zn) concentrations and change the proportions of free chemical species in solution when combined with plant cultivation. The aim of this study was to assess the soluble, available,,and total Cu and Zn concentrations and the distribution of their chemical species in the solution in a Hapludalf soil with a history of pig slurry application and plant cultivation. The study was conducted in undisturbed soil columns that originated from an -year-long experiment conducted at the experimental unit of the Federal University of Santa Maria in Santa Maria, southern Brazil. The soil was a Typic Hapludalf soil fertilized with pig slurry at rates of , , , and  m() ha(-). The soil was collected from depth intervals of -., .-., .-., .-., -., and .-. m before and after cultivation with black oat and maize in a greenhouse to assess the total and available Cu and Zn concentrations and to extract the solution. The soil solution concentrations of the main cations, anions, and dissolved organic carbon (DOC) and pH were assessed. The distribution of Cu and Zn chemical species was assessed using the Visual Minteq software. The history of  pig slurry applications increased the concentration of Cu and Zn in surface soil intervals, but the concentration of Cu also increased in the soil solution at depth. The phytotoxicity caused by Cu and Zn may not occur even after several years of pig slurry application because the plants provide soil conditions in which chemical species complexed with dissolved organic carbon predominate and Cu and Zn in free forms are present only in small amounts.", (C) 2015 Elsevier B.V. All rights reserved.,"De Conti, L|Ceretta, CA|Ferreira, PAA|Lourenzi, CR|Girotto, E|Lorensini, F|Tiecher, TL|Marchezan, C|Anchieta, MG|Brunetto, G",AGRICULTURE ECOSYSTEMS & ENVIRONMENT,poaceae pig slurry speciation modeling dissolved organic carbon,10.1016/j.agee.2015.09.040
157,WOS:000377452800009,2016,The Influence of Model Structure Uncertainty on Water Quality Assessment,RUNOFF APPLICABILITY TRANSPORT SUPPORT,"Physically-based mathematical water quality models are known as potentially effective tools to simulate the temporal and spatial variations of water quality variables along rivers. Each model relies on specific sets of assumptions and equations to simulate the physico-biochemical processes, which influence on its simulation results. This paper aims to improve the insight in the uncertainties related to state-of-the-art river physico-biochemical water quality modelling. Sensitivity analysis is applied to the processes implemented in three most popular commercial software packages: MIKE, InfoWorks RS and InfoWorks ICM. This is done for the Molse Neet river case study. Firstly, the physico-biochemical processes are screened to obtain a preliminary assessment on the critical processes and to determine the processes that require more detailed comparison. Then, local sensitivity analysis is carried out to specify the sensitive parameters and processes. Results show that the hydrodynamic results, heat transfer rate and reaeration simulations cause large differences in model simulation outputs for water temperature and dissolved oxygen concentrations. The ignorance of processes related to sediment transport, phytoplankton and bacteria has a significant influence on the higher values of organic matter and lower values of dissolved oxygen concentrations. The three models show consensus on the main pollutant sources explaining organic matter and nitrate concentrations, but disagree on the main factors explaining the DO concentrations.",,"Nguyen, TT|Willems, P",WATER RESOURCES MANAGEMENT,infoworksicm infoworksrs mike11 model structure uncertainty river water quality model sensitivity analysis,10.1007/s11269-016-1330-x
158,WOS:000241572400011,2006,Optimal control of open-channel flow using adjoint sensitivity analysis,MANNINGS ROUGHNESS COEFFICIENTS CONTAMINANT RELEASES HAZARD MITIGATION WAVE CONTROL IDENTIFICATION OPTIMIZATION RIVERS MODEL,"An optimal flow control methodology based on adjoint sensitivity analysis for controlling nonlinear open channel flows with complex geometries is presented. The adjoint equations, derived from the nonlinear Saint-Venant equations, are generally capable of evaluating the time-dependent sensitivities with respect to a variety of control variables under complex flow conditions and cross-section shapes. The internal boundary conditions of the adjoint equations at a confluence (junction) derived by the variational approach make the flow control model applicable to solve optimal flow control problems in a channel network over a watershed. As a result, an optimal flow control software package has been developed, in which two basic modules, i.e., a hydrodynamic module and a bound constrained optimization module using the limited-memory quasi-Newton algorithm, are integrated. The effectiveness and applicability of this integrated optimal control tool are demonstrated thoroughly by implementing flood diversion controls in rivers, from one reach with a single or multiple floodgates (with or without constraints), to a channel network with multiple floodgates. This new optimal flow control model can be generally applied to make optimal decisions in real-time flood control and water resource management in a watershed.",,"Ding, Y|Wang, SSY",JOURNAL OF HYDRAULIC ENGINEERING-ASCE,open channel flow sensitivity analysis floods geometry optimization,10.1061/(ASCE)0733-9429(2006)132:11(1215)
159,WOS:000415699500014,2017,Matrix-free algorithm for the optimization of multidisciplinary systems,EQUALITY CONSTRAINED OPTIMIZATION NONLINEAR AEROELASTIC SYSTEMS LARGE-SCALE OPTIMIZATION INEXACT NEWTON METHOD KRYLOV-SCHUR METHODS DESIGN OPTIMIZATION SENSITIVITY-ANALYSIS BOUNDARY-CONDITIONS ENGINEERING DESIGN SQP METHOD,"Multidisciplinary engineering systems are usually modeled by coupling software components that were developed for each discipline independently. The use of disparate solvers complicates the optimization of multidisciplinary systems and has been a long-standing motivation for optimization architectures that support modularity. The individual discipline feasible (IDF) formulation is particularly attractive in this respect. IDF achieves modularity by introducing optimization variables and constraints that effectively decouple the disciplinary solvers during each optimization iteration. Unfortunately, the number of variables and constraints can be significant, and the IDF constraint Jacobian required by most conventional optimization algorithms is prohibitively expensive to compute. Furthermore, limited-memory quasi-Newton approximations, commonly used for large-scale problems, exhibit linear convergence rates that can struggle with the large number of design variables introduced by the IDF formulation. In this work, we show that these challenges can be overcome using a reduced-space inexact-Newton-Krylov algorithm. The proposed algorithm avoids the need for the explicit constraint Jacobian and Hessian by using a Krylov iterative method to solve the Newton steps. The Krylov method requires matrix-vector products, which can be evaluated in a matrix-free manner using second-order adjoints. The Krylov method also needs to be preconditioned, and a key contribution of this work is a novel and effective preconditioner that is based on approximating a monolithic solution of the (linearized) multidisciplinary system. We demonstrate the efficacy of the algorithm by comparing it with the popular multidisciplinary feasible formulation on two test problems.",,"Dener, A|Hicken, JE",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,multidisciplinary design optimization individual discipline feasible inexact-newton-krylov matrix-free second-order adjoint preconditioning,10.1007/s00158-017-1734-0
160,WOS:000238297500003,2006,"Optimal crop planning and water resources allocation in a coastal groundwater basin, Orissa, India",LINEAR DECISION RULES OPTIMIZATION MODEL RIVER-BASIN IRRIGATION MANAGEMENT OPERATION DESIGN PROJECT,"Intensive rice cultivation in a coastal groundwater basin in Balasore district of Orissa province (eastern India) during monsoon and winter seasons has resulted in extensive pumping of groundwater by a network of shallow, mini-deep and deep tubewells. Particularly, shallow tubewell owners using centrifugal pumps are unable to lift groundwater during winter seasons due to rapid drawdown of groundwater table below suction lift caused by mini-deep and deep tubewell owners. The seawater intrusion front is also progressing inland in an alarming rate. To develop a long-term sustainable land and water management strategies for the aforementioned issues in humid regions, the district administration realized the need for crop planning and water resources management policies in deterministic and stochastic regimes. As non-structural measure, the Deterministic linear programming (DLP) and chance-constrained linear programming (CCLP) models were developed to allocate available land and water resources optimally on seasonal basis so as to maximize the net annual return from the study area, considering net irrigation water requirement of crops as stochastic variable. These models were solved using the quantitative systems for business (QSB) software package. Sensitivity analysis of the models has been carried out by varying three ranges of cropping scenarios (,  and % deviation from the existing cropping pattern) and combinations of surface water and groundwater at various risk levels (, ,  and %). The total groundwater available and withdrawal in the region are . x () and . x () km(), respectively. The study reveals that % deviation of the existing cropping pattern is the optimal that satisfies the minimum food requirement and maintain geo-hydrological balance of the basin. The sensitivity analysis of conjunctive use of surface water and groundwater shows that % surface water and % groundwater availability as the optimum water allocation level. The proposed cropping and water resources allocation policies of the developed models were found to be socio-economically acceptable that maintained the balance of the entire system, considering all the constraints and restrictions imposed.", (c) 2005 Elsevier B.V. All rights reserved.,"Sethi, LN|Panda, SN|Nayak, MK",AGRICULTURAL WATER MANAGEMENT,coastal groundwater basin conjunctive use optimal cropping sensitivity analysis uncertainty water resources allocation,10.1016/j.agwat.2005.11.009
161,WOS:000278842000024,2010,"Reducing lost-sales rate in (T,R,L) inventory model with controllable lead time",BACKORDERS MIXTURE REDUCTION DISCOUNT,"In order to establish a good image and to enhance customer's loyalty, many efforts such as upgrading the servicing facilities, maintaining a high quality of products and increasing expenditure on advertisement could be made by a selling shop. Naturally, an extra-added cost must be spent for these efforts and it is expected to have a result to reduce the shortage cost of lost-sales and the total expected annual cost. This paper explores a probabilistic inventory model with optimal lost-sales caused by investment due to two different types of cost functions. We consider that the lead time can be shortened at an extra crashing cost, which depends on the length of the lead time. Moreover, we assume that the lost-sales rate can also be reduced by capital investment. The purpose of this paper is to establish a (T,R,L) inventory model with controllable lead time and to analyze the effects of increasing two different types of investments to reduce the lost-sales rate, in which the review period, lead time and lost-sales rate are treated as decision variables. We first formulate the basic periodic review model mathematically with the capital investment to reduce lost-sales rate. Then two models are discussed, one with normally distributed protection interval demand and another with distribution-free case. For each model, two investment cost functional forms, logarithmic and power, are employed for lost-sales rate reduction. Two computational algorithms with the help of the software Matlab are furnished to determine the optimal solution. In addition, six numerical examples and sensitivity analysis are presented to illustrate the theoretical results and obtain some managerial insights. Finally, the effect of lost-sales rate reduction is investigated. By framing this new model, we observe that a significant amount of savings can be easily achieved to increase the competitive edge in business. The results in the numerical examples indicate that the savings of expected annual total cost are realized through lost-sales reduction.", (C) 2010 Elsevier Inc. All rights reserved.,"Annadurai, K|Uthayakumar, R",APPLIED MATHEMATICAL MODELLING,inventory lost sales rate variable lead time minimax distribution-free procedure optimization,10.1016/j.apm.2010.02.035
162,WOS:000323981900016,2013,Analyzing the effects of geological and parameter uncertainty on prediction of groundwater head and travel time,MULTIPLE-POINT STATISTICS GLUE METHODOLOGY CAPTURE ZONE FLOW SIMULATION MODEL HETEROGENEITY INCOHERENCE CALIBRATION DENMARK,"Uncertainty of groundwater model predictions has in the past mostly been related to uncertainty in the hydraulic parameters, whereas uncertainty in the geological structure has not been considered to the same extent. Recent developments in theoretical methods for quantifying geological uncertainty have made it possible to consider this factor in groundwater modeling. In this study we have applied the multiple-point geostatistical method (MPS) integrated in the Stanford Geostatistical Modeling Software (SGeMS) for exploring the impact of geological uncertainty on groundwater flow patterns for a site in Denmark. Realizations from the geostatistical model were used as input to a groundwater model developed from Modular three-dimensional finite-difference ground-water model (MODFLOW) within the Groundwater Modeling System (GMS) modeling environment. The uncertainty analysis was carried out in three scenarios involving simulation of groundwater head distribution and travel time. The first scenario implied  stochastic geological models all assigning the same hydraulic parameters for the same geological units. In the second scenario the same  geological models were subjected to model optimization, where the hydraulic parameters for each of them were estimated by calibration against observations of hydraulic head and stream discharge. In the third scenario each geological model was run with  randomized sets of parameters. The analysis documented that the uncertainty on the conceptual geological model was as significant as the uncertainty related to the embedded hydraulic parameters.",,"He, X|Sonnenborg, TO|Jorgensen, F|Hoyer, AS|Moller, RR|Jensen, KH",HYDROLOGY AND EARTH SYSTEM SCIENCES,,10.5194/hess-17-3245-2013
163,WOS:000264171200019,2009,A Life Cycle Comparison of Alternative Cheese Packages,,"A comparative life cycle assessment (LCA) between three different cheese packages (P: completely polypropylene (PP), P: tin and polyethylene (PE), and P: carton and PE) has been carried out for the production, distribution and waste disposal (% landfill) phase. A package for  kg of cheese was selected as the functional unit. SimaPro software (PReConsultants, The Netherlands) was used for the LCA study. The EcoIndicator  method was selected for comparison of the packages. The comparisons show that the total environmental performance of the cheese package types in order from worst to best is P, P, and P. This conclusion was supported by a sensitivity analysis, which was conducted by using different impact assessment methods.",,"Banar, M|Cokaygil, Z",CLEAN-SOIL AIR WATER,ecoindicator 99 food technology landfilling life cycle assessment packaging simapro7,10.1002/clen.200700185
164,WOS:000292726900029,2011,Development of a screening method to assess flood risk on Danish national roads and highway systems,,"A method to assess flood risk on Danish national roads in a large area in the middle and southern part of Jutland, Denmark, was developed for the Danish Road Directorate. Flood risk has gained renewed focus due to the climate changes in recent years and extreme rain events are expected to become more frequent in the future. The assessment was primarily based on a digital terrain model (DTM) covering , km() in a . x . m grid. The high-resolution terrain model was chosen in order to get an accurate estimation of the potential flooding in the road area and in the immediate vicinity, but also put a high requirement on the methods, hardware and software applied. The outcome of the analysis was detailed maps (as GIS layers) illustrating the location of depressions with depths, surface area and volume data for each depression. Furthermore, preferential flow paths, catchment boundaries and ranking of each depression were calculated. The ranking was based on volume of depressions compared with upstream catchment and a sensitivity analysis of the runoff coefficient. Finally, a method for assessing flood risk at a more advanced level (hydrodynamic simulation of surface and drainage) was developed and used on a specific blue spot as an example. The case study shows that upstream catchment, depressions, drainage system, and use of hydrodynamic calculations have a great influence on the result. Upstream catchments can contribute greatly to the flooding.",,"Nielsen, NH|Larsen, MRA|Rasmussen, SF",WATER SCIENCE AND TECHNOLOGY,flood risk maps predicting flood risk of highway systems high-resolution dtm gis analysis sensitivity analysis climate change screening method decision support tool,10.2166/wst.2011.157
165,WOS:000344297900001,2014,A Novel Closed-Form Solution for Circular Openings in Generalized Hoek-Brown Media,BRITTLE-PLASTIC ROCK GROUND RESPONSE DISPLACEMENTS STRENGTH TUNNEL,"A novel closed-formsolution is presented in this paper for the estimation of displacements around circular openings in a brittle rock mass subject to a hydrostatic stress field. The rock mass is assumed to be elastic-brittle-plastic media governed by the generalized Hoek-Brown yield criterion. The present closed-form solution was validated by employing the existing analytical solutions. Results of several example cases are analyzed to show that, with the simplified assumption, a novel closed-form solution is derived and found to be in an excellent agreement with those obtained by using the exact integration method with mathematical software. Parametric sensitivity analysis is carried out and the parameter a(r), tends to be the sensitive factor. As a closed-form solution that does not require transformation technique and the use of any numerical method, this work can provide a better choice in the preliminary design for circular opening.",,"Meng, QX|Wang, W",MATHEMATICAL PROBLEMS IN ENGINEERING,,10.1155/2014/870835
166,WOS:000262497400004,2008,Fidelity of Network Simulation and Emulation: A Case Study of TCP-Targeted Denial of Service Attacks,,"In this article, we investigate the differences between simulation and emulation when conducting denial of service (DoS) attack experiments. As a case study, we consider low-rate TCP-targeted DoS attacks. We design constructs and tools for emulation testbeds to achieve a level of control comparable to simulation tools. Through a careful sensitivity analysis, we expose difficulties in obtaining meaningful measurements from the DETER, Emulab, and WAIL testbeds with default system settings. We find dramatic differences between simulation and emulation results for DoS experiments. Our results also reveal that software routers such as Click provide a flexible experimental platform, but require understanding and manipulation of the underlying network device drivers. Our experiments with commercial Cisco routers demonstrate that they are highly susceptible to the TCP-targeted attacks when ingress/egress IP filters are used.",,"Chertov, R|Fahmy, S|Shroff, NB",ACM TRANSACTIONS ON MODELING AND COMPUTER SIMULATION,simulation emulation testbeds tcp congestion control denial of service attacks low-rate tcp-targeted attacks,10.1145/1456645.1456649
167,WOS:000361481300001,2015,Multisite Assessment of Hydrologic Processes in Snow-Dominated Mountainous River Basins in Colorado Using a Watershed Model,CONTERMINOUS UNITED-STATES LAND-COVER DATABASE CLIMATE-CHANGE SWAT MODEL AUTOMATIC CALIBRATION SENSITIVITY-ANALYSIS GLOBAL OPTIMIZATION VALIDATION SIMULATIONS COMPLETION,"Hydrologic fluxes in mountainous watersheds are particularly important as these areas often provide a significant source of freshwater for more arid surrounding lowlands. The state of Colorado in the United States comprises a principal snow catchment area, with all major headwater river basins in Colorado providing substantial water flows to surrounding western and midwestern states. The ability to represent and quantify hydrologic processes controlling the generation and movement of water in headwater basins of Colorado therefore has significant implications for effective management of water resources in the western United States under varying climatic and land-use conditions. In the research reported in this paper, hydrologic modeling was applied to four snow-dominated, mountainous basins of Colorado [i.e.,the river basins of ()Cache la Poudre, ()Gunnison, ()San Juan, and ()Yampa] to evaluate the relevance of specific hydrologic components (i.e.,evapotranspiration, snow processes, groundwater processes, surface runoff, and so on) in the complex, high-elevation watersheds. The soil and water assessment tool (SWAT) model was calibrated and tested for multiple river locations within each basin using monthly naturalized flows over the - period. The model was able to adequately simulate streamflows at all locations within the four basins. Monthly patterns of precipitation, snowfall, evapotranspiration (ET), and total water yield were similar for all the basins, while subsurface lateral flow was the dominant hydrologic pathway, contributing between  and % to gross basin water yields on an average annual basis. Overall, results indicated the strong influence of snowmelt and groundwater processes on amounts and timing of streamflows in the study basins. Hence, enhanced representation of these processes may be essential to improve hydrological estimation using computer software in snowmelt-driven mountainous basins. In particular, examination of monthly streamflow residuals indicated that the normality and independence of model residuals, which are often assumed in parameter estimation and uncertainty analysis, were not always satisfied.",,"Foy, C|Arabi, M|Yen, H|Gironas, J|Bailey, RT",JOURNAL OF HYDROLOGIC ENGINEERING,watershed modeling hydrological processes mountainous watersheds snow processes soil and water assessment tool (swat),10.1061/(ASCE)HE.1943-5584.0001130
168,WOS:000361583900032,2015,Comparison of different methods for calculating thermal bridges: Application to wood-frame buildings,SENSITIVITY-ANALYSIS MATERIAL SELECTION WALLS MODEL SIMULATION,"Nowadays thermal bridges losses in building design (standard or dynamic simulations) are generally evaluated using heat transmission coefficients from a database of usual cases. Numerous studies exist on the thermal bridges of classic constructions (concrete, brick). The originality of this paper is to deal with the particular case of the wood-frame construction. In this case, catalogs with heat transmission coefficients have been integrated into building energy simulation software used by building engineers. First, this paper proposes a scientific hindsight and a critical overview of existing calculation methods of thermal bridges. Simulations are made in steady state conditions according to the European standard and with dynamic conditions. A series of calculations was performed in order to validate the presented method. The results for wood stud thermal bridges showed that the values that are mainly used by engineering offices often lead to important errors due to the standard method and rounding choice. Secondly different models are proposed to correctly apply the thermal behavior of thermal bridges to some examples of wood-frame structure. These are based on recent articles covering the methods for thermal bridges calculation in dynamic conditions. This section shows that the most accurate models depend on the consideration of the inertia of the wood stud that concentrates all the mass of the wall unlike more conventional configurations.", (C) 2015 Published by Elsevier Ltd.,"Viot, H|Sempey, A|Pauly, M|Mora, L",BUILDING AND ENVIRONMENT,thermal bridge energy consumption dynamic heat flow inertia thermal simulation,10.1016/j.buildenv.2015.07.017
169,WOS:000306034100006,2012,Sensitivity analysis for volcanic source modeling quality assessment and model selection,UNCERTAINTY IMPORTANCE MEASURE AKAIKE INFORMATION CRITERION COUPLED REACTION SYSTEMS SURFACE DEFORMATION RATE COEFFICIENTS F-TEST INDEXES MOTION,"The increasing knowledge and understanding of volcanic sources has led to the development and implementation of sophisticated and complex mathematical models with the main goal of describing field and experimental data. Quantification of the model's ability in describing the data becomes fundamental for a realistic estimate of the model parameters. The analysis of sensitivity can help us in identifying the parameters that significantly affect the model's output and in assessing its quality factor. In this paper, we describe the Global Sensitivity Analysis (GSA) methods based both on Fourier Amplitude Sensitivity Test and on the Sobol' approach and discuss their implementation in a Mat lab software tool (GSAT). We also introduce a new criterion for model selection based on sensitivity analysis. The proposed approach is tested and applied to quantify the fitting ability of an analytic volcanic source model on a synthetic deformation data. Results show the validity of the method, against the traditional approaches, in supporting the volcanic model selection and the flexibility of the GSAT software tool in analyzing the model sensitivity. (C) ", Elsevier Ltd. All rights reserved.,"Cannavo, F",COMPUTERS & GEOSCIENCES,sensitivity analysis inverse problem volcanic source modeling model selection,10.1016/j.cageo.2012.03.008
170,WOS:000187422700003,2004,MULINO-DSS: a computer tool for sustainable use of water resources at the catchment scale,EXAMPLE DESIGN MODELS,"MULINO, an ongoing project financed by the European Commission, has released the prototype of a Decision Support System software (mDSS) for the sustainable management of water resources at the catchment scale. The software integrates socio-economic and environmental modelling, with geo-spatial information and multi-criteria analysis. The policy background refers to the EU Water Framework Directive. The challenging multi-disciplinary context was approached by developing an innovative and dynamic implementation of the DPSIR framework, originally proposed by the European Environmental Agency. In mDSS integrated assessment modelling provides the values of quantitative indicators to be used for transparent and participated decisions, through the application of value functions, weights and decision rules chosen by the end user. Simple routines for the sensitivity analysis and comparison of alternative weight vectors also provides effective decision support by exploring and finding compromises between conflicting interests/perspectives in a multi-stakeholder context.", (C) 2003 Published by Elsevier B.V on behalf of IMACS.,"Giupponi, C|Mysiak, J|Fassio, A|Cogan, V",MATHEMATICS AND COMPUTERS IN SIMULATION,dss software water resources sustainable use catchment modelling,10.1016/j.matcom.2003.07.003
171,WOS:000392285600091,2016,Identifying critical architectural components with spectral analysis of fault trees,SOFTWARE ARCHITECTURE RELIABILITY-ANALYSIS SYSTEMS MODELS TOOLS,"We increasingly rely on software-intensive embedded systems. Increasing size and complexity of these hardware/software systems makes it necessary to evaluate reliability at the system architecture level. One aspect of this evaluation is sensitivity analysis, which aims at identifying critical components of the architecture. These are the components of which unreliability contributes the most to the unreliability of the system. In this paper, we propose a novel approach for sensitivity analysis based on spectral analysis of fault trees. We show that measures obtained with our approach are both consistent and complementary with respect to the recognized metrics in the literature.", (C) 2016 Elsevier B.V. All rights reserved.,"Ayav, T|Sozer, H",APPLIED SOFT COMPUTING,hardware/software architecture evaluation reliability analysis fault trees fourier analysis sensitivity analysis importance analysisa,10.1016/j.asoc.2016.06.042
172,WOS:000231782200001,2005,Scenario-based simulation of runoff-related pesticide entries into small streams on a landscape level,SURFACE WATER FIELD,"The prediction of runoff-related pesticide entry into surface waters on a landscape level usually requires considerable efforts with regard to input data, time, and personnel. Therefore, the need for an easy to use simulation tool with easily accessible input data, for example from already existing public sources, is obvious. In this paper, we present a simulation tool for the simulation of pesticide entry from arable land into adjacent streams. Our aim was to develop a tool applicable on the landscape level using ""real world data"" from numerous sites and for the simulation of parameter case studies concerning particular parameters at single sites. We used the ratio of exposure to toxicity (REXTOX) model proposed by the OECD, which had been successfully validated in the study area as part of a previous study and which was extended to calculate pesticide concentrations in adjacent streams. We simulated the pesticide entry on the landscape level at  sites in small streams situated in the central lowland of Germany with winter wheat, barley, and sugar beat as the main agricultural. crops. A sensitivity analysis indicated that the most significant model parameters were the width of the no-application zone and the degree of plant interception. The simulation was carried out for the  most frequently detected substances found in the study area using eight different environmental scenarios, covering variation of the width of the no-application zone, climate, and seasonal scenarios. The highest in-stream concentrations were predicted for a scenario using no ( m) buffer zone in conjunction with increased precipitation. According to the predicted concentrations, the risk for the aquatic communities was estimated based on standard toxicity tests and the application of a safety factor. The simulation results are presented both by means of risk maps for the study area showing the simulated pesticide concentration and the resulting ecological risk for numerous sites under varying scenarios and by case study diagrams with focus on the model behavior under the influence of single parameters. Risk maps confirmed the importance of no-application (buffer) zones for the levels of pesticide input. They also indicated the importance of the existing no-application zones for certain compounds and in some cases the need for a further evaluation of these regulations. The simulation tool was implemented as a standard PC software combining the REXTOX model with a geographical information system and can be used on any current personal computer. All input data was taken from public sources of German authorities. With little effort the tool should be applicable for other areas with similar data quality.", (C) 2005 Elsevier Inc. All rights reserved.,"Probst, M|Berenzen, N|Lentzen-Godding, A|Schulz, R",ECOTOXICOLOGY AND ENVIRONMENTAL SAFETY,risk assessment pesticides runoff buffer zones simulation modeling landscape level climate change risk mitigation,10.1016/j.ecoenv.2005.04.012
173,WOS:000323457200002,2013,Comparative study on simulation performances of CORSIM and VISSIM for urban street network,ACTUATED SIGNAL SYSTEM MICROSCOPIC SIMULATION MODEL CALIBRATION TRAFFIC FLOW VALIDATION BEHAVIOR,"With the progress of simulation technologies, many transportation simulation packages were developed. However, little information is available to the users in applying these models to the most appropriate situations, or even seldom with the simulation accuracy of the individual model. This study conducts a comparative analysis of two popular simulation models (VISSIM and CORSIM), based on their simulation performances on an urban transportation network. Road network and field traffic data from North Bund, Hongkou District, Shanghai, China were used as the simulation background and input. Sensitivity analysis was carried out to compare the performance of both models based on four key indices, namely software usability, average control delay, average queuing length, and cross-sectional traffic volume. Advantages of each simulator were identified based on comparison analyses of simulations with different levels of congestion and intersection geospatial scales. The main performance difference was found lying in the default parameter configuration within the models, including driver behavior settings, traffic environment settings, and vehicle types, etc. Consequently, it was recommended that analysts should choose their appropriate tools based on intersection type and level of saturation within the simulation case.", (c) 2013 Elsevier B.V. All rights reserved.,"Sun, D|Zhang, LH|Chen, FX",SIMULATION MODELLING PRACTICE AND THEORY,micro-simulation model urban transportation network comparative study signal intersections sensitivity analysis,10.1016/j.simpat.2013.05.007
174,WOS:000223528100004,2004,POLCAGE 1.0 - a possibilistic life-cycle assessment model for evaluating alternative transportation fuels,FUZZY OUTRANKING SYSTEMS ENERGY,"A composite software model for the comparative life-cycle assessment (LCA) of  different fuel options for the Philippine automotive transport sector was developed. It is based on the GREET fuel-cycle inventory model developed by the Argonne National Laboratory for the United States Department of Energy. GREET .a is linked to an impact assessment submodel using the Danish environmental design of industrial products (EDIP) method. This combined inventory-impact assessment model is enhanced further with possibilistic uncertainty propagation (PUP) and possibilistic compromise programming (PCP) features that allow the  alternatives to be ranked in the presence of multiple criteria and uncertain data. Sensitivity and scenario analysis can also be performed within the composite model. Some current and anticipated Philippine conditions, including electricity generation mix, are incorporated in the prototype's built-in database. The software model, designated as POLCAGE . (possibilistic LCA using GREET and EDIP), is coded in Microsoft Excel and Visual Basic. The model's capabilities and features are demonstrated using a case study based on its default scenario. (C) ", Elsevier Ltd. All rights reserved.,"Tan, RR|Culaba, AB|Purvis, MRI",ENVIRONMENTAL MODELLING & SOFTWARE,life-cycle assessment (lca) decision support system (dss) alternative fuels,10.1016/j.envsoft.2003.10.004
175,WOS:000342532200014,2014,Natural ventilation design: An analysis of predicted and measured performance,,"We present a study of natural ventilation design during the early (conceptual) stage of a building's design, based on a field study in a naturally ventilated office in California where we collected data on occupants' window use, local weather conditions, indoor environmental conditions, and air change rates based on tracer-gas decay. We performed uncertainty and sensitivity analyses to determine which design parameters have most impact on the uncertainty associated with ventilation performance predictions. Using the results of the field study along with wind-tunnel measurements and other detailed analysis, we incrementally improved our early-design-stage model. The improved model's natural ventilation performance predictions were significantly more accurate than those of the first draft early-stage-design model that employed model assumptions typical during initial design. This process highlighted significant limitations in the EnergyPlus software's models of occupant-driven window control. We conclude with recommendations on key design parameters including window control, wind pressure coefficients and weather data resolution to help improve early-design-stage predictions of natural ventilation performance using EnergyPlus. (C) ", Elsevier Ltd. All rights reserved.,"Belleri, A|Lollini, R|Dutton, SM",BUILDING AND ENVIRONMENT,natural ventilation early-design-stage uncertainty analysis air change rates occupant behavior airflow network,10.1016/j.buildenv.2014.06.009
176,WOS:000336664900001,2014,Mathematical Analysis for the Optimization of a Design in a Facultative Pond: Indicator Organism and Organic Matter,WASTE STABILIZATION PONDS COMPUTATIONAL FLUID-DYNAMICS HELMINTH EGGS REMOVAL MATURATION PONDS PERFORMANCE EVALUATION HYDRAULIC PERFORMANCE TREATMENT EFFICIENCY COLIFORM REMOVAL BAFFLES MODEL,"Stabilization ponds are easy to operate and their maintenance is simple. Treatment is carried out naturally and they are recommended in developing countries. The main disadvantage of these systems is the large land area they occupy. The aim of this study was to perform an optimization in the design and cost of a facultative pond, considering a mathematical analysis of the traditional methodology to determine the model constraints (fecal coliforms and organic matter). Matlab optimization toolbox was used for nonlinear programming. A facultative pond with the traditional method was designed and then the optimization system was applied. Both analyses meet the treated water quality requirements for the discharge to the receiving bodies. The results show a reduction of hydraulic retention time by . days, and a decrease in the area of . percent over the traditional method. A sensitivity analysis of the mathematical model is included. It is recommended to realize a full-scale study in order to verify the results of the optimization.",,"Martinez, FC|Cansino, AT|Garcia, MAA|Kalashnikov, V|Rojas, RL",MATHEMATICAL PROBLEMS IN ENGINEERING,,10.1155/2014/652509
177,WOS:000263595500028,2009,The effect of field conditions on low Reynolds number flow in a wetland,LONGITUDINAL DISPERSION EMERGENT VEGETATION RESISTANCE DIFFUSION STREAMS MODEL,"Stormwater runoff has been an environmental concern since the s. Green infrastructure, such as constructed stormwater wetlands (CSWs), is a tool in stormwater management, however, little is known about the hydraulic diffusion processes that impact water quality in low flow, laminar (i.e. baseflow) conditions. Diffusion provides the mechanisms that distribute and mix water through a CSW and therefore how pollutants will be spread through the CSW impacting the water quality. Laboratory experiments were performed by Nepf, H.M., Sullivan, J.A., Zavistoski, R.A. [. A model for diffusion within emergent vegetation. Limnology and Oceanography, (), S-], and Serra, T., Fernando, H.J.S., Rodriquez, R.V. [. Effects of emergent vegetation on lateral diffusion in wetland. Water Research, (), - to examine the effect of plant density on diffusion in laminar flow conditions. Nepf, H.M. [. Drag, turbulence, and diffusion in flow through emergent vegetation. Water Resources Research, (), - proposed a model predicting the diffusion coefficient based upon the plant density for both laminar and turbulent flow conditions. The present study examines the effect of field conditions on diffusion in a laminar flow field and verifies the diffusion model created by Nepf, H.M. [. Drag, turbulence, and diffusion in flow through emergent vegetation. Water Resources Research, (), -. The results from the present study show that the laminar flow model, based solely on mechanical diffusion, is not sufficient for field conditions and the total diffusion model must be used. The variability in flow conditions and stem diameter found in the field produce pockets of turbulence and dead zones that must be considered to predict the diffusion coefficients in low flow CSWs. A sensitivity analysis of the dead zone term shows that the laboratory, field and diffusion models lie within an acceptable theoretical range for the observed or predicted diffusion coefficient. in addition, a model was created using the Danish Hydraulic Institutes Mike  software. Model results indicate that non-uniform velocities significantly affect the diffusion coefficient and a range of diffusion coefficients should be considered when designing CSWs. (C) ", Elsevier Ltd. All rights reserved.,"Burke, EN|Wadzuk, BM",WATER RESEARCH,wetlands diffusion laminar flow hydrodynamic model,10.1016/j.watres.2008.10.027
178,WOS:000178672100002,2002,The potential of latent semantic analysis for machine grading of clinical case summaries,COMPREHENSION TEXT,"`Objective: This paper introduces latent semantic analysis (LSA), a machine learning method for representing the meaning of words, sentences, and texts. LSA induces a high-dimensional semantic space from reading a very large amount of texts. The meaning of words and texts can be represented as vectors in this space and hence can be compared automatically and objectively. Psychological theory: A generative theory of the mental lexicon based on LSA is described. The word vectors LSA constructs are context free, and each word, irrespective of how many meanings or senses it has, is represented by a single vector. However, when a word is used in different contexts, context appropriate word senses emerge. Current applications: Several applications of LSA to educational software are described, involving the ability of LSA to quickly compare the content of texts, such as an essay written by a student and a target essay. Potential medical applications: An LSA-based software tool is sketched for machine grading of clinical case summaries written by medical students.", (C) 2002 Elsevier Science (USA). All rights reserved.,"Kintsch, W",JOURNAL OF BIOMEDICAL INFORMATICS,latent semantic analysis knowledge representation cognitive science,10.1016/S1532-0464(02)00004-7
179,WOS:000230023700002,2005,Applicability of neuro-fuzzy techniques in predicting ground-water vulnerability: a GIS-based sensitivity analysis,AQUIFER VULNERABILITY HYDRAULIC-PROPERTIES MODELS WATER CLASSIFICATION NETWORKS FLOW MANAGEMENT TRANSPORT SELECTION,"Modeling groundwater vulnerability reliably and cost effectively for non-point source (NPS) pollution at a regional scale remains a major challenge. In recent years, Geographic Information Systems (GIS), neural networks and fuzzy logic techniques have been used in several hydrological studies. However, few of these research studies have undertaken an extensive sensitivity analysis. The overall objective of this research is to examine the sensitivity of neuro-fuzzy models used to predict groundwater vulnerability in a spatial context by integrating GIS and neuro-fuzzy techniques. The specific objectives are to assess the sensitivity of neuro-fuzzy models in a spatial domain using GIS by varying (i) shape of the fuzzy sets, (ii) number of fuzzy sets, and (iii) learning and validation parameters (including rule weights). The neuro-fuzzy models were developed using NEFCLASS-J software on a JAVA platform and were loosely integrated with a GIS. Four plausible parameters which are critical in transporting contaminants through the soil profile to the groundwater, included soil hydrologic group, depth of the soil profile, soil structure (pedality points) of the A horizon, and landuse. In order to validate the model predictions, coincidence reports were generated among model inputs, model predictions, and well/spring contamination data for NO-N. A total of  neuro-fuzzy models were developed for selected sub-basins of Illinois River Watershed, AR. The sensitivity analysis showed that neuro-fuzzy models were sensitive to the shape of the fuzzy sets, number of fuzzy sets, nature of the rule weights, and validation techniques used during the learning processes. Compared to bell-shaped and triangular-shaped membership functions, the neuro-fuzzy models with a trapezoidal membership function were the least sensitive to the various permutations and combinations of the learning and validation parameters. Over all, Models  and  showed relatively higher coincidence with well contamination data than other models. The strength of this method is that it offers a means of dealing with imprecise data, therefore, is a viable option for regional and continental scale environmental modeling where imprecise data prevail. The neuro-fuzzy models, however, should only be used as a tool within a broader framework of GIS, remote sensing and solute transport modeling to assess groundwater vulnerability along with functional, mechanistic and stochastic models.", (c) 2004 Elsevier B.V. All rights reserved.,"Dixon, B",JOURNAL OF HYDROLOGY,gis gps no3-n modeling fuzzy logic neural networks,10.1016/j.jhydrol.2004.11.010
180,WOS:000321313800102,2013,Techno-economic analysis of two bio-oil upgrading pathways,BIOMASS FAST PYROLYSIS HYDROGEN-PRODUCTION GREEN GASOLINE MODEL FUELS,"We evaluate the economic feasibility for two bio-oil upgrading pathways: two-stage hydrotreating followed by fluid catalytic cracking (FCC) or single-stage hydrotreating followed by hydrocracking. In the hydrotreating/FCC pathway, two options are available as the hydrogen source for hydrotreating: merchant hydrogen or hydrogen from natural gas reforming. The primary products of the hydrotreating/FCC pathway are commodity chemicals whereas the primary products for the hydrotreating/hydrocracking pathway are transportation fuels and hydrogen. The two pathways are modeled using Aspen Plus (R) for a  metric tons/day facility. Equipment sizing and cost calculations are based on Aspen Economic Evaluation (R) software. The bio-oil yield via fast pyrolysis is assumed to be % of biomass. We calculate the internal rate of return (IRR) for each pathway as a function of feedstock cost, fixed capital investment (FCI), hydrogen and catalyst costs, and facility revenues. The results show that a facility employing the hydrotreating/ FCC pathway with hydrogen production via natural gas reforming option generates the highest IRR of .%. Sensitivity analysis demonstrates that product yield, FCI, and biomass cost have the greatest impacts on facility IRR. Monte-Carlo analysis shows that two-stage hydrotreating and FCC of the aqueous phase bio-oil with hydrogen produced via natural gas reforming has a relatively low risk for project investment.", (c) 2013 Published by Elsevier B.V.,"Zhang, YN|Brown, TR|Hu, GP|Brown, RC",CHEMICAL ENGINEERING JOURNAL,fast pyrolysis bio-oil upgrading commodity chemicals transportation fuels hydrogen,10.1016/j.cej.2013.01.030
181,WOS:000382049400008,2016,"Parameterization, sensitivity analysis, and inversion: an investigation using groundwater modeling of the surface-mined Tivoli-Guidonia basin (Metropolitan City of Rome, Italy)",WATER MODELS UNCERTAINTY IMPORTANCE FLOW TRANSPORT VALIDATION SYSTEM SITE USA,"With respect to model parameterization and sensitivity analysis, this work uses a practical example to suggest that methods that start with simple models and use computationally frugal model analysis methods remain valuable in any toolbox of model development methods. In this work, ground-water model calibration starts with a simple parameterization that evolves into a moderately complex model. The model is developed for a water management study of the TivoliGuidonia basin (Rome, Italy) where surface mining has been conducted in conjunction with substantial dewatering. The approach to model development used in this work employs repeated analysis using sensitivity and inverse methods, including use of a new observation-stacked parameter importance graph. The methods are highly parallelizable and require few model runs, which make the repeated analyses and attendant insights possible. The success of a model development design can be measured by insights attained and demonstrated model accuracy relevant to predictions. Example insights were obtained: () A long-held belief that, except for a few distinct fractures, the travertine is homogeneous was found to be inadequate, and () The dewatering pumping rate is more critical to model accuracy than expected. The latter insight motivated additional data collection and improved pumpage estimates. Validation tests using three other recharge and pumpage conditions suggest good accuracy for the predictions considered. The model was used to evaluate management scenarios and showed that similar dewatering results could be achieved using  % less pumped water, but would require installing newly positioned wells and cooperation between mine owners.",,"La Vigna, F|Hill, MC|Rossetto, R|Mazza, R",HYDROGEOLOGY JOURNAL,groundwater management numerical modeling inverse modeling carbonate rocks italy,10.1007/s10040-016-1393-z
182,WOS:000300251600008,2012,Coupling hydrogeological with surface runoff model in a Poltva case study in Western Ukraine,HETEROGENEOUS POROUS-MEDIA WATER-GROUNDWATER INTERACTIONS URBAN DRAINAGE SYSTEMS SENSITIVITY-ANALYSIS HYDROLOGIC-RESPONSE STREAM DEPLETION AQUIFER RESPONSE OVERLAND-FLOW MASS-TRANSFER STORM RUNOFF,"This paper presents the hydrological coupling of the software framework OpenGeoSys (OGS) with the EPA Storm Water Management Model (SWMM). Conceptual models include the Saint Venant equation for river flow, the D Darcy equations for confined and unconfined groundwater flow, a two-way hydrological coupling flux in a compartment coupling approach (conductance concept), and Lagrangian particles for solute transport in the river course. A SWMM river-OGS aquifer inter-compartment coupling flux is examined for discharging groundwater in a systematic parameter sensitivity analysis. The parameter study involves a small perturbation (first-order) sensitivity analysis and is performed for a synthetic test example base-by-base through a comprehensive range of aquifer parametrizations. Through parametrization, the test cases enables to determine the leakance parameter for simulating streambed clogging and non-ocillatory river-aquifer water exchange rates with the sequential (partitioned) coupling scheme. The implementation is further tested with a hypothetical but realistic D river-D aquifer model of the Poltva catchment, where discharging groundwater in the upland area affects the river-aquifer coupling fluxes downstream in the river course (propagating feedbacks). Groundwater contribution in the moving river water is numerically determined with Lagrangian particles. A numerical experiment demonstrates that the integrated river-aquifer model is a serviceable and realistic constituent in a complete compartment model of the Poltva catchment.",,"Delfs, JO|Blumensaat, F|Wang, WQ|Krebs, P|Kolditz, O",ENVIRONMENTAL EARTH SCIENCES,integrated surface-subsurface flow modelling urban water conductance concept sensitivity analysis random walk particle tracking (rwpt) poltva basin (western ukraine),10.1007/s12665-011-1285-4
183,WOS:000374807600014,2016,Towards uncertainty quantification and parameter estimation for Earth system models in a component-based modeling framework,EQUIFINALITY,"Component-based modeling frameworks make it easier for users to access, configure, couple, run and test numerical models. However, they do not typically provide tools for uncertainty quantification or data-based model verification and calibration. To better address these important issues, modeling frameworks should be integrated with existing, general-purpose toolkits for optimization, parameter estimation and uncertainty quantification. This paper identifies and then examines the key issues that must be addressed in order to make a component-based modeling framework interoperable with general-purpose packages for model analysis. As a motivating example, one of these packages, DAKOTA, is applied to a representative but nontrivial surface process problem of comparing two models for the longitudinal elevation profile of a river to observational data. Results from a new mathematical analysis of the resulting nonlinear least squares problem are given and then compared to results from several different optimization algorithms in DAKOTA. (C) ", Elsevier Ltd. All rights reserved.,"Peckham, SD|Kelbert, A|Hill, MC|Hutton, EWH",COMPUTERS & GEOSCIENCES,model uncertainty modeling frameworks component-based modeling optimization inverse problems nonlinear least squares parameter estimation longitudinal river elevation profiles,10.1016/j.cageo.2016.03.005
184,WOS:000273363700015,2009,BAT - The Bayesian analysis toolkit,MODEL,"We describe the development of a new toolkit for data analysis. The analysis package is based on Bayes' Theorem, and is realized with the use of Markov Chain Monte Carlo. This gives access to the full posterior probability distribution. Parameter estimation, limit setting and uncertainty propagation are implemented in a straightforward manner.", (C) 2009 Elsevier B.V. All rights reserved.,"Caldwell, A|Kollar, D|Kroninger, K",COMPUTER PHYSICS COMMUNICATIONS,data analysis markov chain monte carlo,10.1016/j.cpc.2009.06.026
185,WOS:000356741300001,2015,Multi-objective model auto-calibration and reduced parameterization: Exploiting gradient-based optimization tool for a hydrologic model,AUTOMATIC CALIBRATION MULTICRITERIA METHODS SENSITIVITY-ANALYSIS GLOBAL OPTIMIZATION SOIL-MOISTURE SWAT MODULE INFORMATION VALIDATION ALGORITHM,"Multi-objective model optimization methods have been extensively studied based on evolutionary algorithms, but less on gradient-based algorithms. This study demonstrates a framework for multi-objective model calibration/optimization using gradient-based optimization tools. Model-independent software Parameter ESTimation (PEST) was used to auto-calibrate ISWAT, a modified version of the distributed hydrologic model Soil and Water Assessment Tool (SWAT), in the Shenandoah River watershed. The time-series processor TSPROC was used to combine multiple objectives into the auto-calibration process. Two sets of roughness coefficients for main channels, one assigned and calibrated according on soil types and one determined via empirical equations, were examined for stream discharge simulation. Five different weighting alternatives were investigated for their effects on ISWAT calibrations. Results showed that using Manning's roughness coefficients obtained from empirical equations improves simulation results and calibration efficiency. Applying a two-step weighting alternative to different observation groups would provide the best calibration results. (C) ", Elsevier Ltd. All rights reserved.,"Wang, Y|Brubaker, K",ENVIRONMENTAL MODELLING & SOFTWARE,pest swat tsproc multi-objectives auto-calibration,10.1016/j.envsoft.2015.04.001
186,WOS:000254090100016,2008,Numerical simulation of 3D bubbles rising in viscous liquids using a front tracking method,FREE-BOUNDARY PROBLEMS GAS BUBBLE MULTIPHASE FLOW REYNOLDS-NUMBER SURFACE-TENSION FLUID VOLUME RISE DYNAMICS DEFORMATION,"The rise of bubbles in viscous liquids is not only a very common process in many industrial applications, but also an important fundamental problem in fluid physics. An improved numerical algorithm based on the front tracking method, originally proposed by Tryggvason and his co-workers, has been validated against experiments over a wide range of intermediate Reynolds and Bond numbers using an axisymmetric model [J. Hua, J. Lou, Numerical simulation of bubble rising in viscous liquid, J. Comput. Phys.  () -]. In the current paper, this numerical algorithm is further extended to simulate D bubbles rising in viscous liquids with high Reynolds and Bond numbers and with large density and viscosity ratios representative of the common air-water two-phase flow system. To facilitate the D front tracking simulation, mesh adaptation is implemented for both the front mesh on the bubble surface and the background mesh. On the latter mesh, the governing Navier-Stokes equations for incompressible, Newtonian flow are solved in a moving reference frame attached to the rising bubble. Specifically, the equations are solved using a finite volume scheme based on the Semi-Implicit Method for Pressure-Linked Equations (SIMPLE) algorithm, and it appears to be robust even for high Reynolds numbers and high density and viscosity ratios. The D bubble surface is tracked explicitly using an adaptive, unstructured triangular mesh. The numerical model is integrated with the software package PARAMESH, a block-based adaptive mesh refinement (AMR) tool developed for parallel computing. PARAMESH allows background mesh adaptation as well as the solution of the governing equations in parallel on a supercomputer. Further, Peskin distribution function is applied to interpolate the variable values between the front and the background meshes. Detailed sensitivity analysis about the numerical modeling algorithm has been performed. The current model has also been applied to simulate a number of cases of D gas bubbles rising in viscous liquids, e.g. air bubbles rising in water. Simulation results are compared with experimental observations both in aspect of terminal bubble shapes and terminal bubble velocities. In addition, we applied this model to simulate the interaction between two bubbles rising in a liquid, which illustrated the model's capability in predicting the interaction dynamics of rising bubbles.", (C) 2007 Elsevier Inc. All rights reserved.,"Hua, J|Stene, JF|Lin, P",JOURNAL OF COMPUTATIONAL PHYSICS,computational fluid dynamics incompressible flow multiphase flow bubble rising simple algorithm front tracking method adaptive mesh refinement moving reference frame,10.1016/j.jcp.2007.12.002
187,WOS:000230708100004,2005,"Determination of the optimal installation capacity of small hydro-power plants through the use of technical, economic and reliability indices",,"One of the most important issues in planning Small Hydro-Power Plants (SHPPs) of the ""run-off river"" type is to determine the optimal installation capacity of the SHPP and estimate its optimal annual energy value. In this paper, a method to calculate the annual energy is presented, as is the program developed using Excel software. This program analyzes and estimates the most important economic indices of an SHPP using the sensitivity analysis method. Another program, developed by Matlab software, calculates the reliability indices for a number of units of an SHPP with a specified load duration curve using the Monte Carlo method. Ultimately, comparing the technical, economic and reliability indices will determine the optimal installation capacity of an SHPP. By applying the above-mentioned algorithm to a sample SHPP named ""Nari"" (located in the northern part of Iran), the optimal capacity of . MW is obtained. (c) ", Elsevier Ltd. All rights reserved.,"Hosseini, SMH|Forouzbakhsh, F|Rahimpoor, M",ENERGY POLICY,installation capacity small hydro-power plant (shpp) economic analysis monte carlo method,10.1016/j.enpol.2004.03.007
188,WOS:000231963300008,2005,Parameterization based shape optimization: theory and practical implementation aspects,DESIGN SENSITIVITY-ANALYSIS TOPOLOGY OPTIMIZATION MECHANICAL SYSTEMS EFFICIENT HOMOGENIZATION TOOL,"Purpose - To present an approach to parameterization based shape optimization of statically loaded structures and to propose its practical implementation. Design/methodology/approach - In order to establish a convenient shape parameterization, the design element technique is employed. A rational Bezier body is used to serve as the design element. The design element is used to retrieve the nodal geometrical data of finite elements (FEs). Their field geometrical data are obtained using the FE own internal functions. For practical implementation it is proposed to establish the optimization cycle by two separately running processes. The data exchange is established by using self-descriptive and platform-independent XML conforming data files. Findings - The proposed approach offers an unified approach to shape optimization of skeletal, as well as continuous structures. Structural shape may be varied smoothly with a relative small set of design variables. The employment of a gradient-based optimization algorithm assures computational efficiency. Research limitations/implications - The aspects of FE mesh deterioration are not considered in this work. This would be necessary if for the actual problem at hand major and excessively non-uniform shape changes of the FE mesh are expected. Practical implications - A useful source of information for someone who is planning to develop a general or special-purpose integrated structural analysis and shape optimization software. Originality/value - The paper offers a rather simple, but quite powerful approach to structural shape optimization together with practical hints for its computational implementation.",,"Kegl, M",ENGINEERING COMPUTATIONS,optimization techniques structures numerical analysis,10.1108/02644400510603041
189,WOS:000378360600044,2016,Tools for investigating the prior distribution in Bayesian hydrology,PARAMETER-ESTIMATION INFERENCE MODELS CALIBRATION,"Bayesian inference is one of the most popular tools for uncertainty analysis in hydrological modeling. While much emphasis has been placed on the selection of appropriate likelihood functions within Bayesian hydrology, few researchers have evaluated the importance of the prior distribution in deriving appropriate posterior distributions. This paper describes tools for the evaluation of parameter sensitivity to the prior distribution to provide guidelines for defining meaningful priors. The tools described here consist of two measurements, the Kullback-Leibler Divergence (KLD) and the prior information elasticity. The Kullback-Leibler Divergence (KLD) is applied to calculate differences between the prior and posterior distributions for different cases. The prior information elasticity is then used to quantify the responsiveness of the KLD values to the change of prior distributions and length of available data. The tools are demonstrated via a Bayesian framework using an MCMC algorithm for a conceptual hydrologic model with both synthetic and real cases. The results of the application of this toolkit suggest the prior distribution can have a significant impact on the posterior distribution and should be more routinely assessed in hydrologic studies.", (C) 2016 Elsevier B.V. All rights reserved.,"Tang, YT|Marshall, L|Sharma, A|Smith, T",JOURNAL OF HYDROLOGY,bayesian hydrological modeling prior distribution parameter sensitivity kullback-leibler divergence elasticity,10.1016/j.jhydrol.2016.04.032
190,WOS:000413669300001,2017,Numerical Study of the Performance and Emission of a Diesel-Syngas Dual Fuel Engine,METHANOL STEAM REFORMER MECHANISM REDUCTION PARTIAL OXIDATION N-HEPTANE COMBUSTION GASIFICATION SIMULATION OPERATION MIXTURES GAS,"Based on the theory of direct relation graph (DRG) and the sensitivity analysis, a reduced mechanism for the diesel-syngas dual fuel was constructed. Three small thresholds were applied in the process of the detailed mechanism simplification by DRG, and a skeletal mechanism with  elements and the  elementary reactions was obtained. According to the framework of the skeletal mechanism, the time-consuming approach of sensitivity analysis was employed for further simplification, and the skeletal mechanism was further reduced to the size of  elements and  reactions. The Chemkin software with the detailed mechanism was utilized to calculate the effect of syngas addition on the combustion characteristics of diesel combustion. The findings showed that the addition of syngas could reduce the ignition delay time and increase the laminar flame speed. Based on the reduced mechanism and engine parameters, a D model of the engine was constructed with the Forte code. The D model was adopted to study the effect of syngas addition on the performance and exhaust emissions of the engine and the relevant data of the experiment was used in the calibration of the D model.",,"Feng, SQ",MATHEMATICAL PROBLEMS IN ENGINEERING,,10.1155/2017/6825079
191,WOS:000090149600007,2000,An object-oriented structural optimization program,SHAPE OPTIMIZATION,"In this paper, implementation concepts of a structural optimization software using object-oriented programming (OOP) in CS ++ is presented. A brief mathematical formulation of structural optimization and continuum-based sensitivity analysis is presented. The requirements of a computational optimization environment are derived from this formulation. The OOP characteristics are analysed and this paradigm is employed in the implementation of design variables, structural performance functionals, velocity fields, design model and mathematical programming algorithms using CI-Jr. Finally, the program obtained is applied to D linear elastic examples of sizing and shape optimization.",,"Silva, CAC|Bittencourt, ML",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,structural optimization sensitivity analysis c plus object-oriented programming linear elasticity,10.1007/s001580050146
192,WOS:000380760400073,2016,Life Cycle Assessment and Life Cycle Cost Analysis of Magnesia Spinel Brick Production,CEMENT INDUSTRY ENERGY LCA,"Sustainable use of natural resources in the production of construction materials has become a necessity both in Europe and Turkey. Construction products in Europe should have European Conformity (CE) and Environmental Product Declaration (EPD), an independently verified and registered document in line with the European standard EN . An EPD certificate can be created by performing a Life Cycle Assessment (LCA) study. In this particular work, an LCA study was carried out for a refractory brick production for environmental assessment. In addition to the LCA, the Life Cycle Cost (LCC) analysis was also applied for economic assessment. Firstly, a cradle-to-gate LCA was performed for one ton of magnesia spinel refractory brick. The CML IA method included in the licensed SimaPro .. software was chosen to calculate impact categories (namely, abiotic depletion, global warming potential, acidification potential, eutrophication potential, human toxicity, ecotoxicity, ozone depletion potential, and photochemical oxidation potential). The LCC analysis was performed by developing a cost model for internal and external cost categories within the software. The results were supported by a sensitivity analysis. According to the results, the production of raw materials and the firing process in the magnesia spinel brick production were found to have several negative effects on the environment and were costly.",,"Ozkan, A|Gunkaya, Z|Tok, G|Karacasulu, L|Metesoy, M|Banar, M|Kara, A",SUSTAINABILITY,cml method firing process global warming potential life cycle assessment (lca) life cycle cost (lcc) magnesia spinel brick refractory production,10.3390/su8070662
193,WOS:000353491500001,2015,Integrated Location-Production-Distribution Planning in a Multiproducts Supply Chain Network Design Model,GENETIC ALGORITHM APPROACH SENSITIVITY-ANALYSIS FACILITY LOCATION PLANT LOCATION OPTIMIZATION MANAGEMENT SYSTEM,"This paper proposes integrated location, production, and distribution planning for the supply chain network design which focuses on selecting the appropriate locations to build a new plant and distribution center while deciding the production and distribution of the product. We examine a multiechelon supply chain that includes suppliers, plants, and distribution centers and develop a mathematical model that aims at minimizing the total cost of the supply chain. In particular, the mathematical model considers the decision of how many plants and distribution centers to open and where to open them, as well as the allocation in each echelon. The LINGO software is used to solve the model for some problem cases. The study conducts various numerical experiments to illustrate the applicability of the developed model. Results show that, in small and medium size of problem, the optimal solution can be found using this solver. Sensitivity analysis is also conducted and shows that customer demand parameter has the greatest impact on the optimal solution.",,"Yu, VF|Normasari, NME|Luong, HT",MATHEMATICAL PROBLEMS IN ENGINEERING,,10.1155/2015/473172
194,WOS:000349077200002,2015,Second law and sensitivity analysis of large ME-TVC desalination units,MULTIPLE-EFFECT EVAPORATION THERMAL-ANALYSIS EXERGY ANALYSIS SYSTEMS PERFORMANCE ENERGY,"Large number of low temperature multi-effect thermal vapor compression (ME-TVC) desalination units have been installed recently in most of the GCC countries. The new trend of combining ME-TVC with conventional multi-effect led to tremendous increase in the unit size more than eight times during a very short period. The unit size capacity of this technology is currently available with  million imperial gallons per day (MIGD), and research studies are expected to increase this unit capacity up to  MIGD in the near future. Hence, this technology becomes highly attractive and competitive against multi stage flash desalination system. A mathematical model of ME-TVC desalination system is developed in this paper, using Engineering Equation Solver Software. This model is used to evaluate and improve the system performance of some new commercial ME-TVC units with capacities of ., ., and . MIGD using energy and exergy analysis. A sensitivity analysis is also presented in this paper to investigate the system performance of Al-Jubail ME-TVC unit in KSA, which is considered as the largest ME-TVC desalination plant in the world. Results showed that the first effect was found to be responsible for about % of the total effects exergy destruction in Al-Jubail, compared to % in ALBA and % in Umm Al-Nar. Results also showed that the specific exergy destruction is reduced significantly by increasing the number of effects as well as working at lower top brine temperatures.",,"Binamer, A",DESALINATION AND WATER TREATMENT,thermal vapor multi-effect exergy desalination,10.1080/19443994.2013.852481
195,WOS:000168416500008,2001,A computational methodology for shape optimization of structures in frictionless contact,SUPERCONVERGENT PATCH RECOVERY SENSITIVITY ANALYSIS FORMULATION ALGORITHMS SOLIDS,"This paper presents a computational methodology for shape optimization of structures in frictionless contact. which provides a basis for developing user-friendly and efficient shape optimization software. For evaluation it has been implemented as a subsystem of a general finite element software. The overall design and main principles of operation of this software are outlined. The parts connected to shape optimization are described in more detail. The key building blocks are: analytic sensitivity analysis, an adaptive finite element method, an accurate contact solver. and a sequential convex programing optimization algorithm. Results for three model application examples are presented, in which the contact pressure and the effective stress are optimized.", (C) 2001 Elsevier Science B.V. All rights reserved.,"Hilding, D|Torstenfelt, B|Klarbring, A",COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,shape optimization frictionless contact finite element method sensitivity analysis adaptive meshing,10.1016/S0045-7825(00)00310-8
196,WOS:000355870800008,2015,Stability analysis of rotary power flow controller,FACTS DEVICES SYSTEMS,"This paper deals with the stability analysis of a rotary power flow controller (RPFC). RPFC is a newly presented flexible alternating current transmission system device based on the rotary phase shifting transformer (RPST) configuration. This device comprises a shunt and a series transformer and two RPSTs. In this paper, a linear model is developed for RPFC in the dq-reference frame. The model includes two RPSTs, shunt, and series transformers. Stability of the device is studied using eigenvalues of the state matrix. Effects of system parameters such as resistance, reactance, and phase difference of the shunt and series transformers and RPSTs on stability are investigated using eigenvalue sensitivity analysis. Simulation results using PSCAD/EMTDC software and analytical results based on eigenvalue analysis are presented to evaluate effects of different parameters on stability of RPFC."," Copyright (c) 2014 John Wiley & Sons, Ltd.","Khayami, MT|Shayanfar, H|Kazemi, A",INTERNATIONAL JOURNAL OF NUMERICAL MODELLING-ELECTRONIC NETWORKS DEVICES AND FIELDS,dynamic stability eigenvalue analysis rotary power flow controller (rpfc) stability analysis small-signal stability,10.1002/jnm.2025
197,WOS:000333446100014,2014,Study of MEPDG Sensitivity Using Nonparametric Regression Procedures,,"Because the new Mechanistic-Empirical Pavement Design Guide (MEPDG) includes numerous inputs, a sensitivity analysis using the Monte Carlo approach is not practical because it requires thousands of MEPDG runs. Instead, nonparametric regression procedures can be very useful to perform MEPDG sensitivity analysis. In this study, nonparametric regression procedures such as multivariate adaptive regression splines and gradient boosting machine are employed to identify inputs that contribute significantly to the outputs. Thirty inputs are used to randomly generate  input combinations by using Latin hypercube sampling. Using four-layer pavement geometry [two asphalt concrete (AC), base, and subgrade layers], simulations are run in MEPDG software to produce a time series of predicted distresses such as roughness, rutting, and cracking. Sensitivity analysis resulted in three groups of inputs to which the output is ()highly sensitive, ()moderately sensitive, and ()minimally sensitive. Results show that roughness is highly sensitive to traffic input variables such as annual average daily truck traffic (AADTT), percentage of trucks in the design lane, and thickness of bottom AC layer. AC rutting is highly affected by AADTT, percentage of trucks in design direction, and tire pressure. Three major factors for total rutting, longitudinal cracking, and alligator cracking are AADTT, percentage of trucks in design direction, and thickness of bottom AC layer. In addition to these, alligator cracking is highly sensitive to percentage of air voids in the bottom AC layer. Transverse cracking is highly sensitive to the percentage of trucks in the design lane, percentage of Class  vehicles, plastic limit, thickness of base layer, effective binder content of top AC layer, and climate. Among all of the inputs, the thickness of the AC layer is highly interactive with other input variables.",,"Tarefder, RA|Sumee, N|Storlie, C",JOURNAL OF COMPUTING IN CIVIL ENGINEERING,sensitivity analysis statistics sampling pavements mechanistic-empirical pavement design guide sensitivity analysis advanced statistical analysis latin hypercube sampling nonparametric regression confidence intervals,10.1061/(ASCE)CP.1943-5487.0000239
198,WOS:000331916100026,2014,Life cycle assessment of corn-based ethanol production in Argentina,TILLAGE SYSTEMS BIOENERGY PRODUCTION UNITED-STATES LAND-USE EMISSIONS BIOFUELS IMPACTS PAMPAS ENERGY DYNAMICS,"The promotion of biofuels as energy for transportation in the world is mainly driven by the perspective of oil depletion, the concerns about energy security and global warming. In Argentina, the legislation has imposed the use of biofuels in blend with fossil fuels ( to %) in the transport sector. The aim of this paper is to assess the environmental impact of corn-based ethanol production in the province of Santa Fe in Argentina based on the life cycle assessment methodology. The studied system includes from raw materials production to anhydrous ethanol production using dry milling technology. The system is divided into two subsystems: agricultural system and refinery system. The treatment of stillage is considered as well as the use of co-products (distiller's dried grains with solubles), but the use and/or application of the produced biofuel is not analyzed: a cradle-to-gate analysis is presented. As functional unit,  MJ of anhydrous ethanol at biorefinery is chosen. Two life cycle impact assessment methods are selected to perform the study: Eco-indicator  and ReCiPe. SimaPro is the life cycle assessment software used. The influence of the perspectives on the model is analyzed by sensitivity analysis for both methods. The two selected methods identify the same relevant processes. The use of fertilizers and resources, seeds production, harvesting process, corn drying, and phosphorus fertilizers and acetamide-anillide-compounds production are the most relevant processes in agricultural system. For refinery system, corn production, supplied heat and burned natural gas result in the higher contributions. The use of distiller's dried grains with solubles has an important positive environmental impact.", (C) 2013 Elsevier B.V. All rights reserved.,"Pieragostini, C|Aguirre, P|Mussati, MC",SCIENCE OF THE TOTAL ENVIRONMENT,life cycle assessment corn-based ethanol eco-indicator 99 recipe sensitivity analysis perspectives analysis,10.1016/j.scitotenv.2013.11.012
199,WOS:000404559900023,2017,Simulating the Fate and Transport of Coal Seam Gas Chemicals in Variably-Saturated Soils Using HYDRUS,HYDRAULIC CONDUCTIVITY VADOSE ZONE REACTIVE TRANSPORT TRANSFORMATION PRODUCTS NITROSAMINE FORMATION CONSTRUCTED WETLANDS SENSITIVITY ANALYSIS AGRICULTURAL SOILS SOLUTE TRANSPORT FIELD CONDITIONS,"The HYDRUS-D and HYDRUS (D/D) computer software packages are widely used finite element models for simulating the one-, and two- or three-dimensional movement of water, heat, and multiple solutes in variably-saturated media, respectively. While the standard HYDRUS models consider only the fate and transport of individual solutes or solutes subject to first-order degradation reactions, several specialized HYDRUS add-on modules can simulate far more complex biogeochemical processes. The objective of this paper is to provide a brief overview of the HYDRUS models and their add-on modules, and to demonstrate possible applications of the software to the subsurface fate and transport of chemicals involved in coal seam gas extraction and water management operations. One application uses the standard HYDRUS model to evaluate the natural soil attenuation potential of hydraulic fracturing chemicals and their transformation products in case of an accidental release. By coupling the processes of retardation, first-order degradation and convective-dispersive transport of the biocide bronopol and its degradation products, we demonstrated how natural attenuation reduces initial concentrations by more than a factor of hundred in the top  cm of the soil. A second application uses the UnsatChem module to explore the possible use of coal seam gas produced water for sustainable irrigation. Simulations with different irrigation waters (untreated, amended with surface water, and reverse osmosis treated) provided detailed results regarding chemical indicators of soil and plant health, notably SAR, EC and sodium concentrations. A third application uses the HP module to analyze trace metal transport involving cation exchange and surface complexation sorption reactions in a soil leached with coal seam gas produced water following some accidental water release scenario. Results show that the main process responsible for trace metal migration in soil is complexation of naturally present trace metals with inorganic ligands such as (bi)carbonate that enter the soil upon infiltration with alkaline produced water. The examples were selected to show how users can tailor the required model complexity to specific needs, such as for rapid screening or risk assessments of various chemicals nder generic soil conditions, or for more detailed site-specific analyses of actual subsurface pollution problems.",,"Mallants, D|Simunek, J|van Genuchten, MT|Jacques, D",WATER,hydrus coal seam gas contaminant transport trace elements hp1,10.3390/w9060385
200,WOS:000325733400001,2013,APPLICATION OF SENSITIVITY ANALYSIS FOR ASSESSMENT OF ENERGY AND ENVIRONMENTAL ALTERNATIVES IN THE MANUFACTURE BY USING ANALYTIC HIERARCHY PROCESS,CLEANER PRODUCTION PAPER-MILL INDUSTRY IRAN,"Multi-criteria decision making (MCDM) was used to make comparative analysis of projects or heterogeneous measures for prioritization criteria and subcriteria simultaneously in a complex situation. The aim of this paper is to determine the alternatives and the sensitivity of main factors affecting water and energy consumption as well as environmental impact in a recycled paper manufacturing by using analytic hierarchy process (AHP). The AHP enables one to consider all the elements of decision process in a model, and to compare criteria and subcriteria of the model to find the best alternative. The AHP technique is applied through specific software package with user-friendly interfaces called Expert Choice. The results indicated that reduction of water consumption is the most important alternative for sustainable development in a recycled paper mill in Iran. Also, good housekeeping is the most sensitive criterion affecting the alternatives. The paper illustrates how the AHP method can help industrial management to overcome the energy usage and environmental impact in the manufacture.",,"Ghorbannezhad, P|Azizi, M|Ray, C|Yoo, C|Ramazani, O",ENVIRONMENT PROTECTION ENGINEERING,,10.5277/epe130301
201,WOS:000174870000001,2002,Optimization of biofiltration for odor control: Model development and parameter sensitivity,HYDROGEN-SULFIDE BIOFILTERS VALIDATION REMOVAL DESIGN BEHAVIOR BIOMASS,"A dynamic model that describes the mass transport and attenuation of odor-causing air emissions (i.e., hydrogen sulfide and other reduced sulfur compounds) in a biofiltration unit was developed and incorporated into a software package called Biofilter(TM). Mechanisms included advective flow, mass transfer from the bulk phase to the biofilm, biofilm internal diffusion, and biological reaction in the biofilm. A dimensionless analysis revealed that the mass transport and attenuation of target compounds can be characterized by several dimensionless groups. Model equations were converted to ordinary differential equations using orthogonal collocation and the resulting ordinary differential equations were solved using the DGEAR algorithm. Numerical solutions were verified by comparing model simulations to analytical solutions. The model simulations showed that the existence of a water layer surrounding the biofilm in a biofiltration unit lowers the removal efficiency of hydrogen sulfide. A sensitivity analysis of model parameters (including the film transfer coefficient, biofilm diffusivity, biofilm thickness, maximum specific biomass growth rate, yield coefficient, half-saturation coefficient, and initial active biomass concentration) using data from two biofilters located at the Cedar Rapids (Iowa) Water Pollution Control Facilities, showed that biofilm. internal diffusion and biofilm. kinetics have a significant effect on hydrogen sulfide removal, while external mass transfer has little effect. Water Environ. Res., ,  ().",,"Li, HB|Crittenden, JC|Mihelcic, JR|Hautakangas, H",WATER ENVIRONMENT RESEARCH,biofiltration biofilter biotrickling filter modeling odors hydrogen sulfide volatile organic compounds wastewater,10.2175/106143002X139703
202,WOS:000355262900012,2015,A meta-heuristic solution for automated refutation of complex software systems specified through graph transformations,ANT COLONY OPTIMIZATION FLY MODEL CHECKING SEARCH,"One of the best approaches for verifying software systems (especially safety critical systems) is the model checking in which all reachable states are generated from an initial state. All of these states are searched for errors or desirable patterns. However, the drawback for many real and complex systems is the state space explosion in which model checking cannot generate all the possible states. In this situation, designers can use refutation to check refusing a property rather than proving it. In refutation, it is very important to handle the state space for finding errors efficiently. In this paper, we propose an efficient solution to implement refutation in complex systems modeled by graph transformation. Since meta-heuristic algorithms are efficient solutions for searching in the problems with very large state spaces, we use them to find errors (e.g., deadlocks) in systems which cannot be verified through existing model checking approaches due to the state space explosion. To do so, we employ a Particle Swarm Optimization (PSO) algorithm to consider only a subset of states (called population) in each step of the algorithm. To increase the accuracy, we propose a hybrid algorithm using PSO and Gravitational Search Algorithm (GSA). The proposed approach is implemented in GROOVE, a toolset for designing and model checking graph transformation systems. The experiments show improved results in terms of accuracy, speed and memory usage in comparison with other existing approaches.", (C) 2015 Elsevier B.V. All rights reserved.,"Rafe, V|Moradi, M|Yousefian, R|Nikanjam, A",APPLIED SOFT COMPUTING,model checking refutation pso gsa graph transformation system state space explosion,10.1016/j.asoc.2015.04.032
203,WOS:000318057900003,2013,A model-independent Particle Swarm Optimisation software for model calibration,RAINFALL-RUNOFF MODELS SHUFFLED COMPLEX EVOLUTION PAMPA DEL TAMARUGAL GLOBAL OPTIMIZATION SENSITIVITY-ANALYSIS DIFFERENTIAL EVOLUTION UNCERTAINTY ASSESSMENT REGIONAL AQUIFER PARAMETERS ALGORITHM,"This work presents and illustrates the application of hydroPSO, a novel multi-OS and model-independent R package used for model calibration. hydroPSO allows the modeller to perform a standard modelling work flow including, sensitivity analysis, parameter calibration, and assessment of the calibration results, using a single piece of software. hydroPSO implements several state-of-the-art enhancements and fine-tuning options to the Particle Swarm Optimisation (PSO) algorithm to meet specific user needs. hydroPSO easily interfaces the calibration engine to different model codes through simple ASCII files and/or R wrapper functions for exchanging information on the calibration parameters. Then, optimises a user-defined goodness-of-fit measure until a maximum number of iterations or a convergence criterion are met. Finally, advanced plotting functionalities facilitate the interpretation and assessment of the calibration results. The current hydroPSO version allows easy parallelization and works with single-objective functions, with multi-objective functionalities being the subject of ongoing development. We compare hydroPSO against standard algorithms (SCE_UA, DE, DREAM, SPSO-, and GML) using a series of benchmark functions. We further illustrate the application of hydroPSO in two real-world case studies: we calibrate, first, a hydrological model for the Ega River Basin (Spain) and, second, a groundwater flow model for the Pampa del Tamarugal Aquifer (Chile). Results from the comparison exercise indicate that hydroPSO is: i) effective and efficient compared to commonly used optimisation algorithms, ii) ""scalable"", i.e. maintains a high performance for increased problem dimensionality, and iii) versatile to adapt to different response surfaces of the objective function. Case study results highlight the functionality and ease of use of hydroPSO to handle several issues that are commonly faced by the modelling community such as: working on different operating systems, single or batch model execution, transient- or steady-state modelling conditions, and the use of alternative goodness-of-fit measures to drive parameter optimisation. Although we limit the application of hydroPSO to hydrological models, flexibility of the package suggests it can be implemented in a wider range of models requiring some form of parameter optimisation. (C) ", Elsevier Ltd. All rights reserved.,"Zambrano-Bigiarini, M|Rojas, R",ENVIRONMENTAL MODELLING & SOFTWARE,global optimisation evolutionary algorithm surface water modelling groundwater modelling swat-2005 modflow-2005 r,10.1016/j.envsoft.2013.01.004
204,WOS:000306076900004,2012,A GENERAL TECHNIQUE FOR COUPLING TWO ARBITRARY METHODS IN STRESS ANALYSIS,ELEMENT-FREE GALERKIN FINITE-ELEMENT BOUNDARY BEM FEM,"In this paper, a general technique for coupling two arbitrary methods is presented. The problem domain is decomposed into two sub-domains. Afterwards, a sensitivity analysis at the interface of each sub-domain is carried out. Sensitivity matrices of the two sub-domains are used to find the coupling matrix equation. Unknowns at the interface are then found by solving the equations. The size of the matrix equation is very small in comparison with coefficient matrices of each sub-domain. The present method allows the black box coupling of different methods, even commercial software without having access to the matrices created by the methods.",,"Hematiyan, MR|Khosravifard, A|Mohammadi, M",INTERNATIONAL JOURNAL OF COMPUTATIONAL METHODS,fem bem mesh-free methods radial point interpolation method coupling,10.1142/S0219876212400270
205,WOS:000269966500005,2009,Thermodynamics of Autothermal Wood Gasification,BIOMASS PERFORMANCE CHAR,"This work extensively studies the thermodynamics of air blown autothermal wood gasification at adiabatic conditions. To this end, the software package HSC Chemistry (R) was used to determine the composition of the synthesis gas at thermodynamic equilibrium. This software operates on the basis of Gibb's energy minimization. In the model, dry and ash-free wood has been represented by CH.O-(.). Dry air has been modeled as a mixture of oxygen, nitrogen and argon. As the calculations were carried out with respect to adiabatic conditions, it was necessary to determine the adiabatic flame temperature via beat and mass balances. This approach of adopting adiabatic conditions, though generally not taken into consideration in thermodynamic studies, is seen as beneficial, providing additional information with regard to the gasifier operating point. A sensitivity analysis was conducted. The influence of parameters; like equivalence ratio, water content of wood fuel and air preheating on adiabatic flame temperature and cold gas efficiency is discussed. For air at ambient temperature, the highest cold gas efficiency is achieved with an equivalence ratio of about .. This was dependent to some small degree on the water content of the fuel wood. With increased air preheating, the maximum cold gas efficiency is increased and shifted to lower equivalence ratios. For wet wood, it transpired to be more efficient to use the sensible beat from the synthesis gas for drying of the fuel rather than preheating of the air. Finally, calculated values are compared to measurements from a circulating fluidised bed gasifier."," (C) 2009 American institute of Chemical Engineers Environ Prog, 28: 347-354, 2009","Mevissen, N|Schulzke, T|Unger, CA|Mac an Bhaird, S",ENVIRONMENTAL PROGRESS & SUSTAINABLE ENERGY,thermodynamic equilibrium simulation solid carbon boundary cold gas efficiency,10.1002/ep.10393
206,WOS:000245766800002,2007,Methods and object-oriented software for FE reliability and sensitivity analysis with application to a bridge structure,OPTIMIZATION,"This paper addresses the growing demand for finite-element software with capabilities to incorporate uncertainty in the input parameters. Reliability and response sensitivity algorithms are implemented in the general-purpose finite-element software OpenSees, which employs an object-oriented programming approach to achieve a sustainable software with focus on maintainability and extensibility. The product is a comprehensive and freely available library of software tools for finite-element reliability and response sensitivity analysis. A numerical example involving a detailed model of a highway bridge with inelastic material behavior and  random variables is presented to demonstrate features of the methodology and the software. Importance vectors are employed to rank the input parameters according to their relative influence on the structural reliability. The required response sensitivities are obtained by an extensive implementation of the direct differentiation method.",,"Haukaas, T|Kiureghian, AD",JOURNAL OF COMPUTING IN CIVIL ENGINEERING,,10.1061/(ASCE)0887-3801(2007)21:3(151)
207,WOS:000310476400006,2012,"Parameterization of a numerical 2-D debris flow model with entrainment: a case study of the Faucon catchment, Southern French Alps",CLAY-SHALE BASIN BARCELONNETTE BASIN SIMULATION-MODELS CHECK DAMS RUNOUT LANDSLIDES AVALANCHES EVENTS FRANCE PREDICTION,"The occurrence of debris flows has been recorded for more than a century in the European Alps, accounting for the risk to settlements and other human infrastructure that have led to death, building damage and traffic disruptions. One of the difficulties in the quantitative hazard assessment of debris flows is estimating the run-out behavior, which includes the run-out distance and the related hazard intensities like the height and velocity of a debris flow. In addition, as observed in the French Alps, the process of entrainment of material during the run-out can be - times in volume with respect to the initially mobilized mass triggered at the source area. The entrainment process is evidently an important factor that can further determine the magnitude and intensity of debris flows. Research on numerical modeling of debris flow entrainment is still ongoing and involves some difficulties. This is partly due to our lack of knowledge of the actual process of the uptake and incorporation of material and due the effect of entrainment on the final behavior of a debris flow. Therefore, it is important to model the effects of this key erosional process on the formation of run-outs and related intensities. In this study we analyzed a debris flow with high entrainment rates that occurred in  at the Faucon catchment in the Barcelonnette Basin (Southern French Alps). The historic event was back-analyzed using the Voellmy rheology and an entrainment model imbedded in the RAMMS -D numerical modeling software. A sensitivity analysis of the rheological and entrainment parameters was carried out and the effects of modeling with entrainment on the debris flow run-out, height and velocity were assessed.",,"Hussin, HY|Luna, BQ|van Westen, CJ|Christen, M|Malet, JP|van Asch, TWJ",NATURAL HAZARDS AND EARTH SYSTEM SCIENCES,,10.5194/nhess-12-3075-2012
208,WOS:000178643700005,2002,A computer program for a Monte Carlo analysis of sensitivity in equations of environmental modelling obtained from experimental data,SYSTEMS,"In the construction of mathematical models from experimental data, it is possible to determine equations that model relations using several methodologies [Un nuevo algoritmo para la modelizacion de sistemas altamente estructurados (); Env. Model. Software  () ; Guide to Statistics , (); Regression models (); Ecosystems and Sustainable Development II ()]. These methodologies build equations that are in line with the experimental data and they analyse a dimension of adjustment and a dimension of error or distance between the experimental data and the data that is produced by the model. There are studies of sensitivity of the sample of data, as found by Bolado and Alonso [Proceedings SAMO ]. The authors consider that it is useful to obtain new parameters that relate the sensitivity of the equations to the variations that are produced by the experimental data. This will allow the selection of the model according to new criteria. On the one hand, the authors present a theoretical study of sensitivity of the models according to different points of view. On the other hand, they discuss a computing algorithm that allows the analysis of sensitivity (and stability) of the mathematical equations, which are built from any methodology. An interface has been incorporated into this algorithm to allow a graphic visualisation of the effects that are produced when modifications of the model are carried out.", (C) 2002 Elsevier Science Ltd. All rights reserved.,"Verdu, F|Villacampa, Y",ADVANCES IN ENGINEERING SOFTWARE,sensitivity analysis environmental modelling monte carlo,10.1016/S0965-9978(02)00023-6
209,WOS:000312884600010,2013,"Development of a Long-term, Ecologically Oriented Dam Release Plan for the Lake Baiyangdian Sub-basin, Northern China",RESERVOIR OPERATION RIVER-BASIN ALLOCATION,"Using China's Lake Baiyangdian sub-basin for a case study, we developed an ecologically oriented dam release plan that can be used to define an optimal dam operation scheme that provides both the environmental flows required by bodies of water and wetlands downstream from the Xidayang Reservoir dam and enough water for agricultural, and industrial water users. In addition, we evaluated the benefits that might be provided by modifying releases of water from the reservoir. To attain ecological sustainability in the sub-basin, we used the supply for each water user as a decision variable based on three objectives: () to achieve sustainable socioeconomic development; () to keep the water volume as close as possible to the ideal environmental flows in the urban rivers of Baoding City; and () to keep the water amount as close as possible to Lake Baiyangdian's ideal environmental water requirements. We used the ideal-point method to provide dimensionless values for the first objective, and then used a weighting method to integrate the three objectives into a single holistic goal. We then used the GAMS/CONOPT software to solve the nonlinear model and predict the optimal results. We discuss the optimal water allocation and ecologically oriented dam release plans for the three scenarios. To determine the limitations of the method, we performed a sensitivity analysis, and discuss the optimal results for different weightings of objectives provided by decision-makers. The results of the optimization analysis provide a set of effective compromises among the target objectives that can guide future management of water releases from the reservoir.",,"Yang, W|Yang, ZF",WATER RESOURCES MANAGEMENT,ecologically oriented dam release plans multi-objective optimization environmental water requirements lake baiyangdian,10.1007/s11269-012-0198-7
210,WOS:000385907200039,2016,MINFIT: A Spreadsheet-Based Tool for Parameter Estimation in an Equilibrium Speciation Software Program,SURFACE COMPLEXATION MODELS URANIUM(VI) ADSORPTION CONSISTENT MODEL TITRATION DATA LAYER MODEL SORPTION OXIDE MONTMORILLONITE PHOSPHATE HEMATITE,"Determination of equilibrium constants describing chemical reactions in the aqueous phase and at solid-water interface relies on inverse modeling and parameter estimation. Although there are existing tools available, the steep learning curve prevents the wider community of environmental engineers and chemists to adopt those tools. Stemming from classical chemical equilibrium codes, MINEQL+ has been one of the most widely used chemical equilibrium software programs. We developed a spreadsheet-based tool, which we are calling MINFIT, that interacts with MINEQL+ to perform parameter estimations that optimize model fits to experimental data sets. MINFIT enables automatic and convenient screening of a large number of parameter sets toward the optimal solutions by calling MINEQL+ to perform iterative forward calculations following either exhaustive equidistant grid search or randomized search algorithms. The combined use of the two algorithms can securely guide the searches for the global optima. We developed interactive interfaces so that the optimization processes are transparent. Benchmark examples including both aqueous and surface complexation problems illustrate the parameter estimation and associated sensitivity analysis. MINFIT is accessible at http://minfit.strikingly.com.",,"Xie, XF|Giammar, DE|Wang, ZM",ENVIRONMENTAL SCIENCE & TECHNOLOGY,,10.1021/acs.est.6b03399
211,WOS:000297595300005,2011,Shape optimisation of preform design for precision close-die forging,FINITE-ELEMENT METHOD TOPOLOGY OPTIMIZATION STRUCTURAL OPTIMIZATION EVOLUTIONARY PROCEDURE SENSITIVITY-ANALYSIS HOMOGENIZATION SIMULATION ALGORITHM BLADE,"Preform design is an essential stage in forging especially for parts with complex shapes. In this paper, based on the evolutionary structural optimisation (ESO) concept, a topological optimisation method is developed for preform design. In this method, a new criterion for element elimination and addition on the workpiece boundary surfaces is proposed to optimise material distribution. To improve the quality of the boundary after element elimination, a boundary smoothing technique is developed using B-spline curve approximation. The developed methods are programmed using C# code and integrated with DEFORM D software package. Two D case problems including forging of an aerofoil shape and forging of rail wheel are evaluated using the developed method. The results suggest that the developed topology optimisation method is an efficient approach for preform design optimisation.",,"Lu, B|Ou, HA|Cui, ZS",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,preform design topology optimisation forging,10.1007/s00158-011-0668-1
212,WOS:000242178600020,2006,Colloid-facilitated solute transport in variably saturated porous media: Numerical model and experimental verification,ALLUVIAL GRAVEL AQUIFER DEEP-BED FILTRATION CONTAMINANT TRANSPORT PARTICLE-TRANSPORT LABORATORY COLUMN ANION EXCLUSION IONIC-STRENGTH SAND COLUMNS WATER SOILS,"Strongly sorbing chemicals (e. g., heavy metals, radionuclides, pharmaceuticals, and explosives) in porous media are associated predominantly with the solid phase, which is commonly assumed to be stationary. However, recent field- and laboratory-scale observations have shown that in the presence of mobile colloidal particles (e.g., microbes, humic substances, clays, and metal oxides), colloids can act as pollutant carriers and thus provide a rapid transport pathway for strongly sorbing contaminants. To address this problem, we developed a one-dimensional numerical model based on the HYDRUS-D software package that incorporates mechanisms associated with colloid and colloid-facilitated solute transport in variably saturated porous media. The model accounts for transient variably saturated water flow, and for both colloid and solute movement due to advection, diffusion, and dispersion, as well as for solute movement facilitated by colloid transport. The colloid transport module additionally considers the processes of attachment/detachment to/from the solid phase and/or the air-water interface, straining, and/or size exclusion. Various blocking and depth dependent functions can be used to modify the attachment and straining coefficients. The solute transport module uses the concept of two-site sorption to describe nonequilibrium adsorption-desorption reactions to the solid phase. The module further assumes that contaminants can be sorbed onto surfaces of both deposited and mobile colloids, fully accounting for the dynamics of colloid movement between different phases. Application of the model is demonstrated using selected experimental data from published saturated column experiments, conducted to investigate the transport of Cd in the presence of Bacillus subtilis spores in alluvial gravel aquifer media. Numerical results simulating bacteria transport, as well as the bacteria-facilitated Cd transport, are compared with experimental results. A sensitivity analysis of the model to various parameters is also presented.",,"Simunek, J|He, CM|Pang, LP|Bradford, SA",VADOSE ZONE JOURNAL,,10.2136/vzj2005.0151
213,WOS:000406135200014,2017,Evaluating the SWMM LID Editor rain barrel option for the estimation of retention potential of rainwater harvesting systems,WATER RUNOFF,"The low-impact development (LID) Editor rain barrel option of release . of the EPA storm water management model (SWMM) software does not allow the consideration of demand-driven behaviour of domestic rain water harvesting (RWH) systems to evaluate their runoff retention potential. This paper compares the results of the LID Editor with those obtained by a detailed demand-driven tank model scheme - used as a benchmark - and developed using basic functions of SWMM. The comparison showed the LID Editor-based model to generally overestimate the benchmark model in the evaluation of both volumetric and peak retention efficiency. The high variability of the results of the comparison suggests the use of the LID Editor rain barrel option for long-term simulation but not for single event analysis. A sensitivity analysis revealed that the overestimation provided by the rain barrel option is significant for tanks smaller than m(), tank sizes of major diffusion for domestic RWH.",,"Campisano, A|Catania, FV|Modica, C",URBAN WATER JOURNAL,low-impact development rain water harvesting swmm lid editor retention efficiency,10.1080/1573062X.2016.1254259
214,WOS:000239466700009,2006,The comparison of four dynamic systems-based software packages: Translation and sensitivity analysis,WETNESS,"Dynamic model development for describing complex ecological systems continues to grow in popularity. For both academic research and project management, understanding the benefits and limitations of systems-based software could improve the accuracy of results and enlarge the user audience. A Surface Wetness Energy Balance (SWEB) model for canopy surface wetness has been translated into four software packages and their strengths and weaknesses were compared based on 'novice' user interpretations. We found expression-based models such as Simulink and GoldSim with Expressions were able to model the SWEB more accurately; however, stock and flow-based models such as STELLA, Madonna, and GoldSim with Flows provided the user a better conceptual understanding of the ecologic system. Although the original objective of this study was to identify an 'appropriate' software package for predicting canopy surface wetness using SWEB, our outcomes suggest that many factors must be considered by the stakeholders when selecting a model because the modeling software becomes part of the model and of the calibration process. These constraints may include user demographics, budget limitations, built-in sensitivity and optimization tools, and the preference of user friendliness vs. computational power. Furthermore, the multitude of closed proprietary software may present a disservice to the modeling community, creating model artifacts that originate somewhere deep inside the undocumented features of the software, and masking the underlying properties of the model. (c) ", Elsevier Ltd. All rights reserved.,"Rizzo, DM|Mouser, PJ|Whitney, DH|Mark, CD|Magarey, RD|Voinov, AA",ENVIRONMENTAL MODELLING & SOFTWARE,model comparison dynamic simulation system-based models canopy surface energy balance,10.1016/j.envsoft.2005.07.009
215,WOS:000411869000009,2017,A Scenario Based Impact Assessment of Trace Metals on Ecosystem of River Ganges Using Multivariate Analysis Coupled with Fuzzy Decision-Making Approach,WATER-QUALITY MANAGEMENT HEAVY-METALS INDIA BASIN FISH,"The growing consciousness about the health risks associated with environmental pollutants has brought a major shift in global concern towards prevention of hazardous/trace metals discharge in water bodies. Majority of these trace metals gets accumulated in the body of aquatic lives, which are considered as potential indicators of hazardous content. This results in an ecological imbalance in the form of poisoning, diseases and even death of fish and other aquatic lives, and ultimately affect humans through food chain. Trace metals such as Cd, Cr, Cu, Mn, Ni, Pb and Zn originated from various industrial operations containing metallic solutions and agricultural practices, have been contributing significantly to cause aquatic pollution. The present study develops a novel approach of expressing sustainability of river's ecosystem based on health of the fish by coupling fuzzy sensitivity analysis into multivariate analysis. A systematic methodology has been developed by generating monoplot, two dimensional biplot and rotated component matrix (using 'Analyze it' and 'SPSS' software), which can simultaneously identify critical trace metals and their industrial sources, critical sampling stations, and adversely affected fish species along with their interrelationships. A case study of assessing the impact of trace metals on the aquatic life of river Ganges, India has also been presented to demonstrate effectiveness of the model. The clusters pertaining to various water quality parameters have been identified using Principal Component Analysis (PCA) to determine actual sources of pollutants and their impact on aquatic life. The fuzzy sensitivity analysis reveals the cause-effect relationship of these critical parameters. The study suggests pollution control agencies to enforce appropriate regulations on the wastewater dischargers responsible for polluting river streams with a particular kind of trace metal(s).",,"Srinivas, R|Singh, AP|Sharma, R",WATER RESOURCES MANAGEMENT,aquatic ecosystem fuzzy decision-making impact assessment multivariate analysis river ecosystem water quality,10.1007/s11269-017-1738-y
216,WOS:000273938000054,2009,"Optimal testing resource allocation during module testing considering cost, testing effort and reliability",SOFTWARE-RELIABILITY ERROR-DETECTION MODELS PROGRAMS,"Software reliability is one of the most important quality attributes of commercial software. During software testing, software reliability growth models (SRGMs) are commonly used to describe the phenomenon of failure occurrence and/or fault removal which consequently enhancements software reliability. Large software systems are developed by integrating a number of relatively small and independent modules, which are tested independently during module testing phase. The amount of testing resource available is limited which is desired to be consumed judiciously so as to optimize the testing process. In this paper we formulate a resource allocation problem of minimizing the cost of software testing under available amount of testing resource, given a reliability constraint. We use a flexible SRGM considering testing effort which, depending upon the values of parameters, can describe either exponential or S-shaped failure pattern of software modules. A systematic and sequential Algorithm is proposed to solve the optimization problem formulated. Numerical examples are given to illustrate the formulation and solution procedures. Sensitivity analysis is performed to examine the behavior of some parameters of SRGM with most significant influence. (C) ", Elsevier Ltd. All rights reserved.,"Jha, PC|Gupta, D|Yang, B|Kapur, PK",COMPUTERS & INDUSTRIAL ENGINEERING,software reliability growth models modular software testing resource optimal allocation sensitivity analysis,10.1016/j.cie.2009.05.001
217,WOS:000311245300006,2012,Combining explanatory crop models with geospatial data for regional analyses of crop yield using field-scale modeling units,CLIMATE-CHANGE SCENARIOS SIMULATION-MODELS PRECISION AGRICULTURE SPATIAL-RESOLUTION COMPUTER-PROGRAM DECISION-SUPPORT RICE PRODUCTION WINTER-WHEAT GIS IMPACTS,"Crop models are used to predict yield and resource requirements as well as to evaluate different climate or management scenarios at a specific site. However, problems involving land use or global climate change encompass larger, more diverse, spatial scales and would benefit from simulating over broader areas using high-resolution, spatially-distributed data. A geospatial interface was developed to combine the explanatory potato crop model SPUDSIM with the geographic information system (GIS) software ArcGIS using the scripting language Python. Multiple geospatial input data layers were incorporated, including weather, soil, management, and land use. Modeling units (MUs) were defined as homogeneous field-scale areas created by the intersection of the input layers. Crop production was simulated for each unique combination of climate, soil, and management for MUs classified as cropland. The outputs (crop yield, water use, and nitrogen uptake) were mapped to show the spatial distribution within each county and aggregated to the county-level over the region of interest. An example was provided for potato production in Maine and illustrates how potential crop yield varies spatially over the state. The geospatial crop model showed evidence of both spatial and temporal variability of crop yield at the county level. The interface was designed to be flexible and easy to apply to applications such as evaluating crop production capacity and response under different scenarios.", Published by Elsevier B.V.,"Resop, JP|Fleisher, DH|Wang, QG|Timlin, DJ|Reddy, VR",COMPUTERS AND ELECTRONICS IN AGRICULTURE,crop modeling geographic information systems spatial aggregation regional analysis sensitivity analysis land use change,10.1016/j.compag.2012.08.001
218,WOS:000277498900014,2010,Sediment and pollutant load modelling using an integrated urban drainage modelling toolbox: an application of City Drain,COMBINED SEWER SYSTEMS UNCERTAINTY ANALYSIS COPPER LOADS DATA SETS CALIBRATION RUNOFF EROSION PREDICTION TRANSPORT SURFACE,"Numerical and computational modelling of flow and pollutant dynamics in urban drainage systems is becoming more and more integral to planning and design. The main aim of integrated flow and pollutant models is to quantify the efficiency of different measures at reducing the amount of pollutants discharged into receiving water bodies and minimise the consequent negative water quality impact. The open source toolbox CITY DRAIN developed in the Matlab/Simulink environment, which was designed for integrated modelling of urban drainage systems, is used in this work. The goal in this study was to implement and test computational routines for representing sediment and pollutant loads in order to evaluate catchment surface pollution. Tested models estimate the accumulation, erosion and transport of pollutants-aggregately-on urban surfaces and in sewers. The toolbox now includes mathematical formulations for accumulation of pollutants during dry weather period and their wash-off during rainfall events. The experimental data acquired in a previous research project carried out by the Environmental Engineering Research Centre (CIIA) at the Universidad de los Andes in Bogota (Colombia) was used for the calibration of the models. Different numerical approaches were tested for their ability to calibrate to the sediment transport conditions. Initial results indicate, when there is more than one peak during the rainfall event duration, wash-off processes probably can be better represented using a model based on the flow instead of the rainfall intensity. Additionally, it was observed that using more detailed models (compared with an instantaneous approach) for representing pollutant accumulation do not necessarily lead to better results.",,"Rodriguez, JP|Achleitner, S|Moderl, M|Rauch, W|Maksimovic, C|McIntyre, N|Diaz-Granados, MA|Rodriguez, MS",WATER SCIENCE AND TECHNOLOGY,bogota city build-up and wash-off processes calibration and uncertainty analysis city drain toolbox sediment and pollutant load modelling,10.2166/wst.2010.139
219,WOS:000281772900010,2010,INTEGRATION OF UNCERTAINTIES INTO INTERNAL CONTAMINATION MONITORING,IDEAS GUIDELINES DOSIMETRY,"Potential internal contaminations of workers are monitored by periodic bioassays interpreted in terms of intake and committed effective dose through biokinetic and dosimetric models. After a prospective evaluation of exposure at a workplace, a suitable monitoring program can be defined by the choice of measurement techniques and frequency of measurements. However, the actual conditions of exposure are usually not well defined and the measurements are subject to errors. In this study we took into consideration the uncertainties associated with a routine monitoring program in order to evaluate the minimum intake and dose detectable for a given level of confidence. Major sources of uncertainty are the contamination time, the size distribution and absorption into blood of the incorporated particles, and the measurement errors. Different assumptions may be applied to model uncertain knowledge, which lead to different statistical approaches. The available information is modeled here by classical or Bayesian probability distributions. These techniques are implemented in the OPSCI software under development. This methodology was applied to the monitoring program of workers in charge of plutonium purification at the AREVA NC reprocessing facility (La Hague, France). A sensitivity analysis was carried out to determine the important parameters for the minimum detectable dose. The methods presented here may be used for assessment of any other routine monitoring program through the comparison of the minimum detectable dose for a given confidence level with dose constraints. Health Phys. ():-; ",,"Davesne, E|Casanova, P|Chojnacki, E|Paquet, F|Blanchardon, E",HEALTH PHYSICS,computer calculations contamination internal effective dose plutonium,10.1097/HP.0b013e3181cd3d47
220,WOS:000322557200002,2013,Comparison of sediment transport computations using hydrodynamic versus hydrologic models in the Simiyu River in Tanzania,BED-LOAD TRANSPORT SENSITIVITY-ANALYSIS PARAMETERS SWAT TOOL,"This paper presents the results of a study that compares the sediment routing of the Simiyu River using the hydrologic model, Soil and Water Assessment Tool (SWAT) and the D hydrodynamic simulation software for Rivers and Estuaries (SOBEK-RE) model. Routing in SWAT is completed using the simplified Bagnold's equation and in the SOBEK-RE model is undertaken using the Saint Venant equation. The upstream boundary conditions for the routing modules were derived from the subcatchments sediment yields that were estimated by SWAT using the Modified Universal Soil Loss Equation (MUSLE). The sediment loads extrapolated or interpolated from the sediment rating curve for the catchment outlet were used for calibration and validation purposes. The SWAT model predicted an erosion rate of . Mt/yr. The total sediment load transported to the main outlet of the catchment simulated by the SWAT and SOBEK-RE models was equal to . and . Mt/yr, respectively. Thus the models computed a net erosion in the channels of . Mt/yr (SWAT) and . Mt/yr (SOBEK-RE). When comparing the results of the models for the different reaches of the main channel and main tributaries, the models showed different results both in magnitude and in sign (erosion/deposition). However, in a situation where data is scarce (such as grain size, channel geometry), the more complex hydrodynamic model does not necessarily lead to more reliable results. (c) ", Elsevier Ltd. All rights reserved.,"van Griensven, A|Popescu, L|Abdelhamid, MR|Ndomba, PM|Beevers, L|Betrie, GD",PHYSICS AND CHEMISTRY OF THE EARTH,sediment routing sediment transport swat sober-re simiyu river basin,10.1016/j.pce.2013.02.003
221,WOS:000326685400009,2013,Uncertainty analysis in urban drainage modelling: should we break our back for normally distributed residuals?,PARAMETER-ESTIMATION CALIBRATION,"This study presents results on the assessment of the application of a Bayesian approach to evaluate the sensitivity and uncertainty associated with urban rainfall-runoff models. The software MICA was adopted, in which the prior information about the parameters is updated to generate the parameter posterior distribution. The likelihood function adopted in MICA assumes that the residuals between the measured and modelled values have a normal distribution. This is a trait of many uncertainty/sensitivity procedures. This study compares the results from three different scenarios: (i) when normality of the residuals was checked but if they were not normal then nothing was done (unverified); (ii) normality assumption was checked, verified (using data transformations) and a weighting strategy was used that gives more importance to high flows; and (iii) normality assumption was checked and verified, but no weights were applied. The modelling implications of such scenarios were analysed in terms of model efficiency, sensitivity and uncertainty assessment. The overall results indicated that verifying the normality assumption required the models to fit a wider portion of the hydrograph, allowing a more detailed inspection of parameters and processes simulated in both models. Such an outcome provided important information about the advantages and limitations of the models' structure.",,"Dotto, CBS|Deletic, A|McCarthy, DT",WATER SCIENCE AND TECHNOLOGY,bayesian approach normality assumption uncertainty analysis urban drainage models,10.2166/wst.2013.360
222,WOS:000412192800042,2017,Dynamic optimization of beer fermentation: Sensitivity analysis of attainable performance vs. product flavour constraints,VERTICAL ELECTRICAL FURNACE PERLITE GRAIN EXPANSION BATCH DISTILLATION OPTIMAL OPERATION FORMULATION SIMULATION MODEL,"The declining alcohol industry in the UK and the concurrent surge in supply and variety of beer products has created extremely competitive environment for breweries, many of which are pursuing the benefits of process intensification and optimization. To gain insight into the brewing process, an investigation into the influence of by-product threshold levels on obtainable fermentation performance has been performed, by computing optimal operating temperature profiles for a range of constraint levels on by-product concentrations in the final product. The DynOpt software package has been used, converting the continuous control vector optimization problem into nonlinear programming (NLP) form via collocation on finite elements, which has then been solved with an interior point algorithm. This has been performed for increasing levels of time discretization, by means of a range of initializing solution profiles, for a wide spectrum of imposed by-product flavour constraints. Each by-product flavour threshold affects process performance in a unique way. Results indicate that the maximum allowable diacetyl concentration in the final product has very strong influence on batch duration, with lower limits requiring considerably longer batches. The maximum allowable ethyl acetate concentration is shown to dictate the attainable ethanol concentration, and lower limits adversely affect the desired high alcohol content in the final product. (C) ", Elsevier Ltd. All rights reserved.,"Rodman, AD|Gerogiorgis, DI",COMPUTERS & CHEMICAL ENGINEERING,beer fermentation dynamic optimization multi-objective optimization orthogonal collocation on finite elements sensitivity analysis flavour constraints,10.1016/j.compchemeng.2017.06.024
223,WOS:000087885500003,2000,Buckling design optimization of complex built-up structures with shape and size variables,SENSITIVITY ANALYSIS,"The design optimization of buckling behaviour is studied for complex built-up structures composed of various kinds of elements and implemented within JIFEX, a general-purpose software for finite element analysis and design optimization. The direct and adjoint methods of sensitivity analysis for critical buckling loads are presented with detailed computational procedures. Particularly, the variations of prebuckling stresses and external loads have been accounted, for. The design model and solution methods presented in this paper are available for both shape and size optimization, and buckling optimization can also be combined with static, frequency and dynamic response optimization. The numerical examples show the applications of the buckling optimization method and the effectiveness of the methods and the program of this paper.",,"Gu, YX|Zhao, GZ|Zhang, HW|Kang, Z|Grandhi, RV",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,shape optimization size optimization buckling natural frequency dynamic response built-up structures,10.1007/s001580050101
224,WOS:000413886300011,2017,Process simulation and techno economic analysis of renewable diesel production via catalytic decarboxylation of rubber seed oil - A case study in Malaysia,TECHNOECONOMIC ASSESSMENT,"This work describes the economic feasibility of hydroprocessed diesel fuel production via catalytic decarboxylation of rubber seed oil in Malaysia. A comprehensive techno-economic assessment is developed using Aspen HYSYS V. software for process modelling and economic cost estimates. The profitability profile and, minimum fuels selling price of this synthetic fuels production using rubber seed oil as biomass feedstock are assessed under a set of assumptions for what can be plausibly be achieved in -years framework. In this study, renewable diesel processing facility is modelled to be capable of processing , L of inedible oil per day and producing a total of  million litre of renewable diesel product per annual with assumed annual operational days of . With the forecasted renewable diesel retail price of . RM per kg, the pioneering renewable diesel project investment offers an assuring return of investment of .% and net return as high as . million RM. Sensitivity analysis conducted showed that renewable diesel production cost is most sensitive to rubber seed oil price and hydrogen gas price, reflecting on the relative importance of feedstock prices in the overall profitability profile. (C) ", Elsevier Ltd. All rights reserved.,"Cheah, KW|Yusup, S|Singh, HKG|Uemura, Y|Lam, HL",JOURNAL OF ENVIRONMENTAL MANAGEMENT,techno-economic catalytic decarboxylation rubber seed oil renewable diesel minimum fuel selling price profitability profile,10.1016/j.jenvman.2017.05.053
225,WOS:000390878800008,2016,Analyzing Complex Treatment Effects in Nonrandomized Observational Studies: The Case of Retention of Students in Grade,ACADEMIC-PERFORMANCE CAUSAL INFERENCE ACHIEVEMENT,"Should low-achieving students be promoted to the next grade or be retained (held back) in the prior grade? This special section presents a discussion of the application of marginal structural models to the challenging problem of estimating the effect of promotion versus retention in grade on math scores in elementary school. Vandecandelaere, De Fraine, Van Damme, and Vansteelandt provide a didactic presentation of the marginal structural modeling approach, noting retention is a time-varying treatment because promoted low-achieving students may be retained in a subsequent grade. Steiner, Park, and Kim's commentary presents a detailed analysis of the treatment effects being estimated in same-age versus same-grade comparisons from the perspective of the potential outcomes model. Reshetnyak, Cham, and Kim's commentary clarifies the conditions under which same-age versus same-grade comparisons might be preferred; they also identify methods of further improving the estimation of retention effects. In their rejoinder, Vandecandelaere and Vansteelandt discuss tradeoffs in comparing the promoted and retained groups and highlight sensitivity analysis as a method of probing the robustness of treatment effect estimates. Our hope is that this combined didactic presentation and critical evaluation will encourage researchers to add marginal structural models to their methodological toolkits.",,"West, SG",MULTIVARIATE BEHAVIORAL RESEARCH,observational study time-varying treatment marginal structural modeling propensity scores grade retention same-age versus same-grade comparison,10.1080/00273171.2016.1251756
226,WOS:000390183000039,2016,Identifiability of sorption parameters in stirred flow-through reactor experiments and their identification with a Bayesian approach,NONEQUILIBRIUM SOLUTE TRANSPORT CHAIN MONTE-CARLO POROUS-MEDIA MODELS ADSORPTION EQUILIBRIUM KINETICS VALUES,"This paper addresses the methodological conditions particularly experimental design and statistical inference ensuring the identifiability of sorption parameters from breakthrough curves measured during stirred flow-through reactor experiments also known as continuous flow stirred-tank reactor (CSTR) experiments. The equilibrium-kinetic (EK) sorption model was selected as nonequilibrium parameterization embedding the Kd approach. Parameter identifiability was studied.formally on the equations governing outlet concentrations. It was also studied numerically on  simulated CSTR experiments on a soil with known equilibrium-kinetic sorption parameters. EK sorption parameters can not be identified from a single breakthrough curve of a CSTR experiment, because K-d,K- and k(-) were diagnosed collinear. For pairs of CSTR experiments, Bayesian inference allowed to select the correct models of sorption and error among sorption alternatives. Bayesian inference was conducted with SAMCAT software (Sensitivity Analysis and Markov Chain simulations Applied to Transfer models) which launched the simulations through the embedded simulation engine GNU-MCSim, and automated their configuration and post-processing. Experimental designs consisting in varying flow rates between experiments reaching equilibrium at contamination stage were found optimal, because they simultaneously gave accurate sorption parameters and predictions. Bayesian results were comparable to maximum likehood method but they avoided convergence problems, the marginal likelihood allowed to compare all modeK and credible interval gave directly the uncertainty of sorption parameters theta. Although these findings are limited to the specific conditions studied here, in particular the considered sorption model, the chosen parameter values and error structure, they help in the conception and analysis of future CSTR experiments with radionuclides whose kinetic behaviour is suspected. (C) ", Elsevier Ltd. All rights reserved.,"Nicoulaud-Gouin, V|Garcia-Sanchez, L|Giacalone, M|Attard, JC|Martin-Garin, A|Bois, FY",JOURNAL OF ENVIRONMENTAL RADIOACTIVITY,bayesian inference sorption parameters identifiability mcmc convergence monitoring equilibrium kinetic model,10.1016/j.jenvrad.2016.06.008
227,WOS:000382269000125,2016,Irrigation water demand of selected agricultural crops in Germany between 1902 and 2010,LEAF-AREA INDEX CLIMATE-CHANGE IMPACT ASSESSMENT NORTHERN GERMANY HUMID CLIMATE REQUIREMENTS EUROPE AVAILABILITY ENGLAND MODEL,"Irrigation water demand (IWD) is increasing worldwide, including in regions such as Germany that are characterized with low precipitation levels, yet grow water-demanding crops such as sugar beets, potatoes, and vegetables. This study aimed to calculate and analyze the spatial and temporal changes in the IWD of four crops spring barley, oat, winter wheat, and potato between  and  in Germany by using the modeling software AgroHyd Farmmodel. Climatic conditions in Germany continued to change over the investigation period, with an increase in temperature of . K/yr and an increase in precipitation of  mm/yr. Nevertheless, no significant increasing or decreasing trend in IWD was noted in the analysis. The IWD for the investigated crops in the area of the current ""Federal Republic of Germany"" over the  years was  mm/yr, varying between  and  mm/yr. Changes in cropping pattern and cultivated area over the last century caused large differences in the IWD calculated for each administrative district. The mean annual IWD of over the study period (which was divided into  parts) varied between , Mm()/yr in the earliest period (-) and  Mm()/yr in the latest period (-). Policy and management measures to adapt to climate change are currently being debated in Germany. The presented results suggest that the effects of the choice of crops (in this case, changes in cropping pattern in the German nation states) had a stronger influence on regional water resources than those of climate variability. Thus, the influence of climate change on water resources is relativized which brings an important input into the debate.", (C) 2016 Elsevier BM. All rights reserved.,"Drastig, K|Prochnow, A|Libra, J|Koch, H|Rolinski, S",SCIENCE OF THE TOTAL ENVIRONMENT,in igation water demand inigation trend agrohyd fanninodel,10.1016/j.scitotenv.2016.06.206
228,WOS:000185985700004,2003,Structural design optimization on thermally induced vibration,TRANSIENT HEAT-CONDUCTION PRECISE TIME INTEGRATION SENSITIVITY ANALYSIS DYNAMIC LOADS DERIVATIVES,"The numerical method of design optimization for structural thermally induced vibration is originally studied in this paper and implemented in, the software JIFEX The direct and adjoint methods of sensitivity analysis for thermal-induced vibration coupled with both linear and non-linear transient heat conduction is firstly proposed. Based on the finite element method, the linear structural dynamics is treated simultaneously with linear and non-linear transient heat conduction. In the heat conduction, the non-linear factors include the radiation and temperature-dependent materials. The sensitivity analysis of transient linear and non-linear heat conduction is performed with the precise time integration method; and then, the sensitivity analysis of structural transient responses is performed by the Newmark method. Both the direct method and the adjoint method are employed to derive the sensitivity equations of thermal vibration. In the adjoint method, two adjoint vectors of structure and of heat conduction are used to derive the adjoint equations. The coupling effect of heat conduction on thermal vibration in the sensitivity analysis is particularly investigated. With the coupling sensitivity analysis, the optimization model is constructed and solved by the sequential linear programming or sequential quadratic programming algorithm. Numerical examples are given to validate the proposed methods and to demonstrate the importance of the coupled design optimization."," Copyright (C) 2003 John Wiley Sons, Ltd.","Chen, BS|Gu, YX|Zhang, HW|Zhao, GZ",INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,heat conduction thermal vibration sensitivity analysis design optimization precise time integration thermal coupled structural system,10.1002/nme.814
229,WOS:000353971300004,2015,Enhancing the Characterization of Epistemic Uncertainties in PM2.5 Risk Analyses,PARTICULATE AIR-POLLUTION LONG-TERM EXPOSURE UNITED-STATES FOLLOW-UP 6 CITIES MORTALITY FINE COHORT VETERANS QUALITY,"The Environmental Benefits Mapping and Analysis Program (BenMAP) is a software tool developed by the U.S. Environmental Protection Agency (EPA) that is widely used inside and outside of EPA to produce quantitative estimates of public health risks from fine particulate matter (PM.). This article discusses the purpose and appropriate role of a risk analysis tool to support risk management deliberations, and evaluates the functions of BenMAP in this context. It highlights the importance in quantitative risk analyses of characterization of epistemic uncertainty, or outright lack of knowledge, about the true risk relationships being quantified. This article describes and quantitatively illustrates sensitivities of PM. risk estimates to several key forms of epistemic uncertainty that pervade those calculations: the risk coefficient, shape of the risk function, and the relative toxicity of individual PM. constituents. It also summarizes findings from a review of U.S.-based epidemiological evidence regarding the PM. risk coefficient for mortality from long-term exposure. That review shows that the set of risk coefficients embedded in BenMAP substantially understates the range in the literature. We conclude that BenMAP would more usefully fulfill its role as a risk analysis support tool if its functions were extended to better enable and prompt its users to characterize the epistemic uncertainties in their risk calculations. This requires expanded automatic sensitivity analysis functions and more recognition of the full range of uncertainty in risk coefficients.",,"Smith, AE|Gans, W",RISK ANALYSIS,benmap epidemiology health risk pm2 5 risk analysis uncertainty,10.1111/risa.12236
230,WOS:000269046900012,2009,IPH-TRIM3D-PCLake: A three-dimensional complex dynamic model for subtropical aquatic ecosystems,,"This paper presents IPH-TRIMD-PCLake, a three-dimensional complex dynamic model for subtropical aquatic ecosystems. It combines a spatially explicit hydrodynamic model with a water-quality and biotic model of ecological interactions. The software, which is freely available for research purposes, has a graphical user-friendly interface and a flexible design that allows the user to vary the complexity of the model. It also has built-in analysis tools such as Monte Carlo sensitivity analysis, a genetic algorithm for calibration, and plotting tools. (C) ", Elsevier Ltd. All rights reserved.,"Fragoso, CR|van Nes, EH|Janse, JH|Marques, DD",ENVIRONMENTAL MODELLING & SOFTWARE,aquatic ecosystem model cascading trophic effects subtropical ecosystems 3d model,10.1016/j.envsoft.2009.05.006
231,WOS:000309267300007,2012,Comparison of empirical and numerical methods in tunnel stability analysis,,"The stability of a tunnel can be evaluated using mathematical solutions, empirical methods, or numerical modelling. Mathematical solutions are precise methods; however the need to conduct mathematical calculations usually decreases the user's desire to use this method. Empirical methods are based on the experience gathered by researchers in various parts of the world whereas numerical modelling utilises computing power and, using various modelling techniques, can be a precise way of solving very complex problems. In this method the environment and the geometry can be set by the user. This method allows the user to conduct sensitivity analysis. In this article, empirical methods and numerical modelling using UDEC software were used to conduct a stability analysis of the access tunnel at the Shahriar dam crest, which was one of the most important tunnels of this project. In addition, numerical modelling was used to predict the stresses and deformations around the perimeter of the tunnel, and select the most suitable ground support system. The results obtained from both methods were compared for selection of the best suited support system. The results indicated that the empirical methods presented similar results to the results of numerical modelling at the first stages of tunnel design in jointed rocks. Therefore, in the absence of sufficient information for numerical analysis, the results of the empirical method can be used for this project.",,"Rahmani, N|Nikbakhtan, B|Ahangari, K|Apel, D",INTERNATIONAL JOURNAL OF MINING RECLAMATION AND ENVIRONMENT,mathematical analyses empirical methods numerical modelling tunnel udec,10.1080/17480930.2011.611615
232,WOS:000256053900010,2008,A comparative life cycle analysis of two different juice packages,,"Packaging wastes have a portion of -% in total municipal solid waste (MSW) in Turkey, and they have to be evaluated from production to final disposal from the environmental point of view. The concern about the environmental impacts of packages has been dealt with using several approaches in environmental management, such as risk assessment, environmental impact assessment, environmental auditing, energy analysis, material flow analysis, and life cycle analysis (LCA). The main purpose of this research was to investigate the life cycle environmental impact of glass bottles and beverage cartons. This LCA study was performed by using SimaPro (PRe Consultants, The Netherlands) software. Individual and comparative life cycle analysis of two packages was performed depending on a consumer who lives in Eskisehir city. For that aim, SimaPro was used, and the data to run the software was gathered from package producers, the database of the software, and the literature. Life cycle comparisons of the two juice packages among themselves and also within themselves were carried out by using EcoIndicator  on the basis of climate change, ecotoxicity, acidification/eutrophication, and fossil fuels. Sensitivity analysis was performed to evaluate the effects of transportation. According to comparison figures, the environmental load of glass bottles is higher than beverage carton's load for all the impact categories. This result is also supported by the sensitivity analysis.",,"Banar, M|Cokaygil, Z",ENVIRONMENTAL ENGINEERING SCIENCE,lca glass bottle beverage carton packaging waste simapro7,10.1089/ees.2007.0079
233,WOS:000395607700003,2017,Infiltration under snow cover: Modeling approaches and predictive uncertainty,ENERGY-BALANCE HYDRAULIC CONDUCTIVITY WATER EQUIVALENT MELT SIMULATIONS PILOT POINTS TEMPERATURE INDEX RADIATION PARAMETER SURFACE,"Groundwater recharge from snowmelt represents a temporal redistribution of precipitation. This is extremely important because the rate and timing of snowpack drainage has substantial consequences to aquifer recharge patterns, which in turn affect groundwater availability throughout the rest of the year. The modeling methods developed to estimate drainage from a snowpack, which typically rely on temporally dense point-measurements or temporally-limited spatially-dispersed calibration data, range in complexity from the simple degree-day method to more complex and physically-based energy balance approaches. While the gamut of snowmelt models are routinely used to aid in water resource management, a comparison of snowmelt models' predictive uncertainties had previously not been done. Therefore, we established a snowmelt model calibration dataset that is both temporally dense and represents the integrated snowmelt infiltration signal for the Vers Chez le Brandt research catchment, which functions as a rather unique natural lysimeter. We then evaluated the uncertainty associated with the degree-day, a modified degree-day and energy balance snowmelt model predictions using the null space Monte Carlo approach. All three melt models underestimate total snowpack drainage, underestimate the rate of early and midwinter drainage and overestimate spring snowmelt rates. The actual rate of snowpack water loss is more constant over the course of the entire winter season than the snowmelt models would imply, indicating that mid-winter melt can contribute as significantly as springtime snow melt to groundwater recharge in low alpine settings. Further, actual groundwater recharge could be between  and % greater than snowmelt models suggest, over the total winter season. This study shows that snowmelt model predictions can have considerable uncertainty, which may be reduced by the inclusion of more data that allows for the use of more complex approaches such as the energy balance method. Further, our study demonstrated that an uncertainty analysis of model predictions is easily accomplished due to the low computational demand of the models and efficient calibration software and is absolutely worth the additional investment. Lastly, development of a systematic instrumentation that evaluates the distributed, temporal evolution of snowpack drainage is vital for optimal understanding and management of cold-climate hydrologic systems.", (C) 2017 Elsevier B.V. All rights reserved.,"Meeks, J|Moeck, C|Brunner, P|Hunkeler, D",JOURNAL OF HYDROLOGY,uncertainty snowmelt energy balance day degree recharge karst groundwater,10.1016/j.jhydrol.2016.12.042
234,WOS:000337570800007,2014,Fuzzy logic approach and sensitivity analysis for agent-based crowd injury modeling,SYSTEMS BIOLOGY SIMULATION EXPLANATION DYNAMICS PATHWAY DESIGN,"A crowd is a group of people attending a public gathering with some joint purpose, such as protesting against the government or celebrating an event. In some countries, these kinds of activities are the only way to express public displeasure with their government. The government's reactions to such activities may or may not be tolerant. For this reason, such situations must be eliminated by recognizing when and how they are likely to occur, and then providing guidelines to mitigate them. In urban areas, police and military forces use non-lethal weapons (NLWs), such as rubber bullets or clubs, to control a violent and destructive crowd. In order to estimate the results of this engagement, ensuring minimum injuries and reaching an optimal end state, simulating such actions in a virtual environment is necessary. In this work, a fuzzy logic-based crowd injury model for determining the physical effects of NLWs is proposed. Fuzzy logic concepts can be applied to a problem by using linguistic rules, which are determined by problem domain experts. A group of police and military officers were consulted for a set of injury model rules, and those rules were then included in the simulation platform. Sensitivity analysis has been conducted to analyze parameters in the model. As a proof of the concept, a prototype system was implemented using the Repast Simphony agent-based simulation toolkit. Simulation results illustrated the effectiveness of the simulation framework.",,"Kugu, E|Li, J|McKenzie, FD|Sahingoz, OK",SIMULATION-TRANSACTIONS OF THE SOCIETY FOR MODELING AND SIMULATION INTERNATIONAL,agent-based simulation fuzzy logic design sensitivity analysis crowd injury,10.1177/0037549713518598
235,WOS:000414818700006,2017,Assessment of environmental impacts and operational costs of the implementation of an innovative source-separated urine treatment,WASTE-WATER TREATMENT LIFE-CYCLE ASSESSMENT TREATMENT PLANTS NUTRIENT MANAGEMENT REMOVAL ALTERNATIVES SYSTEM FOCUS,"Innovative treatment technologies and management methods are necessary to valorise the constituents of wastewater, in particular nutrients from urine (highly concentrated and can have significant impacts related to artificial fertilizer production). The FP project, ValuefromUrine, proposed a new two-step process (called VFU) based on struvite precipitation and microbial electrolysis cell (MEC) to recover ammonia, which is further transformed into ammonium sulphate. The environmental and economic impacts of its prospective implementation in the Netherlands were evaluated based on life cycle assessment (LCA) methodology and operational costs. In order to tackle the lack of stable data from the pilot plant and the complex effects on wastewater treatment plant (WWTP), process simulation was coupled with LCA and costs assessment using the Python programming language. Additionally, particular attention was given to the propagation and analysis of inputs uncertainties. Five scenarios of VFU implementation were compared to the conventional treatment of  m() of wastewater. Inventory data were obtained from SUMO software for the WWTP operation. LCA was based on Brightway software (using ecoinvent database and ReCiPe method). The results, based on  iterations sampled from inputs distributions (foreground parameters, ecoinvent background data and market prices), showed a significant advantage of VFU technology, both at a small and decentralized scale and at a large and centralized scale (% confidence intervals not including zero values). The benefits mainly concern the production of fertilizers, the decreased efforts at the WWTP, the water savings from toilets flushing, as well as the lower infrastructure volumes if the WWTP is redesigned (in case of significant reduction of nutrients load in wastewater). The modelling approach, which could be applied to other case studies, improves the representativeness and the interpretation of results (e.g. complex relationships, global sensitivity analysis) but requires additional efforts (computing and engineering knowledge, longer calculation time). Finally, the sustainability assessment should be refined in the future with the development of the technology at larger scale to update these preliminary conclusions before its commercialization. (C) ", Elsevier Ltd. All rights reserved.,"Igos, E|Besson, M|Gutierrez, TN|de Faria, ABB|Benetto, E|Barna, L|Ahmadi, A|Sperandio, M",WATER RESEARCH,source-separated urine treatment process simulation sustainability assessment innovative technology integrated modelling,10.1016/j.watres.2017.09.016
236,WOS:000356196000011,2015,PUQ; A code for non-intrusive uncertainty propagation in computer simulations,STRENGTH MAXIMUM SCIENCE CHAOS,"We present a software package for the non-intrusive propagation of uncertainties in input parameters through computer simulation codes or mathematical models and associated analysis; we demonstrate its use to drive micromechanical simulations using a phase field approach to dislocation dynamics. The PRISM uncertainty quantification framework (PUQ) offers several methods to sample the distribution of input variables and to obtain surrogate models (or response functions) that relate the uncertain inputs with the quantities of interest (QoIs); the surrogate models are ultimately used to propagate uncertainties. PUQ requires minimal changes in the simulation code, just those required to annotate the QoI(s) for its analysis. Collocation methods include Monte Carlo, Latin Hypercube and Smolyak sparse grids and surrogate models can be obtained in terms of radial basis functions and via generalized polynomial chaos. PUQ uses the method of elementary effects for sensitivity analysis in Smolyak runs. The code is available for download and also available for cloud computing in nanoHUB. PUQ orchestrates runs of the nanoPLASTICITY tool at nanoHUB where users can propagate uncertainties in dislocation dynamics simulations using simply a web browser, without downloading or installing any software. Program summary Program title: PUQ Catalogue identifier: AEWP_v_ Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEWP_v_.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: MIT license No. of lines in distributed program, including test data, etc.:  No. of bytes in distributed program, including test data, etc.:  Distribution format: tar.gz Programming language: Python, C. Computer: Workstations. Operating system: Linux, Mac OSX. Classification: ., ., ., External routines: SciPy, Matplotlib, hpy Nature of problem: Uncertainty propagation and creation of response surfaces. Solution method: Generalized Polynomial Chaos (gPC) using Smolyak sparse grids. Running time: PUQ performs uncertainty quantification and sensitivity analysis by running a simulation multiple times using different values for input parameters. Its run time will be the product of the run time of the chosen simulation code and the number of runs required to achieve the desired accuracy.", (C) 2015 Elsevier B.V. All rights reserved.,"Hunt, M|Haley, B|McLennan, M|Koslowski, M|Murthy, J|Strachan, A",COMPUTER PHYSICS COMMUNICATIONS,uncertainty quantification surrogate model sensitivity analysis,10.1016/j.cpc.2015.04.011
237,WOS:000395108800010,2017,Pattern Mixture Models for Quantifying Missing Data Uncertainty in Longitudinal Invariance Testing,STRUCTURAL EQUATION MODELS FACTORIAL INVARIANCE DROP-OUT PSYCHOLOGICAL-RESEARCH NONIGNORABLE DROPOUT SENSITIVITY-ANALYSIS CLINICAL-TRIALS INCOMPLETE DATA MIXED MODELS GROWTH,"Many psychology applications assess measurement invariance of a construct (e.g., depression) over time. These applications are often characterized by few time points (e.g., ), but high rates of dropout. Although such applications routinely assume that the dropout mechanism is ignorable, this assumption may not always be reasonable. In the presence of nonignorable dropout, fitting a conventional longitudinal factor model (LFM) to assess longitudinal measurement invariance can yield misleading inferences about the level of invariance, along with biased parameter estimates. In this article we develop pattern mixture longitudinal factor models (PM-LFMs) for quantifying uncertainty in longitudinal invariance testing due to an unknown, but potentially nonignorable, dropout mechanism. PM-LFMs are a kind of multiple group model wherein observed missingness patterns define groups, LFM parameters can differ across these pattern-groups subject to identification constraints, and marginal inference about longitudinal invariance is obtained by pooling across pattern-groups. When dropout is nonignorable, we demonstrate via simulation that conventional LFMs can indicate longitudinal noninvariance, even when invariance holds in the overall population; certain PM-LFMs are shown to ameliorate this problem. On the other hand, when dropout is ignorable, PM-LFMs are shown to provide results comparable to conventional LFMs. Additionally, we contrast PM-LFMs to a latent mixture approach for accommodating nonignorable dropoutwherein missingness patterns can differ across latent groups. In an empirical example assessing longitudinal invariance of a harsh parenting construct, we employ PM-LFMs to assess sensitivity of results to assumptions about nonignorable missingness. Software implementation and recommendations for practice are discussed.",,"Sterba, SK",STRUCTURAL EQUATION MODELING-A MULTIDISCIPLINARY JOURNAL,longitudinal factor model longitudinal invariance nonignorable missing data pattern mixture model,10.1080/10705511.2016.1250635
238,WOS:000168591400008,2001,MCE-RISK: integrating multicriteria evaluation and CIS for risk decision-making in natural hazards,GEOGRAPHICAL INFORMATION-SYSTEMS,"During the past two decades there have been a wide range of applications for decision-making linking multicriteria evaluation (MCE) and geographic information systems (GIS). However, limited literature reports the development of MCE-GIS software, and the comparison of various MCE-GIS approaches. This paper introduces an MCE-GIS program called MCE-RISK for risk-based decision-making. It consists of a series of modules for data standardisation, weighting, MCE-GIS methods. and sensitivity analysis. The program incorporates different MCE-GIS methods. including weighted linear combination (WLC), the technique for order preference by similarity to ideal solution (TOPSIS), and compromise programming (CP), enabling comparisons between different methods for the same decision problem to be made. An example of decision-making for determining priority areas for a bushfire hazard reduction burning is examined. After implementing the alternative MCE-GIS methods, and comparing final outputs and the computational difficulty involved in the analysis, WLC is recommended. Some caveats on using MCE-GIS methods art: also dis cussed. Although the development of MCE-RISK and its application reported in this paper are specific to risk-based decisionmaking in natural hazards, the program can be used for other environmental decision applications. such as environmental impact assessment and land-use planning."," (C) 2001 Elsevier Science Ltd, All rights reserved.","Chen, KP|Blong, R|Jacobson, C",ENVIRONMENTAL MODELLING & SOFTWARE,risk decision-making multicriteria evaluation cis bushfire prescribed burning,10.1016/S1364-8152(01)00006-8
239,WOS:000304926200026,2012,Numerical Modelling of Waste Stabilization Ponds: Where Do We Stand?,DYNAMIC MATHEMATICAL-MODEL AERATED LAGOON TREATMENT EFFICIENCY MATURATION PONDS STEADY-STATE PAPER-MILL PREDICTION REMOVAL TRANSFORMATION CALIBRATION,"Waste stabilization pond (WSP) technology has been an active area of research for the last three decades. In spite of its relative simplicity of design, operation and maintenance, the various processes taking place in WSP have not been entirely quantified. Lately, modelling has served as an important, low-cost tool for a better description and an improved understanding of the system. Although several papers on individual pond models have been published, there is no specific review on different models developed so far. This paper aims at filling this gap. Models are compared by focussing on their key features like the presence and comprehensiveness of a water quality sub-model in terms of aerobic/anoxic and anaerobic carbon removal and nutrient removal; the type of hydraulic sub-model used (D, D, D or D); the software used for implementation and simulation; and whether or not sensitivity analysis, calibration and validation were done. This paper also recommends future directions of research in this area. In-depth study of the published models reveals a clear evolution over time in the concept of modelling, from just hydraulic empirical models to D ones and from simple first-order water quality models to complex ones which describe key biochemical processes as a set of mathematical equations. Due to the inherent complexity, models tend to focus only on specific aspects whilst ignoring or simplifying others. For instance, many models have been developed that either focus solely on hydrodynamics or solely on biochemical processes. Models which integrate both aspects in detail are still rare. Furthermore, it is evident from the review of the different models that calibration and validation with full-scale WSP data is also scarce. Hence, we believe that there is a need for the development of a comprehensive, calibrated model for waste stabilization ponds that can reliably serve as a support tool for the improvement and optimization of pond design and performance.",,"Sah, L|Rousseau, DPL|Hooijmans, CM",WATER AIR AND SOIL POLLUTION,computational fluid dynamics (cfd) hydrodynamics modelling waste stabilization pond (wsp) water quality,10.1007/s11270-012-1098-4
240,WOS:000248617800003,2007,Automatic sensitivity analysis of a finite volume model for two-dimensional shallow water flows,DENSE URBAN AREA,"Given a numerical model for solving two-dimensional shallow water equations, we are interested in the robustness of the simulation by identifying the rate of change of the water depths and discharges with respect to a change in the bottom friction coefficients. Such a sensitivity analysis can be carried out by computing the corresponding derivatives. Automatic differentiation (AD) is an efficient numerical method, free of approximation errors, to evaluate derivatives of the objective function specified by the computer program, Rubar for example. In this paper AD software tool Tapenade is used to compute forward derivatives. Numerical tests were done to show the robustness of the model and to demonstrate the efficiency of these AD-derivatives.",,"Souhar, O|Faure, JB|Paquier, A",ENVIRONMENTAL FLUID MECHANICS,automatic differentiation hydraulic model sensitivity analysis uncertainty propagation steady flow,10.1007/s10652-007-9028-5
241,WOS:000413245200004,2017,A user-friendly software package for VIC hydrologic model development,GLOBAL SENSITIVITY-ANALYSIS PRECIPITATION PRODUCTS MULTISITE CALIBRATION SOIL-MOISTURE RUNOFF-MODEL WATER BASIN SIMULATION DATASET FLUXES,"The Variable Infiltration Capacity (VIC) hydrologic and river routing model simulates the water and energy fluxes that occur near the land surface and provides useful information regarding the quantity and timing of available water within a watershed system. However, despite its popularity, wider adoption is hampered by the considerable effort required to prepare model inputs and calibrate the model parameters. This study presents a user-friendly software package, named VIC-Automated Setup Toolkit (VIC-ASSIST), accessible through an intuitive MATLAB graphical user interface. VIC-ASSIST enables users to navigate the model building process through prompts and automation, with the intention to promote the use of the model for practical, educational, and research purposes. The automated processes include watershed delineation, climate and geographical input set-up, model parameter calibration, sensitivity analysis, and graphical output generation. We demonstrate the package's utilities in various case studies. (C) ", Elsevier Ltd. All rights reserved.,"Wi, S|Ray, P|Demaria, EMC|Steinschneider, S|Brown, C",ENVIRONMENTAL MODELLING & SOFTWARE,vic hydrologic model vic setup assistant tool matlab graphic user interface automatic calibration sensitivity analysis,10.1016/j.envsoft.2017.09.006
242,WOS:000278842000032,2010,Sensitivity analysis of release time of software reliability models incorporating testing effort with multiple change-points,RESOURCE-ALLOCATION GROWTH-MODEL PREDICTION SYSTEMS UNCERTAINTY INDEXES COST,"To accurately model software failure process with software reliability growth models, incorporating testing effort has shown to be important. In fact, testing effort allocation is also a difficult issue, and it directly affects the software release time when a reliability criteria has to be met. However, with an increasing number of parameters involved in these models, the uncertainty of parameters estimated from the failure data could greatly affect the decision. Hence, it is of importance to study the impact of these model parameters. In this paper, sensitivity of the software release time is investigated through various methods, including one-factor-at-a-time approach, design of experiments and global sensitivity analysis. It is shown that the results from the first two methods may not be accurate enough for the case of complex nonlinear model. Global sensitivity analysis performs better due to the consideration of the global parameter space. The limitations of different approaches are also discussed. Finally, to avoid further excessive adjustment of software release time, interval estimation is recommended for use and it can be obtained based on the results from global sensitivity analysis.", (c) 2010 Elsevier Inc. All rights reserved.,"Li, XA|Xie, M|Ng, SH",APPLIED MATHEMATICAL MODELLING,software reliability release time sensitivity analysis testing effort multiple change-points,10.1016/j.apm.2010.03.006
243,WOS:000308971400030,2012,An integrated assessment tool to define effective air quality policies at regional scale,POLLUTION MODEL PM10 STRATEGIES VALIDATION LONDON EUROPE,"In this paper, the Integrated Assessment of air quality is dealt with at regional scale. First the paper describes the main challenges to tackle current air pollution control, including economic aspects. Then it proposes a novel approach to manage the problem, presenting its mathematical formalization and describing its practical implementation into the Regional Integrated Assessment Tool (RIAT). The main features of the software system are described and some preliminary results on a domain in Northern Italy are illustrated. The novel features in RIAT are then compared to the state-of-the-art in integrated assessment of air quality, for example the ability to handle nonlinearities (instead of the usual linear approach) and the multi-objective framework (alternative to cost-effectiveness and scenario analysis). Then the lessons learned during the RIAT implementation are discussed, focusing on the locality, flexibility and openness of the tool. Finally the areas for further development of air quality integrated assessment are highlighted, with a focus on sensitivity analysis, structural and non technical measures, and the application of parallel computing concepts. (C) ", Elsevier Ltd. All rights reserved.,"Carnevale, C|Finzi, G|Pisoni, E|Volta, M|Guariso, G|Gianfreda, R|Maffeis, G|Thunis, P|White, L|Triacchini, G",ENVIRONMENTAL MODELLING & SOFTWARE,integrated assessment modeling model reduction air quality modeling multi-objective optimization decision support,10.1016/j.envsoft.2012.07.004
244,WOS:000183565000005,2003,Calibration and sensitivity analysis of a river water quality model under unsteady flow conditions,SYSTEMS QUASAR UNCERTAINTY PREDICTION SIMULATION CHANNEL OUSE,"Water quality models generally require a relatively large number of parameters to define their functional relationships, and since prior information on parameter values is limited, these are commonly defined by fitting the model to observed data. In this paper, the identifiability of water quality parameters and the associated uncertainty in model simulations are investigated. A modification to the water quality model `Quality Simulation Along River Systems' is presented in which an improved flow component is used within the existing water quality model framework. The performance of the model is evaluated in an application to the Bedford Ouse river, UK, using a Monte-Carlo analysis toolbox. The essential framework of the model proved to be sound, and calibration and validation performance was generally good. However some supposedly important water quality parameters associated with algal activity were found to be completely insensitive, and hence non-identifiable, within the model structure, while others (nitrification and sedimentation) had optimum values at or close to zero, indicating that those processes were not detectable from the data set examined.", (C) 2003 Elsevier Science B.V. All rights reserved.,"Sincock, AM|Wheater, HS|Whitehead, PG",JOURNAL OF HYDROLOGY,bedford ouse nitrate do bod quality simulation along river systems water quality modelling,10.1016/S0022-1694(03)00127-6
245,WOS:000381039000006,2016,Sensitivity of Flood-Depth Frequency to Watershed-Runoff Change and Sea-Level Rise Using a One-Dimensional Hydraulic Model,IMPACT,"Climate change and sea-level rise are expected to alter the likelihood of extreme events, such as floods, within the design lifetime of infrastructure components. Critical civil infrastructure facilities, including wastewater treatment, transportation, and energy, need site-specific flood contingency plans that reflect the effects of changing climate. This study developed a sensitivity analysis method to assess future flood risk by estimating flood frequency under conditions of higher sea level and streamflow response to increased precipitation intensity. The method was applied to an ungauged location on a tidal estuary in the Mid-Atlantic region as a case study. One-dimensional (D) unsteady flow analysis using a hydraulic analysis software developed by the U.S. Army Corps of Engineers was used to predict discharge and water surface elevation along the estuary reach, subject to prescribed boundary conditions of upstream discharge and downstream water surface elevation. A current-climate flood-depth frequency curve was estimated for the study site based on simulations of high-flow events in the years for which simultaneous upstream and downstream records were available. The simulations were repeated, applying additive water surface elevation (WSEL) perturbations at the downstream boundary (to represent anticipated sea-level rise) and multiplicative event discharge perturbations at the upstream boundary (to represent anticipated change in watershed hydrology). The perturbations were applied separately and together. Revised flood-depth frequency curves were calculated for each set of perturbations. For this location, the % annual exceedance (-year) WSEL is .m (.ft) higher, and the .% annual exceedance (-year) WSEL is m (.ft) higher, than current climate in the worst-case scenario, .m (ft) of sea-level rise and a % increase in event discharge. For that scenario, the current % exceedance (-year) WSEL has a % probability of exceedance (.year). The results indicate that the effects of the upstream and downstream changes are not additive. This research will help infrastructure stakeholders be aware of the flood risk and vulnerability while environmental changes are underway.",,"Feng, YL|Brubaker, KL",JOURNAL OF HYDROLOGIC ENGINEERING,,10.1061/(ASCE)HE.1943-5584.0001378
246,WOS:000222719700006,2004,Interactive software for material parameter characterization of advanced engineering constitutive models,STATE,"The development of an overall strategy to estimate the material parameters for a class of viscoplastic material models is presented. The procedure is automated through the integrated software COMPARE (Constitutive Material PARameter Estimator) that enables the determination of an 'optimum' set of material parameters by minimizing the errors between the experimental test data and the model's predicted response. The core ingredients of COMPARE are (i) primal analysis, which utilizes a finite element-based solution scheme, (ii) sensitivity analysis utilizing a direct-differentiation approach for the material response sensitivities, and (iii) a gradient-based optimization technique of an error/cost function. Now that the COMPARE core code has reached a level of maturity, a graphical user interface (GUI) was deemed necessary. Without such an interface, use of COMPARE was previously restricted to very experienced users with the additional cumbersome, and sometimes tedious, task of preparing the required input files manually. The complexity of the input containing massive amounts of data has previously placed severe limitations on the use of such optimization procedures by the general engineering community. By using C+ + and the Microsoft Foundation Classes to develop a GUI, it is believed that an advanced code such as COMPARE can now make the transition to general usability in an engineering environment. (C) ", Elsevier Ltd. All rights reserved.,"Saleeb, AF|Marks, JR|Wilt, TE|Arnold, SM",ADVANCES IN ENGINEERING SOFTWARE,c plus graphical user interface optimization material characterization viscoplasticity,10.1016/j.advengsoft.2004.03.010
247,WOS:000276920300002,2010,The decision model of task allocation for constrained stochastic distributed systems,COMPUTING SYSTEMS MAXIMIZING RELIABILITY ALGORITHMS OPTIMIZATION,"In distributed systems, an application program is divided into several software modules, which need to be allocated to processors connected by communication links. The distributed system reliability (DSR) could be defined as the probability of successfully completing the distributed program. Previous studies about optimal task allocation with respect to DSR focused on the effects of the inter-connectivity of processors, the failure rates of the processors, and the failure rates of the communication links. We are the first to study the effects of module software reliabilities and module execution frequencies on the optimal task allocation. By viewing each module as a state in the Markov process, we build a task allocation decision model to maximize DSR for distributed systems with % reliable network. In this model, the DSR is derived from the module software reliabilities, the processor hardware reliabilities, the transition probabilities between modules, and the task allocation matrix. Resource constraints of memory space limitation and computation load limitation on each processor are considered. The constraint of total system cost, including the execution cost, the communication cost, and the failure cost, is also considered. We solve the problem by Constraint Programming using the ILOG SOLVER library. We then apply the proposed model to a case extended from previous studies. Finally, a sensitivity analysis is performed to verify the effects of module software reliabilities and processor hardware reliabilities on the DSR and on the task allocation decision. (C) ", Elsevier Ltd. All rights reserved.,"Jou, CC",COMPUTERS & INDUSTRIAL ENGINEERING,task allocation decision model distributed system reliability markov process constraint programming,10.1016/j.cie.2009.04.004
248,WOS:000243927200008,2007,Reliability-based multiobjective optimization for automotive crashworthiness and occupant safety,,"This paper presents a methodology for reliability-based multiobjective optimization of large-scale engineering systems. This methodology is applied to the vehicle crashworthiness design optimization for side impact, considering both structural crashworthiness and occupant safety, with structural weight and front door velocity under side impact as objectives. Uncertainty quantification is performed using two first order reliability method-based techniques: approximate moment approach and reliability index approach. Genetic algorithm-based multiobjective optimization software GDOT, developed in-house, is used to come up with an optimal pareto front in all cases. The technique employed in this study treats multiple objective functions separately without combining them in any form. It shows that the vehicle weight can be reduced significantly from the baseline design and at the same time reduce the door velocity. The obtained pareto front brings out useful inferences about optimal design regions. A decision-making criterion is subsequently invoked to select the ""best"" subset of solutions from the obtained nondominated pareto optimal solutions. The reliability, thus computed, is also checked with Monte Carlo simulations. The optimal solution indicated by knee point on the optimal pareto front is verified with LS-DYNA simulation results.",,"Sinha, K",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,reliability-based multiobjective optimization uncertainty quantification form nondominated points gdot pareto optimal solution knee point automotive crashworthiness occupant safety side impact monte carlo simulation,10.1007/s00158-006-0050-x
249,WOS:000246090900006,2007,Therapeutic drug monitoring of kidney transplant recipients using profiled support vector machines,NEURAL-NETWORKS CYCLOSPORINE TIME PREDICTION REGRESSION PHARMACOKINETICS DOSAGE MODEL,"This paper proposes a twofold approach for therapeutic drug monitoring (TDM) of kidney recipients using support vector machines (SVMs), for both predicting and detecting Cyclosporine A (CyA) blood concentrations. The final goal is to build useful, robust, and ultimately understandable models for individualizing the dosage of CyA. We compare SVMs with several neural network models, such as the multilayer perceptron (MLP), the Elman recurrent network, finite/infinite impulse response networks, and neural network ARMAX approaches. In addition, we present a profile-dependent SVM (PD-SVM), which incorporates a priori knowledge in both tasks. Models are compared numerically, statistically, and in the presence of additive noise. Data from  renal allograft recipients were used to develop the models. Patients followed a standard triple therapy, and CyA trough concentration was the dependent variable. The best results for the CyA blood concentration prediction were obtained using the PD-SVM (mean error of . ng/mL and root-mean-square error of . ng/mL in the validation set) and appeared to be more robust in the presence of additive noise. The proposed PD-SVM improved results from the standard SVM and MLP, specially significant (both numerical and statistically) in the one-against-all scheme. Finally, some clinical conclusions were obtained from sensitivity rankings of the models and distribution of support vectors. We conclude that the PD-SVM approach produces more accurate and robust models than do neural networks. Finally, a software tool for aiding medical decision-making including the prediction models is presented.",,"Camps-Valls, G|Soria-Olivas, E|Perez-Ruixo, JJ|Perez-Cruz, F|Artes-Rodriguez, A|Jimenez-Torres, NV",IEEE TRANSACTIONS ON SYSTEMS MAN AND CYBERNETICS PART C-APPLICATIONS AND REVIEWS,cyclosporine kidney transplantation neural networks sensitivity analysis support vector machines (svms) therapeutic drug monitoring (tdm),10.1109/TSMCC.2007.893279
250,WOS:000237378200026,2006,Simulation modeling of soil and plant nitrogen use in a potato cropping system in the humid and cool environment,FERTILIZER GROWTH DYNAMICS YIELD EFFICIENCY RATES,"With rising environmental concerns for current practices of fertilizer N management in the humid and cool areas, we simulated soil N dynamics and plant N use in the potato (Solanum tuberosum L.) cropping system using the software Stella. The objectives were to predict in-season N requirements by the potato crop, tuber yield, N uptake, N partitioning within root, leaf, stem and tuber, and N loss in the plant-soil system, and to examine the accuracy of using model predictions for N management in potato. The first-order linear and S-shaped growth processes were used in the simulation. The model was unidimensional and used a daily time step. Sensitivity analysis indicated that N inflow in the system was the key trait affecting potato N uptake and tuber yield. The model was validated by comparisons of the predictions with field study datasets at four sites conducted across Quebec, Canada. The simulated daily N uptake by the potato followed a S-growth pattern from the early vegetative stage to full bloom, and a plateau of N uptake appeared at late tuberization. The predicted maximum daily N uptake rate (. kg ha(-) day(-)) occurred at early bloom whereas the predicted maximum N transfer from stems and leaves to tubers (. kg ha(-) day(-)) occurred  weeks after the peak of N uptake. Simultaneously, high daily N uptake occurred when N concentrations in the root zone ranged between  and  kg ha(-). The predicted N uptake and potato tuber yield values were correlated to N inflows in the model (R- = .). The model estimated loss of N was % of the field measurements. Using model balancing the amounts of N needed by crops would lead to optimize plant growth and N use efficiency and to minimize N lost to the environment.", (c) 2006 Elsevier B.V. All rights reserved.,"Li, H|Parent, LE|Karam, A",AGRICULTURE ECOSYSTEMS & ENVIRONMENT,model prediction n balance n partitioning potato tuber yield simulation modeling,10.1016/j.agee.2006.01.013
251,WOS:000280656300004,2010,Simulation model for extended double-ended queueing,QUEUES IMPATIENCE CUSTOMERS,"The purpose of this paper is to extend traditional double-ended queuing models using a simulation approach. Traditional double-ended queuing models assume that one supply queue should satisfy one demand queue through instantaneous pairing. Inter-arrival time is assumed to follow an exponential distribution, with arrivals to the system assumed to occur just one at a time. However, this assumption is frequently violated in many real-world situations. The pairing or batch size can either be multiple or a random variable, and the pairing processing time can be greater than . Inter-arrival time may follow distributions other than exponential. In some cases bulk arrivals may come at the same time, and pairing is not always guaranteed. Because the analytical approach has enormous difficulties obtaining performance measures under these relaxed situations, a simulation approach for extended double-ended queueing processes is presented. This includes an algorithm to find state probabilities and a newly developed simulation procedure. Using this new procedure, sensitivity analyses of performance measures were performed using various input conditions implemented using ProModel and SimRumnner simulation software. A business case is studied to demonstrate the versatility of the proposed approaches. (c) ", Elsevier Ltd. All rights reserved.,"Kim, WK|Yoon, KP|Mendoza, G|Sedaghat, M",COMPUTERS & INDUSTRIAL ENGINEERING,double-ended queue simulation state probability optimization job placement agency,10.1016/j.cie.2010.04.002
252,WOS:000295716700008,2011,Uncertainty propagation or box propagation,INITIAL-VALUE PROBLEMS VALIDATED SOLUTIONS TAYLOR ODES,This paper discusses the use of recently developed techniques and software in the numerical propagation of uncertainties in initial coordinates and/or parameters for initial value problems. We present an approach based on several validated numerical integration techniques but focusing on the propagation of boxes. The procedure uses a multivariable high order Taylor series development of the solution of the system whose Taylor coefficients are calculated via extended automatic differentiation rules for all the basic operations. These techniques are implemented in the recent free-software TIDES. The classical two-body and Lorenz problems are chosen as examples to show the benefits of the approach. The results show that the solution of uncertainties can be approximated in an analytic form by means of a Taylor series and that these techniques can be extremely useful in different practical applications. (C) , Elsevier Ltd. All rights reserved.,"Barrio, R|Rodriguez, M|Abad, A|Serrano, S",MATHEMATICAL AND COMPUTER MODELLING,taylor series method automatic differentiation uncertainty propagation freeware software,10.1016/j.mcm.2011.06.036
253,WOS:000250159500009,2007,Simiyu River catchment parameterization using SWAT model,INFORMATION-SYSTEM APPROACH RAINFALL-RUNOFF MODELS SURFACE SOIL-MOISTURE MONSOON 90,"The paper presents advances in hydrologic modelling of the Simiyu River catchment using the soil and water assessment tool (SWAT). In this study, the SWAT model set-up and subsequent application to the catchment was based on high-resolution data such as land use from  in LandSat TM Satellite,  in Digital Elevation Model and Soil from Soil and Terrain Database for Southern Africa (SOTERSAF). The land use data were reclassified based on some ground truth maps using IDRISI Kilimanjaro software. The Soil data were also reclassified manually to represent different soil hydrologic groups, which are important for the SWAT model set-up and simulations. The SWAT application first involved analysis of parameter sensitivity, which was then used for model auto-calibration that followed hierarchy of sensitive model parameters. The analysis of sensitive parameters and auto-calibration was achieved by sensitivity analysis and auto-calibration options, which are new in the recent version of SWAT, SWAT . The paper discusses the results of sensitivity and auto -calibration analyses, and present optimum model parameters, which are important for operation and water/land management studies (e.g. rain-fed agriculture and erosion/sediment and pollutant transport) in the catchment using SWAT. The river discharge estimates from this and a previous study were compared so as to evaluate performances of the recent hydrologic simulations in the catchment. Results showed that surface water model parameters are the most sensitive and have more physical meaning especially CN (the most sensitive) and SOL-K. Simulation results showed more or less same estimate of river flow at Ndagalu gauging station. The model efficiencies (R-%) in this and in the pervious study during calibration and validation periods were, respectively, ., . and ., .. The low level of model performance achieved in these studies showed that other factors than the spatial land data are greatly important for improvement of flow estimation by SWAT in Simiyu.", (c) 2007 Published by Elsevier Ltd.,"Mulungu, DMM|Munishi, SE",PHYSICS AND CHEMISTRY OF THE EARTH,auto-calibration analysis high-resolution data parameter sensitivity analysis simiyu river catchment swat 2005,10.1016/j.pce.2007.07.053
254,WOS:000405513900006,2017,Bayesian inference of earthquake parameters from buoy data using a polynomial chaos-based surrogate,NUMERICAL TIDAL MODEL UNCERTAINTY QUANTIFICATION DIFFERENTIAL-EQUATIONS FRICTION COEFFICIENTS TSUNAMI SIMULATIONS,"This work addresses the estimation of the parameters of an earthquake model by the consequent tsunami, with an application to the Chile  event. We are particularly interested in the Bayesian inference of the location, the orientation, and the slip of an Okada-based model of the earthquake ocean floor displacement. The tsunami numerical model is based on the GeoClaw software while the observational data is provided by a single DARTa""c buoy. We propose in this paper a methodology based on polynomial chaos expansion to construct a surrogate model of the wave height at the buoy location. A correlated noise model is first proposed in order to represent the discrepancy between the computational model and the data. This step is necessary, as a classical independent Gaussian noise is shown to be unsuitable for modeling the error, and to prevent convergence of the Markov Chain Monte Carlo sampler. Second, the polynomial chaos model is subsequently improved to handle the variability of the arrival time of the wave, using a preconditioned non-intrusive spectral method. Finally, the construction of a reduced model dedicated to Bayesian inference is proposed. Numerical results are presented and discussed.",,"Giraldi, L|Le Maitre, OP|Mandli, KT|Dawson, CN|Hoteit, I|Knio, OM",COMPUTATIONAL GEOSCIENCES,uncertainty quantification bayesian inference polynomial chaos expansion noise model low-rank representation shallow water equation tsunami earthquake inversion,10.1007/s10596-017-9646-z
255,WOS:000283340800004,2010,A DECISION SUPPORT TOOL FOR IRRIGATION INFRASTRUCTURE INVESTMENTS,OPTIMAL ALLOCATION WATER-RESOURCES SYSTEM MODEL MANAGEMENT IMPACTS FARM,"Increasing water scarcity, climate change and pressure to provide water for environmental flows urge irrigators to be more efficient. In Australia, ongoing water reforms and most recently the National Water Security Plan offer incentives to irrigators to adjust their farming practices by adopting water-saving Irrigation infrastructures to match soil, crop and climatic conditions. Water Works is a decision support tool to facilitate irrigators to make long- and short-term irrigation infrastructure investment decisions at the farm level. It helps irrigators to improve the economic efficiency, water use efficiency and environmental performance of their farm businesses. Water Works has been tested, validated and accepted by the irrigation community and researchers in NSW, Australia. The interface of Water Works is user-friendly and flexible. The simulation and optimisation module in Water Works provides an opportunity to evaluate Infrastructure investment decisions to suit their seasonal or long-term water availability. The sensitivity analysis allows substantiation of the impact of major variables Net present value, internal rate of return, benefit cost ratio and payback period are used to analyse the costs and benefits of modern irrigation technology. Application of Water Works using a whole farm-level case study indicates its effectiveness in making long- and short-term investment decisions Water Works can be easily integrated into commercial software such as spreadsheets, GIS, real-time data acquisition and control systems to further enhance its usability. Water Works can also be used in regional development planning."," Copyright (C) 2009 John Wiley & Sons, Ltd.","Khan, S|Mushtaq, S|Chen, C",IRRIGATION AND DRAINAGE,decision support tool water management seasonal and long-term investment optimisation simulation benefit-cost analysis whole farm water trading water saving,10.1002/ird.501
256,WOS:000335707200019,2014,FReET: Software for the statistical and reliability analysis of engineering problems and FReET-D: Degradation module,LATIN HYPERCUBE SAMPLES REINFORCEMENT CORROSION INPUT VARIABLES CONCRETE SIZE SIMULATION CARBONATION DURABILITY FRACTURE MODEL,"The objective of the paper is to present methods and software for the efficient statistical, sensitivity and reliability assessment of engineering problems. Attention is given to small-sample techniques which have been developed for the analysis of computationally intensive problems. The paper shows the possibility of ""randomizing"" computationally intensive problems in the manner of the Monte Carlo type of simulation. In order to keep the number of required simulations at an acceptable level, Latin Hypercube Sampling is utilized. The technique is used for both random variables and random fields. Sensitivity analysis is based on non-parametric rank-order correlation coefficients. Statistical correlation is imposed by the stochastic optimization technique - simulated annealing. A hierarchical sampling approach has been developed for the extension of the sample size in Latin Hypercube Sampling, enabling the addition of simulations to a current sample set while maintaining the desired correlation structure. The paper continues with a brief description of the user-friendly implementation of the theory within FReET commercial multipurpose reliability software. FReET-D software is capable of performing degradation modeling, in which a large number of reinforced concrete degradation models can be utilized under the main FReET software engine. Some of the interesting applications of the software are referenced in the paper. (C) ", Elsevier Ltd. All rights reserved.,"Novak, D|Vorechovsky, M|Teply, B",ADVANCES IN ENGINEERING SOFTWARE,statistical analysis sensitivity reliability monte carlo simulation latin hypercube sampling simulated annealing random fields material degradation,10.1016/j.advengsoft.2013.06.011
257,WOS:000266225700018,2009,MVC2: A MATLAB graphical interface toolbox for second-order multivariate calibration,TRILINEAR DECOMPOSITION ALGORITHM PARALLEL FACTOR-ANALYSIS PARTIAL LEAST-SQUARES CURVE RESOLUTION ADVANTAGE BILINEARIZATION SENSITIVITY PREDICTION,"This work reports the release of Multivariate Calibration  (MVC), a MATLAB graphical interface toolbox for implementing several second-order multivariate calibration methodologies. The toolbox accepts a variety of input data formats, arranged in either matrices or vectors (i.e., unfolded matrices), and contained in ASCII files. It allows one to manually select working sensor regions and plot landscapes for selected samples. The development of each model and its subsequent application to unknown samples is straightforward. Prediction results are produced along with analytical figures of merit and standard concentration errors, as calculated by modern concepts of uncertainty propagation.", (c) 2009 Elsevier B.V. All rights reserved.,"Olivieri, AC|Wu, HL|Yu, RQ",CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS,second-order multivariate calibration matlab program graphical interface figures of merit,10.1016/j.chemolab.2009.02.005
258,WOS:000365335000031,2016,A toolbox using the stochastic optimization algorithm MIPT and ChemCAD for the systematic process retrofit of complex chemical processes,MULTIOBJECTIVE OPTIMIZATION EVOLUTIONARY ALGORITHMS FLOWSHEET OPTIMIZATION PROCESS SIMULATORS GENETIC ALGORITHM DESIGN METHODOLOGY PLANTS PERSPECTIVES INTEGRATION,"Global optimization techniques using powerful algorithms have led to a wide range of applications to increase the efficiency of chemical processes. Nevertheless, the performance for optimization of process models is limited by a certain complexity, especially accounting for existing processes (retrofit). Due to the great combinatorial diversity of possible alternatives a systematic approach is essential. The local integration of modifications in the overall process leads to changes in internal streams. Therefore new operating points have to be found and resulting effects on the plant performance have to be evaluated. An optimization framework for the purpose of retrofitting using a rigorous process simulation tool is proposed to fulfill this task. Here, flowsheet simulation software packages are offering a high performance for the prediction of new operation points for following units, and for units affected by recycle streams. An optimization approach using flowsheet simulation software and the stochastic optimization algorithm Molecular-Inspired Parallel Tempering (MIPT) implemented in the programming software Matlab (TM) is presented. Both of these programs are linked via OPC (OLE for process control), a standard communication platform. The toolbox provides a quick evaluation of the process by searching for the global optimum. The MIPT algorithm is suitable for large optimization problems and can handle constraints and infeasibilities. The usage of a rigorous process simulator is providing a high accuracy of the thermodynamic results which is necessary to evaluate the influence of the new process design. Furthermore, a simulation model of the industrial plant can directly be used for the optimization. A complex multicomponent separation process with recycle streams is used to demonstrate the advantages of the proposed toolbox. To simplify the user input a graphical user interface was programmed. The results of a sensitivity analysis and the optimization for different feed compositions are presented. (C) ", Elsevier Ltd. All rights reserved.,"Otte, D|Lorenz, HM|Repke, JU",COMPUTERS & CHEMICAL ENGINEERING,optimization molecular-inspired parallel tempering multicomponent separation process retrofit stochastic algorithm chemcad (tm),10.1016/j.compchemeng.2015.08.023
259,WOS:000378954000023,2016,SEVIRI PrePro: A novel software tool for the pre-processing of SEVIRI geostationary orbit EO data products,ECOSYSTEMS,"The Spinning Enhanced Visible and. Infrared Imager (SEVIRI) is a geostationary orbit multispectral sensor on-board the Meteosat second Generation (MSG) platform, acquiring Earth Observation (EO) data over Earth's land surface from the optical to infrared parts of electromagnetic spectrum every  min. From the sensor a series of operational products are also provided to the user's community at no cost via EUMETSAT or LSA SAF portals. Herein, an open access stand-alone software product developed in Java programming language is presented for automating key pre-processing steps to all the SEVIRI operationally distributed products. The software tool, named Seviri PrePro, makes use of present day multi-core processors and is able to process very large datasets in a short time period, making it appropriate as well for use in a High Performance Computing (HPC) environment. The practical usefulness of the toolkit is also demonstrated herein using as a case study the SEVIRI evapotranspiration (ET) product. The development of SEVIRI PrePro is of significant importance to the SEVIRI users' community and is also very timely given that, to our knowledge, no similar software tool is freely distributed at present. Its use is anticipated to make a significant contribution to a large number of practical applications requiring use of SEVIRI data, including but not limited, weather forecasting and global climate monitoring at a range of geographical scales. (C) ", Elsevier Ltd. All rights reserved.,"Petropoulos, GP|Anagnostopoulos, V",ENVIRONMENTAL MODELLING & SOFTWARE,earth observation seviri pre-processing operational products software tool,10.1016/j.envsoft.2016.03.015
260,WOS:000278195100021,2010,The IAEA DEEP desalination economic model: A critical review,COSTS POWER,"The IAEA DEEP software has been used worldwide for the economic evaluation of desalination plants (thermal or electrical) coupled with various energy sources (nuclear, fossil fueled or renewable). Throughout the years, the software was updated constantly. Such updates included the user interface and model structure but not the economic models. Previous continuous development was culminated in the development of the DEEP . version which has been recently released in . This paper presents a step forwards in the continuous effort to maintain high standards and reliability of DEEP. It also scrutinizes methods used, assumptions made, and constants or default values originally used. The validity of calculations as well as the identification of the most important parameters is presented. Sensitivity analysis is used to identify the most important parameters in the DEEP model. Overall, the review proves that both the DEEP economic model and software implementation are solid for economic evaluation of dual purpose plants. Based on results presented and recommendations made, a new version of DEEP is expected to be released in  which will address minor issues and improvements.", (C) 2010 Elsevier B.V. All rights reserved.,"Kavvadias, KC|Khamis, I",DESALINATION,nuclear seawater desalination cogeneration desalination economic evaluation program (deep) sensitivity analysis,10.1016/j.desal.2010.02.032
261,WOS:000327903400013,2013,Addressing ten questions about conceptual rainfall-runoff models with global sensitivity analyses in R,MULTIOBJECTIVE CALIBRATION ENVIRONMENTAL-MODEL CATCHMENT MODEL CLIMATE-CHANGE UNCERTAINTY PERFORMANCE INDEXES PARAMETERS AUSTRALIA HYDROLOGY,"Sensitivity analysis (SA) is generally recognized as a worthwhile step to diagnose and remedy difficulties in identifying model parameters, and indeed in discriminating between model structures. An analysis of papers in three journals indicates that SA is a standard omission in hydrological modeling exercises. We provide some answers to ten reasonably generic questions using the Morris and Sobol SA methods, including to what extent sensitivities are dependent on parameter ranges selected, length of data period, catchment response type, model structures assumed and climatic forcing. Results presented demonstrate the sensitivity of four target functions to parameter variations of four rainfall runoff models of varying complexity (- parameters). Daily rainfall, streamflow and pan evaporation data are used from four -year data sets and from five catchments in the Australian Capital Territory (ACT) region. Similar results are obtained using the Morris and Sobol methods. It is shown how modelers can easily identify parameters that are insensitive, and how they might improve identifiability. Using a more complex objective function, however, may not result in all parameters becoming sensitive. Crucially, the results of the SA can be influenced by the parameter ranges selected. The length of data period required to characterize the sensitivities assuredly is a minimum of five years. The results confirm that only the simpler models have well-identified parameters, but parameter sensitivities vary between catchments. Answering these ten questions in other case studies is relatively easy using freely available software with the Hydromad and Sensitivity packages in R.", (C) 2013 Elsevier B.V. All rights reserved.,"Shin, MJ|Guillaume, JHA|Croke, BFW|Jakeman, AJ",JOURNAL OF HYDROLOGY,sensitivity analysis rainfall-runoff model identifiability,10.1016/j.jhydrol.2013.08.047
262,WOS:000382746100002,2016,Pressure drop across wire mesh demister in desalination plants using Eulerian-Eulerian modeling and computational fluid dynamics simulation,CURVED VANE DEMISTERS SEPARATION EFFICIENCY MIST ELIMINATOR FLASH DESALINATION HEAT-TRANSFER FLOW PERFORMANCE CFD CONDENSERS DESIGN,"This study focuses on the development of design correlation for pressure drop in wire mesh demisters, used in the multistage flash desalination process (MSF) as well as similar evaporation and flashing units found in other industrial processes. Development of the correlation is based on numerical simulation of the demister using steady-state and two-dimensional model for the flow of vapor and brine droplets through the demister. An Eulerian model was used to model the system and the resulting model equations were solved using a commercial computational fluid dynamics software (FLUENT). The system model was formed of three zones, which include the vapor space above and below the demister and the demister. In addition, the demister was approximated as a porous media. A sensitivity analysis of the model revealed that vapor velocity, demister packing density and height, and the inlet flashed-off vapor composition are the main parameters that affect demister performance. Consequently, numerical data were used to correlate pressure drop across the demister as a function of operating and design parameters. The developed correlation was validated using data from real MSF plants. Analysis indicated that the correlation predictions and experimental data were consistent and showed good agreement with an error less than %.",,"Al-Rabiah, D|Al-Fulaij, H|Ettouney, H",DESALINATION AND WATER TREATMENT,desalination eulerian modeling multistage flashing cfd demister,10.1080/19443994.2015.1102774
263,WOS:000378854600008,2016,"Towards the integration of process design, control and scheduling: Are we getting closer?",MODEL-PREDICTIVE CONTROL INTEGER DYNAMIC OPTIMIZATION CONSTRAINED LINEAR-SYSTEMS OPTIMAL GRADE TRANSITION CHEMICAL-PROCESSES BATCH PROCESSES UNCERTAINTY FRAMEWORK FLEXIBILITY PARAMETERS,"The integration of design and control, control and scheduling and design, control and scheduling, all have been core PSE challenges. While significant progress has been achieved over the years, it is fair to say that at the moment there is not a generally accepted methodology and/or ""protocol"" for such an integration - it is also interesting to note that currently, there is not a commercially available software [or even in a prototype form] system to fully support such an activity. Here, we present the foundations for such an integrated framework and especially a software platform that enables such integration based on research developments over the last  years. In particular, we describe PAROC, a prototype software system which allows for the representation, modeling and solution of integrated design, scheduling and control problems. Its main features include: (i) a high-fidelity dynamic model representation, also involving global sensitivity analysis, parameter estimation and mixed integer dynamic optimization capabilities; (ii) a suite/toolbox of model approximation methods; (iii) a host of multi-parametric programming solvers for mixed continuous/integer problems; (iv) a state-space modeling representation capability for scheduling and control problems; and (v) an advanced toolkit for multi-parametric/explicit Model Predictive Control and moving horizon reactive scheduling problems. Algorithms that enable the integration capabilities of the systems for design, scheduling and control are presented on a case of a series of cogeneration units. (C) ", Elsevier Ltd. All rights reserved.,"Pistikopoulos, EN|Diangelakis, NA",COMPUTERS & CHEMICAL ENGINEERING,multi-parametric receding horizon policies control design optimization optimal scheduling integration,10.1016/j.compchemeng.2015.11.002
264,WOS:000303035400007,2012,Comparison of different uncertainty techniques in urban stormwater quantity and quality modelling,FORMAL BAYESIAN METHOD SENSITIVITY-ANALYSIS STREAMFLOW SIMULATION HYDRAULIC-PROPERTIES GLUE APPROACH WATER OPTIMIZATION CALIBRATION PARAMETER QUANTIFICATION,"Urban drainage models are important tools used by both practitioners and scientists in the field of stormwater management. These models are often conceptual and usually require calibration using local datasets. The quantification of the uncertainty associated with the models is a must, although it is rarely practiced. The International Working Group on Data and Models, which works under the IWA/IAHR Joint Committee on Urban Drainage, has been working on the development of a framework for defining and assessing uncertainties in the field of urban drainage modelling. A part of that work is the assessment and comparison of different techniques generally used in the uncertainty assessment of the parameters of water models. This paper compares a number of these techniques: the Generalized Likelihood Uncertainty Estimation (GLUE), the Shuffled Complex Evolution Metropolis algorithm (SCEM-UA), an approach based on a multi-objective auto-calibration (a multialgorithm, genetically adaptive multi-objective method, AMALGAM) and a Bayesian approach based on a simplified Markov Chain Monte Carlo method (implemented in the software MICA). To allow a meaningful comparison among the different uncertainty techniques, common criteria have been set for the likelihood formulation, defining the number of simulations, and the measure of uncertainty bounds. Moreover, all the uncertainty techniques were implemented for the same case study, in which the same stormwater quantity and quality model was used alongside the same dataset. The comparison results for a well-posed rainfall/runoff model showed that the four methods provide similar probability distributions of model parameters, and model prediction intervals. For ill-posed water quality model the differences between the results were much wider; and the paper provides the specific advantages and disadvantages of each method. In relation to computational efficiency (i.e. number of iterations required to generate the probability distribution of parameters), it was found that SCEM-UA and AMALGAM produce results quicker than GLUE in terms of required number of simulations. However, GLUE requires the lowest modelling skills and is easy to implement. All non-Bayesian methods have problems with the way they accept behavioural parameter sets, e.g. GLUE, SCEM-UA and AMALGAM have subjective acceptance thresholds, while MICA has usually problem with its hypothesis on normality of residuals. It is concluded that modellers should select the method which is most suitable for the system they are modelling (e.g. complexity of the model's structure including the number of parameters), their skill/knowledge level, the available information, and the purpose of their study. (C) ", Elsevier Ltd. All rights reserved.,"Dotto, CBS|Mannina, G|Kleidorfer, M|Vezzaro, L|Henrichs, M|McCarthy, DT|Freni, G|Rauch, W|Deletic, A",WATER RESEARCH,urban drainage models uncertainties parameter probability distributions bayesian inference glue scem-ua mica amalgam mcmc multi-objective auto-calibration,10.1016/j.watres.2012.02.009
265,WOS:000369512800012,2016,Integration of a Three-Dimensional Process-Based Hydrological Model into the Object Modeling System,JGRASS-NEWAGE SYSTEM DISTRIBUTED MODEL FRAMEWORK ENERGY BUDGETS BASIN WATER TIME FLOW TERRAIN,"The integration of a spatial process model into an environmental modeling framework can enhance the model's capabilities. This paper describes a general methodology for integrating environmental models into the Object Modeling System (OMS) regardless of the model's complexity, the programming language, and the operating system used. We present the integration of the GEOtop model into the OMS version . and illustrate its application in a small watershed. OMS is an environmental modeling framework that facilitates model development, calibration, evaluation, and maintenance. It provides innovative techniques in software design such as multithreading, implicit parallelism, calibration and sensitivity analysis algorithms, and cloud-services. GEOtop is a physically based, spatially distributed rainfall-runoff model that performs three-dimensional finite volume calculations of water and energy budgets. Executing GEOtop as an OMS model component allows it to: () interact directly with the open-source geographical information system (GIS) uDig-JGrass to access geo-processing, visualization, and other modeling components; and () use OMS components for automatic calibration, sensitivity analysis, or meteorological data interpolation. A case study of the model in a semi-arid agricultural catchment is presented for illustration and proof-of-concept. Simulated soil water content and soil temperature results are compared with measured data, and model performance is evaluated using goodness-of-fit indices. This study serves as a template for future integration of process models into OMS.",,"Formetta, G|Capparelli, G|David, O|Green, TR|Rigon, R",WATER,watershed model environmental modeling framework automatic calibration software integration,10.3390/w8010012
266,WOS:000406818500009,2017,Estimates of plant density of wheat crops at emergence from very low altitude UAV imagery,AERIAL VEHICLE UAV WINTER-WHEAT PRECISION AGRICULTURE SPACING MEASUREMENT VEGETATION INDEXES ROW VISION RECONSTRUCTION POPULATION SYSTEMS,"Plant density is useful variable that determines the fate of the wheat crop. The most commonly used method for plant density quantification is based on visual counting from ground level. The objective of this study is to develop and evaluate a method for estimating wheat plant density at the emergence stage based on high resolution imagery taken from UAV at very low altitude with application to high throughput phenotyping in field conditions. A Sony ILCE alpha L RGB camera with  Mpixels and equipped with a  mm focal length lens was flying aboard an hexacopter at  to  m altitude at about  m/s speed. This allows getting ground resolution between . mm to . mm, while providing -% overlap between images. The camera was looking with  degrees zenith angle in a compass direction perpendicular to the row direction to maximize the cross section viewed of the plants and minimize the effect of the wind created by the rotors. Agisoft photoscan software was then used to derive the position of the cameras for each image. Images were then projected on the ground surface to finally extract subsamples used to estimate the plant density. The extracted images were first classified to separate the green pixels from the background and the rows were then identified and extracted. Finally, image object (group of connected green pixels) was identified on each row and the number of plants they contain was estimated using a Support Vector Machine whose training was optimized using a Particle Swarm Optimization. Three experiments were conducted in Greoux, Avignon and Clermont sites with some variability in the sowing dates, densities, genotypes, flight altitude, and growth stage at the time of the image acquisition. The application of the method on the  samples available over the three sites provides a RMSE and relative RMSE on estimates of . plants/m() and .% with a bias of . plants/m(). However, differences in performances were observed between the three sites, mostly related to the growth stage at the time of the flight. Plants should have between one to two leaves when images are taken. Further, a specific sensitivity analysis shows that the ground resolution of the images should be better than . mm. Finally, the repeatability of the method is good especially when images are taken from similar observational geometries. The current limits and possible improvements of the method proposed are finally discussed.", (C) 2017 Elsevier Inc. All rights reserved.,"Jin, XL|Liu, SY|Baret, F|Hemerle, M|Comar, A",REMOTE SENSING OF ENVIRONMENT,plant density unmanned aerial vehicle computer vision algorithm particle swarm optimization (pso)-support vector machine (svm) winter wheat,10.1016/j.rse.2017.06.007
267,WOS:000303082000011,2012,An environmental and economic analysis for geotube coastal structures retaining dredge material,EROSION,"This paper investigates the environmental and economic sensitivity of coastal structures for two different construction methods: a traditional rubble mound structure and a geotube coastal structure using dredged material. The analysis is undertaken for two projects: a small scale coastal protection project using a revetment and a medium size capital harbour expansion using a breakwater. This work provides further insight into previously published work by Sheehan et al. () on the economic aspects of geotube technology and identifies the optimum method of construction for each type of coastal structure. An economic sensitivity analysis is undertaken on the key logistical parameters involved in the construction of these coastal structures. An environmental sensitivity analysis focuses on the CO emissions produced from the construction of the coastal structures for both construction methods. These sensitivity analyses are undertaken using a decision support software program (DMMAP), developed to assist users at the planning stages of a project to achieve sustainable dredge material management. The key logistical parameters are analysed to generate environmental and economic ranking tables. The analyses highlight that the size of the structure and the distance to the source of the quarry material are crucial factors in determining the optimum construction method. This work shows that geotubes are a viable alternative to traditional rubble mound coastal structures. It also shows that traditional construction methods may be more economical than geotube structures when considering small coastal structures. In general, the larger the scale of the project the greater the potential savings in CO emissions and cost that can be achieved through the use of geotube technology. Geotubes, with the use of dredge material, may provide a sustainable beneficial use for dredge material and offer a serious economic and environmental alternative to traditional rubble mound structures.", (C) 2012 Elsevier B.V. All rights reserved.,"Sheehan, C|Harrington, J",RESOURCES CONSERVATION AND RECYCLING,geotube revetment breakwater coastal structures dredging beneficial use,10.1016/j.resconrec.2012.01.011
268,WOS:000356347800059,2015,Integrated regional ecological risk assessment of multi-ecosystems under multi-disasters: a case study of China,SAMPLING-BASED METHODS SENSITIVITY-ANALYSIS UNCERTAINTY MODEL,"Using China as a case study, this paper explores the integrated regional ecological risk assessment of multiple stressors and multiple receptors on a large spatial scale. The objective is to provide scientific data to support ecological risk identification and prevention. To carry out this assessment, ten natural disasters were chosen as risk sources, and twenty-two ecosystems were chosen as risk receptors. The vulnerability of environment where these ecosystems existed was taken into consideration. Using the software platform GIS, the ecological risk of each disaster was evaluated, the integrated assessment for all disasters was compiled, and the integrated risk of different ecosystems was obtained. All results were shown in assessment maps. The results show that forty-five percent of the ecosystems' areas in China face high or medium ecological risks. This result indicates that the establishment of ecosystem protection and ecological risk prevention mechanisms in China is still a long-term, difficult task, requiring the rational use and conservation of forests, meadows, farmland, wetlands, and other ecosystems alike is of great necessity. The uncertainty analysis of risk assessment using the Monte Carlo Simulation method demonstrated the results to be reliable and credible.",,"Xu, XG|Xu, LF|Yan, L|Ma, LY|Lu, YL",ENVIRONMENTAL EARTH SCIENCES,ecological risk integrated assessment natural disaster uncertainty analysis china,10.1007/s12665-015-4079-2
269,WOS:000260920100004,2008,ON THE SENSITIVITY OF DESIRABILITY FUNCTIONS FOR MULTIRESPONSE OPTIMIZATION,,"Desirability functions have been one of the most important multiresponse optimization technique since the early eighties. Main reasons for this popularity might be counted as the convenience of the implementation of the method and it's availability in many experimental design software packages. Technique itself involves somehow subjective parameters such as the importance coefficients between response characteristics that are used to calculate overall desirability, weights used in determining the shape of each individual response and the size of the specification band of the response. However, the impact of these sensitive parameters on the solution set is mostly uninvestigated. This paper proposes a procedure to analyze the sensitivity of the important characteristic parameters of desirability functions and their impact on pareto-optimal solution set. The proposed procedure uses the experimental design tools on the solution space and estimates a prediction equation on the overall desirability to identify the sensitive parameters. For illustration, a classical desirability example is selected from the literature and results are given along with the discussion.",,"Aksezer, CS",JOURNAL OF INDUSTRIAL AND MANAGEMENT OPTIMIZATION,desirability functions parametric sensitivity analysis multiresponse optimization,10.3934/jimo.2008.4.685
270,WOS:000299324200005,2012,Yield improvement analysis with parameter-screening factorials,SENSITIVITY-ANALYSIS SYSTEM MODEL,"This paper presents a technique for the critical parameter analysis of the disk drive manufacturing process. The objective of the work is to improve the manufacturing yield by tuning the parameters that significantly affect the yield. Several techniques were studied including the sensitivity analysis framework, which is currently used at several disk drive plants. From our initial experiments, we found that the sensitivity analysis results were not sufficiently good and the interactions between parameters were not identified. We then designed a new technique based on factorial designs, the parameter-screening factorials algorithm. Our method can work with a large number of inputs within reasonable computing time, and can identify both the parameter and the interaction effects. The results can be obtained more quickly and are better in comparison with the currently used technique. Moreover, by applying the technique to the full list instead of the pre-selected list of the manufacturing parameters, we discovered that the parameters watch list previously identified by the experts should be adjusted to include some extra parameters. After the results were validated by the experts, we designed software that automates the critical parameter analysis process. The software should greatly benefit the daily yield analysis at the disk drive manufacturing plant greatly.", (C) 2011 Elsevier B.V. All rights reserved.,"Yamwong, W|Achalakul, T",APPLIED SOFT COMPUTING,critical parameter identification yield improvement analysis factorial designs,10.1016/j.asoc.2011.11.021
271,WOS:000295845900012,2011,Groundwater drawdown at Nankou site of Beijing Plain: model development and calibration,FLOW,"Water shortage and groundwater pollution have become two primary environmental concerns to Beijing since the s. The local aquifers, as the dominant sources for domestic and agricultural water supply, are depleting due to groundwater abstraction and continuous drought in recent years with rapid urbanization and increasing water consumption. Therefore, understanding the hydrogeological system is fundamental for a sustainable water resources management. In this article, the numerical analysis of a -D regional groundwater flow model for the Nankou area is presented. The hydrogeological system is reproduced according to sparsely distributed boreholes data. The numerical analysis is carried out using the scientific software OpenGeoSys, which is based on the finite element method. The model calibration and sensitivity analysis are accomplished with inverse methods by applying a model independent parameter estimation system (PEST). The results of the calibrated model show reasonable agreements with observed water levels. The transient groundwater flow simulations reflect the observed drawdown of the last  years and show the formation of a depression cone in an intensively pumped area.",,"Sun, F|Shao, HB|Kalbacher, T|Wang, WQ|Yang, ZS|Huang, ZF|Kolditz, O",ENVIRONMENTAL EARTH SCIENCES,groundwater modeling opengeosys pest nankou,10.1007/s12665-011-0957-4
272,WOS:000242724500019,2006,Uncertainty analysis for regional-scale reserve selection,SITE SELECTION SPECIES DISTRIBUTION CONSERVATION DESIGN BIODIVERSITY NETWORKS MODELS PERSISTENCE PROBABILITIES CONNECTIVITY,"Methods for reserve selection and conservation planning often ignore uncertainty. For example, presence-absence observations and predictions of habitat models are used as inputs but commonly assumed to be without error We applied information-gap decision theory to develop uncertainty analysis methods for reserve selection. Our proposed method seeks a solution that is robust in achieving a given conservation target, despite uncertainty in the data. We maximized robustness in reserve selection through a novel method, ""distribution discounting,"" in which the site- and species-specific measure of conservation value (related to species-specific occupancy probabilities) was penalized by an error measure (in our study, related to accuracy of statistical prediction). Because distribution discounting can be implemented as a modification of input files, it is a computationally efficient solution for implementing uncertainty analysis into reserve selection. Thus, the method is particularly useful for high-dimensional decision problems characteristic of regional conservation assessment. We implemented distribution discounting in the zonation reserve-selection algorithm that produces a hierarchy of conservation priorities throughout the landscape. We applied it to reserve selection for seven priority fauna in a landscape in New South Wales, Australia. The distribution discounting method can be easily adapted for use with different kinds of data (e.g., probability of occurrence or abundance) and different landscape descriptions (grid or patch based) and incorporated into other reserve-selection algorithms and software.",,"Moilanen, A|Wintle, BA|Elith, J|Burgman, M",CONSERVATION BIOLOGY,distribution discounting distribution smoothing information-gap decision theory reserve-network design site-selection algorithm spatial reserve design zonation,10.1111/j.1523-1739.2006.00560.x
273,WOS:000318057900004,2013,A long-term sensitivity analysis of the denitrification and decomposition model,NITROUS-OXIDE EMISSIONS DRAINED ILLINOIS AGROECOSYSTEMS GREENHOUSE-GAS EMISSIONS SOIL ORGANIC-CARBON DNDC MODEL AGRICULTURAL SOILS N2O EMISSIONS COMPUTATIONAL EXPERIMENTS MECHANISTIC MODEL RAINFALL EVENTS,"Although sensitivity analysis (SA) was conducted on the DeNitrification-DeComposition (DNDC) model, a global SA over a long period of time is lacking. We used a method of Bayesian analysis of computer code outputs (BACCO) with the Gaussian emulation machine for sensitivity analysis software (GEM-SA) to conduct a long-term SA of DNDC for predicting the annual change of soil organic carbon (dSOC), nitrous oxide emission (NO) and grain yield of spring wheat. Twenty seven non-weather input parameters with wide ranges were selected for SA using weather data recorded from Three Hills, Alberta over  years (-). The SA had two steps: ) a preliminary BACCO GEM-SA was conducted to identify a more accurate emulator sampling method and to screen out parameters with insignificant influence on model outcomes; and ) final BACCO GEM-SA was conducted with optimal input design set for emulator training runs varying only the significant input parameters. Results indicated that the Maximin Latin Hypercube sampling method outperformed the LP-x method with higher emulator accuracy. Most of the  input parameters contributed little to the three outputs by the first step BACCO GEM-SA. In the second step of BACCO GEM-SA there were only three (in the case of dSOC) and six (in the cases of NO and yield) input parameters whose influence contributed to more than % of the total output variances by their total effects. Among the selected parameters, initial soil organic carbon and clay content are very important and were important in determining results for all three outputs. Sensitivities of some parameters, such as clay content and urea fertilizer amount changed dramatically over the years. This indicates that a single year SA may overestimate or underestimate a long-term parameter effect on the model prediction. The two-step procedure with the BACCO GEM-SA method improved the accuracy of SA and provided important information for model validation and parameterization.", Crown Copyright (C) 2013 Published by Elsevier Ltd. All rights reserved.,"Qin, XB|Wang, H|Li, YE|Li, Y|McConkey, B|Lemke, R|Li, CS|Brandt, K|Gao, QZ|Wan, YF|Liu, S|Liu, YT|Xu, C",ENVIRONMENTAL MODELLING & SOFTWARE,dndc long-term global sensitivity analysis bacco gem-sa,10.1016/j.envsoft.2013.01.005
274,WOS:000266261300006,2009,Traffic Parameters Estimation to Predict Road Side Pollutant Concentrations using Neural Networks,NO2 CONCENTRATIONS MODELS STREET AIR,"The analysis aims to evaluate which is the most important among traffic parameters (flows, queues length, occupancy degree, and travel time) to forecast CO and CH concentrations. The study area was identified by Notarbartolo Road and bounded by LibertA Street and Sciuti Street in the urban area of Palermo in Southern Italy. In this area, various loop detectors and one pollution-monitoring site were located. Traffic data related to the pollution-monitoring site immediately near the road link were estimated by Simulation of Urban MObility (SUMO) traffic microsimulator software using as input the flows measured by loop detectors on other links of road network. Traffic and weather data were used as input variables to predict pollutant concentrations by using neural networks. Finally, after a sensitivity analysis, it was showed that queues length were the mostly correlated traffic parameters to pollutant concentrations.",,"Galatioto, F|Zito, P",ENVIRONMENTAL MODELING & ASSESSMENT,microsimulator traffic parameters neural networks pollutant concentrations,10.1007/s10666-007-9129-z
275,WOS:000368869200001,2016,A GUI platform for uncertainty quantification of complex dynamical models,RAINFALL-RUNOFF MODELS GLOBAL SENSITIVITY MEASURES AUTOMATIC CALIBRATION OPTIMIZATION INDEXES DESIGN MACHINE OUTPUT,"Uncertainty quantification (UQ) refers to quantitative characterization and reduction of uncertainties present in computer model simulations. It is widely used in engineering and geophysics fields to assess and predict the likelihood of various outcomes. This paper describes a UQ platform called UQ-PyL (Uncertainty Quantification Python Laboratory), a flexible software platform designed to quantify uncertainty of complex dynamical models. UQ-PyL integrates different kinds of UQ methods, including experimental design, statistical analysis, sensitivity analysis, surrogate modeling and parameter optimization. It is written in Python language and runs on all common operating systems. UQ-PyL has a graphical user interface that allows users to enter commands via pull-down menus. It is equipped with a model driver generator that allows any computer model to be linked with the software. We illustrate the different functions of UQ-PyL by applying it to the uncertainty analysis of the Sacramento Soil Moisture Accounting Model. We will also demonstrate that UQ-PyL can be applied to a wide range of applications. (C)  The Authors.", Published by Elsevier Ltd.,"Wang, C|Duan, QY|Tong, CH|Di, ZH|Gong, W",ENVIRONMENTAL MODELLING & SOFTWARE,uncertainty quantification design of experiments sensitivity analysis surrogate modeling parameter optimization uq-pyl,10.1016/j.envsoft.2015.11.004
276,WOS:000358627500004,2015,"Individual-based modeling of soil organic matter in NetLogo: Transparent, user-friendly, and open",SENSITIVITY-ANALYSIS NITROGEN DYNAMICS CARBON MICROBIOLOGY MINERALIZATION SIMULATION COMPLEX PARAMETERIZATION POPULATIONS COMPONENTS,"Soil organic matter dynamics are essential for terrestrial ecosystem functions as they affect biogeochemical cycles and, thus, the provision of plant nutrients or the release of greenhouse gases to the atmosphere. Most of the involved processes are driven by microorganisms. To investigate and understand these processes, individual-based models allow analyzing complex microbial systems' behavior based on rules and conditions for individual entities within these systems, taking into account local interactions and individual variations. Here, we present a streamlined, user-friendly and open version of the individual-based model INDISIM-SOM, which describes the mineralization of soil carbon and nitrogen. It was implemented in NetLogo, a widely used and easily accessible software platform especially designed for individual-based simulation models. Including powerful means to observe the model behavior and a standardized documentation, this increases INDISIM-SOM's range of potential uses and users, and facilitates the exchange among soil scientists as well as between different modeling approaches. (C) ", Elsevier Ltd. All rights reserved.,"Banitz, T|Gras, A|Ginovart, M",ENVIRONMENTAL MODELLING & SOFTWARE,individual-based model soil organic matter soil microorganisms mineralization nitrification netlogo,10.1016/j.envsoft.2015.05.007
277,WOS:000383298800002,2016,A software framework for probabilistic sensitivity analysis for computationally expensive models,STOCHASTIC PREDICTIONS POLYMERIC NANOCOMPOSITES CORRELATED PARAMETERS OPTIMIZATION SIMULATIONS UNCERTAINTY PROPAGATION VARIABLES INDEXES DESIGN,"We provide a sensitivity analysis toolbox consisting of a set of Matlab functions that offer utilities for quantifying the influence of uncertain input parameters on uncertain model outputs. It allows the determination of the key input parameters of an output of interest. The results are based on a probability density function (PDF) provided for the input parameters. The toolbox for uncertainty and sensitivity analysis methods consists of three ingredients: () sampling method, () surrogate models, () sensitivity analysis (SA) method. Numerical studies based on analytical functions associated with noise and industrial data are performed to prove the usefulness and effectiveness of this study. (C) ", Elsevier Ltd. All rights reserved.,"Vu-Bac, N|Lahmer, T|Zhuang, X|Nguyen-Thoi, T|Rabczuk, T",ADVANCES IN ENGINEERING SOFTWARE,uncertainty quantification random sampling penalized spline regression sensitivity analysis matlab toolbox,10.1016/j.advengsoft.2016.06.005
278,WOS:000405522600004,2017,Estimation of vertical water fluxes from temperature time series by the inverse numerical computer program FLUX-BOT,FLOW DIFFUSIVITY HEAT,"The application of heat as a hydrological tracer has become a standard method for quantifying water fluxes between groundwater and surface water. The typical application is to estimate vertical water fluxes in the shallow subsurface beneath streams or lakes. For this purpose, time series of temperatures in the surface water and in the sediment are measured and evaluated by a vertical D representation of heat transport by advection and conduction. Several analytical solutions exist to calculate the vertical water flux from the measured temperatures. Although analytical solutions can be easily implemented, they are restricted to specific boundary conditions such as a sinusoidal upper temperature boundary. Numerical solutions offer higher flexibility in the selection of the boundary conditions. This, in turn, reduces the effort of data preprocessing, such as the extraction of the diurnal temperature variation from the raw data. Here, we present software to estimate water fluxes based on temperaturesFLUX-BOT. FLUX-BOT is a numerical code written in MATLAB that calculates vertical water fluxes in saturated sediments based on the inversion of measured temperature time series observed at multiple depths. FLUX-BOT applies a centred Crank-Nicolson implicit finite difference scheme to solve the one-dimensional heat advection-conduction equation. FLUX-BOT includes functions for the inverse numerical routines, functions for visualizing the results, and a function for performing uncertainty analysis. We present applications of FLUX-BOT to synthetic and to real temperature data to demonstrate its performance.",,"Munz, M|Schmidt, C",HYDROLOGICAL PROCESSES,heat tracing numerical solution surface water groundwater interaction temperature time series vertical water flux,10.1002/hyp.11198
279,WOS:000340977000075,2014,Groundwater fluxes in a shallow seasonal wetland pond: The effect of bathymetric uncertainty on predicted water and solute balances,MASS-BALANCE DEPENDENT ECOSYSTEMS LAKES MODEL DISCHARGE STORAGE RN-222 VOLUME CALIBRATION AUSTRALIA,"The successful management of groundwater dependent shallow seasonal wetlands requires a sound understanding of groundwater fluxes. However, such fluxes are hard to quantify. Water volume and solute mass balance models can be used in order to derive an estimate of groundwater fluxes within such systems. This approach is particularly attractive, as it can be undertaken using measurable environmental variables, such as; rainfall, evaporation, pond level and salinity. Groundwater fluxes estimated from such an approach are subject to uncertainty in the measured variables as well as in the process representation and in parameters within the model. However, the shallow nature of seasonal wetland ponds means water volume and surface area can change rapidly and non-linearly with depth, requiring an accurate representation of the wetland pond bathymetry. Unfortunately, detailed bathymetry is rarely available and simplifying assumptions regarding the bathymetry have to be made. However, the implications of these assumptions are typically not quantified. We systematically quantify the uncertainty implications for eight different representations of wetland bathymetry for a shallow seasonal wetland pond in South Australia. The predictive uncertainty estimation methods provided in the Model-Independent Parameter Estimation and Uncertainty Analysis software (PEST) are used to quantify the effect of bathymetric uncertainty on the modelled fluxes. We demonstrate that bathymetry can be successfully represented within the model in a simple parametric form using a cubic Sexier curve, allowing an assessment of bathymetric uncertainty due to measurement error and survey detail on the derived groundwater fluxes compared with the fixed bathymetry models. Findings show that different bathymetry conceptualisations can result in very different mass balance components and hence process conceptualisations, despite equally good fits to observed data, potentially leading to poor management decisions for the wetlands. Model predictive uncertainty increases with the crudity of the bathymetry representation, however, approximations that capture the general shape of the wetland pond such as a power law or Bezier curve show only a small increase in prediction uncertainty compared to the full dGPS surveyed bathymetry, implying these may be sufficient for most modelling purposes.", (C) 2014 Elsevier B.V. All rights reserved.,"Trigg, MA|Cook, PG|Brunner, P",JOURNAL OF HYDROLOGY,wetland ponds bathymetry uncertainty pest solute balance bezier curve,10.1016/j.jhydrol.2014.06.020
280,WOS:000224375900007,2004,A tool for risk-based management of surface water quality,SENSITIVITY-ANALYSIS UNCERTAINTY MODELS EUTROPHICATION PREDICTION PHOSPHORUS SYSTEMS FUTURE,"Water quality Risk Analysis Tool (WaterRAT) is software for supporting decision-making in surface water quality management. The philosophy behind the software is that uncertainty in water quality model predictions is inevitably high due to model equation error, parameter error, and limited definition of boundary conditions and management objectives. Using sensitivity and uncertainty analyses based on Monte Carlo simulation and first order methods, WaterRAT allows the modeller to identify the significant uncertainties, and evaluate the degree to which they control decision-making risk. WaterRAT has a library of river and lake water quality models of varying complexity, and these can be applied at a wide range of temporal and spatial scales, allowing the model design to be responsive to both the modelling task and the data constraints. (C) ", Elsevier Ltd. All rights reserved.,"McIntyre, NR|Wheater, HS",ENVIRONMENTAL MODELLING & SOFTWARE,water quality uncertainty risk decision-making,10.1016/j.envsoft.2003.12.003
281,WOS:000245438200012,2007,The boundary-quality penalty: a quantitative method for approximating species responses to fragmentation in reserve selection,UNCERTAINTY ANALYSIS SITE SELECTION DESIGN NETWORKS BIODIVERSITY LANDSCAPE MODELS CONSERVATION PERSISTENCE PROBABILITIES,"Aggregation of reserve networks is generally considered desirable for biological and economic reasons: aggregation reduces negative edge effects and facilitates metapopulation dynamics, which plausibly leads to improved persistence of species. Economically, aggregated networks are less expensive to manage than fragmented ones. Therefore, many reserve-design methods use qualitative heuristics, such as distance-based criteria or boundary-length penalties to induce reserve aggregation. We devised a quantitative method that introduces aggregation into reserve networks. We call the method the boundary-quality penalty (BQP) because the biological value of a land unit (grid cell) is penalized when the unit occurs close enough to the edge of a reserve such that a fragmentation or edge effect would reduce population densities in the reserved cell. The BQP can be estimated for any habitat model that includes neighborhood (connectivity) effects, and it can be introduced into reserve selection software in a standardized manner We used the BQP in a reserve-design case study of the Hunter Valley of southeastern Australia. The BQP resulted in a more highly aggregated reserve network structure. The degree of aggregation required was specified by observed (albeit modeled) biological responses to fragmentation. Estimating the effects of fragmentation on individual species and incorporating estimated effects in the objective function of reserve-selection algorithms is a coherent and defensible way to select aggregated reserves. We implemented the BQP in the context of the Zonation method, but it could as well be implemented into any other spatially explicit reserve-planning framework.",,"Moilanen, A|Wintle, BA",CONSERVATION BIOLOGY,boundary length connectivity edge effect fragmentation habitat model model averaging reserve selection site-selection algorithm zonation,10.1111/j.1523-1739.2006.00625.x
282,WOS:000362135900019,2015,Evaluation and comparison of open source program solutions for automatic seed counting on digital images,SOFTWARE SHAPE IDENTIFICATION YIELD SIZE,"Seed number quantification is an essential agronomic parameter conducted mostly manually or by mechanical counters, both with obvious limitations. Digital image analysis provides a reliable and robust alternative to accurately calculate many biological features. This study presents and evaluates the performance of four open-source image-analysis programs i.e. ImageJ, CellProfiler, P-TRAP and SmartGrain to count crop seeds from digital images captured by camera and scanner. It also evaluates ImageJ program for automated seed counting using macro containing RenyiEntropy threshold algorithm. Digital images of cereal crop seeds were acquired i.e. wheat, barley, maize, rye, oat, sorghum, triticale and rice. All images contained  seeds per image present in an area of approx.  cm(). RenyiEntropy threshold increased the seed count accuracy of ImageJ from digital camera images. Generally, seed counts from digital camera images of all crops were accurate, but software-crop combination had significant (p < .) difference from reference value. Among image analysis programs, ImageJ produced mostly higher seed count across all observed crops than other programs. Mean seed counts from scanned images of maize were observed only by CellProfiler and P-TRAP, with other programs inappropriate due to high inaccuracy. These results suggest CellProfiler as a reliable image analysis program for seed counting from digital images. Benchmark test was also performed to compare speed of analysis. The automated seed count produced by image analysis programs described here allows faster, reliable and reproducible analysis, compared to standard manual method. To our knowledge this is the first study on using CellProfiler program for crop seed counting from digital images.", (C) 2015 Elsevier B.V. All rights reserved.,"Mussadiq, Z|Laszlo, B|Helyes, L|Gyuricza, C",COMPUTERS AND ELECTRONICS IN AGRICULTURE,cellprofiler imagej maize p-trap smartgrain wheat,10.1016/j.compag.2015.08.010
283,WOS:000255770300011,2008,Tools to support a model-based methodology for emission/immission and benefit/cost/risk analysis of wastewater systems that considers uncertainty,NO. 1 RWQM1 TREATMENT PLANTS WWTP DESIGN QUALITY COSTS BENCHMARKING,"This paper presents a set of tools developed to support an innovative methodology to design and upgrade wastewater treatment systems in a probabilistic way. For the first step, data reconstruction, two different tools were developed, one for situations where data are available and another one where no data are available. The second step, modelling and simulation, implied the development of a new simulation platform and of distributed computation software to deal with the simulation load generated by the third step, uncertainty analysis, with Monte Carlo simulations of the system over one year, important dynamics and stiff behaviour. For the fourth step, evaluation of alternatives, the evaluator tool processes the results of the simulations and plots the relevant information regarding the robustness of the process against input and parameters uncertainties, as well as concentration-duration curves for the risk of non-compliance with effluent and receiving water quality limits. This paper illustrates the merits of these tools to make the innovative methodology of practical interest. The design practice should move from conventional procedures suited for the relatively fixed context of emission limits, to more advanced, transparent and cost-effective procedures appropriate to cope with the flexibility and complexity introduced by integrated water management approaches. (c) ", Elsevier Ltd. All rights reserved.,"Benedetti, L|Bixio, D|Claeys, F|Vanrolleghem, PA",ENVIRONMENTAL MODELLING & SOFTWARE,wastewater treatment plant design cost-benefit analysis risk modelling and simulation software tools grid computing,10.1016/j.envsoft.2008.01.001
284,WOS:000270759400017,2009,Quantifying predictive uncertainty for a mountain-watershed model,HYDROLOGIC-MODELS AUTOMATIC CALIBRATION GLOBAL OPTIMIZATION SWAT MODEL VALIDATION SENSITIVITY,"Watershed models require calibration before they are utilized as a decision-making tool. This paper describes a rigorous sensitivity analysis, automated parameter estimation and evaluation of prediction uncertainty for a Watershed Analysis Risk Management Framework (WARMF) model of the Turkey Creek Watershed. Sensitivity analysis was conducted using UCODE calibration and uncertainty-analysis software. Simulated stream flow is strongly sensitive to  of the  parameters evaluated: hydraulic conductivity, field capacity, total porosity, precipitation weighting factor, evaporation magnitude, evaporation skewness and snow melting rates; and parameter sensitivity is dependent on site-specific climate and soil conditions. Simulated stream flow matched observed stream flow fairly well with an R() value of ., Nash-Sutcliffe coefficient of efficiency (NSE) value of . and Root Mean Squared Error (RMSE) of . m()/s. The calibrated model was used to predict changes in stream flow that would result from changes in land use, including development of forested areas in parts of the watershed to commercial and residential areas. As expected, new development resulted in increased peak flows and reduced low flows. Uncertainty associated with all model parameters, including those not estimated by calibration by enhancing the parameter variance/covariance matrix, was considered when evaluating prediction uncertainties. Seventy percent of the time, predicted flows had uncertainties less than % with more of the uncertainty during low flow conditions.", (C) 2009 Elsevier B.V. All rights reserved.,"Geza, M|Poeter, EP|McCray, JE",JOURNAL OF HYDROLOGY,sensitivity analysis automatic calibration prediction uncertainty ucode warmf,10.1016/j.jhydrol.2009.07.025
285,WOS:000399845800057,2017,"Risk assessment of pesticides and other stressors in bees: Principles, data gaps and perspectives from the European Food Safety Authority",HONEYBEES APIS-MELLIFERA HORNET VESPA-VELUTINA YELLOW-LEGGED HORNET NEONICOTINOID INSECTICIDES ECOSYSTEM SERVICES POLLEN CONSUMPTION AETHINA-TUMIDA COLONY FAILURE EXPOSURE IMPACTS,"Current approaches to risk assessment in bees do not take into account co-exposures from multiple stressors. The European Food Safety Authority (EFSA) is deploying resources and efforts to move towards a holistic risk assessment approach of multiple stressors in bees. This paper describes the general principles of pesticide risk assessment in bees, including recent developments at EFSA dealing with risk assessment of single and multiple pesticide residues and biological hazards. The EFSA Guidance Document on the risk assessment of plant protection products in bees highlights the need for the inclusion of an uncertainty analysis, other routes of exposures and multiple stressors such as chemical mixtures and biological agents. The EFSA risk assessment on the survival, spread and establishment of the small hive beetle, Aethina tumida, an invasive alien species, is provided with potential insights for other bee pests such as the Asian hornet, Vespa velutina. Furthermore, data gaps are identified at each step of the risk assessment, and recommendations are made for future research that could be supported under the framework of Horizon . Finally, the recent work conducted at EFSA is presented, under the over-arching MUST-B project (""EU efforts towards the development of a holistic approach for the risk assessment on MUltiple STressors in Bees"") comprising a toolbox for harmonised data collection under field conditions and a mechanistic model to assess effects from pesticides and other stressors such as biological agents and beekeeping management practices, at the colony level and in a spatially complex landscape. Future perspectives at EFSA include the development of a data model to collate high quality data to calibrate and validate the model to be used as a regulatory tool. Finally, the evidence collected within the framework of MUST-B will support EFSA's activities on the development of a holistic approach to the risk assessment of multiple stressors in bees. In conclusion, EFSA calls for collaborative action at the EU level to establish a common and open access database to serve multiple purposes and different stakeholders. (C)  The Authors.", Published by Elsevier B.V.,"Rortais, A|Arnold, G|Dorne, JL|More, SJ|Sperandio, G|Streissl, F|Szentes, C|Verdonck, F",SCIENCE OF THE TOTAL ENVIRONMENT,multiple stressors honeybee colony health modelling indicator data collection research needs,10.1016/j.scitotenv.2016.09.127
286,WOS:000344530400002,2014,An SMT Based Method for Optimizing Arithmetic Computations in Embedded Software Code,PROGRAMS EXAMPLES,"We present a new method for optimizing the source code of embedded control software with the objective of minimizing implementation errors in the linear fixed-point arithmetic computations caused by overflow, underflow, and truncation. Our method relies on the use of the satisfiability modulo theory (SMT) solver to search for alternative implementations that are mathematically equivalent but require a smaller bit-width, or implementations that use the same bit-width but have a larger error-free dynamic range. Our systematic search of the bounded implementation space is based on a new inductive synthesis procedure, which is guaranteed to find a valid solution as long as such solution exists. Furthermore, we propose an incremental optimization procedure, which applies the synthesis procedure only to small code regions-one at a time-as opposed to the entire program, which is crucial for scaling the method up to programs of realistic size and complexity. We have implemented our new method in a software tool based on the Clang/LLVM compiler frontend and the Yices SMT solver. Our experiments, conducted on a set of representative benchmarks from embedded control and digital signal processing applications, show that the method is both effective and efficient in optimizing arithmetic computations in embedded software code.",,"Eldib, H|Wang, C",IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS,fixed point arithmetic inductive program synthesis satisfiability modulo theory (smt) solver superoptimization,10.1109/TCAD.2014.2341931
287,WOS:000262884900019,2009,Kinetic modelling of methane decomposition in a tubular solar reactor,AEROSOL FLOW REACTOR HYDROGEN-PRODUCTION THERMAL-DECOMPOSITION NATURAL-GAS RAPID DECOMPOSITION CHEMICAL REACTOR PYROLYSIS CARBON DISSOCIATION DECARBONIZATION,"Solar methane cracking is a promising pathway to produce hydrogen and carbon black with the bonus of zero CO() emission. A kinetic simulation of the methane decomposition in a tubular solar chemical reactor prototype is presented. This reactor is composed of four independent tubular reaction zones inserted in a graphite cavity receiver. Chemical reaction modelling is carried out thanks to the Dsmoke software, using a detailed kinetic scheme for the wide range modelling of alkane transformation. First. a kinetic analysis of the chemical system is presented to determine the sequence of methane cracking and a sensitivity analysis of the results on temperature (in the range - K) and on natural gas composition is performed. Then, a kinetic simulation of the solar reactor is proposed and implemented, in which each tubular reaction zone is modelled by three plug-flow reactors in series representing the pre-heating, isothermal, and cooling zones of the reactor. It predicts the evolution of gas species concentrations as a function of residence time. Comparisons with experimental results between  K and  K show good agreement for CH() conversion, and CH() and H() off-gas compositions.", (c) 2008 Elsevier B.V. All rights reserved.,"Rodat, S|Abanades, S|Coulie, J|Flamant, G",CHEMICAL ENGINEERING JOURNAL,methane cracking kinetic model plug-flow reactor hydrogen production solar reactor,10.1016/j.cej.2008.09.008
288,WOS:000306623200008,2012,Sensitivity analysis of some critical factors affecting simulated intrusion volumes during a low pressure transient event in a full-scale water distribution system,HYDRAULIC TRANSIENTS LEAKAGE PIPES,"Intrusion events caused by transient low pressures may result in the contamination of a water distribution system (DS). This work aims at estimating the range of potential intrusion volumes that could result from a real downsurge event caused by a momentary pump shutdown. A model calibrated with transient low pressure recordings was used to simulate total intrusion volumes through leakage orifices and submerged air vacuum valves (AVVs). Four critical factors influencing intrusion volumes were varied: the external head of (untreated) water on leakage orifices, the external head of (untreated) water on submerged air vacuum valves, the leakage rate, and the diameter of AVVs' outlet orifice (represented by a multiplicative factor). Leakage orifices' head and AVVs' orifice head levels were assessed through fieldwork. Two sets of runs were generated as part of two statistically designed experiments. A first set of  runs was based on a complete factorial design in which each factor was varied over  levels. A second set of  runs was based on a latin hypercube design, better suited for experimental runs on a computer model. The simulations were conducted using commercially available transient analysis software. Responses, measured by total intrusion volumes, ranged from  to  L. A second degree polynomial was used to analyze the total intrusion volumes. Sensitivity analyses of both designs revealed that the relationship between the total intrusion volume and the four contributing factors is not monotonic, with the AVVs' orifice head being the most influential factor. When intrusion through both pathways occurs concurrently, interactions between the intrusion flows through leakage orifices and submerged AVVs influence intrusion volumes. When only intrusion through leakage orifices is considered, the total intrusion volume is more largely influenced by the leakage rate than by the leakage orifices' head. The latter mainly impacts the extent of the area affected by intrusion. (C) ", Elsevier Ltd. All rights reserved.,"Ebacher, G|Besner, MC|Clement, B|Prevost, M",WATER RESEARCH,water distribution system intrusion volumes sensitivity analysis transient analysis downsurge event,10.1016/j.watres.2012.05.006
289,WOS:000241686100002,2006,Application of the Morris algorithm for sensitivity analysis of the REALM model for the Goulburn irrigation system,2ND-ORDER SCREENING METHOD,"The REALM modelling shell is widely used in Australia as a water allocation modelling tool. It has been used to develop the Goulburn System Model (GSM) of the Goulburn, Broken, Loddon and Campaspe Rivers in northeastern Victoria. REALM represents the river and irrigation system as a network of storages and carriers. The model has been optimised to best represent the water harvesting and allocation for use by water management authorities. The model is analysed to assess the sensitivity of a subset of the model outputs, to a subset of the system parameters. The New Morris algorithm uses sampling paths generated in the space of the parameters, to generate points at which the model is run (to generate the model outputs). These model runs are then used to estimate the first and second-order effects of the parameters on the outputs. The results illustrate the mild linkage of the Goulburn and Broken systems, and the Broken system also shows differences between minimum and average outflows. The Goulburn is more sensitive to some of the numerical convergence parameters used in the allocation software, while the Broken is less sensitive to these factors. The numerical convergence factors also lead to important second-order effects.",,"Braddock, RD|Schreider, SY",ENVIRONMENTAL MODELING & ASSESSMENT,sensitivity analysis water allocation model realm,10.1007/s10666-005-9029-z
290,WOS:000302212800003,2012,Effect of Temporal and Spatial Rainfall Resolution on HSPF Predictive Performance and Parameter Estimation,HYDROLOGICAL SIMULATION PROGRAM AUTOMATIC CALIBRATION MODEL PARAMETERS VARIABILITY RUNOFF PRECIPITATION RADAR UNCERTAINTY IMPACT FLOW,"Watershed-scale rainfall-runoff models are used for environmental management and regulatory modeling applications, but their effectiveness is limited by predictive uncertainties associated with model input data. This study evaluated the effect of temporal and spatial rainfall resolution on the predictive performance of Hydrological Simulation Program-Fortran (HSPF) using manual and automatic calibration procedures. Furthermore, the effect of automatic parameter estimation on the physical significance of calibrated parameter values was evaluated. Temporal resolutions examined included  min,  min,  h, and  h, and spatial resolution effects evaluated included the effect of a spatially averaged network of four rain gauges and Next-Generation Radar (NEXRAD) for selected rain events. Model efficiencies ranged from . to . when individual rain gauges (RG, RG, RG, and RG) were used one at a time. Model efficiency improved and ranged from . to . when a spatially averaged network of four rain gauges was used. The effect of temporal resolution on model performance varied with rain gauge location in the watershed and with use of a single gauge or spatially averaged rain gauges for model calibration. Rainfall resolution has a strong influence on parameter estimation because, to achieve high model performance, parameter values must shift whenever the resolution of the rainfall data is changed. Despite a shift in parameter values as a result of changes in rainfall resolution, the results showed that Parameter Estimation Software (PEST)-calibrated values remained within their parameter bounds. In summary, results obtained from a medium-sized Piedmont watershed in Georgia, USA, revealed that model performance was more sensitive to spatial resolution than temporal resolution. DOI: ./(ASCE)HE.-..", (C) 2012 American Society of Civil Engineers.,"Mohamoud, YM|Prieto, LM",JOURNAL OF HYDROLOGIC ENGINEERING,hspf spatial resolution temporal resolution parameter estimation model performance watershed modeling,10.1061/(ASCE)HE.1943-5584.0000457
291,WOS:000283561900012,2010,"A model for the development of a power production system in Greece, Part I: Where RES do not meet EU targets",RENEWABLE ELECTRICITY POLICIES ENERGY SECTOR PLANTS CONSUMPTION CHALLENGES STRATEGIES EMISSIONS CAPACITY THAILAND,"This paper studies the electricity production system of the Greek Interconnected Electric Production System using a model created with the software package WASP-IV. The period of study is from  to . It consists of three scenarios using three different criteria: energy, environmental and economic. The three scenarios are the business as usual, the lignite and the natural gas. Subsequently, a sensitivity analysis is carried out for the annual growth rate of electricity consumption and load demand. The paper examines how the three criteria change, when there are no other energy sources beyond those already in use (lignite, oil, natural gas, biomass, solar, wind and hydropower) with no CO capture policies and with the electricity production from Renewable Energy Sources not to reach the targets of the European Union for . In a second paper, three other scenarios examine production with the Renewable Energy Sources to reach the targets of the European Union for . (C) ", Elsevier Ltd. All rights reserved.,"Kalampalikas, NG|Pilavachi, PA",ENERGY POLICY,electricity production system greek interconnected electric system co2 emissions,10.1016/j.enpol.2010.05.038
292,WOS:000303381300019,2012,Chloride migration in groundwater for a tannery belt in Southern India,NATURAL RECHARGE AQUIFER PARAMETERS SOLUTE TRANSPORT TAMIL-NADU TERRAIN BASIN WATER SITE,"Groundwater in a tannery belt in Southern India is being polluted by the discharge of untreated effluents from  operating tanneries. Total dissolved solids and chloride (Cl-) measurements in open wells in the tannery cluster vary from , to , and , to , mg/l, respectively. A mass transport model was constructed using Visual MODFLOW Premium . software to investigate the chloride migration in an area of . km(). Input to the chloride migration model was a groundwater flow model that considered steady and transient conditions. This model was calibrated with field observations; and sensitivity analysis was carried out whereby model parameters, viz., conductivity, dispersivity, and source concentration were altered slightly, and the effect on calibration statistics was evaluated. Results indicated that hydraulic conductivity played a more sensitive role than did dispersivity. The Cl- migration was mainly through advection rather than dispersion. It was found that even if the pollutant load reduced to % of the present level, the Cl- concentration in groundwater, even after  years, would not be reduced to the permissible limit of drinking water in the tannery belt.",,"Mondal, NC|Singh, VP",ENVIRONMENTAL MONITORING AND ASSESSMENT,shallow aquifer tannery industry groundwater pollution chloride migration southern india,10.1007/s10661-011-2156-x
293,WOS:000379138500008,2016,A probabilistic projection of the transient flow equations with random system parameters and internal boundary conditions,FREQUENCY-RESPONSE METHOD POLYNOMIAL CHAOS WATER-HAMMER PIPELINES DESIGN,"This paper presents a novel probabilistic approach based on the polynomial chaos expansion that can model the uncertainty propagation from the beginning of a waterhammer simulation and not as an afterthought. Uncertainties are considered in pipe diameter, friction coefficient, and wave speed, as well as internal boundary conditions of leaks and blockages. The polynomial chaos expansion solver results are in an excellent agreement with those calculated by using a model employing the traditional method of characteristics. The probabilistic polynomial chaos approach has the advantage of being robust and more efficient than other non-intrusive methods such as Monte Carlo simulation, which requires thousands of iterations for sharp solutions. The polynomial chaos approach is further extended to solve for randomness in frequency domain using the transfer matrix method with results of comparable accuracy. With further developments, this probabilistic approach can be integrated within existing network modelling software for practical hydraulic engineering problems.",,"Sattar, AMA",JOURNAL OF HYDRAULIC RESEARCH,blockages leaks pipelines polynomial chaos expansion probabilistic analysis random variable transient flow waterhammer equations,10.1080/00221686.2016.1140682
294,WOS:000232093200009,2006,Sensitivity analysis of the strain criterion for multidimensional scaling,SPECTRAL FUNCTIONS OPTIMIZATION,"Multidimensional scaling (MDS) is a collection of data analytic techniques for constructing configurations of points from dissimilarity information about interpoint distances. Classsical MDS assumes a fixed matrix of dissimilarities. However, in some applications, e.g., the problem of inferring -dimensional molecular structure from bounds on interatomic distances, the dissimilarities are free to vary, resulting in optimization problems with a spectral objective function. A perturbation analysis is used to compute first- and second-order directional derivatives of this function. The gradient and Hessian are then inferred as representers of the derivatives. This coordinate-free approach reveals the matrix structure of the objective and facilitates writing customized optimization software. Also analyzed is the spectrum of the Hessian of the objective.", (c) 2004 Elsevier B.V. All rights reserved.,"Lewis, RM|Trosset, MW",COMPUTATIONAL STATISTICS & DATA ANALYSIS,classical multidimensional scaling principal coordinate analysis distance matrices distance geometry spectral decomposition perturbation analysis,10.1016/j.csda.2004.07.011
295,WOS:000233651000004,2005,A review of probabilistic risk assessment of contaminated land,MONTE-CARLO-SIMULATION POLYCYCLIC AROMATIC-HYDROCARBONS SOIL REMEDIATION GOALS NON-IONIZED CHEMICALS SUPERFUND SITES UNCERTAINTY ANALYSIS EXPOSURE ASSESSMENT CLEANUP LEVELS HEALTH-RISKS HETEROGENEOUS AQUIFER,"Background, Aims and Scope. The management and decisions concerning restoration of contaminated land often require in-depth risk analyses. An environmental risk assessment is generally described as proceeding in four separate steps: hazard identification, dose-response assessment, exposure assessment, and risk characterization. The risk assessment should acknowledge and quantify the uncertainty in risk predictions. This can be achieved by applying probabilistic methods which, although they have been available for many years, are still not generally used. Risk assessment of contaminated land is an area where probabilistic methods have proved particularly useful. Many reports have appeared in the literature, mostly by North American researchers. The aim of this review is to summarize the experience gained so far, provide a number of useful examples, and suggest what may be done to promote probabilistic methods in Europe and the rest of the world. Methods. The available literature has been explored through searches in the major scientific and technical databases, WWW resources, textbooks and direct contacts with active researchers. A calculation example was created using standard simulation software. Results and Discussion. Uncertainty and variability are part of every risk assessment. Much work on risks from contaminated soil has focussed on exposure, and choice and structure of the exposure model is then a basic uncertainty factor. Other factors, e.g. parameter uncertainty, are easier to characterize. Variability can be separated into inter-individual, spatial and temporal components. Both uncertainty and variability in the exposure variables can be investigated using Monte Carlo simulation methods. These simulations enable not only the estimation of the probability for a given risk or exposure, but also add information on the sensitivity of the various input variables. This will assist the assessor in further refining the risk analysis. The large number of applications published encompasses soil contamination by lead, arsenic, chromium, uranium, polychlorinated bipheryls (PCB), polycyclic aromatic hydrocarbons (PAH), hexachloro benzene, pentachlorophenol and chlorinated solvents. Probabilistic risk assessments have been used in widely different settings, such as the metallurgical industry (mining and smelting operations), manufacturing, gas plants, wood impregnation, infrastructure, and waste landfills. Site-specific remediation goals can be specified using probabilistic methods, and a guideline document has been issued within the US Superfund programme. The usability of probabilistic risk assessment is illustrated by a calculation example. The current Swedish generic guideline value for benzo[a]pyrene in contaminated soil, with ingestion of vegetables as the major route of exposure, is compared with a probabilistic estimate. The toxicological reference value corresponds well with the upper th percentile of the estimated variability in intake, but does not account for uncertainty in the partition coefficients. Conclusions and Outlook. The probabilistic approach to risk assessment has proved its value in characterizing variability and uncertainty, and thereby contributing to a more informed and transparent decision-in a king process. The management of contaminated land is a major environmental application for probabilistic risk assessments. A substantial number of studies have been published and the method is now well established in the scientific community. This development has progressed further in the United States than elsewhere, but similar applications are now being reported from Europe and Asia. Probabilistic risk assessment is used to derive soil guideline values in the United Kingdom, and other countries may be anticipated to follow. However, efficient use of probabilistic methods for risk assessment of contaminated land requires certain components. There is a requirement for quality assurance and transparency that can be met by guidelines specifying data requirements and which items to report on. Both federal and state governments in the United States have issued such guidelines, and we see a similar need from a European perspective. A second component, necessary for a successful implementation of probabilistic methods, is education. We have ourselves developed undergraduate curricula, but we also see a need for continuous education of risk assessors and decision makers. The third component required is case studies, showing how probabilistic risk assessment can be implemented successfully in the cleanup of contaminated land. Most published studies originate from the United States, so here too there is a need for the rest of the world to catch LIP. In addition to the three components mentioned, there is an obvious need to develop and improve methods and practice of risk communication.",,"Oberg, T|Bergback, B",JOURNAL OF SOILS AND SEDIMENTS,"exposure assessment monte carlo simulation multimedia model point estimate probabilistic risk assessment, probability distributions risk analysis sensitivity analysis uncertainty variability",10.1065/jss2005.08.143
296,WOS:000321088500001,2013,A review of Bayesian belief networks in ecosystem service modelling,COLUMBIA RIVER BASIN LAND MANAGEMENT ALTERNATIVES DECISION-SUPPORT TOOLS GROUNDWATER CONTAMINATION UNCERTAINTY ANALYSIS ADAPTIVE MANAGEMENT RESOURCE MANAGEMENT EXPERT KNOWLEDGE AUSTRALIA SYSTEMS,"A wide range of quantitative and qualitative modelling research on ecosystem services (ESS) has recently been conducted. The available models range between elementary, indicator-based models and complex process-based systems. A semi-quantitative modelling approach that has recently gained importance in ecological modelling is Bayesian belief networks (BBNs). Due to their high transparency, the possibility to combine empirical data with expert knowledge and their explicit treatment of uncertainties, BBNs can make a considerable contribution to the ESS modelling research. However, the number of applications of BBNs in ESS modelling is still limited. This review discusses a number of BBN-based ESS models developed in the last decade. A SWOT analysis highlights the advantages and disadvantages of BBNs in ESS modelling and pinpoints remaining challenges for future research. The existing BBN models are suited to describe, analyse, predict and value ESS. Nevertheless, some weaknesses have to be considered, including poor flexibility of frequently applied software packages, difficulties in eliciting expert knowledge and the inability to model feedback loops. (c) ", Elsevier Ltd. All rights reserved.,"Landuyt, D|Broekx, S|D'hondt, R|Engelen, G|Aertsens, J|Goethals, PLM",ENVIRONMENTAL MODELLING & SOFTWARE,bayesian belief networks ecosystem services expert based systems graphical models,10.1016/j.envsoft.2013.03.011
297,WOS:000357125300022,2015,A 2-D process-based model for suspended sediment dynamics: a first step towards ecological modeling,SAN-FRANCISCO BAY JOAQUIN DELTA SEA-LEVEL PABLO BAY CALIFORNIA TRANSPORT SACRAMENTO SENSORS MARSH FLUX,"In estuaries suspended sediment concentration (SSC) is one of the most important contributors to turbidity, which influences habitat conditions and ecological functions of the system. Sediment dynamics differs depending on sediment supply and hydrodynamic forcing conditions that vary over space and over time. A robust sediment transport model is a first step in developing a chain of models enabling simulations of contaminants, phytoplankton and habitat conditions. This works aims to determine turbidity levels in the complex-geometry delta of the San Francisco estuary using a process-based approach (DelftD Flexible Mesh software). Our approach includes a detailed calibration against measured SSC levels, a sensitivity analysis on model parameters and the determination of a yearly sediment budget as well as an assessment of model results in terms of turbidity levels for a single year, water year (WY) . Model results show that our process-based approach is a valuable tool in assessing sediment dynamics and their related ecological parameters over a range of spatial and temporal scales. The model may act as the base model for a chain of ecological models assessing the impact of climate change and management scenarios. Here we present a modeling approach that, with limited data, produces reliable predictions and can be useful for estuaries without a large amount of processes data.",,"Achete, FM|van der Wegen, M|Roelvink, D|Jaffe, B",HYDROLOGY AND EARTH SYSTEM SCIENCES,,10.5194/hess-19-2837-2015
298,WOS:000188167100002,2003,Uncertainty analysis of hydrologic and water quality predictions for a small watershed using SWAT2000,SOIL HYDRAULIC-PROPERTIES RECHARGE RATE ESTIMATION SPATIAL VARIABILITY ARID ENVIRONMENTS SOLUTE TRANSPORT RISK-ASSESSMENT PART 1 MODEL CONSTRAINTS VARIABLES,"Hydrologic and water quality (H/WQ) models are being used with increasing frequency to devise alternative pollution control strategies. It has been recognized that such models may have a large degree of uncertainty associated with their predictions, and that this uncertainty can significantly impact the utility of the model. In this study, ARRAMIS (Advanced Risk & Reliability Assessment Model) software package was used to analyze the uncertainty of the SWAT (Soil and Water Assessment Tool) outputs concerning nutrients and sediment losses from agricultural lands. ARRAMIS applies Monte Carlo simulation technique connected with Latin hypercube sampling (LHS) scheme. This technique is applied to the Warner Creek watershed located in the Piedmont physiographic region of Maryland, and it provides an interval estimate of a range of values with an associated probability instead of a point estimate of a particular pollutant constituent. Uncertainty of model outputs was investigated using LHS scheme with restricted pairing for the model input sampling. Probability distribution functions (pdfs) for each of the  model simulations were constructed from these results. Model output distributions of interest in this analysis were stream flow, sediment, organic nitrogen (organic-N), organic phosphorus (organic-P), nitrate, ammonium, and mineral phosphorus (mineral-P) transported with water. Developed probability distribution functions for the model provided information with desirable probability. Results indicate that consideration of input parameter uncertainty produces % less mean stream flow along with approximately .% larger sediment loading than obtained using mean input parameters. On the contrary, mean of outputs regarding nutrients such as nitrate, ammonia, organic-N, and organic-P (but not mineral-P) were almost the same as the one using mean input parameters. The uncertainty in predicted stream flow and sediment loading is large, but that for nutrient loadings is the same as that of the corresponding input parameters. This study concluded that using a best possible distribution for the input parameters to reflect the impact of soils and land use diversity in a small watershed on SWAT model outputs may be more accurate than using average values for each input parameter.",,"Sohrabi, TM|Shirmohammadi, A|Chu, TW|Montas, H|Nejadhashemi, AP",ENVIRONMENTAL FORENSICS,uncertainty swat2000 latin hypercube sampling monte carlo nonpoint pollution nutrient,10.1080/714044368
299,WOS:000313918200054,2013,Stochastic approach to municipal solid waste landfill life based on the contaminant transit time modeling using the Monte Carlo (MC) simulation,EARTHEN BARRIERS COMPACTED CLAY SATURATED SOIL TRANSPORT MIGRATION DIFFUSION SORPTION SITES DECAY,"The paper is concerned with application and benefits of MC simulation proposed for estimating the life of a modern municipal solid waste (MSW) landfill. The software Crystal Ball (R) (CB), simulation program that helps analyze the uncertainties associated with Microsoft (R) Excel models by MC simulation, was proposed to calculate the transit time contaminants in porous media. The transport of contaminants in soil is represented by the one-dimensional (D) form of the advection-dispersion equation (ADE). The computer program CONTRANS written in MATLAB language is foundation to simulate and estimate the thickness of landfill compacted clay liner. In order to simplify the task of determining the uncertainty of parameters by the MC simulation, the parameters corresponding to the expression Z taken from this program were used for the study. The tested parameters are: hydraulic gradient (HG), hydraulic conductivity (HC), porosity (POROS), linear thickness (TH) and diffusion coefficient (EDC). The principal output report provided by CB and presented in the study consists of the frequency chart, percentiles summary and statistics summary. Additional CB options provide a sensitivity analysis with tornado diagrams. The data that was used include available published figures as well as data concerning the Mittal Steel Poland (MSP) S.A. in Krakow, Poland. This paper discusses the results and show that the presented approach is applicable for any MSW landfill compacted clay liner thickness design.", (C) 2012 Elsevier B.V. All rights reserved.,"Bieda, B",SCIENCE OF THE TOTAL ENVIRONMENT,poland mc simulation cb (r) sensitivity analysis advection dispersion equation stochastic process,10.1016/j.scitotenv.2012.10.032
300,WOS:000305299400013,2012,Adjoint sensitivity in PDE constrained least squares problems as a multiphysics problem,,"Purpose - The purpose of this paper is to provide a framework for the implementation of an adjoint sensitivity formulation for least-squares partial differential equations constrained optimization problems exploiting a multiphysics finite elements package. The estimation of the diffusion coefficient in a Poisson-type diffusion equation is used as an example. Design/methodology/approach - The authors derive the adjoint formulation in a continuous setting allowing to attribute to the direct and adjoint states the role of different fields to be solved for. They are one-way coupled through the mismatch between measured and direct states acting as a source term in the adjoint equation. Having solved for the direct and adjoint state, the sensitivity of the cost function with respect to the design variables can then be obtained by a suitable post-processing procedure. This sensitivity can then be used to efficiently solve the least-squares problem. Findings - The authors derived the adjoint formulation in a continuous setting allowing the direct and adjoint states to be attributed the role of different fields to be solved. They are one-way coupled through the mismatch between measured and direct states acting as a source term in the adjoint equation. It is found that, having solved for the direct and adjoint state, the sensitivity of the cost function with respect to the design variables can then be obtained by a suitable post-processing procedure. Research limitations/implications - This paper implies that modern multiphysics finite elements packages provide a flexible and extendable software environment for the experimentation with different adjoint formulations. Such tools are therefore expected to become increasingly important in solving notoriously difficult partial differential equation (PDE)-constrained least-squares problems. The framework also provides the possibility of experimentation with different regularization techniques (total variation and multiscale techniques for instance) to handle the ill-posedness of the problem. Originality/value - In this paper the adjoint sensitivity computation is casted as a multiphysics problem allowing for a flexible and extendable implementation.",,"Lahaye, D|Mulckhuyse, W",COMPEL-THE INTERNATIONAL JOURNAL FOR COMPUTATION AND MATHEMATICS IN ELECTRICAL AND ELECTRONIC ENGINEERING,sensitivity analysis differential equations computation computer software adjoint sensitivity non-linear least squares problems multiphysics finite elements software,10.1108/03321641211209780
301,WOS:000407370700097,2017,Modeling Nitrogen Dynamics in a Waste Stabilization Pond System Using Flexible Modeling Environment with MCMC,SENSITIVITY-ANALYSIS CONSTRUCTED WETLAND WATER TREATMENT PREDICTIVE UNCERTAINTY NUTRIENT RECOVERY GLUE METHODOLOGY BAYESIAN METHOD UNITED-STATES REMOVAL PERFORMANCE,"This study presents an approach for obtaining realization sets of parameters for nitrogen removal in a pilot-scale waste stabilization pond (WSP) system. The proposed approach was designed for optimal parameterization, local sensitivity analysis, and global uncertainty analysis of a dynamic simulation model for the WSP by using the R software package Flexible Modeling Environment (R-FME) with the Markov chain Monte Carlo (MCMC) method. Additionally, generalized likelihood uncertainty estimation (GLUE) was integrated into the FME to evaluate the major parameters that affect the simulation outputs in the study WSP. Comprehensive modeling analysis was used to simulate and assess nine parameters and concentrations of ON-N, NH-N and NO-N. Results indicate that the integrated FME-GLUE-based model, with good Nash-Sutcliffe coefficients (.-.) and correlation coefficients (.-.), successfully simulates the concentrations of ON-N, NH-N and NO-N. Moreover, the Arrhenius constant was the only parameter sensitive to model performances of ON-N and NH-N simulations. However, Nitrosomonas growth rate, the denitrification constant, and the maximum growth rate at  degrees C were sensitive to ON-N and NO-N simulation, which was measured using global sensitivity.",,"Mukhtar, H|Lin, YP|Shipin, OV|Petway, JR",INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH AND PUBLIC HEALTH,flexiblemodeling environment waste stabilization pond nitrogen dynamic parameterization sensitivity mcmc glue global uncertainty,10.3390/ijerph14070765
302,WOS:000325074400040,2013,Estimation of nitrate load from septic systems to surface water bodies using an ArcGIS-based software,GROUND-WATER RIPARIAN ZONE DENITRIFICATION NITROGEN MODEL TRANSPORT SIMULATION TOPOGRAPHY AQUIFERS STREAM,"Nitrate, as a commonly identified groundwater and surface water pollutant, poses serious threats to human health and the environment. One important source of nitrate in the environment is due to wastewater treatment using Onsite Sewage Treatment and Disposal Systems (OSTDS) (a.k.a., septic systems). To facilitate water resources and environmental management, an ArcGIS-Based Nitrate Load Estimation Toolkit (ArcNLET) is developed to simulate nitrate transport and estimate nitrate load from septic systems and collocated fertilizer applications in groundwater to surface water bodies. It is a screening tool based on a simplified conceptual model of groundwater flow and nitrate transport. It is used in this study to estimate nitrate load from thousands of septic systems to surface water bodies in two neighborhoods located in Jacksonville, FL, USA, where nitrate due to septic systems is believed to be one of the reasons of nutrient enrichment and an isotope study indicates that denitrification is significant. A global sensitivity analysis is performed to identify critical parameters for model calibration, and the most critical parameter is the first-order decay coefficient used to simulate the denitrification process. Hydraulic conductivities at different soil zones have different levels of influence on simulated nitrate concentrations at different locations. By manually adjusting model parameters, simulated shapes of water table and nitrate concentration agree reasonably with average field observations, suggesting that ArcNLET is able to simulate spatial variability of field observations. Estimated nitrate loads exhibit spatial variability, which is useful to facilitate decisions on the conversion of OSTDS into sewers in certain areas for reducing nitrate load from septic systems to surface water bodies.",,"Wang, LY|Ye, M|Rios, JF|Fernandes, R|Lee, PZ|Hicks, RW",ENVIRONMENTAL EARTH SCIENCES,gis-based screening model nitrate transport denitrification nitrate loads sensitivity analysis morris method,10.1007/s12665-013-2283-5
303,WOS:000408861800155,2017,Improving Thermal Comfort of Low-Income Housing in Thailand through Passive Design Strategies,TROPICAL HUMID REGION BUILDINGS STANDARDS ADAPTATION HOUSES,"In Thailand, the delivery of adequate low-income housing has historically been overshadowed by politics with cost and quantity being prioritised over quality, comfort and resilience. In a country that experiences hot and humid temperatures throughout the year, buildings need to be adaptable to the climate to improve the thermal comfort of inhabitants. This research is focused on identifying areas for improving the thermal performance of these housing designs. Firstly, dynamic thermal simulations were run on a baseline model using the adaptive thermal comfort model CIBSE TM for assessment. The three criteria defined in CIBSE TM were used to assess the frequency and severity of overheating in the buildings. The internal temperature of the apartments was shown to exceed the thermal comfort threshold for these criteria throughout the year. The internal operating daily temperatures of the apartment remain high, ranging from a maximum of . degrees C to a minimum of . degrees C. Based on these findings, five criteria were selected to be analysed for sensitivity to obtain the key parameters that influence the thermal performance and to suggest possible areas for improvement. The computer software package Integrated Environmental SolutionsVirtual Environment (IES-VE) was used to perform building energy simulations. Once the baseline conditions were identified, the software packages SimLab. and RStudio were used to carry out the sensitivity analysis. These results indicated that roof material and the presence of a balcony have the greatest influence on the system. Incorporating insulation into the roof reduced the mean number of days of overheating by .%. Removing the balcony increased the number of days of overheating by .% due to significant reductions in internal ventilation.",,"Bhikhoo, N|Hashemi, A|Cruickshank, H",SUSTAINABILITY,thermal comfort low income housing thailand tropical climates dynamic thermal simulations sensitivity analysis,10.3390/su9081440
304,WOS:000235218600050,2005,Comparison of deterministic and Monte Carlo methods in shielding design,,"In shielding calculation, deterministic methods have some advantages and also some disadvantages relative to other kind of codes, such as Monte Carlo. The main advantage is the short computer time needed to find solutions while the disadvantages are related to the often-used build-up factor that is extrapolated from high to low energies or with unknown geometrical conditions, which can lead to significant errors in shielding results. The aim of this work is to investigate how good are some deterministic methods to calculating low-energy shielding, using attenuation coefficients and build-up factor corrections. Commercial software MicroShield . has been used as the deterministic code while MCNP has been used as the Monte Carlo code. Point and cylindrical sources with slab shield have been defined allowing comparison between the capability of both Monte Carlo and deterministic methods in a day-by-day shielding calculation using sensitivity analysis of significant parameters, such as energy and geometrical conditions.",,"Oliveira, AD|Oliveira, C",RADIATION PROTECTION DOSIMETRY,,10.1093/prd/nci187
305,WOS:000407603800012,2017,Sensitivity analysis of DEM prediction for sliding wear by single iron ore particle,DISCRETE ELEMENT METHOD BALL MILLS SIMULATION MODEL PERFORMANCE CONTACT MECHANISMS MOTION STEEL LIFE,"Purpose - Sliding wear is a common phenomenon in the iron ore handling industry. Large-scale handling of iron ore bulk-solids causes a high amount of volume loss from the surfaces of bulk-solids-handling equipment. Predicting the sliding wear volume from equipment surfaces is beneficial for efficient maintenance of worn equipment. Recently, the discrete element method (DEM) simulations have been utilised to predict the wear by bulk-solids. However, the sensitivity of wear prediction subjected to DEM parameters has not been systemically investigated at single particle level. To ensure the wear predictions by DEM are accurate and stable, this study aims to conduct the sensitivity analysis at the single particle level. Design/methodology/approach - In this research, pin-on-disc wear tests are modelled to predict the sliding wear by individual iron ore particles. The Hertz-Mindlin ( no slip) contact model is implemented to simulate interactions between particle ( pin) and geometry ( disc). To quantify the wear from geometry surface, a sliding wear equation derived from Archard's wear model is adopted in the DEM simulations. The accuracy of the pin-on-disc wear test simulation is assessed by comparing the predicted wear volume with that of the theoretical calculation. The stability is evaluated by repetitive tests of a reference case. At the steady-state wear, the sensitivity analysis is done by predicting sliding wear volumes using the parameter values determined by iron ore-handling conditions. This research is carried out using the software EDEM (R) ... Findings - Numerical errors occur when a particle passes a joint side of geometry meshes. However, this influence is negligible compared to total wear volume of a wear revolution. A reference case study demonstrates that accurate and stable results of sliding wear volume can be achieved. For the sliding wear at steady state, increasing particle density or radius causes more wear, whereas, by contrast, particle Poisson's ratio, particle shear modulus, geometry mesh size, rotating speed, coefficient of restitution and time step have no impact on wear volume. As expected, increasing indentation force results in a proportional increase. For maintaining wear characteristic and reducing simulation time, the geometry mesh size is recommended. To further reduce simulation time, it is inappropriate using lower particle shear modulus. However, the maximum time step can be increased to % T-R without compromising simulation accuracy. Research limitations/implications - The applied coefficient of sliding wear is determined based on theoretical and experimental studies of a spherical head of iron ore particle. To predict realistic volume loss in the iron ore-handling industry, this coefficient should be experimentally determined by taking into account the non-spherical shapes of iron ore particles. Practical implications - The effects of DEM parameters on sliding wear are revealed, enabling the selections of adequate values to predict sliding wear in the iron ore-handling industry. Originality/value - The accuracy and stability to predict sliding wear by using EDEM (R) .. are verified. Besides, this research accelerates the calibration of sliding wear prediction by DEM.",,"Chen, GM|Schott, DL|Lodewijks, G",ENGINEERING COMPUTATIONS,discrete element method pin-on-disc bulk-solids-handling wear prediction,10.1108/EC-07-2016-0265
306,WOS:000358997900004,2015,Sensitivity Analysis for Bayesian Hierarchical Models,LINEAR MIXED MODELS LOCAL INFLUENCE MARGINAL DENSITIES INFERENCE APPROXIMATIONS PERTURBATION,"Prior sensitivity examination plays an important role in applied Bayesian analyses. This is especially true for Bayesian hierarchical models, where interpretability of the parameters within deeper layers in the hierarchy becomes challenging. In addition, lack of information together with identifiability issues may imply that the prior distributions for such models have an undesired influence on the posterior inference. Despite its importance, informal approaches to prior sensitivity analysis are currently used. They require repetitive re-fits of the model with ad-hoc modified base prior parameter values. Other formal approaches to prior sensitivity analysis suffer from a lack of popularity in practice, mainly due to their high computational cost and absence of software implementation. We propose a novel formal approach to prior sensitivity analysis, which is fast and accurate. It quantifies sensitivity without the need for a model re-fit. Through a series of examples we show how our approach can be used to detect high prior sensitivities of some parameters as well as identifiability issues in possibly over-parametrized Bayesian hierarchical models.",,"Roos, M|Martins, TG|Held, L|Rue, H",BAYESIAN ANALYSIS,base prior formal local sensitivity measure bayesian robustness calibration hellinger distance bayesian hierarchical models identifiability overparametrisation,10.1214/14-BA909
307,WOS:000353715400015,2015,LCI Databases Sensitivity Analysis of the Environmental Impact of the Injection Molding Process,LIFE-CYCLE ASSESSMENT,"During the last decades, society's concern for the environment has increased. Specific tools like the Life Cycle Assessment (LCA), and software and databases to apply this method have been developed to calculate the environmental burden of products or processes. Calculating the environmental impact of plastic products is relevant as the global plastics production rose to  million tons in . Among the different ways of processing plastics, the injection molding process is one of the most used in the industry worldwide. In this paper, a sensitivity analysis of the environmental impact of the injection molding process has been carried out. In order to perform this study, the EcoInvent database inventory for injection molding, and the data from which this database is created, have been studied. Generally, when an LCA of a product is carried out, databases such as EcoInvent, where materials, processes and transports are characterized providing average values, are used to quantify the environmental impact. This approach can be good enough in some cases but in order to assess a specific production process, like injection molding, a further level of detail is needed. This study shows how the final results of environmental impact differ for injection molding when using the PVC's, PP's or PET's data. This aspect suggests the necessity of studying, in a more precise way, this process, to correctly evaluate its environmental burden. This also allows us to identify priority areas and thereby actions to develop a more sustainable way of manufacturing plastics.",,"Elduque, A|Javierre, C|Elduque, D|Fernandez, A",SUSTAINABILITY,,10.3390/su7043792
308,WOS:000419225500038,2017,Quantifying Roughness Coefficient Uncertainty in Urban Flooding Simulations through a Simplified Methodology,POLYNOMIAL CHAOS SWMM MODEL INUNDATION FLOW CALIBRATION PARAMETERS CATCHMENT SYSTEMS,"A methodology is presented which can be used in the evaluation of parametric uncertainty in urban flooding simulation. Due to the fact that such simulations are time consuming, the following methodology is proposed: (a) simplification of the description of the physical process; (b) derivation of a training data set; (c) development of a data-driven surrogate model; (d) use of a forward uncertainty propagation scheme. The simplification comprises the following steps: (a) unit hydrograph derivation using a D hydrodynamic model; (b) calculation of the losses in order to determine the effective rainfall depth; (c) flood event simulation using the principle of the proportionality and superposition. The above methodology was implemented in an urban catchment located in the city of Athens, Greece. The model used for the first step of the simplification was FLOW-RD, whereas the well-known SWMM software (US Environmental Protection Agency, Washington, DC, USA) was used for the second step of the simplification. For the training data set derivation, an ensemble of  Unit Hydrographs was derived with the FLOW-RD model. The parameters which were modified in order to produce this ensemble were the Manning coefficients in the two friction zones (residential and urban open space areas). The surrogate model used to replicate the unit hydrograph derivation, using the Manning coefficients as an input, was based on the Polynomial Chaos Expansion technique. It was found that, although the uncertainties in the derived results have to be taken into account, the proposed methodology can be a fast and efficient way to cope with dynamic flood simulation in an urban catchment.",,"Bellos, V|Kourtis, IM|Moreno-Rodenas, A|Tsihrintzis, VA",WATER,urban flooding swmm flow-r2d uncertainty surrogate models polynomial chaos expansion,10.3390/w9120944
309,WOS:000267193100001,2009,Evaluating uncertainty in integrated environmental models: A review of concepts and tools,RAINFALL-RUNOFF MODELS GENERALIZED POLYNOMIAL CHAOS WATER-QUALITY MODELS SENSITIVITY-ANALYSIS RISK-ASSESSMENT AUTOMATIC CALIBRATION GLOBAL OPTIMIZATION RELIABILITY METHODS HYDROLOGIC-MODELS DIFFERENTIAL-EQUATIONS,"This paper reviews concepts for evaluating integrated environmental models and discusses a list of relevant software-based tools. A simplified taxonomy for sources of uncertainty and a glossary of key terms with ""standard'' definitions are provided in the context of integrated approaches to environmental assessment. These constructs provide a reference point for cataloging  different model evaluation tools. Each tool is described briefly (in the auxiliary material) and is categorized for applicability across seven thematic model evaluation methods. Ratings for citation count and software availability are also provided, and a companion Web site containing download links for tool software is introduced. The paper concludes by reviewing strategies for tool interoperability and offers guidance for both practitioners and tool developers.",,"Matott, LS|Babendreier, JE|Purucker, ST",WATER RESOURCES RESEARCH,,10.1029/2008WR007301
310,WOS:000331341400011,2014,Adaptive stochastic Galerkin FEM,GENERALIZED POLYNOMIAL CHAOS FINITE-ELEMENT-METHOD ELLIPTIC SPDES CONVERGENCE PDES,"A framework for residual-based a posteriori error estimation and adaptive mesh refinement and polynomial chaos expansion for general second order linear elliptic PDEs with random coefficients is presented. A parametric, deterministic elliptic boundary value problem on an infinite-dimensional parameter space is discretized by means of a Galerkin projection onto finite generalized polynomial chaos (gpc) expansions, and by discretizing each gpc coefficient by a FEM in the physical domain. An anisotropic residual-based a posteriori error estimator is developed. It contains bounds for both contributions to the overall error: the error due to gpc discretization and the error due to Finite Element discretization of the gpc coefficients in the expansion. The reliability of the residual estimator is established. Based on the explicit form of the residual estimator, an adaptive refinement strategy is presented which allows to steer the polynomial degree adaptation and the dimension adaptation in the stochastic Galerkin discretization, and, embedded in the gpc adaptation loop, also the Finite Element mesh refinement of the gpc coefficients in the physical domain. Asynchronous mesh adaptation for different gpc coefficients is permitted, subject to a minimal compatibility requirement on meshes used for different gpc coefficients. Details on the implementation with the open-source software framework ALEA are presented; it is generic, and is based on available stiffness and mass matrices of a FEM for the deterministic, nonparametric nominal problem evaluated in the FEniCS environment. Preconditioning of the resulting matrix equation and iterative solution are discussed. Numerical experiments in two spatial dimensions for membrane and plane stress boundary value problems on polygons are presented. They indicate substantial savings in total computational complexity due to FE mesh coarsening in high gpc coefficients.", (C) 2013 Elsevier B.V. All rights reserved.,"Eigel, M|Gittelson, CJ|Schwab, C|Zander, E",COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,uncertainty quantification stochastic finite element methods operator equations alea fenics adaptive methods,10.1016/j.cma.2013.11.015
311,WOS:000345254600006,2014,Quantification of the uncertainties related to velocity-area streamgauging data,,"Estimating the contribution of the different error sources in a given streamgauging offers a practical tool to improve the measurement strategy. To address the limitations of the method proposed by the ISO  standard, a generalized approach is introduced for computing the uncertainty associated with velocity-area discharge measurements. Direct computation methods are suggested for estimating the uncertainty components related to the vertical integration of velocity and to the transversal integration of velocity and depth. Discharge extrapolations to the edges and in the top/bottom layers are explicitly taken into account, as well as the distribution of the verticals throughout the cross-section. The new uncertainty analysis method was applied to streamgauging data which are representative of varied site conditions and field procedures. The new method appears to be more versatile than the ISO  method, and better suited to the diversity of streamgauging procedures. It is possible to implement it in discharge computation software such as BAREME.",,"Le Coz, J|Bechon, PM|Camenen, B|Dramais, G",HOUILLE BLANCHE-REVUE INTERNATIONALE DE L EAU,gauging uncertainty velocity area procedure,10.1051/lhb/2014047
312,WOS:000253333700007,2008,Hardware and software efficacy in assessment of fine root diameter distributions,LENGTH MORPHOLOGY IMAGES,"Fine roots constitute the majority of root system surface area and thus most of the nutrient and water absorption surface. Fine roots are, however, the least understood of all plant roots. A sensitivity analysis of several software programs capable of providing root diameter distribution analyses was undertaken to determine if this software was capable of discriminating % changes in diameters of roots in the .-. mm diameter range. Digital images produced by drawing discrete lines, by scanning wires of various diameters, and by scanning roots from several legume species were analyzed and compared. None of the three packages were able to adequately analyze these images. Each introduced artifacts into the data that were severe enough to confound interpretation of the resulting diameter class length histograms at resolutions from  to  pixels (px) mm(-), and root diameters from . to .  mm or larger. One package was, however, clearly superior to the other two for routine digital analysis. All three packages require additional development before they are suitable for routine analysis of fine roots. Due to the  px mm- resolution ceiling with currently available scanners, the smallest roots for which this level of discrimination is possible is .mm diameter. For many agricultural and forest species, up to % of their total root length is less than . mm in diameter. It is concluded that both hardware and software constraints currently inhibit the sensitivity of investigations into fine root diameter shifts in response to environmental conditions.", Published by Elsevier B.V.,"Zobel, RW",COMPUTERS AND ELECTRONICS IN AGRICULTURE,fine roots high resolution scanner digital image analysis diameter distribution root length,10.1016/j.compag.2007.08.002
313,WOS:000301688100024,2012,Analyzing longitudinal clinical trial data with nonignorable missingness and unknown missingness reasons,QUALITY-OF-LIFE LOCAL SENSITIVITY DROP-OUT BAYESIAN-INFERENCE NONRANDOM DROPOUT REPEATED OUTCOMES BINARY DATA MODELS NONRESPONSE REGRESSION,"Longitudinal clinical trials are often plagued by nonmonotone missingness due to both patient dropout and intermittent missingness. Standard analysis assumes that missingness is ignorable. Because the assumption can be questionable, the sensitivity of inferences to alternative assumptions about missingness needs to be evaluated. This need arises in the analysis of a longitudinal prostate cancer quality-of-life (QoL) clinical trial dataset, in which nonmonotone missingness occurs. The choice of the missing data model is studied in the analysis. A local sensitivity analysis method is then applied to analyze the dataset and to investigate the changes in parameter estimates in the neighborhood of the ignorable model. One advantage of the method is that it surmounts computational difficulty and completely avoids evaluating the high-dimensional integrals in the likelihood due to nonmonotone missingness. Another is that it can be implemented using the standard software without excessive additional computation. The method is especially advantageous for large clinical datasets for which alternative approaches can become computationally prohibitive. In addition, the analysis demonstrates the importance of exploiting information on reasons for missingness. When such information is unavailable for some missingness and therefore the missingness types (i.e., dropout versus intermittent missingness) are unknown, a bound analysis is proposed, combined with genetic algorithms, to account for unknown missingness types. The analysis demonstrates the usefulness of the method as a general approach to evaluating the sensitivity of standard analysis to nonignorable nonmonotone missingness in clinical trials.", (C) 2010 Elsevier B.V. All rights reserved.,"Xie, H",COMPUTATIONAL STATISTICS & DATA ANALYSIS,bound analysis clinical trial genetic algorithm missing data multinomial logit model sensitivity analysis,10.1016/j.csda.2010.11.021
314,WOS:000344387000014,2014,Non-stationary extreme value analysis in a changing climate,DIFFERENTIAL EVOLUTION MODEL PROJECTIONS RETURN LEVELS SIMULATIONS EVENTS TEMPERATURE VARIABILITY ENSEMBLE IMPACTS RECORDS,"This paper introduces a framework for estimating stationary and non-stationary return levels, return periods, and risks of climatic extremes using Bayesian inference. This framework is implemented in the Non-stationary Extreme Value Analysis (NEVA) software package, explicitly designed to facilitate analysis of extremes in the geosciences. In a Bayesian approach, NEVA estimates the extreme value parameters with a Differential Evolution Markov Chain (DE-MC) approach for global optimization over the parameter space. NEVA includes posterior probability intervals (uncertainty bounds) of estimated return levels through Bayesian inference, with its inherent advantages in uncertainty quantification. The software presents the results of non-stationary extreme value analysis using various exceedance probability methods. We evaluate both stationary and non-stationary components of the package for a case study consisting of annual temperature maxima for a gridded global temperature dataset. The results show that NEVA can reliably describe extremes and their return levels.",,"Cheng, LY|AghaKouchak, A|Gilleland, E|Katz, RW",CLIMATIC CHANGE,,10.1007/s10584-014-1254-5
315,WOS:000398544700012,2017,Evaluation of TRMM-Precipitation with Rain-Gauge Observation Using Hydrological Model J2000,MEASURING MISSION TRMM WATER-BALANCE SENSITIVITY-ANALYSIS SATELLITE RAINFALL BIAS CORRECTION CLIMATE-CHANGE BASIN VALIDATION SIMULATION AFRICA,"Spatial precipitation is a major input to distributed hydrological models, and the accuracy of runoff predictions greatly depends on its accuracy. Satellite-based precipitation products are expected to offer an alternative to ground-based rainfall estimates in the present and the foreseeable future. In the present study, the suitability of tropical rainfall measuring mission (TRMM) multisatellite precipitation analysis (TMPA) rainfall in driving a distributed hydrological model for runoff prediction was evaluated. For this purpose, a hydrological model from the literature was calibrated and validated using raingauge data on daily time step for simulation of runoff in Kopili River basin (=,km) during -. The calibrated model was then used for simulation of runoff employing specialized software and compared with the observed discharge at Kherunighat gauging site. Evaluation criteria, i.e.,coefficient of correlation (CC), Nash-Sutcliffe coefficient (NSE), percent bias (PBIAS) and root mean square error (RMSE)-observations standard deviation ratio (RSR) were adopted to judge the performance of the model under different rainfall datasets. Simulation using gauge precipitation, the values of CC, NSE, PBIAS, and RSR were found to be ., ., ., and . respectively during calibration and , ., -., and ., respectively, during validation indicating overall good model performance. Furthermore, using the raw TMPA precipitation, the values of CC, NSE, PBIAS, and RSR were found to be ., -., ., and ., respectively during the period  to  and ., -., ., and ., respectively during simulation time period from  to . The moderate value of RSR indicates that the raw TMPA precipitation-based simulation represents the low flow, including rising and recession limbs fairly well, however, in case of the high-flow periods, overpredictions for all years were observed. The evaluation result concluded that the raw TRMM precipitation data are heavily biased from observed precipitation and are incompatible for daily runoff simulation in the study area and bias correction is essential for TRMM precipitation correction. However, after employing adequate bias-correction techniques, the TRMM precipitation performed well with higher degree of accuracy and can be used as an alternative to measured rainfall data due to its high spatial resolution where data are insufficient for runoff prediction, i.e.,for ungauged basins. Moreover, performance of the TRMM precipitation improved when simulated in combination with the gauge precipitation. Therefore, along with the efforts to improve satellite-based precipitation-estimation techniques, it is also important to develop more-effective near-real-time precipitation bias-adjustment techniques for hydrological applications.", (C) 2015 American Society of Civil Engineers.,"Kumar, D|Pandey, A|Sharma, N|Flugel, WA",JOURNAL OF HYDROLOGIC ENGINEERING,hydrological modeling runoff tmpa3b42 v7 bias-correction j2000,10.1061/(ASCE)HE.1943-5584.0001317
316,WOS:000331776000033,2014,Characterisation factors for life cycle impact assessment of sound emissions,ROAD TRAFFIC NOISE SENSITIVITY-ANALYSIS LCA FRAMEWORK,"Noise is a serious stressor affecting the health of millions of citizens. It has been suggested that disturbance by noise is responsible for a substantial part of the damage to human health. However, no recommended approach to address noise impacts was proposed by the handbook for life cycle assessment (LCA) of the European Commission, nor are characterisation factors (CFs) and appropriate inventory data available in commonly used databases. This contribution provides CFs to allow for the quantification of noise impacts on human health in the LCA framework. Noise propagation standards and international reports on acoustics and noise impacts were used to define the model parameters. Spatial data was used to calculate spatially-defined CFs in the form of -by--km maps. The results of this analysis were combined with data from the literature to select input data for representative archetypal situations of emission (e.g. urban day with a frequency of  Hz, rural night at  Hz, etc.). A total of  spatial and  archetypal CFs were produced to evaluate noise impacts at a European level (i.e. EU). The possibility of a user-defined characterisation factor was added to support the possibility of portraying the situation of full availability of information, as well as a highly-localised impact analysis. A Monte Carlo-based quantitative global sensitivity analysis method was applied to evaluate the importance of the input factors in determining the variance of the output. The factors produced are ready to be implemented in the available LCA databases and software. The spatial approach and archetypal approach may be combined and selected according to the amount of information available and the life cycle under study. The framework proposed and used for calculations is flexible enough to be expanded to account for impacts on target subjects other than humans and to continents other than Europe.", (C) 2013 Elsevier B.V. All rights reserved.,"Cucurachi, S|Heijungs, R",SCIENCE OF THE TOTAL ENVIRONMENT,noise noise impacts life cycle lcia lca annoyance,10.1016/j.scitotenv.2013.07.080
317,WOS:000415358000013,2017,Uncertain supply chain network design considering carbon footprint and social factors using two-stage approach,ANALYTIC HIERARCHY PROCESS CONSTRAINED PROGRAMMING-MODEL LIFE-CYCLE ASSESSMENT FACILITY LOCATION FUZZY ENVIRONMENT REVERSE LOGISTICS VENDOR SELECTION GREEN LOGISTICS DECISION-MAKING SERVICE LEVEL,"Sustainable development has become one of the leading global issues over the period of time. Currently, implementation of sustainability in supply chain has been continuously in center of attention due to introducing stringent legislations regarding environmental pollution by various governments and increasing stakeholders' concerns toward social injustice. Unfortunately, literature is still scarce on studies considering all three dimensions (economical, environmental and social) of sustainability for the supply chain. An effective supply chain network design (SCND) is very important to implement sustainability in supply chain. This study proposes an uncertain SCND model that minimizes the total supply chain-oriented cost and determines the opening of plants, warehouses and flow of materials across the supply chain network by considering various carbon emissions and social factors. In this study, a new AHP and fuzzy TOPSIS-based methodology is proposed to transform qualitative social factors into quantitative social index, which is subsequently used in chance-constrained SCND model with an aim at reducing negative social impact. Further, the carbon emission of supply chain is estimated by considering a composite emission that consists of raw material, production, transportation and handling emissions. In the model, a carbon emission cap is imposed on total supply chain to reduce the carbon footprint of supply chain. To solve the proposed model, a code is developed in AMPL software using a nonlinear solver SNOPT. The applicability of the proposed model is illustrated with a numerical example. The sensitivity analysis examines the effects of reducing carbon footprint cap, negative social impacts and varying probability on the total cost of the supply chain. It is observed that a stricter carbon cap over supply chain network leads to opening of more plants across the supply chain. In addition, carbon footprint of supply chain is found to be decreased in certain extent with the reduction in negative social impacts from suppliers. The carbon footprint of the supply chain is found to be reduced with increasing certainty of material supply from the suppliers. The total supply chain cost is observed to be augmented with increasing probability.",,"Das, R|Shaw, K",CLEAN TECHNOLOGIES AND ENVIRONMENTAL POLICY,chance-constrained programming supply chain network design carbon footprint sustainability stochastic programming social sustainability,10.1007/s10098-017-1446-6
318,WOS:000315267500001,2013,Determination of the overlap factor and its enhancement for medium-size tropospheric lidar systems: a ray-tracing approach,GEOMETRICAL FORM-FACTOR FIBEROPTIC OUTPUT RECEIVER SIGNALS,"The problem of overlap factor (OVF) computation and its near-range sensitivity for medium-size aperture (f/, f/) bi-axial tropospheric lidar systems using ray-tracing simulation software is presented. The method revisits both detector and fiber optics coupling alternatives at the telescope focal-plane along with the insertion of a field lens. A sensitivity analysis is carried out as a function of laser divergence, field lens, and detector/fiber positions, detector size, and the fiber's core diameter and numerical aperture. The ray-tracing approach presented here is straightforward and a comparatively much simpler solution than analytical-based methods. Parametric simulations are carried out to show that both approaches are coincident. Insertion of a field lens proves to be an elegant and low sensitivity solution for OVF enhancement, particularly, in the near-range of the lidar.", (C) 2013 Society of Photo-Optical Instrumentation Engineers (SPIE) [DOI: 10.1117/1.JRS.7.073591],"Kumar, D|Rocadenbosch, F",JOURNAL OF APPLIED REMOTE SENSING,overlap factor cross-over laser-telescope function fiber-coupled lidar systems,10.1117/1.JRS.7.073591
319,WOS:000393021400022,2017,ODM: an analytical solution-based tool for reacting oxygen diffusion modelling in mine spoils,COAL STRIP MINES PYRITE OXIDATION THIOBACILLUS-FERROOXIDANS DRAINAGE REMEDIATION WASTE PILE ACID TAILINGS TRANSPORT IRAN SIMULATIONS,"A simple one-dimensional analytical solution is presented to model oxygen diffusion through the pore space of mine spoils containing pyrite. The model incorporates volumetric oxygen consumption terms due to pyrite oxidation, oxidation of Fe+ to Fe+ and bacterial activity. Based on this analytical solution, a graphical user interface (GUI) tool is programmed and designed in MATLAB software. This tool can be used to model transport of oxygen through the mine spoils either with or without a cap. Results of several simulation scenarios of sensitivity analysis showed a significant change in oxygen concentration with varying effective diffusion coefficient of oxygen transport model and simulation time. Efficiency and flexibility of the tool developed here is verified by modelling oxygen transport through the pore space of a coal waste pile (case A) and a copper mine tailings (case B). Maximum depth of oxygen diffusion is obtained approximately equal to  and . m through the cases A and B, respectively.",,"Bahrami, S|Ardejani, FD",ENVIRONMENTAL EARTH SCIENCES,acid mine drainage mine tailings mine waste pyrite oxidation bacterial activity matlab gui,10.1007/s12665-017-6389-z
320,WOS:000346541600018,2014,Synthesis of 4-Chloro-3-nitrobenzotrifluoride: Industrial thermal runaway simulation due to cooling system failure,LIQUID-LIQUID REACTIONS SENSITIVITY-ANALYSIS DYNAMIC SIMULATION SEMIBATCH REACTOR CHEMICAL REACTORS SAFE OPERATION POLYMERIZATION CRITERIA ACID NITRATION,"In pharmaceutical and fine chemical industries, fast and strongly exothermic reactions are often carried out in semibatch reactors (SBRs) to better control the heat evolution by the feeding rate. In fact, for such processes, a thermal runaway event may be triggered whenever the rate of heat removal becomes lower than the rate of heat production. Such a dangerous phenomenon consists in an uncontrolled reactor temperature increase that, occurring in practically adiabatic conditions, can trigger secondary undesired exothermic reactions or worse, decompositions of the whole reacting mixture with consequent reactor pressurization due to uncontrollable gases formation. In this work, dedicated software has been developed and used to simulate a cooling system breakdown in an industrial SBR where the nitration of -Chlorobenzotrifluoride is carried out. The mathematical model is able to simulate both reactor temperature and pressure vs. time profiles thanks to a complete description of both the desired reaction and the unwanted reacting mixture decomposition kinetics. Different accidental scenarios have been simulated, showing both the wide different consequences that can arise from the same initiating event and, therefore, the usefulness of a complete simulation of the hypothesized accidental scenario in the frame of a Quantitative Risk Analysis. (C)  The Institution of Chemical Engineers.", Published by Elsevier B.V. All rights reserved.,"Copelli, S|Derudi, M|Cattaneo, CS|Nano, G|Raboni, M|Torretta, V|Rota, R",PROCESS SAFETY AND ENVIRONMENTAL PROTECTION,thermal runaway cooling system failure aromatic nitrations 4-chlorobenzotrifluoride semibatch reactor decomposition kinetics,10.1016/j.psep.2013.11.006
321,WOS:000289865900008,2011,Automatic differentiation strategy for the local sensitivity analysis of a one-dimensional hydraulic model,,"In this paper, automatic differentiation (AD) techniques are introduced and applied in the local sensitivity analysis of the state function handled by the one-dimensional hydraulic model, Mage. We have proposed the different steps to easily compute automatic derivatives of a given numerical model. More specifically, Tapenade software, in the tangent linear mode (TLM), has been used to calculate derivatives of the model outputs (discharge and water level) with respect to the bottom friction expressed in terms of Strickler relation. We have shown the independent contribution of the main stream and floodplain Strickler coefficients on discharges and water levels. Furthermore, numerical comparison has shown that derivatives computed using the AD tool are more accurate than those using the forward divided differences scheme."," Copyright (C) 2010 John Wiley & Sons, Ltd.","Souhar, O|Faure, JB",INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN FLUIDS,automatic differentiation divided differences hydraulic models sensitivity analysis,10.1002/fld.2263
322,WOS:000235767800009,2006,Comparing deterministic and probabilistic risk assessments - A case study at a closed steel mill in southern Sweden,EXPOSURE ASSESSMENT SOIL SITE UNCERTAINTY VARIABILITY MODELS GREECE LEAD,"Background, Aims and Scope. Contaminated land is a high priority environmental problem in most of Europe and North-America. Sweden is no exception and generic guideline values have been developed for the initial assessment, but site-specific assessments are also needed. The generic guideline values are not applicable when the exposure conditions are different from the typical Swedish conditions or when the site contains a particularly sensitive ecosystem. The Swedish guideline values have, like in man), other countries, been set by using deterministic point estimates for all variables and constants in the used multimedia model. The same approach is common also for site-specific assessments, and a limitation is that it fails to quantify variability and uncertainty. Probabilistic risk assessment provided a method to deal with this problem. Variability and uncertainty in the input parameters (variables or constants) are described by probability distributions, and likewise the output (risk or exposure) is presented as a probability distribution. A substantial number of probabilistic risk assessments for contaminated land at sites in North America, Europe and Asia have been published. However, an extensive review of the literature did not identify an), study where probabilistic risk assessment was applied to a site contaminated by an iron or steel industry. Here we will describe such a case, where we have compared a deterministic point estimate with a probabilistic risk assessment for six elements and benzo[a]pyrene. Methods. The site had different metallurgical plants in operation for more than  years. Most parts of the steel mill were closed by the mid s, and today the site is used by small-sized enterprises. The soil is contaminated with metals from the previous industrial operations. The present owner plans to develop the site and has therefore initiated extensive investigations of soil contamination. Sixty-two soil samples collected between  and  provided a good coverage of the whole site, and were analyzed for the content of different elements and polycyclic aromatic hydrocarbons (PAH). The exposure assessments were focused on six elements with high concentrations compared to the generic guideline values; arsenic (As), lead (Pb), cadmium (Ccl), chromium (Cr), copper (Cu) and zinc (Zn). In addition, benzo[a]pyrene was included due to the high toxicity and comparatively high concentrations. Variability and uncertainty were characterized in a Monte Carlo simulation of exposures (, iterations), and the exposures were evaluated with two land use scenarios; less sensitive use and sensitive use. Results and Discussion. The deterministic point estimates and the probabilistic estimates of the th percentile are in approximately the same ranges in the scenario of less sensitive land use. It is only the exposure for arsenic that is slightly above the toxicological reference value (TRV) in the deterministic assessment. In the probabilistic assessment, the exposure for all elements is below the TRV. The results for sensitive land use are applicable to a scenario where the site is developed for general housing. The deterministic point estimates and the probabilistic estimates of the th percentile are also here in approximately the same ranges, but the exposure exceeds the TRV for arsenic, cadmium and lead. Drinking water, vegetables grown on site and soil ingestion are the major exposure pathways for this scenario. In this assessment, the estimated intake distributions are applicable to a randomly selected individual. The probability distributions used here to characterize the different soil parameters are typically representing both variability and uncertainty, and the same is true the majority of the exposure variables. We therefore decided not to attempt to separate variability and uncertainty at this stage, but with additional data from a more in-depth site investigation it might be possible to achieve this. Conclusions and Outlook. To the best of our knowledge, this study is the first report on a probabilistic risk assessment on a former iron and steel works site. The materials handled by this industry were less toxic than for many other metallurgical operations, but contaminants may still severely limit the options for future land use. This case study shows that probabilistic exposure estimates for a set of soil contaminants can be quite similar to deterministic point estimates. The main difference is instead to be found in the additional information obtained with the probabilistic assessment. The sensitivity analyses show pathways and input variables that contribute most to variations in the total intake of each contaminant, e.g. dermal contact and ingestion of soil, vegetables and drinking water. This information can be used both in the planning of future land use and for active measures to reduce current exposure. The probabilistic assessment also provides information on the magnitude of exposure and the margin of safety. This information may facilitate risk communication between decision-makers and stakeholders. The presentation of results from probabilistic risk assessments is only briefly discussed in the literature and here we see a need for research and opportunities for enhancement. The choice of data analytical tools may then be of importance, since more complex multimedia models are rather difficult to decipher when implemented within traditional spreadsheet software. Some of the research needs are identified here and in a previous review article in this journal.",,"Sander, P|Oberg, T",JOURNAL OF SOILS AND SEDIMENTS,arsenic contaminated soils drinking water exposure assessments monte carlo simulation multimedia models probabilistic risk assessments sensitivity analysis steel mills uncertainties,10.1065/jss2005.10.147
323,WOS:000364452300007,2015,Uncertainties propagations in 1D hydraulic modeling,,"Numerical modeling tools, like Crue the D modeling software developed by CNR, are widely used to analyze hydraulic and hydrological behavior of rivers. Those tools are based on input parameters, with physical or numerical meaning; theses inputs are generally known with some uncertainties. The tool Promethee, developed by IRSN, is able to realize uncertainties propagations, and two kinds of sensibility analysis: the first one, a determinist method (Morris) based on screening, is able to identify factors which influenced outputs variability; the second one, a probabilistic method (FAST) based on variance analysis of outputs regarding inputs variances, performs inputs ranking in function of outputs sensibilities. Uncertainties propagations studies require an important computational capacity; to do so; the Promethee/Crue coupling is used. The coupled tool is able to parameter Crue files for the hydraulic computations, to run lots of computation, and then to analyze results with statistic tools. This coupled tool gives the possibility to realize sensitivity studies by probabilistic method, to parameter realistic and complex model rivers, and to study the influence of several inputs variations.",,"Nguyen, TM|Richet, Y|Balayn, P|Bardet, L",HOUILLE BLANCHE-REVUE INTERNATIONALE DE L EAU,promethee crue9 sensitivity analysis fast morris,10.1051/lhb/20150055
324,WOS:000341218800002,2014,An environmental assessment system for environmental technologies,LIFE-CYCLE ASSESSMENT WASTE MANAGEMENT-SYSTEMS SOLID-WASTE LCA INCINERATION COLLECTION EASEWASTE MODEL,"A new model for the environmental assessment of environmental technologies, EASETECH, has been developed. The primary aim of EASETECH is to perform life-cycle assessment (LCA) of complex systems handling heterogeneous material flows. The objectives of this paper are to describe the EASETECH framework and the calculation structure. The main novelties compared to other LCA software are as follows. First, the focus is put on material flow modelling, as each flow is characterised as a mix of material fractions with different properties and flow compositions are computed as a basis for the LCA calculations. Second, the tool has been designed to allow for the easy set-up of scenarios by using a toolbox, the processes within which can handle heterogeneous material flows in different ways and have different emission calculations. Finally, tools for uncertainty analysis are provided, enabling the user to parameterise systems fully and propagate probability distributions through Monte Carlo analysis. (C) ", Elsevier Ltd. All rights reserved.,"Clavreul, J|Baumeister, H|Christensen, TH|Damgaard, A",ENVIRONMENTAL MODELLING & SOFTWARE,easetech life cycle assessment waste lca model uncertainty flow modelling,10.1016/j.envsoft.2014.06.007
325,WOS:000355985700016,2015,Carbon dioxide utilisation for production of transport fuels: process and economic analysis,FISCHER-TROPSCH SYNTHESIS TECHNOECONOMIC ASSESSMENT ANAEROBIC-DIGESTION DIMETHYL ETHER CO2 CAPTURE GASIFICATION MONOETHANOLAMINE TECHNOLOGIES CHALLENGES LIQUIDS,"Utilising CO as a feedstock for chemicals and fuels could help mitigate climate change and reduce dependence on fossil fuels. For this reason, there is an increasing world-wide interest in carbon capture and utilisation (CCU). As part of a broader project to identify key technical advances required for sustainable CCU, this work considers different process designs, each at a high level of technology readiness and suitable for large-scale conversion of CO into liquid hydrocarbon fuels, using biogas from sewage sludge as a source of CO. The main objective of the paper is to estimate fuel production yields and costs of different CCU process configurations in order to establish whether the production of hydrocarbon fuels from commercially proven technologies is economically viable. Four process concepts are examined, developed and modelled using the process simulation software Aspen Plus (R) to determine raw materials, energy and utility requirements. Three design cases are based on typical biogas applications: () biogas upgrading using a monoethanolamine (MEA) unit to remove CO, () combustion of raw biogas in a combined heat and power (CHP) plant and () combustion of upgraded biogas in a CHP plant which represents a combination of the first two options. The fourth case examines a post-combustion CO capture and utilisation system where the CO removal unit is placed right after the CHP plant to remove the excess air with the aim of improving the energy efficiency of the plant. All four concepts include conversion of CO to CO via a reverse water-gas-shift reaction process and subsequent conversion to diesel and gasoline via Fischer-Tropsch synthesis. The studied CCU options are compared in terms of liquid fuel yields, energy requirements, energy efficiencies, capital investment and production costs. The overall plant energy efficiency and production costs range from -% and .-. pound per litre of liquid fuels, respectively. A sensitivity analysis is also carried out to examine the effect of different economic and technical parameters on the production costs of liquid fuels. The results indicate that the production of liquid hydrocarbon fuels using the existing CCU technology is not economically feasible mainly because of the low CO separation and conversion efficiencies as well as the high energy requirements. Therefore, future research in this area should aim at developing novel CCU technologies which should primarily focus on optimising the CO conversion rate and minimising the energy consumption of the plant.",,"Dimitriou, I|Garcia-Gutierrez, P|Elder, RH|Cuellar-Franca, RM|Azapagic, A|Allen, RWK",ENERGY & ENVIRONMENTAL SCIENCE,,10.1039/c4ee04117h
326,WOS:000394653000006,2017,Inverse heat transfer analysis of radiator central heating systems inside residential buildings using sensitivity analysis,THERMAL DESIGN OPTIMIZATION ENERGY PARAMETERS COEFFICIENTS TEMPERATURE SIMULATION ALGORITHM,"In the present study, a novel inverse heat transfer methodology is presented for thermal energy analysis of residential buildings equipped with radiator Heating, Ventilation, Air Conditioning systems using a combination of numerical simulation and sensitivity analysis. With the use of numerical simulation, thermal analysis of building components is simulated in order to calculate temperature field in the computational domain. With inverse analysis technique, we perform the estimation of some thermal parameters with the use of Levenberg-Marquardt and conjugate gradient methods and reported the results with the use of sensitivity analysis inside the building. To confirm the accuracy of numerical simulation, the problem is also simulated with commercial software such as Carrier and Energy-Plus. Then, thermal parameter estimation results of inverse simulation are presented. The present study illustrates the sufficiency of inverse analysis in order to calculate building thermal design parameters with excellent accuracy and precision with the least possible financial design cost.",,"Moftakhari, A|Aghanajafi, C|Ghazvin, AMC",INVERSE PROBLEMS IN SCIENCE AND ENGINEERING,inverse analysis levenberg-marquardt method conjugate gradient method sensitivity analysis building thermal design,10.1080/17415977.2016.1178258
327,WOS:000283561900013,2010,"A model for the development of a power production system in Greece, Part II: Where RES meet EU targets",,"This paper studies the electricity production system of the Greek Interconnected Electric System using a development model created with the software package WASP-IV. The period of study is from  till . It consists of three scenarios using three different criteria: energy, environmental, and economic. The three scenarios are the Renewable Energy Source (RES), the lignite-RES, and the natural gas-RES. Subsequently, a sensitivity analysis is carried out for annual growth rate of electricity consumption and load demand. It is considered that there are no other energy sources beyond those already in use (lignite, oil, natural gas, biomass, solar, wind, and hydropower), no CO capture policies are implemented, and electricity production from RES meets targets of the European Union in . The present paper completes the study started with the paper ""A model for the development of a power production system in Greece, Part : RES do not meet EU targets"". It is shown that with regard to fossil fuels, the use of natural gas is the best choice. The use of RES, though environmentally friendly, is an expensive solution. (C) ", Elsevier Ltd. All rights reserved.,"Kalampalikas, NG|Pilavachi, PA",ENERGY POLICY,electricity production system electricity consumption co2 emissions,10.1016/j.enpol.2010.05.037
328,WOS:000299625100003,2012,A topological optimization approach for structural design of a high-speed low-load mechanism using the equivalent static loads method,DYNAMIC LOADS SYSTEMS ALGORITHM,"In high-speed low-load mechanisms, the principal loads are the inertial forces caused by the high accelerations and velocities. Hence, mechanical design should consider lightweight structures to minimize such loads. In this paper, a topological optimization method is presented on the basis of the equivalent static loads method. Finite element (FE) models of the mechanism in different positions are constructed, and the equivalent loads are obtained using flexible multibody dynamics simulation. Kinetic DOFs are used to simulate the motion joints, and a quasi-static analysis is performed to obtain the structural responses. The element sensitivity is calculated according to the static-load-equivalent equilibrium, in such a way that the influence on the inertial force is considered. A dimensionless component sensitivity factor (strain energy caused by unit load divided by kinetic energy from unit velocity) is used, which quantifies the significance of each element. Finally, the topological optimization approach is presented on the basis of the evolutionary structural optimization method, where the objective is to find the maximum ratio of strain energy to kinetic energy. In order to show the efficiency of the presented method, we presented two numerical cases. The results of these analyses show that the presented method is more efficient and can be easily implemented in commercial FE analysis software."," Copyright (C) 2011 John Wiley & Sons, Ltd.","Yang, ZJ|Chen, X|Kelly, R",INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,topological optimization high-speed low-load mechanism equivalent static loads method quasi-static analysis sensitivity analysis,10.1002/nme.3253
329,WOS:000245063400008,2007,Parameter estimation and uncertainty analysis for a watershed model,RAINFALL-RUNOFF MODELS SHUFFLED COMPLEX EVOLUTION GROUNDWATER-FLOW MODEL MONTE-CARLO METHODS BAYESIAN-APPROACH METROPOLIS ALGORITHM PREDICTION INTERVALS CATCHMENT MODELS MARKOV-CHAINS CALIBRATION,"Where numerical models are employed as an aid to environmental management, the uncertainty associated with predictions made by such models must be assessed. A number of different methods are available to make such an assessment. This paper explores the use of three such methods, and compares their performance when used in conjunction with a lumped parameter model for surface water flow (HSPF) in a large watershed. Linear (or first-order) uncertainty analysis has the advantage that it can be implemented with virtually no computational burden. While the results of such an analysis can be extremely useful for assessing parameter uncertainty in a relative sense, and ascertaining the degree of correlation between model parameters, its use in analyzing predictive uncertainty is often limited. Markov Chain Monte Carlo (MCMC) methods are far more robust, and can produce reliable estimates of parameter and predictive uncertainty. As well as this, they can provide the modeler with valuable qualitative information on the shape of parameter and predictive probability distributions; these shapes can be quite complex, especially where local objective function optima lie within those parts of parameter space that are considered probable after calibration has been undertaken. Nonlinear calibration-constrained optimization can also provide good estimates of parameter and predictive uncertainty, even in situations where the objective function surface is complex. Furthermore, they can achieve these estimates using far fewer model runs than MCMC methods. However, they do not provide the same amount of qualitative information on the probability structure of parameter space as do MCMC methods, a situation that can be partially rectified by combining their use with an efficient gradient-based search method that is specifically designed to locate different local optima. All methods of parameter and predictive uncertainty analysis discussed herein are implemented using freely-available software. Hence similar studies, or extensions of the present study, can be easily undertaken in other modeling contexts by other modelers. (c) ", Elsevier Ltd. All rights reserved.,"Gallagher, M|Doherty, J",ENVIRONMENTAL MODELLING & SOFTWARE,uncertainty analysis parameter estimation mathematical modeling markov chain monte carlo model calibration,10.1016/j.envsoft.2006.06.007
330,WOS:000236131200002,2006,Discrete element representation of manure products,WHEAT EN-MASSE SIMULATION MODELS PARAMETERS FLOW,"To simulate the machine-product interactions taking place in land application equipment, models of manure products must first be developed and validated. Several parameters must be defined to appropriately represent organic fertilizers in the discrete element method (DEM) framework. The work reported herein was aimed at determining the properties of the virtual product that would allow mimicking the behaviour of manure in the DEM software PFCD. A procedure was developed to generate an assembly of particles within the domain under investigation according to a user-defined particle size distribution, as would be measured by screening. The results generated by this procedure in terms of granulometry of the assembly of particles were very close to the user specifications with errors on the number of particles and on their size averaging .% and .%, respectively. A procedure was also developed to create clusters of particles randomly oriented and located within the modeled domain. The cluster-generation code was tested for clusters made of up to six particles, but could be expanded to include more particles. A calibration procedure based on a virtual direct shear test was developed to define the properties of the resulting virtual manure. A sensitivity analysis was performed to study the influence of parameters defining the linear and Hertz-Mindlin contact constitutive models. The simulations were based on experimental results obtained for pig manure at a total solids (TS) concentration of %. The results showed that numerous parameters have an influence on the behaviour of the virtual product in the direct shear test. Implementing the measured particle size distribution for pig manure at % TS, a friction coefficient of . and a Young's modulus value of . MPa allowed reaching an angle of internal friction of . degrees and an apparent cohesion value of . kPa that favourably compared to the . degrees and . kPa values measured experimentally.", (c) 2005 Elsevier B.V. All rights reserved.,"Landry, H|Lague, C|Roberge, M",COMPUTERS AND ELECTRONICS IN AGRICULTURE,discrete element method dem constitutive models input parameters numerical modeling manure organic fertilizers,10.1016/j.compag.2005.10.004
331,WOS:000355131600020,2015,Effect of bacteria density and accumulated inert solids on the effluent pollutant concentrations predicted by the constructed wetlands model BIO_PORE,WASTE-WATER TREATMENT SENSITIVITY-ANALYSIS SIMULATION GROWTH,"Constructed wetlands are a widely adopted technology for the treatment of wastewater in small communities. The understanding of their internal functioning has increased at an unprecedented pace over recent years, in part thanks to the use of mathematical models. BIO_PORE model is one of the most recent models developed for constructed wetlands. This model was built in the COMSOL Multiphysics (TM) software and implements the biokinetic expressions of Constructed Wetlands Model  (CWM) to describe the fate and transport of organic matter, nitrogen and sulphur in horizontal subsurface-flow constructed wetlands. In previous studies, CWM was extended with the inclusion of two empirical parameters (M-bio_max and M-cap) that proved to be essential to provide realistic bacteria growth rates and dynamics. The aim of the current work was to determine the effect of these two parameters on the effluent pollutant concentrations predicted by the model. To that end, nine simulations, each with a different M-bio_max-M-cap pair, were launched on a high-end multi-processor computer and the effluent COD and ammonia nitrogen concentrations obtained on each simulation were qualitatively compared among them. Prior to this study, a finite element mesh optimization procedure was carried out to reduce computational cost. Results of the mesh optimization procedure indicated that among the  tested meshes of different element size, the mesh utilized for this model in previous studies represented a fair compromise between output accuracy and computation time. Results of the sensitivity analysis showed that the value of M-cap has a dramatic effect on the simulated effluent concentrations of COD and ammonia nitrogen, which clearly decreased for increasing values of this parameter. On the other hand, the model output was also sensitive to the values of M-bio_max, but its effects were less important and no clear relation could be established between its value and the simulated effluent concentration of COD and ammonia nitrogen.", (C) 2014 Elsevier B.V. All rights reserved.,"Samso, R|Blazquez, J|Agullo, N|Grau, J|Torres, R|Garcia, J",ECOLOGICAL ENGINEERING,local sensitivity mesh optimization bacteria growth parallel computing batch,10.1016/j.ecoleng.2014.09.069
332,WOS:000338763900008,2014,Development of a zoning-based environmental-ecological coupled model for lakes: a case study of Baiyangdian Lake in northern China,WATER-QUALITY SENSITIVITY-ANALYSIS ECOSYSTEM MODEL WETLANDS MANAGEMENT RIVERS,"Environmental/ecological models are widely used for lake management as they provide a means to understand physical, chemical, and biological processes in highly complex ecosystems. Most research has focused on the development of environmental (water quality) and ecological models, separately. Limited studies were developed to couple the two models, and in these limited coupled models, a lake was regarded as a whole for analysis (i.e. considering the lake to be one well-mixed box), which is appropriate for small-scale lakes but is not sufficient to capture spatial variations within middle-scale or large-scale lakes. In response to this problem, this paper seeks to establish a zoning-based environmental-ecological coupled model for a lake. Hierarchical cluster analysis was adopted to determine the number of zones in a given lake based on hydrological, water quality, and ecological data analysis. The MIKE  model was used to construct -D hydrodynamics and water quality simulations. STELLA software was used to create a lake ecological model that can simulate the spatial variations of ecological condition based on flow field distribution results generated by MIKE . Baiyangdian Lake, the largest freshwater lake in northern China, was adopted as the study case. The results showed that the new model is promising for predicting spatial variations of ecological conditions in response to changes in lake water quantity and quality, and could be useful for lake management.",,"Zhao, YW|Xu, MJ|Xu, F|Wu, SR|Yin, XA",HYDROLOGY AND EARTH SYSTEM SCIENCES,,10.5194/hess-18-2113-2014
333,WOS:000400594100014,2017,Uncertainty Estimation in Flood Inundation Mapping: An Application of Non-parametric Bootstrapping,MONTHLY STREAMFLOW PREDICTION ARTIFICIAL NEURAL-NETWORKS CONFIDENCE-INTERVALS MODEL CALIBRATION DESIGN FLOODS RUNOFF RISK PRECIPITATION OPTIMIZATION PARAMETERS,"Disaster prevention planning is affected in a significant way by a lack of in-depth understanding of the numerous uncertainties involved with flood delineation and related estimations. Currently, flood inundation extent is represented as a deterministic map without in-depth consideration of the inherent uncertainties associated with variables such as precipitation, streamflow, topographic representation, modelling parameters and techniques, and geospatial operations. The motivation of this study is to estimate uncertainties in flood inundation mapping based on a non-parametric bootstrapping method. The uncertainty is addressed through the application of non-parametric bootstrap sampling to the hydrodynamic modelling software, HEC-RAS, integrated with Geographic Information System (GIS). This approach was used to simulate different water levels and flow rates corresponding to different return periods from the available database. The study area was the Langat River Basin in Malaysia. The results revealed that the inundated land and infrastructure are subject to a flooding hazard of high-frequency events and that the flood damage potential is increasing significantly for residential areas and valuable land-use classes with higher return periods. The proposed methodology, as well as the study outcomes, of this paper could be beneficial to policymakers, water resources managers, insurance companies and other flood-related stakeholders."," Copyright (c) 2017 John Wiley & Sons, Ltd.","Faghih, M|Mirzaei, M|Adamowski, J|Lee, J|El-Shafie, A",RIVER RESEARCH AND APPLICATIONS,flood mapping uncertainty analysis non-parametric bootstrap sampling generalized extreme value distribution,10.1002/rra.3108
334,WOS:000183089700001,2003,Direct treatment of uncertainty: I - Applications in aquatic invertebrate risk assessment and soil metabolism for chlorpyrifos,RESPONSE VARIABILITY DYNAMIC-RESPONSE ACUTE TOXICITY SYSTEMS MODELS,"Environmental processes are wrought with uncertainty. Therefore, an efficient means to propagate uncertainty is advantageous, especially if regulatory decisions are based upon any research or data analysis where uncertainty is present. The Deterministic Equivalent Modeling Method (DEMM) propagates parametric uncertainties in model input parameters to output predictions. DEMM is used to calculate uncertainty in output parameters based upon the direct effect of every uncertain input parameter. Rather than sampling input distributions and running hundreds or thousands of model calculations as in Monte Carlo or Latin Hypercube Sampling, DEMM carries a representation of each distribution throughout the calculation of the dependent variable. An overview of DEMM is provided. Once DEMM algorithms are established using symbolic mathematical software program(s), and the dependent variable expansion hypothesized, then the additional overhead required to set up and solve algebraic or differential systems is small. Examples of DEMM using literature values for chlorpyrifos (a widely used insecticide) effects and fate illustrate DEMM's capability for uncertainty propagation. Determination of chlorpyrifos risk quotients for invertebrates (algebraic system) and chlorpyrifos metabolic fate in soil (differential equation system) are presented. These examples illustrate DEMM methodology on problems of interest in environmental fate and risk assessment. Multiple data sets and field/laboratory observations for chlorpyrifos were assembled and utilized with DEMM to propagate uncertainty in output predictions. Chlorpyrifos environmental fate (environmental degradation and metabolite formation/degradation) and risk for aquatic invertebrates, with uncertainty characterized using DEMM are discussed.",,"Cryer, SA|Applequist, GE",ENVIRONMENTAL ENGINEERING SCIENCE,deterministic equivalent modeling method demm uncertainty chlorpyrifos soil metabolism risk quotient,10.1089/109287503321671375
335,WOS:000250352400010,2007,Parallel computing techniques for sensitivity analysis in optimum structural design,FINITE-ELEMENT-ANALYSIS OPTIMIZATION SYSTEMS ENVIRONMENT LOADS,"Among different activities of the optimum structural design using the gradient-based optimization approaches, design sensitivity analysis is the most time-consuming computational process. By introducing parallel computing techniques for sensitivity computation, significant speedup has been obtained in optimum structural design. Computation of design sensitivities is characteristically uncoupled, thus opening the door to parallelization. In this paper, two types of approaches viz. single-level and multilevel parallelisms are pursued for design sensitivities. The design sensitivities are computed using analytical and finite-difference methods. Numerical studies show that the performance of the parallel algorithms for design sensitivities on message passing systems is very good. Good speedups have been achieved in parallel multilevel sensitivity calculation. The parallel algorithms for design sensitivity analysis have been implemented on message passing parallel systems within the software platform of Parallel Computer Adaptive Language.",,"Umesha, PK|Venuraju, MT|Hartmann, D|Leimbach, KR",JOURNAL OF COMPUTING IN CIVIL ENGINEERING,,10.1016/(ASCE)0887-3801(2007)21:6(463)
336,WOS:000364248800012,2015,Using an ensemble smoother to evaluate parameter uncertainty of an integrated hydrological model of Yanqi basin,MIKE-SHE MODEL QUASI-GEOSTROPHIC MODEL SQUARE-ROOT FILTERS KALMAN FILTER DATA ASSIMILATION SENSITIVITY-ANALYSIS SYSTEM FLOW IDENTIFICATION PROPAGATION,"Model uncertainty needs to be quantified to provide objective assessments of the reliability of model predictions and of the risk associated with management decisions that rely on these predictions. This is particularly true in water resource studies that depend on model-based assessments of alternative management strategies. In recent decades, Bayesian data assimilation methods have been widely used in. hydrology to assess uncertain model parameters and predictions. In this case study, a particular data assimilation algorithm, the Ensemble Smoother with Multiple Data Assimilation (ESMDA) (Emerick and Reynolds, ), is used to derive posterior samples of uncertain model parameters and forecasts for a distributed hydrological model of Yanqi basin, China. This model is constructed using MIKESHE/MIKEsoftware, which provides for coupling between surface and subsurface processes (DHI, a-d). The random samples in the posterior parameter ensemble are obtained by using measurements to update  prior parameter samples generated with a Latin Hypercube Sampling (LHS) procedure. The posterior forecast samples are obtained from model runs that use the corresponding posterior parameter samples. Two iterative sample update methods are considered: one based on an a perturbed observation Kalman filter update and one based on a square root Kalman filter update. These alternatives give nearly the same results and converge in only two iterations. The uncertain parameters considered include hydraulic conductivities, drainage and river leakage factors, van Genuchten soil property parameters, and dispersion coefficients. The results show that the uncertainty in many of the parameters is reduced during the smoother updating process, reflecting information obtained from the observations. Some of the parameters are insensitive and do not benefit from measurement information. The correlation coefficients among certain parameters increase in each iteration, although they generally stay below ..", (C) 2015 Elsevier B.V. All rights reserved.,"Li, N|McLaughlin, D|Kinzelbach, W|Li, WP|Dong, XG",JOURNAL OF HYDROLOGY,uncertainty analysis ensemble smoother with multiple data assimilation unbiased square root filter integrated hydrological model latin hypercube sampling yanqi basin,10.1016/j.jhydrol.2015.07.024
337,WOS:000304453300004,2012,Standardized uncertainty analysis for hydrometry: a review of relevant approaches and implementation examples,,"The water-centric community has continuously made efforts to identify, assess and implement rigorous uncertainty analyses for routine hydrological measurements. This paper reviews some of the most relevant efforts and subsequently demonstrates that the Guide to the expression of uncertainty in measurement (GUM) is a good candidate for estimation of uncertainty intervals for hydrometry. The demonstration is made by implementing the GUM to typical hydrometric applications and comparing the analysis results with those obtained using the Monte Carlo method. The results show that hydrological measurements would benefit from the adoption of the GUM as the working standard, because of its soundness, the availability of software for practical implementation and potential for extending the GUM to hydrological/hydraulic numerical simulations.",,"Muste, M|Lee, K|Bertrand-Krajewski, JL",HYDROLOGICAL SCIENCES JOURNAL-JOURNAL DES SCIENCES HYDROLOGIQUES,uncertainty analysis hydrometric measurements monte carlo uncertainty estimation sensitivity analysis,10.1080/02626667.2012.675064
338,WOS:000089556100004,2000,Estimating labor productivity using probability inference neural network,ESTIMATING CONSTRUCTION PRODUCTIVITY,"This paper discusses the derivation of a probabilistic neural network classification model and its application in the construction industry. The probability inference neural network (PINN) model is based on the same concepts as those of the learning vector quantization method combined with a probabilistic approach. The classification and prediction networks are combined in an integrated network, which required the development of a different training and recall algorithm. The topology and algorithm of the developed model was presented and explained in detail. Portable computer software was developed to implement the training, testing, and recall for PINN. The PINN was tested on real historical productivity data at a local construction company and compared to the classic feedforward back-propagation neural network model. This showed marked improvement in performance and accuracy. In addition, the effectiveness of PINN for estimating labor production rates in the context of the application domain was validated through sensitivity analysis.",,"Lu, M|AbouRizk, SM|Hermann, UH",JOURNAL OF COMPUTING IN CIVIL ENGINEERING,,10.1061/(ASCE)0887-3801(2000)14:4(241)
339,WOS:000290190800017,2011,Advances in concrete arch dams shape optimization,DESIGN,"This paper presents an efficient methodology to find the optimum shape of arch dams. In order to create the geometry of arch dams a new algorithm based on Hermit Splines is proposed. A finite element based shape sensitivity analysis for design-dependent loadings involving body force, hydrostatic pressure and earthquake loadings is implemented. The sensitivity analysis is performed using the concept of mesh design velocity. In order to consider the practical requirements in the optimization model such as construction stages, many geometrical and behavioral constrains are included in the model in comparison with previous researches. The optimization problem is solved via the sequential quadratic programming (SQP) method. The proposed methods are applied successfully to an Iranian arch dam, and good results are achieved. By using such methodology, efficient software for shape optimization of concrete arch dams for practical and reliable design now is available.", (C) 2011 Elsevier Inc. All rights reserved.,"Akbari, J|Ahmadi, MT|Moharrami, H",APPLIED MATHEMATICAL MODELLING,arch dam shape sensitivity analysis finite element modeling shape optimization,10.1016/j.apm.2011.01.020
340,WOS:000417943600002,2017,Parameter sensitivity analysis of a 1-D cold region lake model for land-surface schemes,GLOBAL SENSITIVITY GENERAL-CIRCULATION HYDROLOGIC MODEL ORGANIC-MATTER CLIMATE MODELS WATER-QUALITY GREAT-LAKES UNCERTAINTY ATMOSPHERE HEAT,"Lakes might be sentinels of climate change, but the uncertainty in their main feedback to the atmosphere heat- exchange fluxes - is often not considered within climate models. Additionally, these fluxes are seldom measured, hindering critical evaluation of model output. Analysis of the Canadian Small Lake Model (CSLM), a one-dimensional integral lake model, was performed to assess its ability to reproduce diurnal and seasonal variations in heat fluxes and the sensitivity of simulated fluxes to changes in model parameters, i.e., turbulent transport parameters and the light extinction coefficient (K-d). A C++ open-source software package, Problem Solving environment for Uncertainty Analysis and Design Exploration (PSUADE), was used to perform sensitivity analysis (SA) and identify the parameters that dominate model behavior. The generalized likelihood uncertainty estimation (GLUE) was applied to quantify the fluxes' uncertainty, comparing daily-averaged eddy-covariance observations to the output of CSLM. Seven qualitative and two quantitative SA methods were tested, and the posterior likelihoods of the modeled parameters, obtained from the GLUE analysis, were used to determine the dominant parameters and the uncertainty in the modeled fluxes. Despite the ubiquity of the equifinality issue - different parameter-value combinations yielding equivalent results-the answer to the question was unequivocal: K-d, a measure of how much light penetrates the lake, dominates sensible and latent heat fluxes, and the uncertainty in their estimates is strongly related to the accuracy with which K-d is determined. This is important since accurate and continuous measurements of K-d could reduce modeling uncertainty.",,"Guerrero, JL|Pernica, P|Wheater, H|Mackay, M|Spence, C",HYDROLOGY AND EARTH SYSTEM SCIENCES,,10.5194/hess-21-6345-2017
341,WOS:000300438500006,2012,GIS Water-Balance Approach to Support Surface Water Flood-Risk Management,RESOLUTION TOPOGRAPHIC DATA DIGITAL ELEVATION MODELS DISTRIBUTED MODEL OVERLAND-FLOW URBAN AREAS INUNDATION SIMULATION DRAINAGE RUNOFF SYSTEM,"Controversy has arisen as to whether the lack of appropriate consideration to surface water flood risk in urban spatial planning is reducing the capacity to manage urban flood risk. A screening tool is required which would allow spatial planners to identify potential surface water flood risks and explore their management opportunities. An urban water balance approach is presented. The hypothesis is that key hydrological characteristics, storage volume and location, flow paths, and surface water generation, capture the key processes responsible for surface water flooding. The model is assembled and run by using ESRI ArcGIS software. Surface sinks and their catchment areas are identified by using a Lidar DEM. Excess surface water is calculated by using a runoff coefficient that is applied to rainfall volumes, and no other losses are considered. A surface water accumulation module sums the excess surface water from the catchment area of each sink. A sensitivity analysis of model assumptions demonstrates that these are valid for a screening tool. An informal validation of the model with local authority data revealed that most of the known flood risk locations were highlighted by the model. The model is applied to Keighley and sample results illustrate how knowledge of sink storage can be interpreted to explore opportunities for flood risk management. The model is a useful tool for quickly assessing potential flood risk locations and basic management options. DOI: ./(ASCE)HE.-..", (C) 2012 American Society of Civil Engineers.,"Diaz-Nieto, J|Lerner, DN|Saul, AJ|Blanksby, J",JOURNAL OF HYDROLOGIC ENGINEERING,pluvial flooding surface water gis water balance flood risk keighley uk lidar urban drainage,10.1061/(ASCE)HE.1943-5584.0000416
342,WOS:000275585100011,2010,Development of expert system modeling based decision support system for swine manure management,LOSSES,"Animal waste has always been considered as a resource for agricultural input as biofertilizer. However, the management is becoming more stringent due to environmental regulations. Livestock producers are faced with different manure management options that may be implemented into their operations. Given the expansion of the livestock industry, the implementation of environmental regulations, and the increasing importance of social and health issues, the selection of optimal manure management systems is becoming a strategically important task. Increasingly, integrated decision support systems (DSSs) are becoming necessary to assist decision makers in their evaluation of different manure management alternatives, like, liquid system, semi-solid system, solid system and bio-gas or bio-energy system based on combinations of different manure management sub-systems (collection, storage and application). To address this situation, a user-friendly computer program called Integrated Swine Manure Management (ISMM) is being developed for the Canadian Prairie provinces. Decision criteria including environmental, agronomic, social and health, greenhouse gas emission, and economic factors have been considered for the selection. design, and operation of the DSS. The expert system modeling is based on Visual Basic programming. Decision on adopting a particular combination of systems components is based on performance rating of the overall system. The program is interactive so that weighting factors for the different decision criteria can be varied to suit site-specific considerations. In this paper, the systems approach for development of an integrated liquid manure management system is discussed. Using a case study, sensitivity analysis of different combinations of management components is also reported for systems performance. The decision software compared satisfactorily with other available DSS packages.", (C) 2010 Elsevier B.V. All rights reserved.,"Karmakar, S|NKetia, M|Lague, C|Agnew, J",COMPUTERS AND ELECTRONICS IN AGRICULTURE,manure management systems engineering approach expert system decision support system (dss) decision criteria,10.1016/j.compag.2009.12.009
343,WOS:000285122400004,2010,Efficient solution for Galerkin-based polynomial chaos expansion systems,STOCHASTIC PROJECTION METHOD FINITE-ELEMENT SYSTEMS ITERATIVE SOLUTION LINEAR-SYSTEMS FLUID-FLOW MODELS,"Iterative solvers and preconditioners are widely used for handling the linear system of equations arising from stochastic finite element method (SFEM) formulations, e.g. galerkin-based polynomial chaos (G-P-C) Expansion method. Especially, Preconditioned Conjugate Gradient (PCG) solver and the Incomplete Cholesky (IC) preconditioner are shown to be adequate choices within this context. In this study, approaches for the automated adjustment of the input parameters for these tools are to be introduced. The proposed algorithms aim to enable the use of the PCG solver and IC preconditioner in a black-box fashion. As a result, the requirement of the expertise for using these tools is removed to a certain extend. Furthermore, these algorithms can be used also for the implementation purposes of SFEM's within general purpose software by increasing the ease of the use of these tools and hence leading to an improved user-comfort. (C) ", Elsevier Ltd. All rights reserved.,"Panayirci, HM",ADVANCES IN ENGINEERING SOFTWARE,stochastic finite elements polynomial chaos expansion computational efficiency iterative solvers preconditioners uncertainty quantification,10.1016/j.advengsoft.2010.09.004
344,WOS:000270369500007,2009,Probabilistic Assessment Study of Channels Downstream Slopes Erosion in the Maritime Environment,,"This research deals with the probabilistic simulation and assessment of erosion in the downstream maritime slopes in hop ports (ports with deep approach channels to be able to accommodate the recent vessels generations) with natural side slopes. The study concentrated on the liquefaction effect in the erosion factor, which is the main controllable parameter for this phenomena. The probability of failure for the limit state function represents the erosion factor, which has a liable representation by a normal distribution with parameters mu = . and sigma = ., as a representative limit state function. This research deals with a maritime channel with certain dimensions as an example. The probabilistic simulations for downstream slope erosion were carried out using the Monte Carlo technique by using a probabilistic model. The generated probabilistic histograms of the erosion factor based on one run and different numbers of simulated random samples were determined. Based on these reliability simulation results, the erosion volumes per unit width of the channel were evaluated. Validation and sensitivity analyses were also carried out to ensure more reliability for this research. The study produced a group of guiding regression models for estimates and the determined conclusions related to the evaluated erosion volumes we carefully examined by considering calculation conditions based on a % confidence level with different assumptions. Then preliminary estimates for the eroded volumes (m()/m) in the downstream slope of the channel were evaluated and so used to determine the relevant regression models. These distributions were determined based on a group of assumed realistic conditions, which include variable berm depths and constant downstream slope angles in one simulated group with erosion volumes against downstream slopes depths heights variation and constant berm depths and variable slope angles in another with erosion volumes against downstream slopes angles variation. The limit state functions representing the erosion volumes variation behavior under the different conditions were also determined by using reliable statistical goodness-of-fit software. The research results are presented in a graphical form for the purpose of improving the current application capabilities in the subject and providing practical usage for the unprotected maritime navigation channel, trenches, and maritime downstream slopes.",,"Khalifa, MA|El Ganainy, MA|Nasr, RI",JOURNAL OF COASTAL RESEARCH,maritime downstream slopes hop ports approach channels downstream erosion factor downstream slopes liquefaction probabilistic simulations monte carlo simulation probabilistic slope failure sensitivity analysis downstream erosion volumes limit state functions,10.2112/08-1074.1
345,WOS:000355932400009,2015,"Parameter sensitivity analysis and optimization of Noah land surface model with field measurements from Huaihe River Basin, China",MESOSCALE ETA-MODEL WATERSHED MODEL ENVIRONMENTAL-MODELS UNCERTAINTY CALIBRATION IMPLEMENTATION HYDROLOGY SYSTEMS IMPACT EVAPORATION,"This study aims to identify the parameters that are most important in controlling the Noah land surface model (LSM), the analysis of parameter interactions, and the evaluation of the performance of parameter optimization using the parameter estimation software PEST. We found it necessary to analyze parameter sensitivity in order to properly simulate hydrological variables such as latent heat flux in the Huaihe River Basin, China. The parameters under study in the Noah LSM link thermodynamic and hydrological parts into a complete model. To our knowledge, this parameter interaction in the Noah LSM has never been studied before. There are, however, several studies concerning the influence of vegetation types and climate conditions on parameter sensitivity of the Noah LSM. Three sensitivity analysis methods, the including local sensitivity analysis method SENSAN, regional sensitivity analysis, and Sobol's method, were tested. Five experimental sites in the Huaihe River Basin were chosen to perform the simulations. The results show that the Noah LSM parameter sensitivities were impacted by the choice of the analysis method. The local method SENSAN often produced significant differences in results compared to the two global methods. The parameter interactions investigated made a significant contribution towards elucidating how one process influences another in the Noah LSM. The results show that parameters were not transferable solely based on vegetation types but also rely on climate conditions. According to the sensitivity analysis results, four sensitive parameters were chosen to be optimized using the PEST method. PEST is a widely used method for estimating parameters in models. Root-mean-square error was used to evaluate the effect of the optimization. Generally in all sites, the optimized parameters values perform better than the original parameter values.",,"Hou, T|Zhu, YH|Lu, HS|Sudicky, E|Yu, ZB|Ouyang, F",STOCHASTIC ENVIRONMENTAL RESEARCH AND RISK ASSESSMENT,noah lsm huaihe river sensitivity analysis rsa sobol's method pest,10.1007/s00477-015-1033-5
346,WOS:000245786200002,2007,Stormwater pollutant loads modelling: epistemological aspects and case studies on the influence of field data sets on calibration and verification,REGRESSION-MODELS,"In urban drainage, stormwater quality models have been used by researchers and practitioners for more than  years. Most of them were initially developed for research purposes, and have been later on implemented in commercial software packages devoted to operational needs. This paper presents some epistemological problems and difficulties with practical consequences in the application of stormwater quality models, such as simplified representation of reality, scaling-up, over-parameterisation, transition from calibration to verification and prediction, etc. Two case studies (one to estimate pollutant loads at the outlet of a catchment, one to design a detention tank to reach a given pollutant interception efficiency), with simple and detailed stormwater quality models, illustrate some of the above problems. It is hard to find, if not impossible, an ""optimum"" or ""best"" unique set of parameters values. Model calibration and verification appear to dramatically depend on the data sets used for their calibration and verification. Compared to current practice, collecting more and reliable data is absolutely necessary.",,"Bertrand-Krajewski, JL",WATER SCIENCE AND TECHNOLOGY,calibration epistemology field data modelling sensitivity analysis separate and combined sewers stormwater verification,10.2166/wst.2007.090
347,WOS:000403512500013,2017,"The rocky road to extended simulation frameworks covering uncertainty, inversion, optimization and control",REAL-TIME MANAGEMENT COMPUTATIONAL SCIENCE CO2 STORAGE FLOW ASSIMILATION VERIFICATION ALGORITHMS COMPLEXITY TRANSPORT SELECTION,"In the past decades, simulation frameworks have greatly increased in complexity, due to coupling of models from various disciplines into so-called integrated models. Recently, the combination with tools for uncertainty quantification, inverse modelling, optimization and control started a development towards what we call extended simulation frameworks. While there is an ongoing discussion on quality assurance and reproducibility for simulation frameworks, we have not observed a similar discussion for the extended case. Particularly for extended frameworks, the need for quality assurance is high: The overwhelming range of options and algorithms is unmanageable by a domain expert and opaque to decision makers or the public. The resulting demand for 'intelligent software' with automated configuration can lead to a blind trust in simulation results even if they are incorrect. This is a threatening scenario due to potential consequences in simulation-based engineering or political decisions. In this paper, we analyze the increasing complexity of scientific computing workflows, and discuss the corresponding problems of extended scientific simulation frameworks. We propose a paradigm that regulates the allowable properties of framework components, supports the framework configuration for complex simulations, enforces automatic self-tests of configured frameworks, and communicates automated algorithm choices, potentially critical user settings or convergence issues with adaptive detail level and urgency to the end-user. Our goal is to start transferring the quality assurance discussion in the field of integrated modeling and conventional software frameworks to the area of extended simulation frameworks. With this, we hope to increase the reliability and transparency of (extended) frameworks, framework use and of the corresponding simulation results. (C) ", Elsevier Ltd. All rights reserved.,"Wirtz, D|Nowak, W",ENVIRONMENTAL MODELLING & SOFTWARE,scientific computing extended software frameworks no free lunch reproducibility software trust,10.1016/j.envsoft.2016.10.003
348,WOS:000388092400006,2016,"ALBANY: USING COMPONENT-BASED DESIGN TO DEVELOP A FLEXIBLE, GENERIC MULTIPHYSICS ANALYSIS CODE",EMBEDDED ANALYSIS CAPABILITIES MANAGING SOFTWARE COMPLEXITY FINITE-ELEMENT PARALLEL SIMULATION FRAMEWORK EQUATIONS SYSTEMS LIBRARY,"Albany is a multiphysics code constructed by assembling a set of reusable, general components. It is an implicit, unstructured grid finite element code that hosts a set of advanced features that are readily combined within a single analysis run. Albany uses template-based generic programming methods to provide extensibility and flexibility; it employs a generic residual evaluation interface to support the easy addition and modification of physics. This interface is coupled to powerful automatic differentiation utilities that are used to implement efficient nonlinear solvers and preconditioners, and also to enable sensitivity analysis and embedded uncertainty quantification capabilities as part of the forward solve. The flexible application programming interfaces in Albany couple to two different adaptive mesh libraries; it internally employs generic integration machinery that supports tetrahedral, hexahedral, and hybrid meshes of user specified order. We present the overall design of Albany, and focus on the specifics of the integration of many of its advanced features. As Albany and the components that form it are openly available on the internet, it is our goal that the reader might find some of the design concepts useful in their own work. Albany results in a code that enables the rapid development of parallel, numerically efficient multiphysics software tools. In discussing the features and details of the integration of many of the components involved, we show the reader the wide variety of solution components that are available and what is possible when they are combined within a simulation capability.",,"Salinger, AG|Bartlett, RA|Bradley, AM|Chen, QS|Demeshko, IP|Gao, XJ|Hansen, GA|Mota, A|Muller, RP|Nielsen, E|Ostien, JT|Pawlowski, RP|Perego, M|Phipps, ET|Sun, WC|Tezaur, IK",INTERNATIONAL JOURNAL FOR MULTISCALE COMPUTATIONAL ENGINEERING,partial differential equations finite element analysis template-based generic programming,10.1615/IntJMultCompEng.2016017040
349,WOS:000258484000001,2008,Software framework for parameter updating and finite-element response sensitivity analysis,INELASTIC STRUCTURES RELIABILITY SERVICES PROGRAM,"The finite-element software framework OpenSees is extended with parameter updating and response sensitivity capabilities to support client applications such as reliability, optimization, and system identification. Using software design patterns, member properties, applied loadings, and nodal coordinates can be identified and repeatedly updated in order to create customized finite-element model updating applications. Parameters are identified using a Chain of Responsibility software pattern, where objects in the finite-element model forward a parameterization request to component objects until the request is handled. All messages to identify and update parameters are passed through a Facade that decouples client applications from the finite-element domain of OpenSees. To support response sensitivity analysis, the Strategy design pattern facilitates multiple approaches to evaluate gradients of the structural response, whereas the Visitor pattern ensures that objects in the finite-element domain make the proper contributions to the equations that govern the response sensitivity. Examples demonstrate the software design and the steps taken by representative finite-element model updating and response sensitivity applications.",,"Scott, MH|Haukaas, T",JOURNAL OF COMPUTING IN CIVIL ENGINEERING,,10.1061/(ASCE)0887-3801(2008)22:5(281)
350,WOS:000245063400009,2007,Numerical and visual evaluation of hydrological and environmental models using the Monte Carlo analysis toolbox,RAINFALL-RUNOFF MODELS REGIONALIZED SENSITIVITY ANALYSIS WATER-QUALITY IMPROVED CALIBRATION PHOSPHORUS TRANSFER CONCEPTUAL MODELS UNCERTAINTY CATCHMENT IDENTIFICATION SYSTEMS,"The detailed evaluation of mathematical models and the consideration of uncertainty in the modeling of hydrological and environmental systems are of increasing importance, and are sometimes even demanded by decision makers. At the same time, the growing complexity of models to represent real-world systems makes it more and more difficult to understand model behavior, sensitivities and uncertainties. The Monte Carlo Analysis Toolbox (MCAT) is a Matlab library of visual and numerical analysis tools for the evaluation of hydrological and environmental models. Input to the MCAT is the result of a Monte Carlo or population evolution based sampling of the parameter space of the model structure under investigation. The MCAT can be used off-line, i.e. it does not have to be connected to the evaluated model, and can thus be used for any model for which an appropriate sampling can be performed. The MCAT contains tools for the evaluation of performance, identitiability, sensitivity, predictive uncertainty and also allows for the testing of hypotheses with respect to the model structure used. In addition to research applications, the MCAT can be used as a teaching tool in courses that include the use of mathematical models. (c) ", Elsevier Ltd. All rights reserved.,"Wagener, T|Kollat, J",ENVIRONMENTAL MODELLING & SOFTWARE,evaluation uncertainty analysis sensitivity analysis identifiability hypothesis testing model diagnostics matlab visualization,10.1016/j.envsoft.2006.06.017
351,WOS:000371777100004,2015,Sensitivity of algorithm parameters and objective function scaling in multi-objective optimisation of water distribution systems,OF-THE-ART DISTRIBUTION NETWORKS GENETIC ALGORITHMS EVOLUTIONARY ALGORITHMS OPTIMAL OPERATION NSGA-II DECISION-MAKING TOTAL-COST DESIGN QUALITY,"This paper presents an extensive analysis of the sensitivity of multi-objective algorithm parameters and objective function scaling tested on a large number of parameter setting combinations for a water distribution system optimisation problem. The optimisation model comprises two operational objectives minimised concurrently, the pump energy costs and deviations of constituent concentrations as a water quality measure. This optimisation model is applied to a regional non-drinking water distribution system, and solved using the optimisation software GANetXL incorporating the NSGA-II linked with the network analysis software EPANet. The sensitivity analysis employs a set of performance metrics, which were designed to capture the overall quality of the computed Pareto fronts. The performance and sensitivity of NSGA-II parameters using those metrics is evaluated. The results demonstrate that NSGA-II is sensitive to different parameter settings, and unlike in the single-objective problems, a range of parameter setting combinations appears to be required to reach a Pareto front of optimal solutions. Additionally, inadequately scaled objective functions cause the NSGA-II bias towards the second objective. Lastly, the methodology for performance and sensitivity analysis may be used for calibration of algorithm parameters.",,"Mala-Jetmarova, H|Barton, A|Bagirov, A",JOURNAL OF HYDROINFORMATICS,algorithm parameters multi-objective optimisation performance metrics scaling sensitivity water distribution systems,10.2166/hydro.2015.062
352,WOS:000244915000009,2007,Topology optimization of material-nonlinear continuum structures by the element connectivity parameterization,DESIGN SENSITIVITY-ANALYSIS ELASTOPLASTIC STRUCTURES COMPLIANT MECHANISMS PLASTIC-DEFORMATION CONTACT CRASHWORTHINESS,"The application of the element density-based topology optimization method to nonlinear continuum structures is limited to relatively simple problems such as bilinear elastoplastic material problems. Furthermore, it is very difficult to use analytic sensitivity when a commercial nonlinear finite element code is used. As an alternative to the element density formulation, the element connectivity parameterization (ECP) formulation is developed for the topology optimization of isotropic-hardening elastoplastic or hyperelastic continua by using commercial software. ECP varies the stiffness of zero-length linear elastic links that connect design domain-discretizing finite elements. Unloading was not considered. But the advantages of ECP in material-nonlinear problems were demonstrated: considerably simple analytic sensitivity calculation using a commercial code and simple link stiffness penalization regardless of nonlinear material behaviour."," Copyright (c) 2006 John Wiley & Sons, Ltd.","Yoon, GH|Kim, YY",INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,topology optimization material-nonlinearity element connectivity parameterization,10.1002/nme.1843
353,WOS:000308319300005,2012,A Computational Fluid Dynamic Model for Prediction of Organic Dyes Adsorption from Aqueous Solutions,LOW-COST ADSORBENT WASTE-WATER RED MUD METHYLENE-BLUE INDUSTRY WASTE CONGO-RED BASIC DYE FLY-ASH REMOVAL EQUILIBRIUM,"Modelling of the removal of synthetic dyes from aqueous solutions by adsorbents is important to develop an appropriate treatment plan using adsorption process. This paper presents a computational fluid dynamic model incorporating the Langmuir isotherm scheme and second-order kinetic expression to describe the adsorption process. The governing equation of the model was numerically solved using PHOENICS software to simulate synthetic dyes adsorption from the aqueous system. The experimental results presented in this study and taken from the literature for the removal of synthetic dyes were compared with those results predicted by the numerical model. The predicted outputs of the model match the experimental measurements satisfactory. A sensitivity analysis of the major parameters that influence the percent of dye removal from solution phase has been carried out. Three of the main parameters taken into account were the kinetic rate constant, amount of dye adsorbed at equilibrium and the Langmuir isotherm constant. It was found that the model is most sensitive to the amount of dye adsorbed at equilibrium. This effect is most obvious at the early stages of the adsorption process when the rate of dye removal is very fast. Quantification of the reaction mechanism allows developing an appropriate remediation strategy based on the adsorption process.",,"Ardejani, FD|Badii, K|Farhadi, F|Saberi, MA|Shokri, BJ",ENVIRONMENTAL MODELING & ASSESSMENT,computational fluid dynamic synthetic dyes kinetics adsorption sensitivity analysis,10.1007/s10666-012-9310-x
354,WOS:000186661200005,2003,Development and application of computer simulation tools for ecological risk assessment,FOOD WEBS EXPOSURE MODEL WATER CONTAMINANTS CHEMICALS TOXICITY ROUTES SOIL,"Based on a review of available models for ecological risk estimation, most are site-specific and their applications are limited. However, general models, which can be easily adapted to other sites, remain few, in addition, they are simple and associated with significant uncertainties. In this paper, an approach is introduced for an ecological risk assessment ( ERA) model that can be modified for site-specific conditions. Using computer simulation as a screening tool for ecological risk assessment can assist environmental managers and policy decision-makers in the planning and implementation of potentially highly focused assessments and remediation, should the ERA dictate the need. The model was integrated with a Windows-based interface and interactive database management system (DBMS) as a user-friendly software package. In addition, based on trophic sources, a food web has been integrated into the framework of the DBMS. In an effort to evaluate the model, a case study was implemented to characterize the effects on an ecosystem of replacing electroplated chromium coatings with sputtered tantalum at U. S. Army Yuma and Aberdeen Proving Grounds. Potential exposure pathways included ingestion, inhalation, and dermal absorption for terrestrial animals; root and foliar uptake for plants; and direct absorption for aquatic species. Overall, results showed that the most significant exposure resulted from molybdenum and hexavalent chromium, which posed higher risks to select aquatic and terrestrial species at both sites. On the other hand, tantalum ( with vanadium as the surrogate) resulted in the least risk to all receptors within the studied areas. A sensitivity analysis demonstrated that soil-water distribution coefficients have a significant impact on the results. Based on the results, neither molybdenum nor chromium are recommended as a coating in gun barrels, and further study would be essential to address any affected firing range area. Tantalum is recommended for use, although for those species receiving a slight adverse risk, field investigations that include receptor sampling maybe necessary once soil/sediment and water sampling validates projected concentrations.",,"Lu, HY|Axe, L|Tyson, TA",ENVIRONMENTAL MODELING & ASSESSMENT,ecological risk assessment exposure model heavy metals food web,10.1023/B:ENMO.0000004585.85305.3d
355,WOS:000377792100014,2016,"Application of run-off model as a contribution to the torrential flood risk management in Topiderska Reka watershed, Serbia",CATCHMENTS,"This paper deals with developing of hydrological flow model for the torrential watershed of the Topiderska River, located in the Belgrade macro-region, in which torrential flood events have had destructive human and material consequences. The aim of the paper was to show that model, developed by hydrological software SHETRAN, is useful tool enabling the simulation of physical properties and hydrological processes in watersheds, having a significant place in decision support system within the torrential flood risk management. Results achievement was enabled through three phases: () Sensitivity analysis showed that the closest hydrograph to registered hydrograph is modelled with averaged values of parameters; () model calibration was done by calibration of four main parameters with significant influence on flow component (hydraulic conductivity of soil and rock, Strickler roughness coefficient for overland and river network flow) in case of the torrential flood event from th July ; and () model verification was done by input of calibrated values in case of two another torrential flood events (, ). Statistical analysis of correspondence of modelled and registered discharges showed that due to correlation coefficient (.-.) and determination coefficient (.-.) as well as probability of error according to F test, good results are achieved. Considering obtained results, developed model should be a part of torrential flood risk management in watershed of Topiderska River.",,"Petrovic, AM|Kovacevic-Majkic, J|Milosevic, MV",NATURAL HAZARDS,hydrological model run-off discharge torrential flood topciderska river,10.1007/s11069-016-2269-1
356,WOS:000323349500001,2013,Optimal Design of Signal Controlled Road Networks Using Differential Evolution Optimization Algorithm,EQUILIBRIUM TRANSPORTATION NETWORKS VARIATIONAL INEQUALITY CONSTRAINTS SIMULATED ANNEALING APPROACH AREA TRAFFIC CONTROL GENETIC ALGORITHM SENSITIVITY ANALYSIS ASSIGNMENT TIMINGS FLOW,"This study proposes a traffic congestion minimization model in which the traffic signal setting optimization is performed through a combined simulation-optimization model. In this model, the TRANSYT traffic simulation software is combined with Differential Evolution (DE) optimization algorithm, which is based on the natural selection paradigm. In this context, the EQuilibrium Network Design (EQND) problem is formulated as a bilevel programming problem in which the upper level is the minimization of the total network performance index. In the lower level, the traffic assignment problem, which represents the route choice behavior of the road users, is solved using the Path Flow Estimator (PFE) as a stochastic user equilibrium assessment. The solution of the bilevel EQND problem is carried out by the proposed Differential Evolution and TRANSYT with PFE, the so-called DETRANSPFE model, on a well-known signal controlled test network. Performance of the proposed model is compared to that of two previous works where the EQND problem has been solved by Genetic-Algorithms- (GAs-) and Harmony-Search- (HS-) based models. Results show that the DETRANSPFE model outperforms the GA- and HS-based models in terms of the network performance index and the computational time required.",,"Ceylan, H",MATHEMATICAL PROBLEMS IN ENGINEERING,,10.1155/2013/696374
357,WOS:000249622700001,2007,An integrated framework for multipollutant air quality management and its application in georgia,SOUTHEASTERN UNITED-STATES SOURCE APPORTIONMENT TIME-SERIES POLLUTION OZONE MODEL EMISSIONS MORTALITY HEALTH PM2.5,"Air protection agencies in the United States increasingly confront non-attainment of air quality standards for multiple pollutants sharing interrelated emission origins. Traditional approaches to attainment planning face important limitations that are magnified in the multipollutant context. Recognizing those limitations, the Georgia Environmental Protection Division has adopted an integrated framework to address ozone, fine particulate matter, and regional haze in the state. Rather than applying atmospheric modeling merely as a final check of an overall strategy, photochemical sensitivity analysis is conducted upfront to compare the effectiveness of controlling various precursor emission species and source regions. Emerging software enables the modeling of health benefits and associated economic valuations resulting from air pollution control. Photochemical sensitivity and health benefits analyses, applied together with traditional cost and feasibility assessments, provide a more comprehensive characterization of the implications of various control options. The fuller characterization both informs the selection of control options and facilitates the communication of impacts to affected stakeholders and the public. Although the integrated framework represents a clear improvement over previous attainment-planning efforts, key remaining shortcomings are also discussed.",,"Cohan, DS|Boylan, JW|Marmur, A|Khan, MN",ENVIRONMENTAL MANAGEMENT,air pollution control cost-benefit analysis ozone fine particulate matter state implementation plans attainment,10.1007/s00267-006-0228-4
358,WOS:000387850900012,2016,Two-scale topology design optimization of stiffened or porous plate subject to out-of-plane buckling constraint,STRUCTURAL OPTIMIZATION HOMOGENIZATION METHOD HIERARCHICAL-OPTIMIZATION OPTIMUM STRUCTURE MICROSTRUCTURES PENALIZATION STIFFNESS LAYOUT BEAM,"This paper studies maximum out-of-plane buckling load design of thin bending plates for a given amount of material. Two kinds of plates are considered. One is made of periodic homogeneous porous material. Another is uniformly stiffened solid plate. The plate material, thickness, design domain of its middle plane and boundary conditions are given. The pattern of prescribed in-plane external load or displacements along the part of boundaries, which move freely, is given. Both plate topology and micro-structural topology of porous material or stiffener layout are concurrently optimized. The artificial element material densities in both macro and micro-scale are chosen as design variables. The volume preserving nonlinear density filter is applied to obtain the black-white optimum topology and comparison of its different sensitivities is made to show the reason for oscillation during optimization process in Appendix. The new numerical implementation of asymptotic homogenization method (NIAH, Cheng (Acta Mech Sinica (): -, ) and Cai (Int J Solids Struct (), -, ) is applied to homogenization of periodic plate structures and analytic sensitivity analysis of effective stiffness with respect to the topological design variables in both macro-scale and micro-scale. On basis of that, this paper implements the sensitivity analysis of out-of-plane buckling load by using commercial FEA software and enables the application of gradient-based search algorithm in optimization. Several numerical implementation details are discussed. Three numerical examples are given to show the validity of this method.",,"Cheng, GD|Xu, L",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,two-scale analysis concurrent topology optimization buckling constraints niah method,10.1007/s00158-016-1542-y
359,WOS:000321690600011,2013,Data-driven sensitivity analysis to detect missing data mechanism with applications to structural equation modelling,GOODNESS-OF-FIT IMPROPER SOLUTIONS STANDARD ERRORS VALUES,"Missing data are a common problem in almost all areas of empirical research. Ignoring the missing data mechanism, especially when data are missing not at random (MNAR), can result in biased and/or inefficient inference. Because MNAR mechanism is not verifiable based on the observed data, sensitivity analysis is often used to assess it. Current sensitivity analysis methods primarily assume a model for the response mechanism in conjunction with a measurement model and examine sensitivity to missing data mechanism via the parameters of the response model. Recently, Jamshidian and Mata (Post-modelling sensitivity analysis to detect the effect of missing data mechanism, Multivariate Behav. Res.  (), pp. -) introduced a new method of sensitivity analysis that does not require the difficult task of modelling the missing data mechanism. In this method, a single measurement model is fitted to all of the data and to a sub-sample of the data. Discrepancy in the parameter estimates obtained from the the two data sets is used as a measure of sensitivity to missing data mechanism. Jamshidian and Mata describe their method mainly in the context of detecting data that are missing completely at random (MCAR). They used a bootstrap type method, that relies on heuristic input from the researcher, to test for the discrepancy of the parameter estimates. Instead of using bootstrap, the current article obtains confidence interval for parameter differences on two samples based on an asymptotic approximation. Because it does not use bootstrap, the developed procedure avoids likely convergence problems with the bootstrap methods. It does not require heuristic input from the researcher and can be readily implemented in statistical software. The article also discusses methods of obtaining sub-samples that may be used to test missing at random in addition to MCAR. An application of the developed procedure to a real data set, from the first wave of an ongoing longitudinal study on aging, is presented. Simulation studies are performed as well, using two methods of missing data generation, which show promise for the proposed sensitivity method. One method of missing data generation is also new and interesting in its own right.",,"Jamshidian, M|Yuan, KH",JOURNAL OF STATISTICAL COMPUTATION AND SIMULATION,factor analysis generating missing data incomplete data missing at random missing not at random sensitivity analysis simulation sub-sample,10.1080/00949655.2012.660486
360,WOS:000336668600061,2014,A New Algorithm for Small-Signal Analysis of DC-DC Converters,POWER CONVERTERS STABILITY ANALYSIS PWM CONVERTERS A SURVEY IMPLEMENTATION SYSTEMS,"This paper presents a new approach for small-signal analysis of all types of dc-dc converters with any number of topological modes within a switching cycle. So far, sampled-data modeling and sensitivity analysis are mostly used for such a purpose. In both cases, the switching conditions implicitly appear in the small-signal matrices which increases the complexity of the computation for the system with a larger number of topological modes. Here, we propose an alternative approach based on Filippov's method, which studies the effects of each switching separately. It uses a shooting method with an event detector to locate a periodic steady-state, and, when the Newton-Raphson search process of the shooting method converges, it also gives the Jacobian matrix. The algorithm can be easily implemented in a software program as an analytical tool which is expected to be useful for fast and accurate frequency-domain analysis (by small-signal transfer functions) to facilitate controller design. Moreover, since it uses the Filippov's method, this algorithm can also predict the slow-and fast-timescale instabilities.",,"Mandal, K|Banerjee, S|Chakraborty, C",IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS,dc-dc converters filippov's method resonant converters small-signal analysis,10.1109/TII.2013.2277942
361,WOS:000371989100016,2016,Model development and process simulation of postcombustion carbon capture technology with aqueous AMP/PZ solvent,CO2 CAPTURE DIOXIDE CAPTURE MASS-TRANSFER PILOT-SCALE FLUE-GAS STRUCTURED PACKINGS TERNARY VLE PIPERAZINE 2-AMINO-2-METHYL-1-PROPANOL KINETICS,"This study presents the development, application, and uncertainty analysis of a process simulation model for postcombustion CO capture with an AMP/PZ solvent blend based on state of the art knowledge on AMP/PZ solvent technology. The development includes the improvement of the physical property models of a software package designed for simulation of acid gas treatment and CO capture technologies. The improvement particularly consisted of regression of AMP-PZ binary interaction parameters. The model was applied to a case study of postcombustion CO capture from an Advanced Super Critical Pulverized Coal power plant. Uncertainly analysis was undertaken by validating the physical property models against laboratory measurements reported in literature; by comparing model results with pilot study results, and by evaluating the strength of the model with a novel method called pedigree analysis. The results show that AMP/PZ postcombustion technology performs better than MEA technology on most performance indicators, e.g., the Specific Reboiler Duty is reduced from . GJ/t CO for MEA, to . GJ/t CO for AMP/PZ, and the specific cooling water requirement is reduced from . to . GJ/t CO. Only amine slip to the atmosphere increases with AMP/PZ technology: from . g/t CO to . g/t CO, although this value is still within emission limits from existing regulatory frameworks. The coal power plant net efficiency with AMP/PZ capture amounts to a value of .%(LHV), compared to .%(LHV) for the case without CCS and .%(LHV) in case of CCS with MEA. The uncertainty analysis shows that the model is well capable of predicting experimental and pilot result. The remaining uncertainty is mostly in the reaction kinetics and in the flowsheet design. Validation could be further improved, by more elaborate comparison to independent measures of physical properties, and by comparison of the model outputs to results from large demonstration or commercial size capture plants. (C) ", Elsevier Ltd. All rights reserved.,"van der Spek, M|Arendsen, R|Ramirez, A|Faaij, A",INTERNATIONAL JOURNAL OF GREENHOUSE GAS CONTROL,model development process simulation postcombustion co2 capture amp/pz solvent uncertainty analysis pedigree analysis,10.1016/j.ijggc.2016.01.021
362,WOS:000317749400002,2013,Estimation of uncertainty sources in the projections of Lithuanian river runoff,CLIMATE-CHANGE IMPACTS FLOOD RISK-ASSESSMENT PARAMETER MODELS,"Particular attention is given to the reliability of hydrological modelling results. The accuracy of river runoff projection depends on the selected set of hydrological model parameters, emission scenario and global climate model. The aim of this article is to estimate the uncertainty of hydrological model parameters, to perform sensitivity analysis of the runoff projections, as well as the contribution analysis of uncertainty sources (model parameters, emission scenarios and global climate models) in forecasting Lithuanian river runoff. The impact of model parameters on the runoff modelling results was estimated using a sensitivity analysis for the selected hydrological periods (spring flood, winter and autumn flash floods, and low water). During spring flood the results of runoff modelling depended on the calibration parameters that describe snowmelt and soil moisture storage, while during the low water period-the parameter that determines river underground feeding was the most important. The estimation of climate change impact on hydrological processes in the Merkys and Neris river basins was accomplished through the combination of results from AB, A and B emission scenarios and global climate models (ECHAM and HadCM). The runoff projections of the thirty-year periods (-, -, -) were conducted applying the HBV software. The uncertainties introduced by hydrological model parameters, emission scenarios and global climate models were presented according to the magnitude of the expected changes in Lithuanian rivers runoff. The emission scenarios had much greater influence on the runoff projection than the global climate models. The hydrological model parameters had less impact on the reliability of the modelling results.",,"Kriauciuniene, J|Jakimavicius, D|Sarauskiene, D|Kaliatka, T",STOCHASTIC ENVIRONMENTAL RESEARCH AND RISK ASSESSMENT,lithuanian rivers climate change hbv model calibration sensitivity and uncertainty analysis susa,10.1007/s00477-012-0608-7
363,WOS:000352040700012,2015,A comparative life cycle assessment of conventional hand dryer and roll paper towel as hand drying methods,UNCERTAINTY HYGIENE,"A comparative life cycle assessment, under a cradle to gate scope, was carried out between two hand drying methods namely conventional hand dryer use and dispenser issued roll paper towel use. The inventory analysis for this study was aided by the deconstruction of a hand dryer and dispenser unit besides additional data provided by the Physical Resources department, from the product system manufacturers and information from literature. The LCA software SimaPro, supported by the ecoinvent and US-EI databases, was used towards establishing the environmental impacts associated with the lifecycle stages of both the compared product systems. The Impact + method was used for classification and characterization of these environmental impacts. An uncertainty analysis addressing key input data and assumptions made, a sensitivity analysis covering the use intensity of the product systems and a scenario analysis looking at a US based use phase for the hand dryer were also conducted. Per functional unit, which is to achieve a pair of dried hands, the dispenser product system has a greater life cycle impact than the dryer product system across three of four endpoint impact categories. The use group of lifecycle stages for the dispenser product system, which represents the cradle to gate lifecycle stages associated with the paper towels, constitutes the major portion of this impact. For the dryer product system, the use group of lifecycle stages, which essentially covers the electricity consumption during dryer operation, constitutes the major stake in the impact categories. It is evident from the results of this study that per dry, for a use phase supplied by Ontario's grid ( grid mix scenario) and a United States based manufacturing scenario, the use of a conventional hand dryer (rated at  W and under a  s use intensity) has a lesser environmental impact than with using two paper towels (% recycled content, unbleached and weighing  g) issued from a roll dispenser.", (C) 2015 Elsevier B.V. All rights reserved.,"Joseph, T|Baah, K|Jahanfar, A|Dubey, B",SCIENCE OF THE TOTAL ENVIRONMENT,hand dryer paper towel life cycle assessment impact 2002+ uncertainty analysis,10.1016/j.scitotenv.2015.01.112
364,WOS:000209099900002,2011,ASSESSMENT OF COLLOCATION AND GALERKIN APPROACHES TO LINEAR DIFFUSION EQUATIONS WITH RANDOM DATA,,"We compare the performance of two methods, the stochastic Galerkin method and the stochastic collocation method, for solving partial differential equations (PDEs) with random data. The stochastic Galerkin method requires the solution of a single linear system that is several orders larger than linear systems associated with deterministic PDEs. The stochastic collocation method requires many solves of deterministic PDEs, which allows the use of existing software. However, the total number of degrees of freedom in the stochastic collocation method can be considerably larger than the number of degrees of freedom in the stochastic Galerkin system. We implement both methods using the Trilinos software package and we assess their cost and performance. The implementations in Trilinos are known to be efficient, which allows for a realistic assessment of the computational complexity of the methods. We also develop a cost model for both methods which allows us to examine asymptotic behavior.",,"Elman, HC|Miller, CW|Phipps, ET|Tuminaro, RS",INTERNATIONAL JOURNAL FOR UNCERTAINTY QUANTIFICATION,uncertainty quantification stochastic partial differential equations polynomial chaos stochastic galerkin method stochastic sparse grid collocation karhunen-loeve expansion,10.1615/Int.J.UncertaintyQuantification.v1.i1.20
365,WOS:000265341800001,2009,GUI-HDMR - A software tool for global sensitivity analysis of complex models,STREET CANYON MODEL ENVIRONMENTAL-MODELS RS-HDMR REPRESENTATIONS UNCERTAINTY PARAMETERS INDEXES OUTPUT,"The high dimensional model representation (HDMR) method is a set of tools which can be used to construct a fully functional metamodel and to calculate variance based sensitivity indices very efficiently. Extensions to the existing set of random sampling (RS)-HDMR tools have been developed in order to make the method more applicable for complex models with a large number of input parameters as often appear in environmental modelling. The HDMR software described here combines the RS-HDMR tools and its extensions in one Matlab package equipped with a graphical user interface (GUI). This makes the HDMR method easily available for all interested users. The performance of the GUI-HDMR software has been tested in this paper using two analytical test models, the Ishigami function and the Sobol' g-function. In both cases the model is highly non-linear, non-monotonic and has significant parameter interactions. The developed GUI-HDMR software copes very well with the test cases and sensitivity indices of first and second order could be calculated accurately with only low computational effort. The efficiency of the software has also been compared against other recently developed approaches and is shown to be competitive. GUI-HDMR can be applied to a wide range of applications in all fields, because in principle only one random or quasi-random set of input and output values is required to estimate all sensitivity indices up to second order. The size of the set of samples is however dependent on the problem and can be successively increased if additional accuracy is required. A brief description of its application within a range of modelling environments is given. (C) ", Elsevier Ltd. All rights reserved.,"Ziehn, T|Tomlin, AS",ENVIRONMENTAL MODELLING & SOFTWARE,global sensitivity analysis high dimensional model representation random sampling matlab software graphical user interface,10.1016/j.envsoft.2008.12.002
366,WOS:000240794000013,2006,Sensitivity analysis of differential-algebraic equations and partial differential equations,ADAPTIVE MESH REFINEMENT SYSTEMS SOFTWARE OPTIMIZATION ALGORITHMS,"Sensitivity analysis generates essential information for model development, design optimization, parameter estimation, optimal control, model reduction and experimental design. In this paper we describe the forward and adjoint methods for sensitivity analysis, and outline some of our recent work on theory, algorithms and software for sensitivity analysis of differential-algebraic equation (DAE) and time-dependent partial differential equation (PDE) systems. (c) ", Elsevier Ltd. All rights reserved.,"Petzold, L|Li, ST|Cao, Y|Serban, R",COMPUTERS & CHEMICAL ENGINEERING,sensitivity analysis differential-algebraic equations adjoint method,10.1016/j.compchemeng.2006.05.015
367,WOS:000348756700001,2015,Pi 4U: A high performance computing framework for Bayesian uncertainty quantification of complex models,LIQUID WATER EVOLUTIONARY STRATEGIES PROBABILISTIC APPROACH MARGINAL LIKELIHOOD DYNAMICAL-SYSTEMS INVERSE PROBLEMS UPDATING MODELS SIMULATION OPTIMIZATION RELIABILITY,"We present Pi U,() an extensible framework, for non-intrusive Bayesian Uncertainty Quantification and Propagation (UQ+P) of complex and computationally demanding physical models, that can exploit massively parallel computer architectures. The framework incorporates Laplace asymptotic approximations as well as stochastic algorithms, along with distributed numerical differentiation and task-based parallelism for heterogeneous clusters. Sampling is based on the Transitional Markov Chain Monte Carlo (TMCMC) algorithm and its variants. The optimization tasks associated with the asymptotic approximations are treated via the Covariance Matrix Adaptation Evolution Strategy (CMA-ES). A modified subset simulation method is used for posterior reliability measurements of rare events. The framework accommodates scheduling of multiple physical model evaluations based on an adaptive load balancing library and shows excellent scalability. In addition to the software framework, we also provide guidelines as to the applicability and efficiency of Bayesian tools when applied to computationally demanding physical models. Theoretical and computational developments are demonstrated with applications drawn from molecular dynamics, structural dynamics and granular flow.", (C) 2014 Elsevier Inc. All rights reserved.,"Hadjidoukas, PE|Angelikopoulos, P|Papadimitriou, C|Koumoutsakos, P",JOURNAL OF COMPUTATIONAL PHYSICS,uncertainty quantification parallel computing distributed computing bayesian inference reliability,10.1016/j.jcp.2014.12.006
368,WOS:000259363300011,2008,Methods for assessing uncertainty in fundamental assumptions and associated models for cancer risk assessment,EXPERT JUDGMENT PROBABILITY-DISTRIBUTIONS COMPREHENSIVE REALISM INHALED FORMALDEHYDE PHARMACOKINETIC DATA CLIMATE-CHANGE LUNG-CANCER F344 RAT INFORMATION ELICITATION,"The distributional approach for uncertainty analysis in cancer risk assessment is reviewed and extended. The method considers a combination of bioassay study results, targeted experiments, and expert judgment regarding biological mechanisms to predict a probability distribution for uncertain cancer risks. Probabilities are assigned to alternative model components, including the determination of human carcinogenicity, mode of action, the dosimetry measure for exposure, the mathematical form of the dose-response relationship, the experimental data set(s) used to fit the relationship, and the formula used for interspecies extrapolation. Alternative software platforms for implementing the method are considered, including Bayesian belief networks (BBNs) that facilitate assignment of prior probabilities, specification of relationships among model components, and identification of all output nodes on the probability tree. The method is demonstrated using the application of Evans, Sielken, and co-workers for predicting cancer risk from formaldehyde inhalation exposure. Uncertainty distributions are derived for maximum likelihood estimate (MLE) and th percentile upper confidence limit (UCL) unit cancer risk estimates, and the effects of resolving selected model uncertainties on these distributions are demonstrated, considering both perfect and partial information for these model components. A method for synthesizing the results of multiple mechanistic studies is introduced, considering the assessed sensitivities and selectivities of the studies for their targeted effects. A highly simplified example is presented illustrating assessment of genotoxicity based on studies of DNA damage response caused by naphthalene and its metabolites. The approach can provide a formal mechanism for synthesizing multiple sources of information using a transparent and replicable weight-of-evidence procedure.",,"Small, MJ",RISK ANALYSIS,bayesian belief network cancer risk assessment distributional method expert judgment genotoxicity mode of action uncertainty analysis weight of evidence,10.1111/j.1539-6924.2008.01134.x
369,WOS:000276075900006,2010,Object-oriented design of process line simulation and optimization-A case study in papermaking,MULTIDISCIPLINARY DESIGN PAPER FRAMEWORK SYSTEM FLOW,"Simulation-based optimization for industrial process lines is discussed in this paper. Our approach combines multidisciplinary modeling, modern sensitivity analysis methodology as well as multiobjective optimization by means of object-oriented software design principles. As a result, a simulation and optimization approach that can be extended and modified due to users' needs can be developed. Our approach is illustrated by a real-world example from papermaking industry.",,"Madetoja, E|Tarvainen, P",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,process line optimization multiobjective optimization multidisciplinary modeling object-oriented programming,10.1007/s00158-009-0451-8
370,WOS:000280543800002,2010,An evolutionary optimization of diffuser shapes based on CFD simulations,STRAIGHT 2-DIMENSIONAL DIFFUSERS HEAT-TRANSFER OPTIMIZATION INTERNALLY FINNED TUBES SENSITIVITY ANALYSIS ADJOINT FORMULATION PARALLEL COMPUTERS LAMINAR-FLOW DESIGN PERFORMANCE CHANNELS,"An efficient and robust algorithm is presented for the optimum design of plane symmetric diffusers handling incompressible turbulent flow. The indigenously developed algorithm uses the CFD software: Fluent for the hydrodynamic analysis and employs a genetic algorithm (GA) for optimization. For a prescribed inlet velocity and outlet pressure, pressure recovery coefficient C-p* (the objective function) is estimated computationally for various design options. The CFD software and the GA have been combined in a monolithic platform for a fully automated operation using some special control commands. Based on the developed algorithm, an extensive exercise has been made to optimize the diffuser shape. Different methodologies have been adopted to create a large number of design options. Interestingly, not much difference has been noted in the optimum C-p* values obtained through different approaches. However, in all the approaches, a better design has been obtained through a proper selection of the number of design variables. Finally, the effect of diffuser length on the optimum shape has also been studied."," Copyright (C) 2009 John Wiley & Sons, Ltd.","Ghosh, S|Pratihar, DK|Maiti, B|Das, PK",INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN FLUIDS,incompressible flow optimization genetic algorithm 2d planar diffuser 2d planar duct,10.1002/fld.2124
371,WOS:000175645300003,2002,Automatic versus manual model differentiation to compute sensitivities and solve non-linear inverse problems,,"Emerging tools for automatic differentiation (AD) of computer programs should be of great benefit for the implementation of many derivative-based numerical methods such as those used for inverse modeling. The Odyssee software, one such tool for Fortran  codes, has been tested on a sample model that solves a D non-linear diffusion-type equation. Odyssee offers both the forward and the reverse differentiation modes, that produce the tangent and the cotangent models, respectively. The two modes have been implemented on the sample application. A comparison is made with a manually-produced differentiated code for this model (MD), obtained by solving the adjoint equations associated with the model's discrete state equations. Following a presentation of the methods and tools and of their relative advantages and drawbacks, the performances of the codes produced by the manual and automatic methods are compared, in terms of accuracy and of computing efficiency (CPU and memory needs). The perturbation method (finite-difference approximation of derivatives) is also used as a reference. Based on the test of Taylor, the accuracy of the two AD modes proves to be excellent and as high as machine precision permits, a good indication of Odyssee's capability to produce error-free codes. In comparison, the manually-produced derivatives (MD) sometimes appear to be slightly biased, which is likely due to the fact that a theoretical model (state equations) and a practical model (computer program) do not exactly coincide, while the accuracy of the perturbation method is very uncertain. The MD code largely outperforms all other methods in computing efficiency, a subject of current research for the improvement of AD tools. Yet these tools can already be of considerable help for the computer implementation of many numerical methods, avoiding the tedious task of hand-coding the differentiation of complex algorithms.", (C) 2002 Published by Elsevier Science Ltd.,"Elizondo, D|Cappelaere, B|Faure, C",COMPUTERS & GEOSCIENCES,code differentiation optimization adjoint state data assimilation sensitivity analysis odyssee,10.1016/S0098-3004(01)00048-6
372,WOS:000283842100010,2010,The Nested Event Tree Model with Application to Combating Terrorism,MIXED-INTEGER MODELS PROGRAMMING-PROBLEMS GLOBAL OPTIMIZATION RESOURCES ALLOCATION RISKS,"In this paper, we model and solve the strategic problem of minimizing the expected loss inflicted by a hostile terrorist organization. An appropriate allocation of certain capability-related, intent-related, vulnerability-related, and consequence-related resources is used to reduce the probabilities of success in the respective attack-related actions and to ameliorate losses in case of a successful attack. We adopt a nested event tree optimization framework and formulate the problem as a specially structured nonconvex factorable program. We develop two branch-and-bound schemes based, respectively, on utilizing a convex nonlinear relaxation and a linear outer approximation, both of which are proven to converge to a global optimal solution. We also design an alternative direct mixed-integer programming model representation for this case, and we investigate a fundamental special-case variant for this scheme that provides a relaxation and affords an optimality gap measure. Several range reduction, partitioning, and branching strategies are proposed, and extensive computational results are presented to study the efficacy of different compositions of these algorithmic ingredients, including comparisons with the commercial software BARON. A sensitivity analysis is also conducted to explore the effect of certain key model parameters.",,"Lunday, BJ|Sherali, HD|Glickman, TS",INFORMS JOURNAL ON COMPUTING,combating terrorism outer approximation branch and bound global optimization factorable programs,10.1287/ijoc.1100.0377
373,WOS:000287575500001,2011,Laboratory and numerical modeling of water balance in a layered sloped soil cover with channel flow pathway over mine waste rock,WETTING FRONT INSTABILITY ENGINEERED TEST COVERS CHARACTERISTIC CURVE UNSATURATED SOILS OXYGEN BARRIERS WHISTLE MINE TAILINGS INFILTRATION EVAPORATION ONTARIO,"Macropores developed in barrier layers in soil covers overlying acid-generating waste rock may produce preferential flow through the barrier layers and compromise cover performance. However, little has been published on the effects of preferential flow on water balance in soil covers. In the current study, an inclined, layered soil cover with a -cm-wide sand-filled channel pathway in a silty clay barrier layer was built over reactive waste rock in the laboratory. The channel or preferential flow pathway represented the aggregate of cracks or fissures that may occur in the barrier during compaction and/or climate-induced deterioration. Precipitation, runoff, interflow, percolation, and water content were recorded during the test. A commercial software VADOSE/W was used to simulate the measured water balance and to conduct further sensitivity analysis on the effects of the location of the channel and the saturated hydraulic conductivity of the channel material on water balance. The maximum percolation, .% of the total precipitation, was obtained when the distance between the mid-point of the channel pathway and the highest point on the slope accounted for % of the total horizontal length of the soil cover. The modeled percolation increased steadily with an increase in the hydraulic conductivity of the channel material. Percolation was found to be sensitive to the location of the channel and the saturated hydraulic conductivity of the channel material, confirming that proper cover design and construction should aim at minimizing the development of vertical preferential flow in barrier layers. The sum of percolation and interflow was relatively constant when the location of the channel changed along the slope, which may be helpful in locating preferential flow pathways and repairing the barrier.",,"Song, Q|Yanful, EK",ENVIRONMENTAL EARTH SCIENCES,acid rock drainage channel flow laboratory test soil cover vadose/w water balance,10.1007/s12665-010-0488-4
374,WOS:000186310600007,2003,Direct and adjoint sensitivity analysis of chemical kinetic systems with KPP: II - Numerical validation and applications,VARIATIONAL DATA ASSIMILATION CHEMISTRY DATA ASSIMILATION AIR-QUALITY MODEL OZONE IMPLEMENTATION CODE,"The Kinetic PreProcessor KPP was extended to generate the building blocks needed for the direct and adjoint sensitivity analysis of chemical kinetic systems. An overview of the theoretical aspects of sensitivity calculations and a discussion of the KPP software tools is presented in the companion paper. In this work the correctness and efficiency of the KPP generated code for direct and adjoint sensitivity studies are analyzed through an extensive set of numerical experiments. Direct-decoupled Rosenbrock methods are shown to be cost-effective for providing sensitivities at low and medium accuracies. A validation of the discrete-adjoint evaluated gradients is performed against the finite difference estimates. The accuracy of the adjoint gradients is measured using a reference gradient value obtained with a standard direct-decoupled method. The accuracy is studied for both constant step size and variable step size integration of the forward/adjoint model and the consistency between the discrete and continuous adjoint models is analyzed. Applications of the KPP-. software package to direct and adjoint sensitivity studies, variational data assimilation, and parameter identification are considered for the comprehensive chemical mechanism SAPRC-. (C) ", Elsevier Ltd. All rights reserved.,"Daescu, DN|Sandu, A|Carmichael, GR",ATMOSPHERIC ENVIRONMENT,sensitivity analysis data assimilation parameter identification optimization,10.1016/j.atmosenv.2003.08.020
375,WOS:000328724500002,2014,SGEMS-UQ: An uncertainty quantification toolkit for SGEMS,DISTANCES RESERVOIR MODELS,"While algorithms and methodologies to study uncertainty in the Earth Sciences are constantly evolving, there is currently no free integrated software that allows the general practitioners access to these developments. This paper presents SGEMS-UQ a plugin for the SGEMS platform, that is used to perform distance-based uncertainty analysis on geostatistical simulations, and the resulting forward transfer function responses used in subsurface modeling and engineering. A versatile XML-derived dialect is defined for communicating with external programs that reduces the need for ad-hoc linking of codes, and a relational database system is implemented to automate many of the steps in data mining the spatial and forward model parameters. Through a graphical user interface, one can map a set of realizations and forward transfer function responses into a multidimensional scaling (MDS) space where visualization utilities, and clustering techniques are available. Once mapped in the MDS space, the user can explore linkage between simulation parameters and forward transfer function responses using a module based on a SQL database. Consideration is given to the use of software engineering paradigms and design patterns to produce a code-base that is manageable, efficient, and extensible for future applications, while being scalable to work with large datasets. Finally, we illustrate the versatility of the code-base on an application of modeling uncertainty in reservoir forecasts for an oil reservoir in the West Coast of Africa. (C) ", Elsevier Ltd. All rights reserved.,"Li, L|Boucher, A|Caers, J",COMPUTERS & GEOSCIENCES,uncertainty quantification software design xml sql database,10.1016/j.cageo.2013.09.009
376,WOS:000334003800020,2014,Environmental impact assessment based on dynamic fuzzy simulation,MODELS SYSTEMS,"A new ""quick scan"" method for an expert-/stakeholder-based impact assessment approach is introduced. This approach aims to reduce the complexity of models, to simulate and visualize the system dynamics and to provide a basis for guided discussion with stakeholders. The approach is based on dynamic fuzzy models that can be understood easily and developed by experts and understood and adapted by stakeholders (""white box models""). This open modeling process also forms the basis of the credibility of the simulation results. The quick scan approach is supported by an interactive simulation tool that includes optimization and uncertainty analysis as open source software. (C) ", Elsevier Ltd. All rights reserved.,"Wieland, R|Gutzler, C",ENVIRONMENTAL MODELLING & SOFTWARE,quick scan environmental impact assessment fuzzy modeling dynamic fuzzy simulation,10.1016/j.envsoft.2014.02.001
377,WOS:000260806000010,2008,Evaluation of CO2 fluxes from an agricultural field using a process-based numerical model,LAND-USE CHANGE SOIL CARBON RESPIRATION TEMPERATURE SENSITIVITY DIOXIDE SEQUESTRATION VARIABILITY EMISSIONS TRANSPORT,"During , soil CO fluxes, and meteorological and soil variables were measured at multiple locations in a -ha agricultural field in the Sacramento Valley, California, to evaluate the effects of different tillage practices on CO emissions at the field scale. Field scale CO fluxes were then evaluated using the one-dimensional process-based SOILCO module of the HYDRUS-D software package. This model simulates dynamic interactions between soil water contents, temperature, and soil respiration by numerically solving partial-differential water flow (Richards) and heat and CO transport (convection-dispersion) equations using the finite element method. The model assumes that the overall CO production in the soil profile is the sum of soil and plant respiration, whose optimal values are affected by time, depth, water content, temperature, and CO concentration in the soil profile. The effect of each variable is introduced using various reduction functions that multiply the optimal Soil CO production. Our results show that the numerical model could predict CO fluxes across the soil surface reasonably well using soil hydraulic parameters determined from textural characteristics and the HYDRUS-D software default values for heat transport, CO transport and production parameters without any additional calibration. An uncertainty analysis was performed to quantify the effects of input parameters and soil heterogeneity on predicted soil water contents and CO fluxes. Both simulated volumetric water contents and surface CO fluxes show a significant dependency on soil hydraulic properties.", (C) 2008 Elsevier B.V. All rights reserved.,"Buchner, JS|Simunek, J|Lee, J|Rolston, DE|Hopmans, JW|King, AP|Six, J",JOURNAL OF HYDROLOGY,numerical model carbon dioxide uncertainty analysis soil respiration soilco2 hydrus-1d,10.1016/j.jhydrol.2008.07.035
378,WOS:000315974500021,2013,Distributed computation of large scale SWAT models on the Grid,PERSPECTIVES SYSTEMS TOOL,"The increasing interest in larger spatial and temporal scale models and high resolution input data processing comes at a price of higher computational demand. This price is evidently even higher when common modeling routines such as calibration and uncertainty analysis are involved. Likewise, methods and techniques for reducing computation time in large scale socio-environmental modeling software is growing. Recent advancements in distributed computing such as Grid infrastructure have provided further opportunity to this effort. In the interest of gaining computational efficiency, we developed generic tools and techniques for enabling the Soil and Water Assessment Tool (SWAT) model application to run on the EGEE (Enabling Grids for E-science projects in Europe) Grid. Various program components/scripts were written to split a large scale hydrological model of the Soil and Water Assessment Tool (SWAT), to submit the split models to the Grid, and to collect and merge results into single output format. A three-step procedure was applied to take advantage of the Grid. Firstly, a python script was run in order to split the SWAT model into several sub-models. Then, individual sub-models were submitted in parallel for execution on the Grid. Finally, the outputs of the sub-basins were collected and the reach routing process was performed with another script executing a modified SWAT program. We conducted experimental simulations with multiple temporal and spatial scale hydrological models on the Grid infrastructure. Results showed that, in spite of computing overheads, parallel computation of socio-environmental models on the Grid is beneficial for model applications especially with large spatial and temporal scales. In the end, we conclude by recommending methods for further reducing computational overheads while running large scale model applications on the Grid. (c) ", Elsevier Ltd. All rights reserved.,"Yalew, S|van Griensven, A|Ray, N|Kokoszkiewicz, L|Betrie, GD",ENVIRONMENTAL MODELLING & SOFTWARE,distributed computing grid computing hydrological models swat gridification,10.1016/j.envsoft.2012.08.002
379,WOS:000348201000009,2015,Sensitivity analyses and simulations of a full-scale experimental membrane bioreactor system using the activated sludge model No. 3 (ASM3),RETENTION TIME OPERATION WATER,"An ASM-based model was implemented in the numerical software MATHEMATICA where sensitivity analyses and simulations of a membrane bioreactor (MBR) system were carried out. These results were compared with those obtained using the commercial simulator WEST. Predicted values did not show significant variations between both software and simulations showed that the most influential operational conditions were influent flow rate and concentrations and bioreactor volumes. On the other hand, sensitivity analyses were carried out with both software programs for the same five outputs: COD, ammonium and nitrate concentrations in the effluent, total suspended solids concentration and oxygen uptake rate in the aerobic bioreactor. Similar results were in general obtained in both cases and according to these analyses, the most significant inputs over the model predictions were growth and storage heterotrophic biomass yields and decay coefficient. Other parameters related to the hydrolysis process or to the autotrophic biomass also significantly influenced model outputs.",,"Ruiz, LM|Rodelas, P|Perez, JI|Gomez, MA",JOURNAL OF ENVIRONMENTAL SCIENCE AND HEALTH PART A-TOXIC/HAZARDOUS SUBSTANCES & ENVIRONMENTAL ENGINEERING,simulation modeling sensitivity analysis mbr asm3,10.1080/10934529.2015.981122
380,WOS:000351458000015,2015,Risk Analysis of Water Demand for Agricultural Crops under Climate Change,OPTIMIZATION HBMO ALGORITHM RESERVOIR OPERATION DESIGN UNCERTAINTY DISCRETE NETWORKS STRATEGY IMPACTS SYSTEM,"This paper assesses the risk of increase in water demand for a wide range of irrigated crops in an irrigation network located downstream of the Aidoghmoush Dam in East Azerbaijan by considering climate change conditions for the period -. Atmosphere-ocean global circulation models (AOGCMs) are used to simulate climatic variables such as temperature and precipitation. The Bayesian approach is used to consider uncertainties of AOGCMs. Climate change scenarios of climatic variables are first weighted by using the mean observed temperature-precipitation (MOTP) method, and related probability distribution functions are produced. Outputs of AOGCMs are used as input to water requirement models. Then, produced by using the Monte Carlo method,  samples (discrete values) from the probability distribution functions of monthly downscaled temperature and precipitation in the study area are extracted by using a software for sensitivity and uncertainty analysis. Time series of climatic variables in future periods are then generated (temperature variable to calculate potential evapotranspiration and rainfall variable to calculate effective rainfall). To estimate crop water requirements, crop evapotranspiration (from the product of potential evapotranspiration in the previous step and coefficient of crop computed) and effective precipitation (from time series of the previous step) are calculated. The Food and Agricultural Organization of the United Nations (FAO) methods, FAO- and Penman-Monteith, were used to compute crop and potential evapotranspiration, respectively. Because of lack of required data, potential evapotranspiration in future periods is computed through the relationship of temperature and potential evapotranspiration in the baseline period; the same procedure is conducted for temperature. Net water requirement (NWR) and the risk of changes in water demand volume of crops (e.g., wheat, barley, alfalfa, soybean, feed corn, forage, potato, and walnut orchards) are computed by entering  monthly time series of downscaled temperature and precipitation in future periods. The results indicate that risk of changes in crop water requirements increases by approximately % for a % risk, approximately % for a % risk, and approximately % for a % risk. Also, based on the current cultivated area, on average, the volume of water demand only for the aforementioned crops will be approximately .(() m()/year) with a risk of %, approximately (() m()/year) with a risk of %, and approximately (() m()/year) with a risk of %. Wheat and barley are more resistant and less sensitive to climate change than other crops considered.", (C) 2014 American Society of Civil Engineers.,"Ashofteh, PS|Bozorg-Haddad, O|Marino, MA",JOURNAL OF HYDROLOGIC ENGINEERING,climate change net water requirement risk uncertainty monte carlo,10.1061/(ASCE)HE.1943-5584.0001053
381,WOS:000312813800011,2012,Ecological Risk-O-Meter: a risk assessor and manager software tool for better decision making in ecosystems,SECURITY METER QUANTIFY MODEL,"Increased awareness of environmental issues and their effects on ecological systems and human health drive an interest in developing computational methods to reduce detrimental consequences. For example, there are concerns regarding chlorofluorocarbons and their impact on stratospheric ozone, radon and its effect on human health, coal mining and effects on habitat loss, as well as numerous other issues. However, these issues do not exist in a vacuum nor occur just one at a time. There is a need to assess social and ecological risks comprehensively and account for numerous, inter-related potential risks. Given limited funds available for addressing these issues, how can spending for purposes of environmental and ecological mitigation be optimized? What is the magnitude of overall ecological risk for a given region? Novel software, the Ecological Risk-o-Meter, addresses these questions and concerns. The software tool not only assesses the current environmental and ecological risks, but also takes into account potential solutions and provides guidance as to how spending can be optimized to reducing overall environmental risk. We demonstrate this new tool and show how to optimize the costs of risk reduction in recursive cycles based on feedbacks."," Copyright (c) 2012 John Wiley & Sons, Ltd.","Sahinoglu, M|Simmons, SJ|Cahoon, LB|Morton, S",ENVIRONMETRICS,ecological systems vulnerability threat countermeasure risk-o-meter,10.1002/env.2186
382,WOS:000343322300006,2014,Eulerian-Eulerian modelling and computational fluid dynamics simulation of wire mesh demisters in MSF plants,PLATE MIST ELIMINATORS SEPARATION EFFICIENCY DRAINAGE CHANNELS DROPLET MOTION HEAT-TRANSFER FLOW DESIGN ENTRAINMENT PERFORMANCE CONDENSERS,"Purpose - The purpose of this study is to focus on simulation of wire mesh demisters in multistage flash desalination (MSF) plants. The simulation is made by the use of computational fluid dynamics (CFD) software. Design/methodology/approach - A steady state and two-dimensional (D) model was developed to simulate the demister. The model employs an Eulerian-Eulerian approach to simulate the flow of water vapor and brine droplets in the demister. The computational domain included three zones, which are the vapor space above and below the demister and the demister. The demister zone was modeled as a tube bank arrange or as a porous media. Findings - Sensitivity analysis of the model showed the main parameters that affect demister performance are the vapor velocity and the demister permeability. On the other hand, the analysis showed that the vapor temperature has no effect on the pressure drop across the demister. Research limitations/implications - The developed model was validated against previous literature data as well as real plant data. The analysis shows good agreement between model prediction and data. Originality/value - This work is the first in the literature to simulate the MSF demister using CFD modeling. This work is part of a group effort to develop a comprehensive CFD simulation for the entire flashing stage of the MSF process, which would provide an extremely efficient and inexpensive design and simulation tool to the desalination community.",,"Al-Fulaij, H|Cipollina, A|Micale, G|Ettouney, H|Bogle, D",ENGINEERING COMPUTATIONS,cfd demister desalination eulerian modeling multistage flashing,10.1108/EC-03-2012-0063
383,WOS:000345142000010,2015,Geometric sensitivity of patient-specific finite element models of the spine to variability in user-selected anatomical landmarks,SOFT-TISSUE PROPERTIES LUMBAR SPINE INTERVERTEBRAL DISC MATERIAL PROPERTY SCOLIOSIS COMPRESSION DEFORMITY SEGMENT FORCES TRUNK,"Software to create individualised finite element (FE) models of the osseoligamentous spine using pre-operative computed tomography (CT) data-sets for spinal surgery patients has recently been developed. This study presents a geometric sensitivity analysis of this software to assess the effect of intra-observer variability in user-selected anatomical landmarks. User-selected landmarks on the osseous anatomy were defined from CT data-sets for three scoliosis patients and these landmarks were used to reconstruct patient-specific anatomy of the spine and ribcage using parametric descriptions. The intra-observer errors in landmark co-ordinates for these anatomical landmarks were calculated. FE models of the spine and ribcage were created using the reconstructed anatomy for each patient and these models were analysed for a loadcase simulating clinical flexibility assessment. The intra-observer error in the anatomical measurements was low in comparison to the initial dimensions, with the exception of the angular measurements for disc wedge and zygapophyseal joint (z-joint) orientation and disc height. This variability suggested that CT resolution may influence such angular measurements, particularly for small anatomical features, such as the z-joints, and may also affect disc height. The results of the FE analysis showed low variation in the model predictions for spinal curvature with the mean intra-observer variability substantially less than the accepted error in clinical measurement. These findings demonstrate that intra-observer variability in landmark point selection has minimal effect on the subsequent FE predictions for a clinical loadcase.",,"Little, JP|Adam, CJ",COMPUTER METHODS IN BIOMECHANICS AND BIOMEDICAL ENGINEERING,patient-specific finite element scoliosis thoracolumbar spine ribcage,10.1080/10255842.2013.843673
384,WOS:000395529100004,2017,Monte Carlo Approach for Uncertainty Analysis of Acoustic Doppler Current Profiler Discharge Measurement by Moving Boat,ADCP VELOCITY FRAMEWORK VARIANCE,"This paper presents a method using Monte Carlo simulations for assessing uncertainty of moving-boat acoustic Doppler current profiler (ADCP) discharge measurements using a software tool known as QUant, which was developed for this purpose. Analysis was performed on  data sets from four Water Survey of Canada gauging stations in order to evaluate the relative contribution of a range of error sources to the total estimated uncertainty. The factors that differed among data sets included the fraction of unmeasured discharge relative to the total discharge, flow nonuniformity, and operator decisions about instrument programming and measurement cross section. As anticipated, it was found that the estimated uncertainty is dominated by uncertainty of the discharge in the unmeasured areas, highlighting the importance of appropriate selection of the site, the instrument, and the user inputs required to estimate the unmeasured discharge. The main contributor to uncertainty was invalid data, but spatial inhomogeneity in water velocity and bottom-track velocity also contributed, as did variation in the edge velocity, uncertainty in the edge distances, edge coefficients, and the top and bottom extrapolation methods. To a lesser extent, spatial inhomogeneity in the bottom depth also contributed to the total uncertainty, as did uncertainty in the ADCP draft at shallow sites. The estimated uncertainties from QUant can be used to assess the adequacy of standard operating procedures. They also provide quantitative feedback to the ADCP operators about the quality of their measurements, indicating which parameters are contributing most to uncertainty, and perhaps even highlighting ways in which uncertainty can be reduced. Additionally, QUant can be used to account for self-dependent error sources such as heading errors, which are a function of heading. The results demonstrate the importance of a Monte Carlo method tool such as QUant for quantifying random and bias errors when evaluating the uncertainty of moving-boat ADCP measurements.",,"Moore, SA|Jamieson, EC|Rainville, F|Rennie, CD|Mueller, DS",JOURNAL OF HYDRAULIC ENGINEERING,moving-boat acoustic doppler current profiler (adcp) uncertainty monte carlo probabilistic stream gauging procedures,10.1061/(ASCE)HY.1943-7900.0001249
385,WOS:000387350400006,2016,Efficient uncertainty quantification of a fully nonlinear and dispersive water wave model with random inputs,DYNAMICALLY BIORTHOGONAL METHOD SENSITIVITY-ANALYSIS DIFFERENTIAL-EQUATIONS QUADRATURE-RULES POLYNOMIAL CHAOS NUMERICAL-METHODS OCEAN WAVES PROPAGATION SIMULATIONS FIELDS,"A major challenge in next-generation industrial applications is to improve numerical analysis by quantifying uncertainties in predictions. In this work we present a formulation of a fully nonlinear and dispersive potential flow water wave model with random inputs for the probabilistic description of the evolution of waves. The model is analyzed using random sampling techniques and nonintrusive methods based on generalized polynomial chaos (PC). These methods allow us to accurately and efficiently estimate the probability distribution of the solution and require only the computation of the solution at different points in the parameter space, allowing for the reuse of existing simulation software. The choice of the applied methods is driven by the number of uncertain input parameters and by the fact that finding the solution of the considered model is computationally intensive. We revisit experimental benchmarks often used for validation of deterministic water wave models. Based on numerical experiments and assumed uncertainties in boundary data, our analysis reveals that some of the known discrepancies from deterministic simulation in comparison with experimental measurements could be partially explained by the variability in the model input. Finally, we present a synthetic experiment studying the variance-based sensitivity of the wave load on an offshore structure to a number of input uncertainties. In the numerical examples presented the PC methods exhibit fast convergence, suggesting that the problem is amenable to analysis using such methods.",,"Bigoni, D|Engsig-Karup, AP|Eskilsson, C",JOURNAL OF ENGINEERING MATHEMATICS,free surface water waves generalized polynomial chaos high-performance computing sensitivity analysis uncertainty quantification,10.1007/s10665-016-9848-8
0,WOS:000399845800057,2017,"Risk assessment of pesticides and other stressors in bees: Principles, data gaps and perspectives from the European Food Safety Authority",HONEYBEES APIS-MELLIFERA HORNET VESPA-VELUTINA YELLOW-LEGGED HORNET NEONICOTINOID INSECTICIDES ECOSYSTEM SERVICES POLLEN CONSUMPTION AETHINA-TUMIDA COLONY FAILURE EXPOSURE IMPACTS,"Current approaches to risk assessment in bees do not take into account co-exposures from multiple stressors. The European Food Safety Authority (EFSA) is deploying resources and efforts to move towards a holistic risk assessment approach of multiple stressors in bees. This paper describes the general principles of pesticide risk assessment in bees, including recent developments at EFSA dealing with risk assessment of single and multiple pesticide residues and biological hazards. The EFSA Guidance Document on the risk assessment of plant protection products in bees highlights the need for the inclusion of an uncertainty analysis, other routes of exposures and multiple stressors such as chemical mixtures and biological agents. The EFSA risk assessment on the survival, spread and establishment of the small hive beetle, Aethina tumida, an invasive alien species, is provided with potential insights for other bee pests such as the Asian hornet, Vespa velutina. Furthermore, data gaps are identified at each step of the risk assessment, and recommendations are made for future research that could be supported under the framework of Horizon . Finally, the recent work conducted at EFSA is presented, under the over-arching MUST-B project (""EU efforts towards the development of a holistic approach for the risk assessment on MUltiple STressors in Bees"") comprising a toolbox for harmonised data collection under field conditions and a mechanistic model to assess effects from pesticides and other stressors such as biological agents and beekeeping management practices, at the colony level and in a spatially complex landscape. Future perspectives at EFSA include the development of a data model to collate high quality data to calibrate and validate the model to be used as a regulatory tool. Finally, the evidence collected within the framework of MUST-B will support EFSA's activities on the development of a holistic approach to the risk assessment of multiple stressors in bees. In conclusion, EFSA calls for collaborative action at the EU level to establish a common and open access database to serve multiple purposes and different stakeholders. (C)  The Authors.", Published by Elsevier B.V.,"Rortais, A|Arnold, G|Dorne, JL|More, SJ|Sperandio, G|Streissl, F|Szentes, C|Verdonck, F",SCIENCE OF THE TOTAL ENVIRONMENT,multiple stressors honeybee colony health modelling indicator data collection research needs,10.1016/j.scitotenv.2016.09.127
6,WOS:000340951200003,2014,Numerical and intelligent modeling of triaxial strength of anisotropic jointed rock specimens,FUZZY INFERENCE SYSTEM INTACT ROCKS CONTINUUM MASSES,"The strength of anisotropic rock masses can be evaluated through either theoretical or experimental methods. The latter is more precise but also more expensive and time-consuming especially due to difficulties of preparing high-quality samples. Numerical methods, such as finite element method (FEM), finite difference method (FDM), distinct element method (DEM), etc. have been regarded as precise and low-cost theoretical approaches in different fields of rock engineering. On the other hand, applicability of intelligent approaches such as fuzzy systems, neural networks and decision trees in rock mechanics problems has been recognized through numerous published papers. In current study, it is aimed to theoretically evaluate the strength of anisotropic rocks with through-going discontinuity using numerical and intelligent methods. In order to do this, first, strength data of such rocks are collected from the literature. Then FlAC, a commercially well-known software for FDM analysis, is applied to simulate the situation of triaxial test on anisotropic jointed specimens. Reliability of this simulation in predicting the strength of jointed specimens has been verified by previous researches. Therefore, the few gaps of the experimental data are filled by numerical simulation to prevent unexpected learning errors. Furthermore, a sensitivity analysis is carried out based on the numerical process applied herein. Finally, two intelligent methods namely feed forward neural network and a newly developed fuzzy modeling approach are utilized to predict the strength of above-mentioned specimens. Comparison of the results with experimental data demonstrates that the intelligent models result in desirable prediction accuracy.",,"Asadi, M|Bagheripour, MH",EARTH SCIENCE INFORMATICS,numerical modeling artificial neural networks fuzzy systems strength anisotropy jointed rock,10.1007/s12145-013-0137-z
11,WOS:000231782200001,2005,Scenario-based simulation of runoff-related pesticide entries into small streams on a landscape level,SURFACE WATER FIELD,"The prediction of runoff-related pesticide entry into surface waters on a landscape level usually requires considerable efforts with regard to input data, time, and personnel. Therefore, the need for an easy to use simulation tool with easily accessible input data, for example from already existing public sources, is obvious. In this paper, we present a simulation tool for the simulation of pesticide entry from arable land into adjacent streams. Our aim was to develop a tool applicable on the landscape level using ""real world data"" from numerous sites and for the simulation of parameter case studies concerning particular parameters at single sites. We used the ratio of exposure to toxicity (REXTOX) model proposed by the OECD, which had been successfully validated in the study area as part of a previous study and which was extended to calculate pesticide concentrations in adjacent streams. We simulated the pesticide entry on the landscape level at  sites in small streams situated in the central lowland of Germany with winter wheat, barley, and sugar beat as the main agricultural. crops. A sensitivity analysis indicated that the most significant model parameters were the width of the no-application zone and the degree of plant interception. The simulation was carried out for the  most frequently detected substances found in the study area using eight different environmental scenarios, covering variation of the width of the no-application zone, climate, and seasonal scenarios. The highest in-stream concentrations were predicted for a scenario using no ( m) buffer zone in conjunction with increased precipitation. According to the predicted concentrations, the risk for the aquatic communities was estimated based on standard toxicity tests and the application of a safety factor. The simulation results are presented both by means of risk maps for the study area showing the simulated pesticide concentration and the resulting ecological risk for numerous sites under varying scenarios and by case study diagrams with focus on the model behavior under the influence of single parameters. Risk maps confirmed the importance of no-application (buffer) zones for the levels of pesticide input. They also indicated the importance of the existing no-application zones for certain compounds and in some cases the need for a further evaluation of these regulations. The simulation tool was implemented as a standard PC software combining the REXTOX model with a geographical information system and can be used on any current personal computer. All input data was taken from public sources of German authorities. With little effort the tool should be applicable for other areas with similar data quality.", (C) 2005 Elsevier Inc. All rights reserved.,"Probst, M|Berenzen, N|Lentzen-Godding, A|Schulz, R",ECOTOXICOLOGY AND ENVIRONMENTAL SAFETY,risk assessment pesticides runoff buffer zones simulation modeling landscape level climate change risk mitigation,10.1016/j.ecoenv.2005.04.012
14,WOS:000308971400008,2012,Stochastic cost optimization of DNAPL remediation - Method description and sensitivity study,GROUNDWATER MONITORING DESIGN AQUIFER REMEDIATION GENETIC ALGORITHM TRANSPORT UNCERTAINTY MANAGEMENT MODEL NETWORK SYSTEMS PUMP,"A modeling approach is described for optimizing the design and operation of groundwater remediation at DNAPL sites that considers uncertainty in site and remediation system characteristics, performance and cost model limitations, and measurement uncertainties that affect predictions of remediation performance and cost. The performance model simulates performance and costs for thermal source zone treatment and enhanced bioremediation with statistical compliance rules and real-time operational system monitoring. An inverse solution is employed to estimate model parameters, parameter covariances, and residual prediction error from site data and a stochastic cost optimization algorithm determines design and operation variables that minimize expected net present value cost over Monte Carlo realizations. The method is implemented in the program SCOToolkit. A series of applications to a hypothetical problem yielded expected cost reductions for site remediation as much as % compared to conventional non-optimized approaches, while also increasing the probability of achieving ""no further action"" status in a specified timeframe by more than %. Optimizing monitoring frequency for compliance wells used to make no further action determinations as well as operational monitoring used to make decisions on individual remediation system components reveals tradeoffs between increased direct costs for sampling and analysis versus decreased construction and operating costs that arise because more data increases decision reliability. Optimizing protocols for operational monitoring and heating unit shutdown protocols for thermal source treatment (incremental versus all-or-none shutdown, soil versus groundwater sampling, number and frequency of samples) produced cost savings of more than %. Defining compliance based on confidence limits of a moving time window regression decreased expected cost and lowered failure probability compared to using measured extreme values over a lookback period. Uncertainty in DNAPL source delineation was found to have a large effect on the cost and probability of achieving remediation objectives for thermal source remediation. (C) ", Elsevier Ltd. All rights reserved.,"Parker, J|Kim, U|Kitanidis, P|Cardiff, M|Liu, XY|Beyke, G",ENVIRONMENTAL MODELLING & SOFTWARE,stochastic optimization uncertainty analysis dnapl model calibration thermal source treatment enhanced bioremediation remediation cost,10.1016/j.envsoft.2012.05.002
27,WOS:000382049400008,2016,"Parameterization, sensitivity analysis, and inversion: an investigation using groundwater modeling of the surface-mined Tivoli-Guidonia basin (Metropolitan City of Rome, Italy)",WATER MODELS UNCERTAINTY IMPORTANCE FLOW TRANSPORT VALIDATION SYSTEM SITE USA,"With respect to model parameterization and sensitivity analysis, this work uses a practical example to suggest that methods that start with simple models and use computationally frugal model analysis methods remain valuable in any toolbox of model development methods. In this work, ground-water model calibration starts with a simple parameterization that evolves into a moderately complex model. The model is developed for a water management study of the TivoliGuidonia basin (Rome, Italy) where surface mining has been conducted in conjunction with substantial dewatering. The approach to model development used in this work employs repeated analysis using sensitivity and inverse methods, including use of a new observation-stacked parameter importance graph. The methods are highly parallelizable and require few model runs, which make the repeated analyses and attendant insights possible. The success of a model development design can be measured by insights attained and demonstrated model accuracy relevant to predictions. Example insights were obtained: () A long-held belief that, except for a few distinct fractures, the travertine is homogeneous was found to be inadequate, and () The dewatering pumping rate is more critical to model accuracy than expected. The latter insight motivated additional data collection and improved pumpage estimates. Validation tests using three other recharge and pumpage conditions suggest good accuracy for the predictions considered. The model was used to evaluate management scenarios and showed that similar dewatering results could be achieved using  % less pumped water, but would require installing newly positioned wells and cooperation between mine owners.",,"La Vigna, F|Hill, MC|Rossetto, R|Mazza, R",HYDROGEOLOGY JOURNAL,stochastic optimization uncertainty analysis dnapl model calibration thermal source treatment enhanced bioremediation remediation cost,10.1007/s10040-016-1393-z
35,WOS:000407603800012,2017,Sensitivity analysis of DEM prediction for sliding wear by single iron ore particle,DISCRETE ELEMENT METHOD BALL MILLS SIMULATION MODEL PERFORMANCE CONTACT MECHANISMS MOTION STEEL LIFE,"Purpose - Sliding wear is a common phenomenon in the iron ore handling industry. Large-scale handling of iron ore bulk-solids causes a high amount of volume loss from the surfaces of bulk-solids-handling equipment. Predicting the sliding wear volume from equipment surfaces is beneficial for efficient maintenance of worn equipment. Recently, the discrete element method (DEM) simulations have been utilised to predict the wear by bulk-solids. However, the sensitivity of wear prediction subjected to DEM parameters has not been systemically investigated at single particle level. To ensure the wear predictions by DEM are accurate and stable, this study aims to conduct the sensitivity analysis at the single particle level. Design/methodology/approach - In this research, pin-on-disc wear tests are modelled to predict the sliding wear by individual iron ore particles. The Hertz-Mindlin ( no slip) contact model is implemented to simulate interactions between particle ( pin) and geometry ( disc). To quantify the wear from geometry surface, a sliding wear equation derived from Archard's wear model is adopted in the DEM simulations. The accuracy of the pin-on-disc wear test simulation is assessed by comparing the predicted wear volume with that of the theoretical calculation. The stability is evaluated by repetitive tests of a reference case. At the steady-state wear, the sensitivity analysis is done by predicting sliding wear volumes using the parameter values determined by iron ore-handling conditions. This research is carried out using the software EDEM (R) ... Findings - Numerical errors occur when a particle passes a joint side of geometry meshes. However, this influence is negligible compared to total wear volume of a wear revolution. A reference case study demonstrates that accurate and stable results of sliding wear volume can be achieved. For the sliding wear at steady state, increasing particle density or radius causes more wear, whereas, by contrast, particle Poisson's ratio, particle shear modulus, geometry mesh size, rotating speed, coefficient of restitution and time step have no impact on wear volume. As expected, increasing indentation force results in a proportional increase. For maintaining wear characteristic and reducing simulation time, the geometry mesh size is recommended. To further reduce simulation time, it is inappropriate using lower particle shear modulus. However, the maximum time step can be increased to % T-R without compromising simulation accuracy. Research limitations/implications - The applied coefficient of sliding wear is determined based on theoretical and experimental studies of a spherical head of iron ore particle. To predict realistic volume loss in the iron ore-handling industry, this coefficient should be experimentally determined by taking into account the non-spherical shapes of iron ore particles. Practical implications - The effects of DEM parameters on sliding wear are revealed, enabling the selections of adequate values to predict sliding wear in the iron ore-handling industry. Originality/value - The accuracy and stability to predict sliding wear by using EDEM (R) .. are verified. Besides, this research accelerates the calibration of sliding wear prediction by DEM.",,"Chen, GM|Schott, DL|Lodewijks, G",ENGINEERING COMPUTATIONS,discrete element method pin-on-disc bulk-solids-handling wear prediction,10.1108/EC-07-2016-0265
55,WOS:000417388800006,2017,Direct effect of atmospheric turbulence on plume rise in a neutral atmosphere,LARGE-EDDY SIMULATION DIRECT NUMERICAL-SIMULATION POLLUTANT DISPERSION CROSS-FLOW BOUNDARY-LAYERS FINITE-ELEMENT CFD SIMULATION ENVIRONMENT TERRAINS MODELS,"The direct effect of atmospheric turbulence on plume rise in the current research work is studied through examining the turbulence intensity parameter. A hybrid unsteady Reynolds averaged Navier Stokes (RANS) and large eddy simulation (LES) numerical approach is applied with a new mixed scale sub-grid parameterization technique in the commercial ANSYS Fluent software in order to simulate the buoyant plume behavior in a turbulent crossflow. The accuracy of the simulation method is crosschecked against the wind tunnel data available in the literature. The numerical simulation results in various operating conditions are used to derive a new plume rise formula in which the direct effect of atmospheric turbulence intensity at stack height (I-Air) is explicitly introduced in the plume rise formula. Furthermore, the buoyancy parameter of the flue gas is determined at some distances upstream of the stack top surface to include the whole effects of source buoyancy on the plume rise. The value of I-Air at stack height is obtained by measuring the standard deviation of wind velocity at stack height. The sensitivity analysis showed that by increasing the atmospheric turbulence intensity, the final plume rise decreases because of the updraft and downdraft motions of turbulence and it has been found that there is a linear dependency between the plume rise and ( I-Air)(-.). The quantile-quantile plots show that the new model can predict the simulated plume rise with a deviation factor of . whereas the conventional models overestimate the final plume rise at least by a factor of ..", (C) 2017 Turkish National Committee for Air Pollution Research and Control. Production and hosting by Elsevier B.V. All rights reserved.,"Ashrafi, K|Orkomi, AA|Motlagh, MS",ATMOSPHERIC POLLUTION RESEARCH,neutral numerical model plume rise rans-les method turbulence,10.1016/j.apr.2017.01.001
56,WOS:000331688500003,2014,Parallel flow routing in SWMM 5,URBAN DRAINAGE SYSTEMS SENSITIVITY-ANALYSIS MODEL PERFORMANCE IMPACT,"The hydrodynamic rainfall-runoff and urban drainage simulation model SWMM (Storm Water Management Model) is a state of the art software tool applied likewise in research and practice. In order to reduce the computational burden of long simulation runs and to use the extra power of modern multicore computers, a parallel version of SWMM is presented herein. The challenge has been to modify the software in such minimal way that the resulting code enhancement may find its way into the commercial and non-commercial software tools that depend on SWMM for its calculation engine. A pragmatic approach to identify and enhance only the critical parts of the software in terms of run-time was chosen in order to keep the code changes as low as possible. The enhanced software was first tested for coherence against the original code and then benchmarked on four different input scenarios ranging from a very small village to a medium sized urban area. For the investigated sewer systems a speedup of six to ten times on a twelve core system was realized, thus decreasing the execution time to an acceptable level even for tedious system analysis. (C) ", Elsevier Ltd. All rights reserved.,"Burger, G|Sitzenfrei, R|Kleidorfer, M|Rauch, W",ENVIRONMENTAL MODELLING & SOFTWARE,multi-core openmp parallel computing storm water management model urban drainage modeling,10.1016/j.envsoft.2013.11.002
68,WOS:000239980800018,2006,"Series of experiments for empirical validation of solar gain modeling in building energy simulation codes - Experimental setup, test cell characterization, specifications and uncertainty analysis",SPACE ANALYSIS TOOLS PROGRAMS,"Empirical validation of building energy simulation codes is an important component in understanding the capacity and limitations of the software. Within the framework of Task /Annex  of the International Energy Agency (IEA), a series of experiments was performed in an outdoor test cell. The objective of these experiments was to provide a high-quality data set for code developers and modelers to validate their solar gain models for windows with and without shading devices. A description of the necessary specifications for modeling these experiments is provided in this paper, which includes information about the test site location, experimental setup, geometrical and thermophysical cell properties including estimated uncertainties. Computed overall thermal cell properties were confirmed by conducting a steady-state experiment without solar gains. A transient experiment, also without solar gains, and corresponding simulations from four different building energy simulation codes showed that the provided specifications result in accurate thermal cell modeling. A good foundation for the following experiments with solar gains was therefore accomplished. (c) ", Elsevier Ltd. All rights reserved.,"Manz, H|Loutzenhiser, P|Frank, T|Strachan, PA|Bundi, R|Maxwell, G",BUILDING AND ENVIRONMENT,building energy simulation empirical validation test cell specification,10.1016/j.buildenv.2005.07.020
70,WOS:000262565100007,2009,More efficient PEST compatible model independent model calibration,GLOBAL OPTIMIZATION METHODS RAINFALL-RUNOFF MODELS WATERSHED MODEL MULTIOBJECTIVE CALIBRATION AUTOMATIC CALIBRATION PARAMETER-ESTIMATION SENSITIVITY CATCHMENT SCALE HSPF,"This article describes some of the capabilities encapsulated within the Model Independent Calibration and Uncertainty Analysis Toolbox (MICUT), which was written to support the popular PEST model independent interface. We have implemented a secant version of the Levenberg-Marquardt (LM) method that requires far fewer model calls for local search than the PEST LM methodology. Efficiency studies on three distinct environmental model structures (HSPF, FASST and GSSHA) show that we can find comparable local minima with -% fewer model calls than a conventional model independent LM application. Using the secant LM method for local search, MICUT also supports global optimization through the use of a slightly modified version of a stochastic global search technique called Multi-Level Single Linkage [Rinnooy Kan, A.H.G., Timmer, G., a. Stochastic global optimization methods, part : clustering methods. Math. Program. , -; Rinnooy Kan, A.H.G., Timmer, G., b. Stochastic global optimization methods, part ii: multi level methods. Math. Program. , -.]. Comparison studies with three environmental models suggest that the stochastic global optimization algorithm in MICUT is at least as, and sometimes more efficient and reliable than the global optimization algorithms available in PEST.", Published by Elsevier Ltd.,"Skahill, BE|Baggett, JS|Frankenstein, S|Downer, CW",ENVIRONMENTAL MODELLING & SOFTWARE,calibration efficiency secant version of levenberg-marquardt multi-level single linkage,10.1016/j.envsoft.2008.09.011
71,WOS:000306043900002,2012,MVC3: A MATLAB graphical interface toolbox for third-order multivariate calibration,PARALLEL FACTOR-ANALYSIS TRILINEAR LEAST-SQUARES RESIDUAL TRILINEARIZATION CURVE RESOLUTION FOLIC-ACID 4-WAY CALIBRATION MASS-SPECTROMETRY 2ND-ORDER METHOTREXATE SAMPLES,"A new MATIAB graphical interface toolbox for implementing third-order multivariate calibration methodologies is discussed. Multivariate calibration  (MVC) is a sequel of the already described first-order (MVC) and second-order (MVC) toolboxes. MVC accepts a variety of ASCII data for input, depending on whether the third-order data are vectorized or matricized. If required, data for sample sets are arranged into four-way arrays for processing with several quadrilinear and non-quadrilinear algorithms. Quadrilinear decomposition techniques and latent structured models based on partial least-squares regression and residual trilinearization are included in the software. Appropriate working sensor regions in the three data dimensions can be selected. Model development and its subsequent application to unknown samples are straightforward from the interface. Prediction results are provided along with analytical figures of merit and standard concentration errors, as calculated by modern concepts of uncertainty propagation.", (C) 2012 Elsevier B.V. All rights reserved.,"Olivieri, AC|Wu, HL|Yu, RQ",CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS,third-order multivariate calibration matlab program graphical interface figures of merit,10.1016/j.chemolab.2012.03.018
82,WOS:000236131200002,2006,Discrete element representation of manure products,WHEAT EN-MASSE SIMULATION MODELS PARAMETERS FLOW,"To simulate the machine-product interactions taking place in land application equipment, models of manure products must first be developed and validated. Several parameters must be defined to appropriately represent organic fertilizers in the discrete element method (DEM) framework. The work reported herein was aimed at determining the properties of the virtual product that would allow mimicking the behaviour of manure in the DEM software PFCD. A procedure was developed to generate an assembly of particles within the domain under investigation according to a user-defined particle size distribution, as would be measured by screening. The results generated by this procedure in terms of granulometry of the assembly of particles were very close to the user specifications with errors on the number of particles and on their size averaging .% and .%, respectively. A procedure was also developed to create clusters of particles randomly oriented and located within the modeled domain. The cluster-generation code was tested for clusters made of up to six particles, but could be expanded to include more particles. A calibration procedure based on a virtual direct shear test was developed to define the properties of the resulting virtual manure. A sensitivity analysis was performed to study the influence of parameters defining the linear and Hertz-Mindlin contact constitutive models. The simulations were based on experimental results obtained for pig manure at a total solids (TS) concentration of %. The results showed that numerous parameters have an influence on the behaviour of the virtual product in the direct shear test. Implementing the measured particle size distribution for pig manure at % TS, a friction coefficient of . and a Young's modulus value of . MPa allowed reaching an angle of internal friction of . degrees and an apparent cohesion value of . kPa that favourably compared to the . degrees and . kPa values measured experimentally.", (c) 2005 Elsevier B.V. All rights reserved.,"Landry, H|Lague, C|Roberge, M",COMPUTERS AND ELECTRONICS IN AGRICULTURE,discrete element method dem constitutive models input parameters numerical modeling manure organic fertilizers,10.1016/j.compag.2005.10.004
88,WOS:000277498900014,2010,Sediment and pollutant load modelling using an integrated urban drainage modelling toolbox: an application of City Drain,COMBINED SEWER SYSTEMS UNCERTAINTY ANALYSIS COPPER LOADS DATA SETS CALIBRATION RUNOFF EROSION PREDICTION TRANSPORT SURFACE,"Numerical and computational modelling of flow and pollutant dynamics in urban drainage systems is becoming more and more integral to planning and design. The main aim of integrated flow and pollutant models is to quantify the efficiency of different measures at reducing the amount of pollutants discharged into receiving water bodies and minimise the consequent negative water quality impact. The open source toolbox CITY DRAIN developed in the Matlab/Simulink environment, which was designed for integrated modelling of urban drainage systems, is used in this work. The goal in this study was to implement and test computational routines for representing sediment and pollutant loads in order to evaluate catchment surface pollution. Tested models estimate the accumulation, erosion and transport of pollutants-aggregately-on urban surfaces and in sewers. The toolbox now includes mathematical formulations for accumulation of pollutants during dry weather period and their wash-off during rainfall events. The experimental data acquired in a previous research project carried out by the Environmental Engineering Research Centre (CIIA) at the Universidad de los Andes in Bogota (Colombia) was used for the calibration of the models. Different numerical approaches were tested for their ability to calibrate to the sediment transport conditions. Initial results indicate, when there is more than one peak during the rainfall event duration, wash-off processes probably can be better represented using a model based on the flow instead of the rainfall intensity. Additionally, it was observed that using more detailed models (compared with an instantaneous approach) for representing pollutant accumulation do not necessarily lead to better results.",,"Rodriguez, JP|Achleitner, S|Moderl, M|Rauch, W|Maksimovic, C|McIntyre, N|Diaz-Granados, MA|Rodriguez, MS",WATER SCIENCE AND TECHNOLOGY,bogota city build-up and wash-off processes calibration and uncertainty analysis city drain toolbox sediment and pollutant load modelling,10.2166/wst.2010.139
89,WOS:000400594100014,2017,Uncertainty Estimation in Flood Inundation Mapping: An Application of Non-parametric Bootstrapping,MONTHLY STREAMFLOW PREDICTION ARTIFICIAL NEURAL-NETWORKS CONFIDENCE-INTERVALS MODEL CALIBRATION DESIGN FLOODS RUNOFF RISK PRECIPITATION OPTIMIZATION PARAMETERS,"Disaster prevention planning is affected in a significant way by a lack of in-depth understanding of the numerous uncertainties involved with flood delineation and related estimations. Currently, flood inundation extent is represented as a deterministic map without in-depth consideration of the inherent uncertainties associated with variables such as precipitation, streamflow, topographic representation, modelling parameters and techniques, and geospatial operations. The motivation of this study is to estimate uncertainties in flood inundation mapping based on a non-parametric bootstrapping method. The uncertainty is addressed through the application of non-parametric bootstrap sampling to the hydrodynamic modelling software, HEC-RAS, integrated with Geographic Information System (GIS). This approach was used to simulate different water levels and flow rates corresponding to different return periods from the available database. The study area was the Langat River Basin in Malaysia. The results revealed that the inundated land and infrastructure are subject to a flooding hazard of high-frequency events and that the flood damage potential is increasing significantly for residential areas and valuable land-use classes with higher return periods. The proposed methodology, as well as the study outcomes, of this paper could be beneficial to policymakers, water resources managers, insurance companies and other flood-related stakeholders."," Copyright (c) 2017 John Wiley & Sons, Ltd.","Faghih, M|Mirzaei, M|Adamowski, J|Lee, J|El-Shafie, A",RIVER RESEARCH AND APPLICATIONS,flood mapping uncertainty analysis non-parametric bootstrap sampling generalized extreme value distribution,10.1002/rra.3108
91,WOS:000281772900010,2010,INTEGRATION OF UNCERTAINTIES INTO INTERNAL CONTAMINATION MONITORING,IDEAS GUIDELINES DOSIMETRY,"Potential internal contaminations of workers are monitored by periodic bioassays interpreted in terms of intake and committed effective dose through biokinetic and dosimetric models. After a prospective evaluation of exposure at a workplace, a suitable monitoring program can be defined by the choice of measurement techniques and frequency of measurements. However, the actual conditions of exposure are usually not well defined and the measurements are subject to errors. In this study we took into consideration the uncertainties associated with a routine monitoring program in order to evaluate the minimum intake and dose detectable for a given level of confidence. Major sources of uncertainty are the contamination time, the size distribution and absorption into blood of the incorporated particles, and the measurement errors. Different assumptions may be applied to model uncertain knowledge, which lead to different statistical approaches. The available information is modeled here by classical or Bayesian probability distributions. These techniques are implemented in the OPSCI software under development. This methodology was applied to the monitoring program of workers in charge of plutonium purification at the AREVA NC reprocessing facility (La Hague, France). A sensitivity analysis was carried out to determine the important parameters for the minimum detectable dose. The methods presented here may be used for assessment of any other routine monitoring program through the comparison of the minimum detectable dose for a given confidence level with dose constraints. Health Phys. ():-; ",,"Davesne, E|Casanova, P|Chojnacki, E|Paquet, F|Blanchardon, E",HEALTH PHYSICS,computer calculations contamination internal effective dose plutonium,10.1097/HP.0b013e3181cd3d47
102,WOS:000270369500007,2009,Probabilistic Assessment Study of Channels Downstream Slopes Erosion in the Maritime Environment,,"This research deals with the probabilistic simulation and assessment of erosion in the downstream maritime slopes in hop ports (ports with deep approach channels to be able to accommodate the recent vessels generations) with natural side slopes. The study concentrated on the liquefaction effect in the erosion factor, which is the main controllable parameter for this phenomena. The probability of failure for the limit state function represents the erosion factor, which has a liable representation by a normal distribution with parameters mu = . and sigma = ., as a representative limit state function. This research deals with a maritime channel with certain dimensions as an example. The probabilistic simulations for downstream slope erosion were carried out using the Monte Carlo technique by using a probabilistic model. The generated probabilistic histograms of the erosion factor based on one run and different numbers of simulated random samples were determined. Based on these reliability simulation results, the erosion volumes per unit width of the channel were evaluated. Validation and sensitivity analyses were also carried out to ensure more reliability for this research. The study produced a group of guiding regression models for estimates and the determined conclusions related to the evaluated erosion volumes we carefully examined by considering calculation conditions based on a % confidence level with different assumptions. Then preliminary estimates for the eroded volumes (m()/m) in the downstream slope of the channel were evaluated and so used to determine the relevant regression models. These distributions were determined based on a group of assumed realistic conditions, which include variable berm depths and constant downstream slope angles in one simulated group with erosion volumes against downstream slopes depths heights variation and constant berm depths and variable slope angles in another with erosion volumes against downstream slopes angles variation. The limit state functions representing the erosion volumes variation behavior under the different conditions were also determined by using reliable statistical goodness-of-fit software. The research results are presented in a graphical form for the purpose of improving the current application capabilities in the subject and providing practical usage for the unprotected maritime navigation channel, trenches, and maritime downstream slopes.",,"Khalifa, MA|El Ganainy, MA|Nasr, RI",JOURNAL OF COASTAL RESEARCH,maritime downstream slopes hop ports approach channels downstream erosion factor downstream slopes liquefaction probabilistic simulations monte carlo simulation probabilistic slope failure sensitivity analysis downstream erosion volumes limit state functions,10.2112/08-1074.1
103,WOS:000382269000125,2016,Irrigation water demand of selected agricultural crops in Germany between 1902 and 2010,LEAF-AREA INDEX CLIMATE-CHANGE IMPACT ASSESSMENT NORTHERN GERMANY HUMID CLIMATE REQUIREMENTS EUROPE AVAILABILITY ENGLAND MODEL,"Irrigation water demand (IWD) is increasing worldwide, including in regions such as Germany that are characterized with low precipitation levels, yet grow water-demanding crops such as sugar beets, potatoes, and vegetables. This study aimed to calculate and analyze the spatial and temporal changes in the IWD of four crops spring barley, oat, winter wheat, and potato between  and  in Germany by using the modeling software AgroHyd Farmmodel. Climatic conditions in Germany continued to change over the investigation period, with an increase in temperature of . K/yr and an increase in precipitation of  mm/yr. Nevertheless, no significant increasing or decreasing trend in IWD was noted in the analysis. The IWD for the investigated crops in the area of the current ""Federal Republic of Germany"" over the  years was  mm/yr, varying between  and  mm/yr. Changes in cropping pattern and cultivated area over the last century caused large differences in the IWD calculated for each administrative district. The mean annual IWD of over the study period (which was divided into  parts) varied between , Mm()/yr in the earliest period (-) and  Mm()/yr in the latest period (-). Policy and management measures to adapt to climate change are currently being debated in Germany. The presented results suggest that the effects of the choice of crops (in this case, changes in cropping pattern in the German nation states) had a stronger influence on regional water resources than those of climate variability. Thus, the influence of climate change on water resources is relativized which brings an important input into the debate.", (C) 2016 Elsevier BM. All rights reserved.,"Drastig, K|Prochnow, A|Libra, J|Koch, H|Rolinski, S",SCIENCE OF THE TOTAL ENVIRONMENT,in igation water demand inigation trend agrohyd fanninodel,10.1016/j.scitotenv.2016.06.206
104,WOS:000245786200002,2007,Stormwater pollutant loads modelling: epistemological aspects and case studies on the influence of field data sets on calibration and verification,REGRESSION-MODELS,"In urban drainage, stormwater quality models have been used by researchers and practitioners for more than  years. Most of them were initially developed for research purposes, and have been later on implemented in commercial software packages devoted to operational needs. This paper presents some epistemological problems and difficulties with practical consequences in the application of stormwater quality models, such as simplified representation of reality, scaling-up, over-parameterisation, transition from calibration to verification and prediction, etc. Two case studies (one to estimate pollutant loads at the outlet of a catchment, one to design a detention tank to reach a given pollutant interception efficiency), with simple and detailed stormwater quality models, illustrate some of the above problems. It is hard to find, if not impossible, an ""optimum"" or ""best"" unique set of parameters values. Model calibration and verification appear to dramatically depend on the data sets used for their calibration and verification. Compared to current practice, collecting more and reliable data is absolutely necessary.",,"Bertrand-Krajewski, JL",WATER SCIENCE AND TECHNOLOGY,calibration epistemology field data modelling sensitivity analysis separate and combined sewers stormwater verification,10.2166/wst.2007.090
105,WOS:000269046900012,2009,IPH-TRIM3D-PCLake: A three-dimensional complex dynamic model for subtropical aquatic ecosystems,,"This paper presents IPH-TRIMD-PCLake, a three-dimensional complex dynamic model for subtropical aquatic ecosystems. It combines a spatially explicit hydrodynamic model with a water-quality and biotic model of ecological interactions. The software, which is freely available for research purposes, has a graphical user-friendly interface and a flexible design that allows the user to vary the complexity of the model. It also has built-in analysis tools such as Monte Carlo sensitivity analysis, a genetic algorithm for calibration, and plotting tools. (C) ", Elsevier Ltd. All rights reserved.,"Fragoso, CR|van Nes, EH|Janse, JH|Marques, DD",ENVIRONMENTAL MODELLING & SOFTWARE,aquatic ecosystem model cascading trophic effects subtropical ecosystems 3d model,10.1016/j.envsoft.2009.05.006
106,WOS:000309267300007,2012,Comparison of empirical and numerical methods in tunnel stability analysis,,"The stability of a tunnel can be evaluated using mathematical solutions, empirical methods, or numerical modelling. Mathematical solutions are precise methods; however the need to conduct mathematical calculations usually decreases the user's desire to use this method. Empirical methods are based on the experience gathered by researchers in various parts of the world whereas numerical modelling utilises computing power and, using various modelling techniques, can be a precise way of solving very complex problems. In this method the environment and the geometry can be set by the user. This method allows the user to conduct sensitivity analysis. In this article, empirical methods and numerical modelling using UDEC software were used to conduct a stability analysis of the access tunnel at the Shahriar dam crest, which was one of the most important tunnels of this project. In addition, numerical modelling was used to predict the stresses and deformations around the perimeter of the tunnel, and select the most suitable ground support system. The results obtained from both methods were compared for selection of the best suited support system. The results indicated that the empirical methods presented similar results to the results of numerical modelling at the first stages of tunnel design in jointed rocks. Therefore, in the absence of sufficient information for numerical analysis, the results of the empirical method can be used for this project.",,"Rahmani, N|Nikbakhtan, B|Ahangari, K|Apel, D",INTERNATIONAL JOURNAL OF MINING RECLAMATION AND ENVIRONMENT,mathematical analyses empirical methods numerical modelling tunnel udec,10.1080/17480930.2011.611615
108,WOS:000253663600007,2008,A computational algorithm for the multiple generation of nonlinear mathematical models and stability study,,"This paper presents an algorithm that generates families of mathematical models with nonlinear parameters, and includes the study of linear models, based on the experimental data of the intervening variables. The implementation of this algorithm has been named poly-model and is based on the application of the Gauss-Newton algorithm for obtaining the parameters of nonlinear models [Verdu F. Un Algoritmo para la construccion multiple de modelos matematicos no lineales y el estudio de su estabilidad. Doctoral Tesis. Universidad de Alicante, ]. One of its characteristics is a search among different nonlinear models within the parameters; unlike the methods found in the scientific literature [Camacho Rosales J. Estadistica con SPSS para windows. Ed. Ra-Ma, ; Mathsoft Inc. Splus-. Guide to Statistics. Seattle, ], the user does not intervene in their generation. A pruning criteria has also been introduced that is based on the stability analysis of models generated from perturbations, applying studies carried out by the authors and published in [Verdu F, Villacampa Y. A computer program for a Monte Carlo analysis of sensitivity in equations of environmental modelling obtained from experimental data. Advances in Engineering Software, ]. Object-oriented Pascal has been used in Delphi .", (c) 2007 Published by Elsevier Ltd.,"Verdu, F|Villacampa, Y",ADVANCES IN ENGINEERING SOFTWARE,modelling nonlinear regression sensitivity analysis,10.1016/j.advengsoft.2007.03.004
114,WOS:000224365000004,2004,MVC1: an integrated MatLab toolbox for first-order multivariate calibration,PARTIAL LEAST-SQUARES ORTHOGONAL SIGNAL CORRECTION WAVELENGTH SELECTION PROPAGATION ALGORITHM ERROR PLS,"Multivariate calibration  (MVC), a MatLab(R) toolbox for implementing up to  different first-order calibration methodologies through easily managed graphical user interfaces, is presented. The toolbox accepts different input data formats (either arranged as matrices or vectors contained in raw data files or in already existing MatLab variables) and incorporates many preprocessing algorithms in order to improve prediction capabilities. The development and validation of each model and its subsequent application to unknown samples are straightforward. Prediction results are produced along analytical figures of merit and standard errors calculated by uncertainty propagation. Moreover, the toolbox allows one to manually select working sensor regions, or to automatically find which region provides the minimum error. It also generates many different plots regarding model performance, including outliers detection, facilitating both model evaluation and interpretation.", (C) 2004 Elsevier B.V. All rights reserved.,"Oliveri, AC|Goicoechea, HC|Inon, FA",CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS,,10.1016/j.chemolab.2004.03.004
115,WOS:000245063400009,2007,Numerical and visual evaluation of hydrological and environmental models using the Monte Carlo analysis toolbox,RAINFALL-RUNOFF MODELS REGIONALIZED SENSITIVITY ANALYSIS WATER-QUALITY IMPROVED CALIBRATION PHOSPHORUS TRANSFER CONCEPTUAL MODELS UNCERTAINTY CATCHMENT IDENTIFICATION SYSTEMS,"The detailed evaluation of mathematical models and the consideration of uncertainty in the modeling of hydrological and environmental systems are of increasing importance, and are sometimes even demanded by decision makers. At the same time, the growing complexity of models to represent real-world systems makes it more and more difficult to understand model behavior, sensitivities and uncertainties. The Monte Carlo Analysis Toolbox (MCAT) is a Matlab library of visual and numerical analysis tools for the evaluation of hydrological and environmental models. Input to the MCAT is the result of a Monte Carlo or population evolution based sampling of the parameter space of the model structure under investigation. The MCAT can be used off-line, i.e. it does not have to be connected to the evaluated model, and can thus be used for any model for which an appropriate sampling can be performed. The MCAT contains tools for the evaluation of performance, identitiability, sensitivity, predictive uncertainty and also allows for the testing of hypotheses with respect to the model structure used. In addition to research applications, the MCAT can be used as a teaching tool in courses that include the use of mathematical models. (c) ", Elsevier Ltd. All rights reserved.,"Wagener, T|Kollat, J",ENVIRONMENTAL MODELLING & SOFTWARE,evaluation uncertainty analysis sensitivity analysis identifiability hypothesis testing model diagnostics matlab visualization,10.1016/j.envsoft.2006.06.017
116,WOS:000414818700006,2017,Assessment of environmental impacts and operational costs of the implementation of an innovative source-separated urine treatment,WASTE-WATER TREATMENT LIFE-CYCLE ASSESSMENT TREATMENT PLANTS NUTRIENT MANAGEMENT REMOVAL ALTERNATIVES SYSTEM FOCUS,"Innovative treatment technologies and management methods are necessary to valorise the constituents of wastewater, in particular nutrients from urine (highly concentrated and can have significant impacts related to artificial fertilizer production). The FP project, ValuefromUrine, proposed a new two-step process (called VFU) based on struvite precipitation and microbial electrolysis cell (MEC) to recover ammonia, which is further transformed into ammonium sulphate. The environmental and economic impacts of its prospective implementation in the Netherlands were evaluated based on life cycle assessment (LCA) methodology and operational costs. In order to tackle the lack of stable data from the pilot plant and the complex effects on wastewater treatment plant (WWTP), process simulation was coupled with LCA and costs assessment using the Python programming language. Additionally, particular attention was given to the propagation and analysis of inputs uncertainties. Five scenarios of VFU implementation were compared to the conventional treatment of  m() of wastewater. Inventory data were obtained from SUMO software for the WWTP operation. LCA was based on Brightway software (using ecoinvent database and ReCiPe method). The results, based on  iterations sampled from inputs distributions (foreground parameters, ecoinvent background data and market prices), showed a significant advantage of VFU technology, both at a small and decentralized scale and at a large and centralized scale (% confidence intervals not including zero values). The benefits mainly concern the production of fertilizers, the decreased efforts at the WWTP, the water savings from toilets flushing, as well as the lower infrastructure volumes if the WWTP is redesigned (in case of significant reduction of nutrients load in wastewater). The modelling approach, which could be applied to other case studies, improves the representativeness and the interpretation of results (e.g. complex relationships, global sensitivity analysis) but requires additional efforts (computing and engineering knowledge, longer calculation time). Finally, the sustainability assessment should be refined in the future with the development of the technology at larger scale to update these preliminary conclusions before its commercialization. (C) ", Elsevier Ltd. All rights reserved.,"Igos, E|Besson, M|Gutierrez, TN|de Faria, ABB|Benetto, E|Barna, L|Ahmadi, A|Sperandio, M",WATER RESEARCH,source-separated urine treatment process simulation sustainability assessment innovative technology integrated modelling,10.1016/j.watres.2017.09.016
118,WOS:000286284700004,2011,A methodology for the design and development of integrated models for policy support,RIVER-BASIN MANAGEMENT 10 ITERATIVE STEPS LAND-USE PATTERNS PLANNING-SUPPORT OPTIMIZATION METHODOLOGY ORGANIZATIONAL-CHANGE ENVIRONMENTAL-MODELS SENSITIVITY-ANALYSIS CELLULAR-AUTOMATA SCENARIO ANALYSIS,"The development of Decision Support Systems (DSS) to inform policy making has been increasing rapidly. This paper aims to provide insight into the design and development process of policy support systems that incorporate integrated models. It will provide a methodology for the development of such systems that attempts to synthesize knowledge and experience gained over the past - years from developing a suite of these DSSs for a number of users in different geographical contexts worldwide. The methodology focuses on the overall iterative development process that includes policy makers, scientists and IT-specialists. The paper highlights important tasks in model integration and system development and illustrates these with some practical examples from DSS that have dynamic, spatial and integrative attributes. Crucial integrative features of modelling systems that aim to provide support to policy processes, and to which we refer as integrated Decision Support Systems, are: Synthesis of relevant drivers, processes and characteristics of the real world system at relevant spatial and temporal scales. An integrated approach linking economic, environmental and social domains. Connection to the policy context, interest groups and end-users. Engagement with the policy process. Ability to provide added value to the current decision-making practice. With this paper we aim to provide a methodology for the design and development of these integrated Decision Support Systems that includes the 'hard' elements of model integration and software development as well as the 'softer' elements related to the user-developer interaction and social learning of all groups involved in the process. (C) ", Elsevier Ltd. All rights reserved.,"van Delden, H|Seppelt, R|White, R|Jakeman, AJ",ENVIRONMENTAL MODELLING & SOFTWARE,decision support system (dss) model integration design and development process iterative process social learning policy support,10.1016/j.envsoft.2010.03.021
121,WOS:000168591400008,2001,MCE-RISK: integrating multicriteria evaluation and CIS for risk decision-making in natural hazards,GEOGRAPHICAL INFORMATION-SYSTEMS,"During the past two decades there have been a wide range of applications for decision-making linking multicriteria evaluation (MCE) and geographic information systems (GIS). However, limited literature reports the development of MCE-GIS software, and the comparison of various MCE-GIS approaches. This paper introduces an MCE-GIS program called MCE-RISK for risk-based decision-making. It consists of a series of modules for data standardisation, weighting, MCE-GIS methods. and sensitivity analysis. The program incorporates different MCE-GIS methods. including weighted linear combination (WLC), the technique for order preference by similarity to ideal solution (TOPSIS), and compromise programming (CP), enabling comparisons between different methods for the same decision problem to be made. An example of decision-making for determining priority areas for a bushfire hazard reduction burning is examined. After implementing the alternative MCE-GIS methods, and comparing final outputs and the computational difficulty involved in the analysis, WLC is recommended. Some caveats on using MCE-GIS methods art: also dis cussed. Although the development of MCE-RISK and its application reported in this paper are specific to risk-based decisionmaking in natural hazards, the program can be used for other environmental decision applications. such as environmental impact assessment and land-use planning."," (C) 2001 Elsevier Science Ltd, All rights reserved.","Chen, KP|Blong, R|Jacobson, C",ENVIRONMENTAL MODELLING & SOFTWARE,risk decision-making multicriteria evaluation cis bushfire prescribed burning,10.1016/S1364-8152(01)00006-8
128,WOS:000308971400030,2012,An integrated assessment tool to define effective air quality policies at regional scale,POLLUTION MODEL PM10 STRATEGIES VALIDATION LONDON EUROPE,"In this paper, the Integrated Assessment of air quality is dealt with at regional scale. First the paper describes the main challenges to tackle current air pollution control, including economic aspects. Then it proposes a novel approach to manage the problem, presenting its mathematical formalization and describing its practical implementation into the Regional Integrated Assessment Tool (RIAT). The main features of the software system are described and some preliminary results on a domain in Northern Italy are illustrated. The novel features in RIAT are then compared to the state-of-the-art in integrated assessment of air quality, for example the ability to handle nonlinearities (instead of the usual linear approach) and the multi-objective framework (alternative to cost-effectiveness and scenario analysis). Then the lessons learned during the RIAT implementation are discussed, focusing on the locality, flexibility and openness of the tool. Finally the areas for further development of air quality integrated assessment are highlighted, with a focus on sensitivity analysis, structural and non technical measures, and the application of parallel computing concepts. (C) ", Elsevier Ltd. All rights reserved.,"Carnevale, C|Finzi, G|Pisoni, E|Volta, M|Guariso, G|Gianfreda, R|Maffeis, G|Thunis, P|White, L|Triacchini, G",ENVIRONMENTAL MODELLING & SOFTWARE,integrated assessment modeling model reduction air quality modeling multi-objective optimization decision support,10.1016/j.envsoft.2012.07.004
130,WOS:000183565000005,2003,Calibration and sensitivity analysis of a river water quality model under unsteady flow conditions,SYSTEMS QUASAR UNCERTAINTY PREDICTION SIMULATION CHANNEL OUSE,"Water quality models generally require a relatively large number of parameters to define their functional relationships, and since prior information on parameter values is limited, these are commonly defined by fitting the model to observed data. In this paper, the identifiability of water quality parameters and the associated uncertainty in model simulations are investigated. A modification to the water quality model `Quality Simulation Along River Systems' is presented in which an improved flow component is used within the existing water quality model framework. The performance of the model is evaluated in an application to the Bedford Ouse river, UK, using a Monte-Carlo analysis toolbox. The essential framework of the model proved to be sound, and calibration and validation performance was generally good. However some supposedly important water quality parameters associated with algal activity were found to be completely insensitive, and hence non-identifiable, within the model structure, while others (nitrification and sedimentation) had optimum values at or close to zero, indicating that those processes were not detectable from the data set examined.", (C) 2003 Elsevier Science B.V. All rights reserved.,"Sincock, AM|Wheater, HS|Whitehead, PG",JOURNAL OF HYDROLOGY,bedford ouse nitrate do bod quality simulation along river systems water quality modelling,10.1016/S0022-1694(03)00127-6
134,WOS:000249622700001,2007,An integrated framework for multipollutant air quality management and its application in georgia,SOUTHEASTERN UNITED-STATES SOURCE APPORTIONMENT TIME-SERIES POLLUTION OZONE MODEL EMISSIONS MORTALITY HEALTH PM2.5,"Air protection agencies in the United States increasingly confront non-attainment of air quality standards for multiple pollutants sharing interrelated emission origins. Traditional approaches to attainment planning face important limitations that are magnified in the multipollutant context. Recognizing those limitations, the Georgia Environmental Protection Division has adopted an integrated framework to address ozone, fine particulate matter, and regional haze in the state. Rather than applying atmospheric modeling merely as a final check of an overall strategy, photochemical sensitivity analysis is conducted upfront to compare the effectiveness of controlling various precursor emission species and source regions. Emerging software enables the modeling of health benefits and associated economic valuations resulting from air pollution control. Photochemical sensitivity and health benefits analyses, applied together with traditional cost and feasibility assessments, provide a more comprehensive characterization of the implications of various control options. The fuller characterization both informs the selection of control options and facilitates the communication of impacts to affected stakeholders and the public. Although the integrated framework represents a clear improvement over previous attainment-planning efforts, key remaining shortcomings are also discussed.",,"Cohan, DS|Boylan, JW|Marmur, A|Khan, MN",ENVIRONMENTAL MANAGEMENT,air pollution control cost-benefit analysis ozone fine particulate matter state implementation plans attainment,10.1007/s00267-006-0228-4
135,WOS:000402819500015,2017,Investigation on the effect of geometrical and geotechnical parameters on elongated offshore piles using fuzzy inference systems,PLATFORMS BEHAVIOR,"Among numerous offshore structures used in oil extraction, jacket platforms are still the most favorable ones in shallow waters. In such structures, log piles are used to pin the substructure of the platform to the seabed. The pile's geometrical and geotechnical properties are considered as the main parameters in designing these structures. In this study, ANSYS was used as the FE modeling software to study the geometrical and geotechnical properties of the offshore piles and their effects on supporting jacket platforms. For this purpose, the FE analysis has been done to provide the preliminary data for the fuzzy-logic post-process. The resulting data were implemented to create Fuzzy Inference System (FIS) classifications. The resultant data of the sensitivity analysis suggested that the orientation degree is the main factor in the pile's geometrical behavior because piles which had the optimal operational degree of about A degrees are more sustained. Finally, the results showed that the related fuzzified data supported the FE model and provided an insight for extended offshore pile designs.",,"Aminfar, A|Mojtahedi, A|Ahmadi, H|Aminfar, MH",CHINA OCEAN ENGINEERING,pile soil fem offshore jacket platform pile-soil interaction fuzzy-logic fuzzification,10.1007/s13344-017-0044-z
140,WOS:000280656300004,2010,Simulation model for extended double-ended queueing,QUEUES IMPATIENCE CUSTOMERS,"The purpose of this paper is to extend traditional double-ended queuing models using a simulation approach. Traditional double-ended queuing models assume that one supply queue should satisfy one demand queue through instantaneous pairing. Inter-arrival time is assumed to follow an exponential distribution, with arrivals to the system assumed to occur just one at a time. However, this assumption is frequently violated in many real-world situations. The pairing or batch size can either be multiple or a random variable, and the pairing processing time can be greater than . Inter-arrival time may follow distributions other than exponential. In some cases bulk arrivals may come at the same time, and pairing is not always guaranteed. Because the analytical approach has enormous difficulties obtaining performance measures under these relaxed situations, a simulation approach for extended double-ended queueing processes is presented. This includes an algorithm to find state probabilities and a newly developed simulation procedure. Using this new procedure, sensitivity analyses of performance measures were performed using various input conditions implemented using ProModel and SimRumnner simulation software. A business case is studied to demonstrate the versatility of the proposed approaches. (c) ", Elsevier Ltd. All rights reserved.,"Kim, WK|Yoon, KP|Mendoza, G|Sedaghat, M",COMPUTERS & INDUSTRIAL ENGINEERING,double-ended queue simulation state probability optimization job placement agency,10.1016/j.cie.2010.04.002
142,WOS:000321439500015,2013,Application of Bayesian Networks in Quantitative Risk Assessment of Subsea Blowout Preventer Operations,OFFSHORE SAFETY ASSESSMENT RELIABILITY-ANALYSIS ORGANIZATIONAL-FACTORS FAULT-TREES SYSTEMS METHODOLOGY FACILITIES ACCIDENTS SECURITY PLATFORM,"This article proposes a methodology for the application of Bayesian networks in conducting quantitative risk assessment of operations in offshore oil and gas industry. The method involves translating a flow chart of operations into the Bayesian network directly. The proposed methodology consists of five steps. First, the flow chart is translated into a Bayesian network. Second, the influencing factors of the network nodes are classified. Third, the Bayesian network for each factor is established. Fourth, the entire Bayesian network model is established. Lastly, the Bayesian network model is analyzed. Subsequently, five categories of influencing factors, namely, human, hardware, software, mechanical, and hydraulic, are modeled and then added to the main Bayesian network. The methodology is demonstrated through the evaluation of a case study that shows the probability of failure on demand in closing subsea ram blowout preventer operations. The results show that mechanical and hydraulic factors have the most important effects on operation safety. Software and hardware factors have almost no influence, whereas human factors are in between. The results of the sensitivity analysis agree with the findings of the quantitative analysis. The three-axiom-based analysis partially validates the correctness and rationality of the proposed Bayesian network model.",,"Cai, BP|Liu, YH|Liu, ZK|Tian, XJ|Zhang, YZ|Ji, RJ",RISK ANALYSIS,bayesian networks quantitative risk assessment subsea blowout preventer,10.1111/j.1539-6924.2012.01918.x
145,WOS:000356741300007,2015,A Matlab toolbox for Global Sensitivity Analysis,IDENTIFICATION UNCERTAINTIES MODELS,"Global Sensitivity Analysis (GSA) is increasingly used in the development and assessment of environmental models. Here we present a Matlab/Octave toolbox for the application of GSA, called SAFE (Sensitivity Analysis For Everybody). It implements several established GSA methods and allows for easily integrating others. All methods implemented in SAFE support the assessment of the robustness and convergence of sensitivity indices. Furthermore, SAFE includes numerous visualisation tools for the effective investigation and communication of GSA results. The toolbox is designed to make GSA accessible to non-specialist users, and to provide a fully commented code for more experienced users to complement their own tools. The documentation includes a set of workflow scripts with practical guidelines on how to apply GSA and how to use the toolbox. SAFE is open source and freely available for academic and non-commercial purpose. Ultimately, SAFE aims at contributing towards improving the diffusion and quality of GSA practice in the environmental modelling community. (C)  The Authors.", Published by Elsevier Ltd.,"Pianosi, F|Sarrazin, F|Wagener, T",ENVIRONMENTAL MODELLING & SOFTWARE,global sensitivity analysis matlab octave open-source software,10.1016/j.envsoft.2015.04.009
146,WOS:000266225700018,2009,MVC2: A MATLAB graphical interface toolbox for second-order multivariate calibration,TRILINEAR DECOMPOSITION ALGORITHM PARALLEL FACTOR-ANALYSIS PARTIAL LEAST-SQUARES CURVE RESOLUTION ADVANTAGE BILINEARIZATION SENSITIVITY PREDICTION,"This work reports the release of Multivariate Calibration  (MVC), a MATLAB graphical interface toolbox for implementing several second-order multivariate calibration methodologies. The toolbox accepts a variety of input data formats, arranged in either matrices or vectors (i.e., unfolded matrices), and contained in ASCII files. It allows one to manually select working sensor regions and plot landscapes for selected samples. The development of each model and its subsequent application to unknown samples is straightforward. Prediction results are produced along with analytical figures of merit and standard concentration errors, as calculated by modern concepts of uncertainty propagation.", (c) 2009 Elsevier B.V. All rights reserved.,"Olivieri, AC|Wu, HL|Yu, RQ",CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS,second-order multivariate calibration matlab program graphical interface figures of merit,10.1016/j.chemolab.2009.02.005
149,WOS:000186310600007,2003,Direct and adjoint sensitivity analysis of chemical kinetic systems with KPP: II - Numerical validation and applications,VARIATIONAL DATA ASSIMILATION CHEMISTRY DATA ASSIMILATION AIR-QUALITY MODEL OZONE IMPLEMENTATION CODE,"The Kinetic PreProcessor KPP was extended to generate the building blocks needed for the direct and adjoint sensitivity analysis of chemical kinetic systems. An overview of the theoretical aspects of sensitivity calculations and a discussion of the KPP software tools is presented in the companion paper. In this work the correctness and efficiency of the KPP generated code for direct and adjoint sensitivity studies are analyzed through an extensive set of numerical experiments. Direct-decoupled Rosenbrock methods are shown to be cost-effective for providing sensitivities at low and medium accuracies. A validation of the discrete-adjoint evaluated gradients is performed against the finite difference estimates. The accuracy of the adjoint gradients is measured using a reference gradient value obtained with a standard direct-decoupled method. The accuracy is studied for both constant step size and variable step size integration of the forward/adjoint model and the consistency between the discrete and continuous adjoint models is analyzed. Applications of the KPP-. software package to direct and adjoint sensitivity studies, variational data assimilation, and parameter identification are considered for the comprehensive chemical mechanism SAPRC-. (C) ", Elsevier Ltd. All rights reserved.,"Daescu, DN|Sandu, A|Carmichael, GR",ATMOSPHERIC ENVIRONMENT,sensitivity analysis data assimilation parameter identification optimization,10.1016/j.atmosenv.2003.08.020
153,WOS:000334003800020,2014,Environmental impact assessment based on dynamic fuzzy simulation,MODELS SYSTEMS,"A new ""quick scan"" method for an expert-/stakeholder-based impact assessment approach is introduced. This approach aims to reduce the complexity of models, to simulate and visualize the system dynamics and to provide a basis for guided discussion with stakeholders. The approach is based on dynamic fuzzy models that can be understood easily and developed by experts and understood and adapted by stakeholders (""white box models""). This open modeling process also forms the basis of the credibility of the simulation results. The quick scan approach is supported by an interactive simulation tool that includes optimization and uncertainty analysis as open source software. (C) ", Elsevier Ltd. All rights reserved.,"Wieland, R|Gutzler, C",ENVIRONMENTAL MODELLING & SOFTWARE,quick scan environmental impact assessment fuzzy modeling dynamic fuzzy simulation,10.1016/j.envsoft.2014.02.001
157,WOS:000392568100009,2017,Uncertainty quantification in littoral erosion,SHALLOW-WATER FLOWS GEOMETRIC CHARACTERIZATION SHAPE OPTIMIZATION SENSITIVITY SPACES EVOLUTION SCHEME RISK,We aim at quantifying the impact of flow state uncertainties in littoral erosion to provide confidence bounds on deterministic predictions of bottom morphodynamics. Two constructions of the bathymetry standard deviation are discussed. The first construction involves directional quantile-based extreme scenarios using what is known on the flow state Probability Density Function (PDF) from on site observations. We compare this construction to a second cumulative one using the gradient by adjoint of a functional involving the energy of the system. These ingredients are illustrated for two models for the interaction between a soft bed and a flow in a shallow domain. Our aim is to keep the computational complexity comparable to the deterministic simulations taking advantage of what already available in our simulation toolbox. (C) , Elsevier Ltd. All rights reserved.,"Mohammadi, B",COMPUTERS & FLUIDS,backward propagation quantile uncertainty littoral morphodynamics shallow water equations sensitivity analysis worst-case analysis,10.1016/j.compfluid.2016.10.017
164,WOS:000260920100004,2008,ON THE SENSITIVITY OF DESIRABILITY FUNCTIONS FOR MULTIRESPONSE OPTIMIZATION,,"Desirability functions have been one of the most important multiresponse optimization technique since the early eighties. Main reasons for this popularity might be counted as the convenience of the implementation of the method and it's availability in many experimental design software packages. Technique itself involves somehow subjective parameters such as the importance coefficients between response characteristics that are used to calculate overall desirability, weights used in determining the shape of each individual response and the size of the specification band of the response. However, the impact of these sensitive parameters on the solution set is mostly uninvestigated. This paper proposes a procedure to analyze the sensitivity of the important characteristic parameters of desirability functions and their impact on pareto-optimal solution set. The proposed procedure uses the experimental design tools on the solution space and estimates a prediction equation on the overall desirability to identify the sensitive parameters. For illustration, a classical desirability example is selected from the literature and results are given along with the discussion.",,"Aksezer, CS",JOURNAL OF INDUSTRIAL AND MANAGEMENT OPTIMIZATION,desirability functions parametric sensitivity analysis multiresponse optimization,10.3934/jimo.2008.4.685
167,WOS:000242724500019,2006,Uncertainty analysis for regional-scale reserve selection,SITE SELECTION SPECIES DISTRIBUTION CONSERVATION DESIGN BIODIVERSITY NETWORKS MODELS PERSISTENCE PROBABILITIES CONNECTIVITY,"Methods for reserve selection and conservation planning often ignore uncertainty. For example, presence-absence observations and predictions of habitat models are used as inputs but commonly assumed to be without error We applied information-gap decision theory to develop uncertainty analysis methods for reserve selection. Our proposed method seeks a solution that is robust in achieving a given conservation target, despite uncertainty in the data. We maximized robustness in reserve selection through a novel method, ""distribution discounting,"" in which the site- and species-specific measure of conservation value (related to species-specific occupancy probabilities) was penalized by an error measure (in our study, related to accuracy of statistical prediction). Because distribution discounting can be implemented as a modification of input files, it is a computationally efficient solution for implementing uncertainty analysis into reserve selection. Thus, the method is particularly useful for high-dimensional decision problems characteristic of regional conservation assessment. We implemented distribution discounting in the zonation reserve-selection algorithm that produces a hierarchy of conservation priorities throughout the landscape. We applied it to reserve selection for seven priority fauna in a landscape in New South Wales, Australia. The distribution discounting method can be easily adapted for use with different kinds of data (e.g., probability of occurrence or abundance) and different landscape descriptions (grid or patch based) and incorporated into other reserve-selection algorithms and software.",,"Moilanen, A|Wintle, BA|Elith, J|Burgman, M",CONSERVATION BIOLOGY,distribution discounting distribution smoothing information-gap decision theory reserve-network design site-selection algorithm spatial reserve design zonation,10.1111/j.1523-1739.2006.00560.x
169,WOS:000330675000001,2014,Intelligent Platform for Model Updating in a Structural Health Monitoring System,PARAMETER SELECTION DYNAMICS,"The main aim of this study is to develop an automated smart software platform to improve the time-consuming and laborious process of model updating. We investigate the key techniques of model updating based on intelligent optimization algorithms, that is, accuracy judgment methods for basic finite element model, parameter choice theory based on sensitivity analysis, commonly used objective functions and their construction methods, particle swarm optimization, and other intelligent optimization algorithms. An intelligent model updating prototype software framework is developed using the commercial software systems ANSYS and MATLAB. A parameterized finite element modeling technique is proposed to suit different bridge types and different model updating requirements. An objective function library is built to fit different updating targets. Finally, two case studies are conducted to verify the feasibility of the techniques used by the proposed software platform.",,"Dan, DH|Yang, T|Gong, JX",MATHEMATICAL PROBLEMS IN ENGINEERING,,10.1155/2014/628619
175,WOS:000383298800002,2016,A software framework for probabilistic sensitivity analysis for computationally expensive models,STOCHASTIC PREDICTIONS POLYMERIC NANOCOMPOSITES CORRELATED PARAMETERS OPTIMIZATION SIMULATIONS UNCERTAINTY PROPAGATION VARIABLES INDEXES DESIGN,"We provide a sensitivity analysis toolbox consisting of a set of Matlab functions that offer utilities for quantifying the influence of uncertain input parameters on uncertain model outputs. It allows the determination of the key input parameters of an output of interest. The results are based on a probability density function (PDF) provided for the input parameters. The toolbox for uncertainty and sensitivity analysis methods consists of three ingredients: () sampling method, () surrogate models, () sensitivity analysis (SA) method. Numerical studies based on analytical functions associated with noise and industrial data are performed to prove the usefulness and effectiveness of this study. (C) ", Elsevier Ltd. All rights reserved.,"Vu-Bac, N|Lahmer, T|Zhuang, X|Nguyen-Thoi, T|Rabczuk, T",ADVANCES IN ENGINEERING SOFTWARE,uncertainty quantification random sampling penalized spline regression sensitivity analysis matlab toolbox,10.1016/j.advengsoft.2016.06.005
184,WOS:000262497400004,2008,Fidelity of Network Simulation and Emulation: A Case Study of TCP-Targeted Denial of Service Attacks,,"In this article, we investigate the differences between simulation and emulation when conducting denial of service (DoS) attack experiments. As a case study, we consider low-rate TCP-targeted DoS attacks. We design constructs and tools for emulation testbeds to achieve a level of control comparable to simulation tools. Through a careful sensitivity analysis, we expose difficulties in obtaining meaningful measurements from the DETER, Emulab, and WAIL testbeds with default system settings. We find dramatic differences between simulation and emulation results for DoS experiments. Our results also reveal that software routers such as Click provide a flexible experimental platform, but require understanding and manipulation of the underlying network device drivers. Our experiments with commercial Cisco routers demonstrate that they are highly susceptible to the TCP-targeted attacks when ingress/egress IP filters are used.",,"Chertov, R|Fahmy, S|Shroff, NB",ACM TRANSACTIONS ON MODELING AND COMPUTER SIMULATION,simulation emulation testbeds tcp congestion control denial of service attacks low-rate tcp-targeted attacks,10.1145/1456645.1456649
